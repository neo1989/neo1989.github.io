<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>愚苏记</title>
  
  <subtitle>To no avail but try.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://neo1989.net/"/>
  <updated>2023-06-15T14:23:11.671Z</updated>
  <id>https://neo1989.net/</id>
  
  <author>
    <name>Neo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Way2AI · PyTorch实现神经网络的基本套路</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-utilities/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-utilities/</id>
    <published>2023-06-15T05:53:07.000Z</published>
    <updated>2023-06-15T14:23:11.671Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>本文简单交代了神经网络的基本套路以及部分实用组件，以简化开发过程。</p><h2 id="Set-up">Set up</h2><p>通常我们需要为重复实验设置很多seed，所以我们可以将其打包到一个函数里。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas pd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_seeds</span><span class="params">(seed=<span class="number">1024</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Set seeds for reproducibility."""</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    touch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)  <span class="comment"># multi-GPU</span></span><br><span class="line"></span><br><span class="line">set_seeds(seed=<span class="number">1024</span>)</span><br></pre></td></tr></table></figure><h2 id="Device">Device</h2><p>当我们有大型数据集和更大的模型要训练时，我们可以通过在 GPU 上并行化张量操作来加速。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cuda = <span class="literal">True</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span>(torch.cuda.is_available() <span class="keyword">and</span> cuda) <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">torch.set_default_tensor_type(&#123;<span class="string">"cuda"</span>: <span class="string">"torch.cuda.FloatTensor"</span>, <span class="string">"cpu"</span>: <span class="string">"torch.FloatTensor"</span>&#125;.get(str(device)))</span><br></pre></td></tr></table></figure><h2 id="Load-data">Load data</h2><p>这里依然使用前文引入的螺旋数据作为演示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">url = <span class="string">"http://s3.mindex.xyz/datasets/9378f64fc8dd2817e4c92be0a3bae8e7.csv"</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>) <span class="comment"># load</span></span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>) <span class="comment"># shuffle</span></span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Data shapes</span></span><br><span class="line">X = df[[<span class="string">"X1"</span>, <span class="string">"X2"</span>]].values</span><br><span class="line">y = df[<span class="string">"color"</span>].values</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X: "</span>, np.shape(X))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: "</span>, np.shape(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X:  (1500, 2)</span></span><br><span class="line"><span class="comment"># y:  (1500,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize data</span></span><br><span class="line">plt.title(<span class="string">"Generated non-linear data"</span>)</span><br><span class="line">colors = &#123;<span class="string">"c1"</span>: <span class="string">"red"</span>, <span class="string">"c2"</span>: <span class="string">"yellow"</span>, <span class="string">"c3"</span>: <span class="string">"blue"</span>&#125;</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=[colors[_y] <span class="keyword">for</span> _y <span class="keyword">in</span> y], edgecolors=<span class="string">"k"</span>, s=<span class="number">25</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/147e784e6ecae3fd226abce4f3905550.png" alt=""></p><h2 id="Split-data">Split data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_test_split</span><span class="params">(X, y, train_size)</span>:</span></span><br><span class="line">    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)</span><br><span class="line">    X_test, X_val, y_test, y_val = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (1050, 2), y_train: (1050,)</span></span><br><span class="line"><span class="comment"># X_val: (225, 2), y_val: (225,)</span></span><br><span class="line"><span class="comment"># X_test: (225, 2), y_test: (225,)</span></span><br><span class="line"><span class="comment"># Sample point: [0.17003003 0.63079261] → c3</span></span><br></pre></td></tr></table></figure><h2 id="Label-encoding">Label encoding</h2><p>接下来定义一个 LabelEncoder 来将文本标签编码成唯一的索引。</p><p>这里不再使用 scikit-learn 的 LabelEncoder，因为我们希望能够以我们想要的方式保存和加载我们的实例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelEncoder</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Label encoder for tag labels."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, class_to_index=None)</span>:</span></span><br><span class="line">        self.class_to_index = class_to_index <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = list(self.class_to_index.keys())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.class_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"&lt;LabelEncoder(num_classes=<span class="subst">&#123;len(self)&#125;</span>&gt;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        classes = np.unique(y)</span><br><span class="line">        <span class="keyword">for</span> i, class_ <span class="keyword">in</span> enumerate(classes):</span><br><span class="line">            self.class_to_index[class_] = i</span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = list(self.class_to_index.keys())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        encoded = np.zeros((len(y)), dtype=int)</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            encoded[i] = self.class_to_index[item]</span><br><span class="line">        <span class="keyword">return</span> encoded</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        classes = []</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            classes.append(self.index_to_class[item])</span><br><span class="line">        <span class="keyword">return</span> classes</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;<span class="string">'class_to_index'</span>: self.class_to_index&#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(cls, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"r"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Encode</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">label_encoder.fit(y_train)</span><br><span class="line">label_encoder.class_to_index</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert labels to tokens</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">y_train = label_encoder.encode(y_train)</span><br><span class="line">y_val = label_encoder.encode(y_val)</span><br><span class="line">y_test = label_encoder.encode(y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> enumerate(counts)&#125;</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: c3</span></span><br><span class="line"><span class="comment"># y_train[0]: 2</span></span><br><span class="line"><span class="comment"># counts: [350 350 350]</span></span><br><span class="line"><span class="comment"># weights: &#123;0: 0.002857142857142857, 1: 0.002857142857142857, 2: 0.002857142857142857&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Standardize-data">Standardize data</h2><p>我们需要标准化我们的数据（零均值和单位方差），这样特定特征的大小就不会影响模型学习其权重的方式。</p><p>我们只对输入X进行标准化，因为我们的输出y是类值。</p><p>我们将编写自己的 StandardScaler 类，以便在推理过程中轻松保存和加载它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StandardScaler</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mean=None, std=None)</span>:</span></span><br><span class="line">        self.mean = np.array(mean)</span><br><span class="line">        self.std = np.array(std)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        self.mean = np.mean(X_train, axis=<span class="number">0</span>)</span><br><span class="line">        self.std = np.std(X_train, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scale</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (X - self.mean) / self.std</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">unscale</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (X * self.std) + self.mean</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;<span class="string">"mean"</span>: self.mean.tolist(), <span class="string">"std"</span>: self.std.tolist()&#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(cls, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"r"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the data (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler()</span><br><span class="line">X_scaler.fit(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[0]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[1]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_test[0]: mean: 0.0, std: 1.0</span></span><br><span class="line"><span class="comment"># X_test[1]: mean: -0.0, std: 1.0</span></span><br></pre></td></tr></table></figure><h2 id="DataLoader">DataLoader</h2><p>我们将把数据放在 Dataset 中，并使用 DataLoader 来有效地创建用于训练和验证的批次。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        self.X = X</span><br><span class="line">        self.y = y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"&lt;Dataset(N=<span class="subst">&#123;len(self)&#125;</span>)&gt;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        X = self.X[index]</span><br><span class="line">        y = self.y[index]</span><br><span class="line">        <span class="keyword">return</span> [X, y]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(self, batch)</span>:</span></span><br><span class="line">        <span class="string">"""Processing on a batch."""</span></span><br><span class="line">        <span class="comment"># Get inputs</span></span><br><span class="line">        batch = np.array(batch)</span><br><span class="line">        X = np.stack(batch[:, <span class="number">0</span>], axis=<span class="number">0</span>)</span><br><span class="line">        y = batch[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cast</span></span><br><span class="line">        X = torch.FloatTensor(X.astype(np.float32))</span><br><span class="line">        y = torch.LongTensor(y.astype(np.int32))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_dataloader</span><span class="params">(self, batch_size, shuffle=False, drop_last=False)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.utils.data.DataLoader(</span><br><span class="line">            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,</span><br><span class="line">            shuffle=shuffle, drop_last=drop_last, pin_memory=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>事实上我们并不需要 collate_fn ，但我们可以让它透明（无副作用），因为当我想要对批处理做一些处理的时候，需要用到这个方法。(如：数据padding）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create datasets</span></span><br><span class="line">train_dataset = Dataset(X=X_train, y=y_train)</span><br><span class="line">val_dataset = Dataset(X=X_val, y=y_val)</span><br><span class="line">test_dataset = Dataset(X=X_test, y=y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Datasets:\n"</span></span><br><span class="line">       <span class="string">f"  Train dataset:<span class="subst">&#123;train_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">       <span class="string">f"  Val dataset: <span class="subst">&#123;val_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">       <span class="string">f"  Test dataset: <span class="subst">&#123;test_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">       <span class="string">"Sample point:\n"</span></span><br><span class="line">       <span class="string">f"  X: <span class="subst">&#123;train_dataset[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>\n"</span></span><br><span class="line">       <span class="string">f"  y: <span class="subst">&#123;train_dataset[<span class="number">0</span>][<span class="number">1</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Datasets:</span></span><br><span class="line"><span class="comment">#   Train dataset: &lt;Dataset(N=1050)&gt;</span></span><br><span class="line"><span class="comment">#   Val dataset: &lt;Dataset(N=225)&gt;</span></span><br><span class="line"><span class="comment">#   Test dataset: &lt;Dataset(N=225)&gt;</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: [-1.47355106 -1.67417243]</span></span><br><span class="line"><span class="comment">#   y: 0</span></span><br></pre></td></tr></table></figure><p>之前的文章中都是利用全部的数据进行梯度计算，然而更标准的做法是 <strong>mini-batch</strong> 随机梯度下降，也就是将样本分成多个只有 n(BATCH_SIZE) 个样本的 mini-batch。这就是 Dataloader 派上用场的地方。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create dataloaders</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_dataloader = train_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">val_dataloader = val_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">test_dataloader = test_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">batch_X, batch_y = next(iter(train_dataloader))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Sample batch:\n"</span></span><br><span class="line">       <span class="string">f"  X: <span class="subst">&#123;list(batch_X.size())&#125;</span>\n"</span></span><br><span class="line">       <span class="string">f"  y: <span class="subst">&#123;list(batch_y.size())&#125;</span>\n"</span></span><br><span class="line">       <span class="string">"Sample point:\n"</span></span><br><span class="line">       <span class="string">f"  X: <span class="subst">&#123;batch_X[<span class="number">0</span>]&#125;</span>\n"</span></span><br><span class="line">       <span class="string">f"  y: <span class="subst">&#123;batch_y[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Sample batch:</span></span><br><span class="line"><span class="comment">#   X: [64, 2]</span></span><br><span class="line"><span class="comment">#   y: [64]</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: tensor([ 0.4535, -0.3570], dtype=torch.float64)</span></span><br><span class="line"><span class="comment">#   y: 0</span></span><br></pre></td></tr></table></figure><h2 id="Model">Model</h2><p>我们需要定义一个模型，以便继续给出训练阶段的实用组件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>]  <span class="comment"># 2D</span></span><br><span class="line">HIDDEN_DIM = <span class="number">100</span></span><br><span class="line">DROPOUT_P = <span class="number">.01</span></span><br><span class="line">NUM_CLASSES = len(label_encoder.classes)</span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, dropout_p, num_classes)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = F.relu(self.fc1(x_in))</span><br><span class="line">        z = self.dropout(z)</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span class="line">            dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of MLP(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (droput): Dropout(p=0.01, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=3, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Trainer">Trainer</h2><p>之前的文章，我们一直在编写只使用循环来训练分割后的训练数据，然后在测试集上评估。</p><p>但实际工作中，我们会遵循下面这个过程：</p><ul><li>使用mini-batches进行训练</li><li>在验证集上评估损失，并更新超参</li><li>训练结束后，在测试集上评估模型</li></ul><p>所以我们需要创建 Trainer 类来组织这些过程。</p><p>首先，train_step 用来执行小批量数据训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">    self.model.train()</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">        batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]</span><br><span class="line">        inputs, targets = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">        self.optimizer.zero_grad()  <span class="comment"># reset gradients</span></span><br><span class="line">        z = self.model(inputs)  <span class="comment"># forward pass</span></span><br><span class="line">        J = self.loss_fn(z, targets)</span><br><span class="line">        J.backward()  <span class="comment"># backward pass</span></span><br><span class="line">        self.optimizer.step()  <span class="comment"># Update weights</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cumulative Metrics</span></span><br><span class="line">        loss += (j.detach().item() - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>然后 eval_step，用于验证</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">    self.model.eval()</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    y_trues, x_probs = [], []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.inference_model():</span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">            batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]</span><br><span class="line">            inputs, y_trye = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">            z = self.model(inputs)</span><br><span class="line">            J = self.loss_fn(z, y_true).item()</span><br><span class="line"></span><br><span class="line">            loss += (J - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Store outputs</span></span><br><span class="line">            y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">            y_probs.extend(y_prob)</span><br><span class="line">            y_trues.extend(y_true.cpu().numpy())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, np.vstack(y_trues), np.vstack(y_probs)</span><br></pre></td></tr></table></figure><p>最后 predict_step, 只是用来对数据进行预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">    self.model.eval()</span><br><span class="line">    y_prods = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.inference_model():</span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">            inputs, y_trye = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">            z = self.model(inputs)</span><br><span class="line">            y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">            y_probs.extend(y_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.vstack(y_probs)</span><br></pre></td></tr></table></figure><h2 id="LR-scheduler">LR scheduler</h2><p>我们将向优化器添加一个学习率调度器，以在训练期间调整我们的学习率。</p><p>有许多<a href="%22https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate%22" title="How to adjust learning rate">调度器</a>可供选择，但最受欢迎的是 ReduceLROnPlateau ，它在指标（例如：验证损失）停止改进的时候，减少学习率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the LR scheduler</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="string">"min"</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS * <span class="number">10</span>):</span><br><span class="line">    ...</span><br><span class="line">    train_loss = trainer.train_step(dataloader=train_dataloader)</span><br><span class="line">    val_loss, _, _ = trainer.eval_step(dataloader=val_dataloader)</span><br><span class="line">    scheduler.step(val_loss)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><h2 id="Early-stopping">Early stopping</h2><p>我们不应该拍脑袋训练足够多的epoch，而是应该有个明确的停止标准。</p><p>常见的停止标准，是模型达到一个期望的性能时，即停止训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Early stopping</span></span><br><span class="line"><span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">    best_val_loss = val_loss</span><br><span class="line">    best_model = trainer.model</span><br><span class="line">    _patience = patience  <span class="comment"># reset _patience</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    _patience -= <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> _patience:  <span class="comment"># 0</span></span><br><span class="line">    print(<span class="string">"Stopping early!"</span>)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><p>现在把上面这些放到一起</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">LEARNING_RATE = <span class="number">1e-2</span></span><br><span class="line">NUM_EPOCHS = <span class="number">100</span></span><br><span class="line">PATIENCE = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer &amp; scheduler</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">    optimizer, mode=<span class="string">"min"</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, device, loss_fn=None, optimizer=None, scheduler=None)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set params</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.device = device</span><br><span class="line">        self.loss_fn = loss_fn</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self.scheduler = scheduler</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Train step."""</span></span><br><span class="line">        <span class="comment"># Set model to train mode</span></span><br><span class="line">        self.model.train()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over train batches</span></span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Step</span></span><br><span class="line">            batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">            inputs, targets = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">            self.optimizer.zero_grad()  <span class="comment"># Reset gradients</span></span><br><span class="line">            z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">            J = self.loss_fn(z, targets)  <span class="comment"># Define loss</span></span><br><span class="line">            J.backward()  <span class="comment"># Backward pass</span></span><br><span class="line">            self.optimizer.step()  <span class="comment"># Update weights</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Cumulative Metrics</span></span><br><span class="line">            loss += (J.detach().item() - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Validation or test step."""</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.eval()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line">        y_trues, y_probs = [], []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Step</span></span><br><span class="line">                batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">                inputs, y_true = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">                z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">                J = self.loss_fn(z, y_true).item()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Cumulative Metrics</span></span><br><span class="line">                loss += (J - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line">                y_trues.extend(y_true.cpu().numpy())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, np.vstack(y_trues), np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Prediction step."""</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.eval()</span><br><span class="line">        y_probs = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Forward pass w/ inputs</span></span><br><span class="line">                inputs, targets = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">                z = self.model(inputs)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, num_epochs, patience, train_dataloader, val_dataloader)</span>:</span></span><br><span class="line">        best_val_loss = np.inf</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">            <span class="comment"># Steps</span></span><br><span class="line">            train_loss = self.train_step(dataloader=train_dataloader)</span><br><span class="line">            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)</span><br><span class="line">            self.scheduler.step(val_loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Early stopping</span></span><br><span class="line">            <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">                best_val_loss = val_loss</span><br><span class="line">                best_model = self.model</span><br><span class="line">                _patience = patience  <span class="comment"># reset _patience</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                _patience -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> _patience:  <span class="comment"># 0</span></span><br><span class="line">                print(<span class="string">"Stopping early!"</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Logging</span></span><br><span class="line">            print(</span><br><span class="line">                <span class="string">f"Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> | "</span></span><br><span class="line">                <span class="string">f"train_loss: <span class="subst">&#123;train_loss:<span class="number">.5</span>f&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"val_loss: <span class="subst">&#123;val_loss:<span class="number">.5</span>f&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"lr: <span class="subst">&#123;self.optimizer.param_groups[<span class="number">0</span>][<span class="string">'lr'</span>]:<span class="number">.2</span>E&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"_patience: <span class="subst">&#123;_patience&#125;</span>"</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> best_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trainer module</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, device=device, loss_fn=loss_fn,</span><br><span class="line">    optimizer=optimizer, scheduler=scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">best_model = trainer.train(</span><br><span class="line">    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 1 | train_loss: 0.87488, val_loss: 0.66353, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 2 | train_loss: 0.66368, val_loss: 0.55748, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># Epoch: 67 | train_loss: 0.03002, val_loss: 0.02305, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 68 | train_loss: 0.03011, val_loss: 0.02309, lr: 1.00E-02, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 69 | train_loss: 0.02544, val_loss: 0.02227, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 70 | train_loss: 0.02680, val_loss: 0.02154, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 71 | train_loss: 0.02897, val_loss: 0.02162, lr: 1.00E-02, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 72 | train_loss: 0.02737, val_loss: 0.02190, lr: 1.00E-02, _patience: 1</span></span><br><span class="line"><span class="comment"># Stopping early!</span></span><br></pre></td></tr></table></figure><h2 id="Evaluation">Evaluation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(y_true, y_pred, classes)</span>:</span></span><br><span class="line">    <span class="string">"""Per-class performance metrics."""</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">"overall"</span>: &#123;&#125;, <span class="string">"class"</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">"weighted"</span>)</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"precision"</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"recall"</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"f1"</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"num_samples"</span>] = np.float64(len(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">        performance[<span class="string">"class"</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">"precision"</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">"recall"</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">"f1"</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">"num_samples"</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions</span></span><br><span class="line">test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Determine performance</span></span><br><span class="line">performance = get_metrics(</span><br><span class="line">    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance[<span class="string">"overall"</span>], indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "precision": 0.9956140350877192,</span></span><br><span class="line"><span class="comment">#   "recall": 0.9955555555555555,</span></span><br><span class="line"><span class="comment">#   "f1": 0.9955553580159118,</span></span><br><span class="line"><span class="comment">#   "num_samples": 225.0</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><h2 id="Saving-loading">Saving &amp; loading</h2><p>我们需要保存一些必要的模型数据，以供后续能够完整的加载和使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Save artifacts</span></span><br><span class="line">dir = Path(<span class="string">"mlp"</span>)</span><br><span class="line">dir.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">label_encoder.save(fp=Path(dir, <span class="string">"label_encoder.json"</span>))</span><br><span class="line">X_scaler.save(fp=Path(dir, <span class="string">"X_scaler.json"</span>))</span><br><span class="line">torch.save(best_model.state_dict(), Path(dir, <span class="string">"model.pt"</span>))</span><br><span class="line"><span class="keyword">with</span> open(Path(dir, <span class="string">'performance.json'</span>), <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    json.dump(performance, indent=<span class="number">2</span>, sort_keys=<span class="literal">False</span>, fp=fp)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load artifacts</span></span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line">label_encoder = LabelEncoder.load(fp=Path(dir, <span class="string">"label_encoder.json"</span>))</span><br><span class="line">X_scaler = StandardScaler.load(fp=Path(dir, <span class="string">"X_scaler.json"</span>))</span><br><span class="line">model = MLP(</span><br><span class="line">    input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span class="line">    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line">model.load_state_dict(torch.load(Path(dir, <span class="string">"model.pt"</span>), map_location=device))</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize trainer</span></span><br><span class="line">trainer = Trainer(model=model, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dataloader</span></span><br><span class="line">sample = [[<span class="number">0.106737</span>, <span class="number">0.114197</span>]] <span class="comment"># c1</span></span><br><span class="line">X = X_scaler.scale(sample)</span><br><span class="line">y_filler = label_encoder.encode([label_encoder.classes[<span class="number">0</span>]]*len(X))</span><br><span class="line">dataset = Dataset(X=X, y=y_filler)</span><br><span class="line">dataloader = dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line">y_prob = trainer.predict_step(dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line">label_encoder.decode(y_pred)</span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>本文给出了一个机器学习项目的基本组件， 事实上，还有一些其他的重要组成没有覆盖到。比如：</p><ul><li>文本序列化的Tokenizers</li><li>表征数据的Encoders</li><li>数据padding</li><li>实验跟踪及可视化结果</li><li>超惨优化</li><li>等等</li></ul><p>后续我们会继续学习，至少到这里，我们有了入门深度学习的基础了。</p>]]></content>
    
    <summary type="html">
    
      先上手再说。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 神经网络 (二)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-neural-networks-2/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-neural-networks-2/</id>
    <published>2023-06-12T14:56:13.000Z</published>
    <updated>2023-06-14T05:21:05.688Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>接上篇，本文使用PyTorch实现一个相同的神经网络模型。</p><h2 id="Model">Model</h2><p>我们将使用两个线性连接层，并在前向传播中添加ReLU激活函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, num_classes)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = F.relu(self.fc1(x_in))</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initalize model</span></span><br><span class="line">model = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)</span><br><span class="line">print(model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of MLP(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=3, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><p>训练模型的代码跟之前学到的逻辑回归几乎没有区别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">LEARNING_RATE = <span class="number">1e-2</span></span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values()))</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_fn</span><span class="params">(y_pred, y_true)</span>:</span></span><br><span class="line">    n_correct = torch.eq(y_pred, y_true).sum().item()</span><br><span class="line">    accuarcy = (n_correct / len(y_pred)) * <span class="number">100</span></span><br><span class="line">    <span class="keyword">return</span> accuarcy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.LongTensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS * <span class="number">10</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        predictions = y_pred.max(dim=<span class="number">1</span>)[<span class="number">1</span>]  <span class="comment"># class</span></span><br><span class="line">        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 0.09, accuracy: 98.6</span></span><br><span class="line"><span class="comment"># Epoch: 10 | loss: 0.06, accuracy: 99.0</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.05, accuracy: 99.2</span></span><br><span class="line"><span class="comment"># Epoch: 30 | loss: 0.04, accuracy: 99.6</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.03, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 50 | loss: 0.03, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 60 | loss: 0.03, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 70 | loss: 0.02, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 80 | loss: 0.02, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 90 | loss: 0.02, accuracy: 99.7</span></span><br></pre></td></tr></table></figure><h2 id="Evaluation">Evaluation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predictiions</span></span><br><span class="line">y_prob = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line">y_pred = y_prob.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 1.0,</span></span><br><span class="line"><span class="comment">#     "recall": 1.0,</span></span><br><span class="line"><span class="comment">#     "f1": 1.0,</span></span><br><span class="line"><span class="comment">#     "num_samples": 225.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "c1": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 1.0,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c2": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 1.0,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c3": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 1.0,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/3ac0940bf9a117acde72a4d36241c2b8.png" alt=""></p><p>如你所见，PyTorch的直观和易用性能让我的学习曲线相对平缓。</p><p>需要我们编写的核心代码，只集中在定义模型、定义损失函数和优化器、定义训练循环、验证和测试这个四个部分。</p><p>当然，还有许多细节需要考虑，比如说数据预处理、模型的保存和加载、使用GPU等。</p><h2 id="Initializing-weights">Initializing weights</h2><p>到目前为止，我们都是使用了一个很小的随机值初始化权重，这其实不是让模型在训练阶段能够收敛的最佳方式。</p><p>我们的目标是初始化一个合适的权重，使得我们激活的输出不会消失或者爆炸，因为这两种情况都会阻碍模型收敛。事实上我们可以<a href="https://pytorch.org/docs/stable/nn.init.html" target="_blank" rel="noopener" title="nn.init">自定义权重初始化</a>方法。目前比较常用的是<a href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_normal_" target="_blank" rel="noopener" title="nn.init.xavier_normal_">Xavier初始化方法</a>和<a href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_" target="_blank" rel="noopener" title="nn.init.kaiming_normal_">He初始化方法</a>。</p><p>事实上PyTorch的Linear类默认使用了kaiming_uniform_初始化方法，相关源代码看<a href="https://github.com/pytorch/pytorch/blob/af7dc23124a6e3e7b8af0637e3b027f3a8b3fb76/torch/nn/modules/linear.py#L101" target="_blank" rel="noopener" title="Linear源码">这里</a>，后续我们会学习到更高级的优化收敛的策略如batch normalization。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, num_classes)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        init.xavier_normal_(self.fc1.weight, gain=init.calculate_gain(<span class="string">"relu"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = F.relu(self.fc1(x_in))</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure><h2 id="Dropout">Dropout</h2><p>能够让我们的模型表现的好的最好的技术是增加数据，但这并不总是一个可选项。幸运的是，还有有一些帮助模型更健壮的其他办法，如正则化、dropout等。</p><p>Dropout是在训练过程中允许我们将神经元的输出置0的技术。由于我们每批次都会丢弃一组不同的神经元，所以Dropout可以作为一种采样策略，防止过拟合。</p><p><img src="//s3.mindex.xyz/blog/Courses/2c301aaf51dcbdc7fb1556b1cf547228.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">DROPOUT_P = <span class="number">0.1</span> <span class="comment"># percentage of weights that are dropped each pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, dropout_p, num_classes)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p) <span class="comment"># dropout</span></span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        init.xavier_normal(self.fc1.weight, gain=init.calculate_gain(<span class="string">"relu"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = F.relu(self.fc1(x_in))</span><br><span class="line">        z = self.dropout(z) <span class="comment"># dropout</span></span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span class="line">            dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of MLP(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=3, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Overfitting">Overfitting</h2><p>虽然神经网络很擅长捕捉非线性关系，但它们非常容易对训练数据进行过度拟合，且无法对测试数据进行归纳。</p><p>看看下面的例子，我们使用完全随机的数据，并试图拟合含 $2 * N * C + D $ (其中N=样本数，C=标签，D表示输入纬度) 隐藏神经元的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line">NUM_EPOCHS = <span class="number">500</span></span><br><span class="line">NUM_SAMPLES_PER_CLASS = <span class="number">50</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line">HIDDEN_DIM = <span class="number">2</span> * NUM_SAMPLES_PER_CLASS * NUM_CLASSES + INPUT_DIM <span class="comment"># 2*N*C + D</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate random data</span></span><br><span class="line">X = np.random.rand(NUM_SAMPLES_PER_CLASS * NUM_CLASSES, INPUT_DIM)</span><br><span class="line">y = np.array([[i] * NUM_SAMPLES_PER_CLASS <span class="keyword">for</span> i <span class="keyword">in</span> range(NUM_CLASSES)]).reshape(<span class="number">-1</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X: "</span>, format(np.shape(X)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: "</span>, format(np.shape(y)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X:  (150, 2)</span></span><br><span class="line"><span class="comment"># y:  (150,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (105, 2), y_train: (105,)</span></span><br><span class="line"><span class="comment"># X_val: (23, 2), y_val: (23,)</span></span><br><span class="line"><span class="comment"># X_test: (22, 2), y_test: (22,)</span></span><br><span class="line"><span class="comment"># Sample point: [0.51102894 0.55377194] → 2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the inputs (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler().fit(X_train)</span><br><span class="line">X_train = X_scaler.transform(X_train)</span><br><span class="line">X_val = X_scaler.transform(X_val)</span><br><span class="line">X_test = X_scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.LongTensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span class="line">            dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of MLP(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=302, bias=True)</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=302, out_features=3, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">50</span>==<span class="number">0</span>:</span><br><span class="line">        predictions = y_pred.max(dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># class</span></span><br><span class="line">        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 1.07, accuracy: 43.8</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.94, accuracy: 52.4</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.89, accuracy: 55.2</span></span><br><span class="line"><span class="comment"># Epoch: 60 | loss: 0.87, accuracy: 49.5</span></span><br><span class="line"><span class="comment"># Epoch: 80 | loss: 0.82, accuracy: 63.8</span></span><br><span class="line"><span class="comment"># Epoch: 100 | loss: 0.84, accuracy: 62.9</span></span><br><span class="line"><span class="comment"># Epoch: 120 | loss: 0.75, accuracy: 63.8</span></span><br><span class="line"><span class="comment"># Epoch: 140 | loss: 0.77, accuracy: 60.0</span></span><br><span class="line"><span class="comment"># Epoch: 160 | loss: 0.75, accuracy: 60.0</span></span><br><span class="line"><span class="comment"># Epoch: 180 | loss: 0.75, accuracy: 66.7</span></span><br><span class="line"><span class="comment"># Epoch: 200 | loss: 0.75, accuracy: 67.6</span></span><br><span class="line"><span class="comment"># Epoch: 220 | loss: 0.69, accuracy: 68.6</span></span><br><span class="line"><span class="comment"># Epoch: 240 | loss: 0.75, accuracy: 65.7</span></span><br><span class="line"><span class="comment"># Epoch: 260 | loss: 0.73, accuracy: 71.4</span></span><br><span class="line"><span class="comment"># Epoch: 280 | loss: 0.73, accuracy: 69.5</span></span><br><span class="line"><span class="comment"># Epoch: 300 | loss: 0.71, accuracy: 62.9</span></span><br><span class="line"><span class="comment"># Epoch: 320 | loss: 0.68, accuracy: 69.5</span></span><br><span class="line"><span class="comment"># Epoch: 340 | loss: 0.74, accuracy: 65.7</span></span><br><span class="line"><span class="comment"># Epoch: 360 | loss: 0.68, accuracy: 71.4</span></span><br><span class="line"><span class="comment"># Epoch: 380 | loss: 0.78, accuracy: 63.8</span></span><br><span class="line"><span class="comment"># Epoch: 400 | loss: 0.69, accuracy: 66.7</span></span><br><span class="line"><span class="comment"># Epoch: 420 | loss: 0.75, accuracy: 67.6</span></span><br><span class="line"><span class="comment"># Epoch: 440 | loss: 0.76, accuracy: 69.5</span></span><br><span class="line"><span class="comment"># Epoch: 460 | loss: 0.71, accuracy: 67.6</span></span><br><span class="line"><span class="comment"># Epoch: 480 | loss: 0.66, accuracy: 66.7</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">y_prob = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line">y_pred = y_prob.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 0.45959595959595956,</span></span><br><span class="line"><span class="comment">#     "recall": 0.45454545454545453,</span></span><br><span class="line"><span class="comment">#     "f1": 0.4512987012987013,</span></span><br><span class="line"><span class="comment">#     "num_samples": 22.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "c1": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.5,</span></span><br><span class="line"><span class="comment">#       "recall": 0.375,</span></span><br><span class="line"><span class="comment">#       "f1": 0.42857142857142855,</span></span><br><span class="line"><span class="comment">#       "num_samples": 8.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c2": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.4444444444444444,</span></span><br><span class="line"><span class="comment">#       "recall": 0.5714285714285714,</span></span><br><span class="line"><span class="comment">#       "f1": 0.5,</span></span><br><span class="line"><span class="comment">#       "num_samples": 7.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c3": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.42857142857142855,</span></span><br><span class="line"><span class="comment">#       "recall": 0.42857142857142855,</span></span><br><span class="line"><span class="comment">#       "f1": 0.42857142857142855,</span></span><br><span class="line"><span class="comment">#       "num_samples": 7.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/8b43366da170668bdeaee171c279362d.png" alt=""></p><p>正如你所见，虽然模型在训练集上做到了接近70%的准确率，但模型在测试集上的表现并不能令人满意。</p><p>重要的是我们需要进行实验，从不合适（高偏差）的简单模型开始，并试图改进到良好的拟合，以及避免过拟合。</p><p><img src="//s3.mindex.xyz/blog/Courses/9a3b5a8d871020ccda41430ca7958bc1.png" alt=""></p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Neural networks - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      PyTorch实现一个神经网络。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 神经网络 (一)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-neural-networks-1/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-neural-networks-1/</id>
    <published>2023-06-01T08:16:24.000Z</published>
    <updated>2023-06-08T15:33:37.054Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>本章的目标依然是学习一种模型 $\hat{y}$，能准确对输入 $X$ 及对应的输出 $y$ 进行建模。</p><p>你会注意到神经网络只是我们迄今为止看到的广义线性方法的扩展，但具有非线性激活函数，因为我们的数据是高度非线性的。</p><p><img src="//s3.mindex.xyz/blog/Courses/908c174b73d8c8bb1c1ec3ba9e4cf885.png" alt=""></p><p>$$<br>z_1 = XW_1<br>$$</p><p>$$<br>a_1 = f(z_1)<br>$$</p><p>$$<br>z_2 = a_1W_2<br>$$</p><p>$$<br>\hat{y} = softmax(x)<br>$$</p><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">解释</th></tr></thead><tbody><tr><td style="text-align:center">$N$</td><td style="text-align:center">样本数</td></tr><tr><td style="text-align:center">$D$</td><td style="text-align:center">特征数</td></tr><tr><td style="text-align:center">$H$</td><td style="text-align:center">隐藏神经元</td></tr><tr><td style="text-align:center">$C$</td><td style="text-align:center">标签数</td></tr><tr><td style="text-align:center">$W_1$</td><td style="text-align:center">第一层的权重</td></tr><tr><td style="text-align:center">$z_1$</td><td style="text-align:center">第一层的输出</td></tr><tr><td style="text-align:center">$f$</td><td style="text-align:center">非线性激活函数</td></tr><tr><td style="text-align:center">$a_1$</td><td style="text-align:center">第一层的激活值</td></tr><tr><td style="text-align:center">$W_2$</td><td style="text-align:center">第二层的权重</td></tr><tr><td style="text-align:center">$z_2$</td><td style="text-align:center">第二层的输出</td></tr></tbody></table><h2 id="Set-up">Set up</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">np.random.seed(SEED)</span><br><span class="line">random.seed(SEED)</span><br></pre></td></tr></table></figure><h3 id="Load-data">Load data</h3><p>这里准备了一份非线性可分的螺旋数据来学习。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">url = <span class="string">"http://s3.mindex.xyz/datasets/9378f64fc8dd2817e4c92be0a3bae8e7.csv"</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>) <span class="comment"># load</span></span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>) <span class="comment"># shuffle</span></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/4022744de8e63b11599cdd95aab6ac62.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Data shapes</span></span><br><span class="line">X = df[[<span class="string">"X1"</span>, <span class="string">"X2"</span>]].values</span><br><span class="line">y = df[<span class="string">"color"</span>].values</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X: "</span>, np.shape(X))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: "</span>, np.shape(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X:  (1500, 2)</span></span><br><span class="line"><span class="comment"># y:  (1500,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize data</span></span><br><span class="line">plt.title(<span class="string">"Generated non-linear data"</span>)</span><br><span class="line">colors = &#123;<span class="string">"c1"</span>: <span class="string">"red"</span>, <span class="string">"c2"</span>: <span class="string">"yellow"</span>, <span class="string">"c3"</span>: <span class="string">"blue"</span>&#125;</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=[colors[_y] <span class="keyword">for</span> _y <span class="keyword">in</span> y], edgecolors=<span class="string">"k"</span>, s=<span class="number">25</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/147e784e6ecae3fd226abce4f3905550.png" alt=""></p><h3 id="Split-data">Split data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_test_split</span><span class="params">(X, y, train_size)</span>:</span></span><br><span class="line">    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)</span><br><span class="line">    X_test, X_val, y_test, y_val = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (1050, 2), y_train: (1050,)</span></span><br><span class="line"><span class="comment"># X_val: (225, 2), y_val: (225,)</span></span><br><span class="line"><span class="comment"># X_test: (225, 2), y_test: (225,)</span></span><br><span class="line"><span class="comment"># Sample point: [0.17003003 0.63079261] → c3</span></span><br></pre></td></tr></table></figure><h3 id="Label-encoding">Label encoding</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output vectorizer</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line"></span><br><span class="line"><span class="comment"># FIt on train date</span></span><br><span class="line">label_encoder = label_encoder.fit(y_train)</span><br><span class="line">classes = list(label_encoder.classes_)</span><br><span class="line">print(<span class="string">f"classes: <span class="subst">&#123;classes&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># classes: ['c1', 'c2', 'c3']</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert labels to tokens</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">y_train = label_encoder.transform(y_train)</span><br><span class="line">y_val = label_encoder.transform(y_val)</span><br><span class="line">y_test = label_encoder.transform(y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: c3</span></span><br><span class="line"><span class="comment"># y_train[0]: 2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> enumerate(counts)&#125;</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># counts: [350 350 350]</span></span><br><span class="line"><span class="comment"># weights: &#123;0: 0.002857142857142857, 1: 0.002857142857142857, 2: 0.002857142857142857&#125;</span></span><br></pre></td></tr></table></figure><h3 id="Standardize-data">Standardize data</h3><p>因为 $y$ 是类别值，所以我们只标准化 $X$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the data (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler().fit(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply scaler on training and test data (don't standardize outputs for classification)</span></span><br><span class="line">X_train = X_scaler.transform(X_train)</span><br><span class="line">X_val = X_scaler.transform(X_val)</span><br><span class="line">X_test = X_scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[0]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[1]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_test[0]: mean: 0.0, std: 1.0</span></span><br><span class="line"><span class="comment"># X_test[1]: mean: -0.0, std: 1.0</span></span><br></pre></td></tr></table></figure><h2 id="Linear-model">Linear model</h2><p>在尝试使用神经网络之前，为了解释激活函数，我们先用前面学到的逻辑回归模型来学习我们的数据。</p><p>你会发现一个用线性激活函数的线性模型对我们的数据来说并不是合适的。</p><h3 id="Model-Train">Model &amp; Train</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>] <span class="comment"># X is 2-dimensional</span></span><br><span class="line">HIDDEN_DIM = <span class="number">100</span></span><br><span class="line">NUM_CLASSES = len(classes) <span class="comment"># 3 classes</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, num_classes)</span>:</span></span><br><span class="line">        super(LinearModel, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = self.fc1(x_in) <span class="comment"># linear activation</span></span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = LinearModel(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)</span><br><span class="line"></span><br><span class="line">LEARNING_RATE = <span class="number">1e-2</span></span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values()))</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_fn</span><span class="params">(y_pred, y_true)</span>:</span></span><br><span class="line">    n_correct = torch.eq(y_pred, y_true).sum().item()</span><br><span class="line">    accuracy = (n_correct / len(y_pred)) * <span class="number">100</span></span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.LongTensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">1</span>==<span class="number">0</span>:</span><br><span class="line">        predictions = y_pred.max(dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># class</span></span><br><span class="line">        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 1.18, accuracy: 43.7</span></span><br><span class="line"><span class="comment"># Epoch: 1 | loss: 0.92, accuracy: 55.6</span></span><br><span class="line"><span class="comment"># Epoch: 2 | loss: 0.79, accuracy: 54.5</span></span><br><span class="line"><span class="comment"># Epoch: 3 | loss: 0.74, accuracy: 54.4</span></span><br><span class="line"><span class="comment"># Epoch: 4 | loss: 0.73, accuracy: 53.9</span></span><br><span class="line"><span class="comment"># Epoch: 5 | loss: 0.73, accuracy: 53.9</span></span><br><span class="line"><span class="comment"># Epoch: 6 | loss: 0.74, accuracy: 55.0</span></span><br><span class="line"><span class="comment"># Epoch: 7 | loss: 0.75, accuracy: 55.8</span></span><br><span class="line"><span class="comment"># Epoch: 8 | loss: 0.76, accuracy: 56.2</span></span><br><span class="line"><span class="comment"># Epoch: 9 | loss: 0.77, accuracy: 56.7</span></span><br></pre></td></tr></table></figure><h3 id="Evaluation">Evaluation</h3><p>我们来看一下这个线性模型在螺旋数据上的表现如何。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(y_true, y_pred, classes)</span>:</span></span><br><span class="line">    <span class="string">"""Per-class performance metrics."""</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">"overall"</span>: &#123;&#125;, <span class="string">"class"</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">"weighted"</span>)</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"precision"</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"recall"</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"f1"</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"num_samples"</span>] = np.float64(len(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">        performance[<span class="string">"class"</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">"precision"</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">"recall"</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">"f1"</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">"num_samples"</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">y_prob = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample probability: <span class="subst">&#123;y_prob[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">y_pred = y_prob.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample class: <span class="subst">&#123;y_pred[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># sample probability: tensor([0.3424, 0.0918, 0.5659], grad_fn=&lt;SelectBackward0&gt;)</span></span><br><span class="line"><span class="comment"># sample class: 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 0.5174825174825175,</span></span><br><span class="line"><span class="comment">#     "recall": 0.5155555555555555,</span></span><br><span class="line"><span class="comment">#     "f1": 0.5162093875662788,</span></span><br><span class="line"><span class="comment">#     "num_samples": 225.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "c1": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.5194805194805194,</span></span><br><span class="line"><span class="comment">#       "recall": 0.5333333333333333,</span></span><br><span class="line"><span class="comment">#       "f1": 0.5263157894736841,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c2": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.46153846153846156,</span></span><br><span class="line"><span class="comment">#       "recall": 0.48,</span></span><br><span class="line"><span class="comment">#       "f1": 0.47058823529411764,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c3": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.5714285714285714,</span></span><br><span class="line"><span class="comment">#       "recall": 0.5333333333333333,</span></span><br><span class="line"><span class="comment">#       "f1": 0.5517241379310344,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiclass_decision_boundary</span><span class="params">(model, X, y)</span>:</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].max() + <span class="number">0.1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].max() + <span class="number">0.1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.linspace(x_min, x_max, <span class="number">101</span>), np.linspace(y_min, y_max, <span class="number">101</span>))</span><br><span class="line">    cmap = plt.cm.Spectral</span><br><span class="line"></span><br><span class="line">    X_test = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()</span><br><span class="line">    y_pred = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line">    _, y_pred = y_pred.max(dim=<span class="number">1</span>)</span><br><span class="line">    y_pred = y_pred.reshape(xx.shape)</span><br><span class="line">    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.RdYlBu)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/e722233774a0fd9b91f4d77a3068287d.png" alt=""></p><h2 id="Activation-functions">Activation functions</h2><p>使用广义的线性方法产生了较差的结果，因为我们试图用线性激活函数去学习非线性数据。</p><p>所以我们需要一个可以能让模型学习到数据中的非线性的激活函数。有几种不同的选择，我们稍微探索一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fig size</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Data</span></span><br><span class="line">x = torch.arange(<span class="number">-5.</span>, <span class="number">5.</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sigmoid activation (constrain a value between 0 and 1.)</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Sigmoid activation"</span>)</span><br><span class="line">y = torch.sigmoid(x)</span><br><span class="line">plt.plot(x.numpy(), y.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tanh activation (constrain a value between -1 and 1.)</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">y = torch.tanh(x)</span><br><span class="line">plt.title(<span class="string">"Tanh activation"</span>)</span><br><span class="line">plt.plot(x.numpy(), y.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Relu (clip the negative values to 0)</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">y = F.relu(x)</span><br><span class="line">plt.title(<span class="string">"ReLU activation"</span>)</span><br><span class="line">plt.plot(x.numpy(), y.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show plots</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/499e55bee0461d4b7471247d91ec78b1.png" alt=""></p><p>ReLU激活函数$(max(0, z))$ 是目前为止用的最广泛的激活函数。但每个激活函数都有自己适用场景。比如：如果我们需要输出在0和1之间，那么sigmoid是合适的选择。</p><p>*（在某些情况下，ReLU函数也是不够的。例如，当神经元的输出大多为负时，激活函数的输出为0，这将导致神经元“死去”。为了减轻这种影响，我们可以降低学习率活着使用<a href="https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7" target="_blank" rel="noopener" title="ReLU">“ReLU变种”</a>。 如 Leaky ReLU 或 PRelu，它们会适当倾斜于神经元的负输出。）</p><h2 id="NumPy">NumPy</h2><p>现在，我们创建一个与逻辑回归模型完全相似的多层感知机，但包含一个学习数据中非线性的激活函数。</p><h3 id="Initialize-weights">Initialize weights</h3><p><strong>第一步</strong>: 随机初始化模型的权重$W$。（后面会介绍更有效的初始化策略）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize first layer's weights</span></span><br><span class="line">W1 = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, HIDDEN_DIM)</span><br><span class="line">b1 = np.zeros((<span class="number">1</span>, HIDDEN_DIM))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"W1: <span class="subst">&#123;W1.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"b1: <span class="subst">&#123;b1.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># W1: (2, 100)</span></span><br><span class="line"><span class="comment"># b1: (1, 100)</span></span><br></pre></td></tr></table></figure><h3 id="Model">Model</h3><p><strong>第二步</strong>: 讲输入 $X$ 送到模型中进行前向传播以得到网络的输出。</p><p>首先，我们将输入传给第一层。<br>$$<br>z_1 = XW_1<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># z1 = [NX2] · [2X100] + [1X100] = [NX100]</span></span><br><span class="line">z1 = np.dot(X_train, W1) + b1</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"z1: <span class="subst">&#123;z1.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># z1: (1050, 100)</span></span><br></pre></td></tr></table></figure><p>接下来，我们应用非线性激活函数Relu。<br>$$<br>a_1 = f(z_1)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply activation function</span></span><br><span class="line">a1 = np.maximum(<span class="number">0</span>, z1) <span class="comment"># ReLU</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"a_1: <span class="subst">&#123;a1.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># a_1: (1050, 100)</span></span><br></pre></td></tr></table></figure><p>然着我们将激活函数的输出传给第二层，以获得logit。<br>$$<br>z_2 = a_1W_2<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize second layer's weights</span></span><br><span class="line">W2 = <span class="number">0.01</span> * np.random.randn(HIDDEN_DIM, NUM_CLASSES)</span><br><span class="line">b2 = np.zeros((<span class="number">1</span>, NUM_CLASSES))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"W2: <span class="subst">&#123;W2.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"b2: <span class="subst">&#123;b2.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># W2: (100, 3)</span></span><br><span class="line"><span class="comment"># b2: (1, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># z2 = logits = [NX100] · [100X3] + [1X3] = [NX3]</span></span><br><span class="line">logits = np.dot(a1, W2) + b2</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"logits: <span class="subst">&#123;logits.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample: <span class="subst">&#123;logits[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># logits: (1050, 3)</span></span><br><span class="line"><span class="comment"># sample: [ 0.00017606 -0.0023457   0.00035913]</span></span><br></pre></td></tr></table></figure><p>之后，我们将应用softmax来获得网络的概率输出。</p><p>$$<br>\hat{y} = softmax(z_2)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normalization via softmax to obtain class probabilities</span></span><br><span class="line">exp_logits = np.exp(logits)</span><br><span class="line">y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_hat: <span class="subst">&#123;y_hat.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample: <span class="subst">&#123;y_hat[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_hat: (1050, 3)</span></span><br><span class="line"><span class="comment"># sample: [0.33359304 0.33275285 0.33365411]</span></span><br></pre></td></tr></table></figure><h3 id="Loss">Loss</h3><p><strong>第三步</strong>： 利用交叉熵计算我们分类任务的损失。<br>$$<br>J(\theta) = - \sum_i^K{log(\hat{y}_i)} = - \sum_i^K{log(\frac{e^{W_yX_i}}{\sum_j{e^{WX_i}}})}\\<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss</span></span><br><span class="line">correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])</span><br><span class="line">loss = np.sum(correct_class_logprobs) / len(y_train)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># loss: 1.10</span></span><br></pre></td></tr></table></figure><h3 id="Gradients">Gradients</h3><p><strong>第四步</strong> 计算损失函数 $J(\theta)$ 相对于权重的梯度。</p><p>对于$W_2$的梯度，与前篇逻辑回归的梯度相同，因为 $\hat{y} = softmax(z_2)$</p><p>$$<br>\begin{split}<br>\frac{\partial{J}}{\partial{W_{2j}}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{W_{2j}}}  \\<br>&amp;= - \frac{1}{\hat{y}} \frac{\partial{\hat{y}}}{\partial{W_{2j}}} \\<br>&amp;= - \frac{1}{\frac{e^{a_1 W_{2y}}}{\sum_j{e^{a_1 W}}}} \frac{\sum_j{e^{a_1 W}e^{a_1 W_{2y}} 0 - e^{a_1 W_{2y}} e^{a_1 W_{2j}} a_1 }}{(\sum_j{e^{a_1 W}})^2} \\<br>&amp;= \frac{a_1 e^{a_1 W_{2j}}}{\sum_j{e^{a_1 W}}} \\<br>&amp;= a_1 \hat{y}<br>\end{split}<br>$$</p><p>$$<br>\begin{split}<br>\frac{\partial{J}}{\partial{W_{2y}}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{W_{2y}}}  \\<br>&amp;= - \frac{1}{\hat{y}} \frac{\partial{\hat{y}}}{\partial{W_{2y}}} \\<br>&amp;= - \frac{1}{\frac{e^{a_1 W_{2y}}}{\sum_j{e^{a_1 W}}}} \frac{\sum_j{e^{a_1 W}e^{a_1 W_{2y}} a_1 - e^{W_{2y} a_1}e^{a_1 W_{2y}} a_1}}{(\sum_j{e^{a_1 W}})^2} = \frac{1}{\hat{y}} (a_1 \hat{y}^2 - a_1 \hat{y}) \\<br>&amp;= a_1 (\hat{y} - 1)<br>\end{split}<br>$$</p><p>对于 $W_1$ 的梯度计算有点棘手，因为我们必须要通过两组权重进行反向传播。</p><p>$$<br>\begin{split}<br>\frac{\partial{J}}{\partial{W_1}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{X}} \frac{\partial{X}}{\partial{z_1}}  \frac{\partial{z_1}}{\partial{W_1}} \\<br>&amp;= W_2 (\partial{\hat{y}})(\partial{ReLU})X<br>\end{split}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dJ/dW2</span></span><br><span class="line">dscores = y_hat</span><br><span class="line">dscores[range(len(y_hat)), y_train] -= <span class="number">1</span></span><br><span class="line">dscores /= len(y_train)</span><br><span class="line">dW2 = np.dot(a1.T, dscores)</span><br><span class="line">db2 = np.sum(dscores, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dJ/dW1</span></span><br><span class="line">dhidden = np.dot(dscores, W2.T)</span><br><span class="line">dhidden[a1 &lt;= <span class="number">0</span>] = <span class="number">0</span> <span class="comment"># ReLu backprop</span></span><br><span class="line">dW1 = np.dot(X_train.T, dhidden)</span><br><span class="line">db1 = np.sum(dhidden, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="Update-weights">Update weights</h3><p><strong>第五步</strong> 指定一个学习率来更新权重 $W$，惩罚错误的分类奖励正确的分类。<br>$$<br>W_i = W_i - \alpha \frac{\partial{J}}{\partial{W_i}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Update weights</span></span><br><span class="line">W1 += -LEARNING_RATE * dW1</span><br><span class="line">b1 += -LEARNING_RATE * db1</span><br><span class="line">W2 += -LEARNING_RATE * dW2</span><br><span class="line">b2 += -LEARNING_RATE * db2</span><br></pre></td></tr></table></figure><h3 id="Training">Training</h3><p><strong>第六步</strong>: 重复步骤 2 ~ 5，以最小化损失为目的来训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert tensors to NumPy arrays</span></span><br><span class="line">X_train = X_train.numpy()</span><br><span class="line">y_train = y_train.numpy()</span><br><span class="line">X_val = X_val.numpy()</span><br><span class="line">y_val = y_val.numpy()</span><br><span class="line">X_test = X_test.numpy()</span><br><span class="line">y_test = y_test.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random weights</span></span><br><span class="line">W1 = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, HIDDEN_DIM)</span><br><span class="line">b1 = np.zeros((<span class="number">1</span>, HIDDEN_DIM))</span><br><span class="line">W2 = <span class="number">0.01</span> * np.random.randn(HIDDEN_DIM, NUM_CLASSES)</span><br><span class="line">b2 = np.zeros((<span class="number">1</span>, NUM_CLASSES))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training loop</span></span><br><span class="line"><span class="keyword">for</span> epoch_num <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># First layer forward pass [NX2] · [2X100] = [NX100]</span></span><br><span class="line">    z1 = np.dot(X_train, W1) + b1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply activation function</span></span><br><span class="line">    a1 = np.maximum(<span class="number">0</span>, z1) <span class="comment"># ReLU</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># z2 = logits = [NX100] · [100X3] = [NX3]</span></span><br><span class="line">    logits = np.dot(a1, W2) + b2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Normalization via softmax to obtain class probabilities</span></span><br><span class="line">    exp_logits = np.exp(logits)</span><br><span class="line">    y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])</span><br><span class="line">    loss = np.sum(correct_class_logprobs) / len(y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># show progress</span></span><br><span class="line">    <span class="keyword">if</span> epoch_num%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Accuracy</span></span><br><span class="line">        y_pred = np.argmax(logits, axis=<span class="number">1</span>)</span><br><span class="line">        accuracy =  np.mean(np.equal(y_train, y_pred))</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch_num&#125;</span>, loss: <span class="subst">&#123;loss:<span class="number">.3</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.3</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dJ/dW2</span></span><br><span class="line">    dscores = y_hat</span><br><span class="line">    dscores[range(len(y_hat)), y_train] -= <span class="number">1</span></span><br><span class="line">    dscores /= len(y_train)</span><br><span class="line">    dW2 = np.dot(a1.T, dscores)</span><br><span class="line">    db2 = np.sum(dscores, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dJ/dW1</span></span><br><span class="line">    dhidden = np.dot(dscores, W2.T)</span><br><span class="line">    dhidden[a1 &lt;= <span class="number">0</span>] = <span class="number">0</span> <span class="comment"># ReLu backprop</span></span><br><span class="line">    dW1 = np.dot(X_train.T, dhidden)</span><br><span class="line">    db1 = np.sum(dhidden, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    W1 += <span class="number">-1e0</span> * dW1</span><br><span class="line">    b1 += <span class="number">-1e0</span> * db1</span><br><span class="line">    W2 += <span class="number">-1e0</span> * dW2</span><br><span class="line">    b2 += <span class="number">-1e0</span> * db2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0, loss: 1.098, accuracy: 0.519</span></span><br><span class="line"><span class="comment"># Epoch: 100, loss: 0.541, accuracy: 0.680</span></span><br><span class="line"><span class="comment"># Epoch: 200, loss: 0.305, accuracy: 0.893</span></span><br><span class="line"><span class="comment"># Epoch: 300, loss: 0.135, accuracy: 0.951</span></span><br><span class="line"><span class="comment"># Epoch: 400, loss: 0.091, accuracy: 0.976</span></span><br><span class="line"><span class="comment"># Epoch: 500, loss: 0.069, accuracy: 0.984</span></span><br><span class="line"><span class="comment"># Epoch: 600, loss: 0.056, accuracy: 0.989</span></span><br><span class="line"><span class="comment"># Epoch: 700, loss: 0.048, accuracy: 0.991</span></span><br><span class="line"><span class="comment"># Epoch: 800, loss: 0.043, accuracy: 0.994</span></span><br><span class="line"><span class="comment"># Epoch: 900, loss: 0.039, accuracy: 0.994</span></span><br></pre></td></tr></table></figure><h3 id="Evaluation-2">Evaluation</h3><p>在测试集上评估这个模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLPFromScratch</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        z1 = np.dot(x, W1) + b1</span><br><span class="line">        a1 = np.maximum(<span class="number">0</span>, z1)</span><br><span class="line">        logits = np.dot(a1, W2) + b2</span><br><span class="line">        exp_logits = np.exp(logits)</span><br><span class="line">        y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> y_hat</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluation</span></span><br><span class="line">model = MLPFromScratch()</span><br><span class="line">y_prob = model.predict(X_test)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 0.9826749826749827,</span></span><br><span class="line"><span class="comment">#     "recall": 0.9822222222222222,</span></span><br><span class="line"><span class="comment">#     "f1": 0.9822481383871041,</span></span><br><span class="line"><span class="comment">#     "num_samples": 225.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "c1": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 0.9733333333333334,</span></span><br><span class="line"><span class="comment">#       "f1": 0.9864864864864865,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c2": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.9615384615384616,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 0.9803921568627451,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c3": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.9864864864864865,</span></span><br><span class="line"><span class="comment">#       "recall": 0.9733333333333334,</span></span><br><span class="line"><span class="comment">#       "f1": 0.9798657718120806,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiclass_decision_boundary_numpy</span><span class="params">(model, X, y, savefig_fp=None)</span>:</span></span><br><span class="line">    <span class="string">"""Plot the multiclass decision boundary for a model that accepts 2D inputs.</span></span><br><span class="line"><span class="string">    Credit: https://cs231n.github.io/neural-networks-case-study/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        model &#123;function&#125; -- trained model with function model.predict(x_in).</span></span><br><span class="line"><span class="string">        X &#123;numpy.ndarray&#125; -- 2D inputs with shape (N, 2).</span></span><br><span class="line"><span class="string">        y &#123;numpy.ndarray&#125; -- 1D outputs with shape (N,).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Axis boundaries</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].max() + <span class="number">0.1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].max() + <span class="number">0.1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.linspace(x_min, x_max, <span class="number">101</span>),</span><br><span class="line">                         np.linspace(y_min, y_max, <span class="number">101</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create predictions</span></span><br><span class="line">    x_in = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    y_pred = model.predict(x_in)</span><br><span class="line">    y_pred = np.argmax(y_pred, axis=<span class="number">1</span>).reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot decision boundary</span></span><br><span class="line">    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.RdYlBu)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot</span></span><br><span class="line">    <span class="keyword">if</span> savefig_fp:</span><br><span class="line">        plt.savefig(savefig_fp, format=<span class="string">"png"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary_numpy(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary_numpy(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/0878670f7a00ef84e2037f39161b6fa5.png" alt=""></p><h2 id="Ending">Ending</h2><p>神经网络是机器学习和人工智能领域的基础，我们必须彻底掌握。</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Neural networks - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Numpy实现一个神经网络。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 机器学习之Logistic Regression (二)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-LogisticRegression-2/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-LogisticRegression-2/</id>
    <published>2023-05-29T01:51:29.000Z</published>
    <updated>2023-05-29T06:34:55.041Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>接上篇，本文使用PyTorch实现一个简单的逻辑回归。</p><h2 id="Get-ready">Get ready</h2><p>复用前篇的数据准备及预处理工作，这里直接建模。</p><h2 id="Model">Model</h2><p>我们使用PyTorch的<a href="https://pytorch.org/docs/stable/nn.html#linear-layers" target="_blank" rel="noopener" title="Linear layers">Linear layers</a> 来构建与前篇相同的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, num_classes)</span>:</span></span><br><span class="line">        super(LogisticRegression, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = self.fc1(x_in)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = LogisticRegression(input_dim=INPUT_DIM, num_classes=NUM_CLASSES)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of LogisticRegression(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=2, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Loss">Loss</h2><p>这里使用<a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss" target="_blank" rel="noopener" title="nn.CrossEntropyLoss">交叉熵损失</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">y_pred = torch.randn(<span class="number">3</span>, NUM_CLASSES, requires_grad=<span class="literal">False</span>)</span><br><span class="line">y_true = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(NUM_CLASSES)</span><br><span class="line"><span class="keyword">print</span> (y_true)</span><br><span class="line">loss = loss_fn(y_pred, y_true)</span><br><span class="line">print(<span class="string">f"Loss: <span class="subst">&#123;loss.numpy()&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor([0, 1, 1])</span></span><br><span class="line"><span class="comment"># Loss: 1.0754622220993042</span></span><br></pre></td></tr></table></figure><p>在这个任务中，我们将数据的类别权重纳入到损失函数中，以对抗样本的类别不平衡。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values()))</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br></pre></td></tr></table></figure><h2 id="Metrics">Metrics</h2><p>我们将在训练模型时引入准确度来衡量模型的性能，因为仅查看损失值并不是非常直观。</p><p>后面的章节会介绍相关指标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_fn</span><span class="params">(y_pred, y_true)</span>:</span></span><br><span class="line">    n_correct = torch.eq(y_pred, y_true).sum().item()</span><br><span class="line">    accuracy = (n_correct / len(y_pred)) * <span class="number">100</span></span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line">y_pred = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">y_true = torch.Tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">"Accuracy: &#123;accuracy_fn(y_pred, y_true):.1f&#125;"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Accuracy: &#123;accuracy_fn(y_pred, y_true):.1f&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Optimizer">Optimizer</h2><p>与之前介绍的线性回归一样，这里同样使用Adam优化器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.LongTensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line">NUM_EPOCHS = <span class="number">50</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">        predictions = y_pred.max(dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># class</span></span><br><span class="line">        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 0.71, accuracy: 49.6</span></span><br><span class="line"><span class="comment"># Epoch: 10 | loss: 0.23, accuracy: 93.1</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.14, accuracy: 97.4</span></span><br><span class="line"><span class="comment"># Epoch: 30 | loss: 0.11, accuracy: 98.3</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.09, accuracy: 98.0</span></span><br></pre></td></tr></table></figure><h2 id="Evaluation">Evaluation</h2><p>首先，我们看看一下测试集的准确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">pred_train = F.softmax(model(X_train), dim=<span class="number">1</span>)</span><br><span class="line">pred_test = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample probability: <span class="subst">&#123;pred_test[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">pred_train = pred_train.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">pred_test = pred_test.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample class: <span class="subst">&#123;pred_test[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># sample probability: tensor([0.9934, 0.0066], grad_fn=&lt;SelectBackward0&gt;)</span></span><br><span class="line"><span class="comment"># sample class: 0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy (could've also used our own accuracy function)</span></span><br><span class="line">train_acc = accuracy_score(y_train, pred_train)</span><br><span class="line">test_acc = accuracy_score(y_test, pred_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"train acc: <span class="subst">&#123;train_acc:<span class="number">.2</span>f&#125;</span>, test acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train acc: 0.98, test acc: 0.97</span></span><br></pre></td></tr></table></figure><p>我们还可以根据其他有意义的指标来评估我们的模型，如精确度和召回率。<br>$$<br>accuracy = \frac{TP + FN}{TP + TN + FP + FN}<br>$$<br>$$<br>recall = \frac{TP}{TP + FN}<br>$$<br>$$<br>precision = \frac{TP}{TP + FP}<br>$$<br>$$<br>F1 = 2 * \frac{precision * recall}{precision + recall}<br>$$</p><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">解释</th></tr></thead><tbody><tr><td style="text-align:center">TP</td><td style="text-align:center">truly predicted to be positive and were positive</td></tr><tr><td style="text-align:center">TN</td><td style="text-align:center">truly predicted to negative and where negative</td></tr><tr><td style="text-align:center">FP</td><td style="text-align:center">falsely predicted to be positive but where negative</td></tr><tr><td style="text-align:center">FN</td><td style="text-align:center">falsely predicted to be negative but where positive</td></tr></tbody></table><p>格式化指标，以供前端展示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(y_true, y_pred, classes)</span>:</span></span><br><span class="line">    <span class="string">"""Per-class performance metrics."""</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">"overall"</span>: &#123;&#125;, <span class="string">"class"</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">"weighted"</span>)</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"precision"</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"recall"</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"f1"</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"num_samples"</span>] = np.float64(len(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">        performance[<span class="string">"class"</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">"precision"</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">"recall"</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">"f1"</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">"num_samples"</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br><span class="line"></span><br><span class="line"><span class="comment"># # Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=pred_test, classes=label_encoder.classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 0.9744444444444446,</span></span><br><span class="line"><span class="comment">#     "recall": 0.9733333333333334,</span></span><br><span class="line"><span class="comment">#     "f1": 0.9731408308004051,</span></span><br><span class="line"><span class="comment">#     "num_samples": 150.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "benign": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 0.9310344827586207,</span></span><br><span class="line"><span class="comment">#       "f1": 0.9642857142857143,</span></span><br><span class="line"><span class="comment">#       "num_samples": 58.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "malignant": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.9583333333333334,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 0.9787234042553191,</span></span><br><span class="line"><span class="comment">#       "num_samples": 92.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><p>同样的，利用PyTorch实现的逻辑回归模型建模了一个线性决策边界，我们可视化一下结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiclass_decision_boundary</span><span class="params">(model, X, y)</span>:</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].max() + <span class="number">0.1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].max() + <span class="number">0.1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.linspace(x_min, x_max, <span class="number">101</span>), np.linspace(y_min, y_max, <span class="number">101</span>))</span><br><span class="line">    cmap = plt.cm.Spectral</span><br><span class="line"></span><br><span class="line">    X_test = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()</span><br><span class="line">    y_pred = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line">    _, y_pred = y_pred.max(dim=<span class="number">1</span>)</span><br><span class="line">    y_pred = y_pred.reshape(xx.shape)</span><br><span class="line">    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.RdYlBu)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/8d309a3958c1769ef2403cc39a31dd17.png" alt=""></p><h2 id="Inference">Inference</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inputs for inference</span></span><br><span class="line">X_infer = pd.DataFrame([&#123;<span class="string">"leukocyte_count"</span>: <span class="number">13</span>, <span class="string">"blood_pressure"</span>: <span class="number">12</span>&#125;])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize</span></span><br><span class="line">X_infer = X_scaler.transform(X_infer)</span><br><span class="line"><span class="keyword">print</span> (X_infer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[-0.66859939 -3.09473005]]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">y_infer = F.softmax(model(torch.Tensor(X_infer)), dim=<span class="number">1</span>)</span><br><span class="line">prob, _class = y_infer.max(dim=<span class="number">1</span>)</span><br><span class="line">label = label_encoder.decode(_class.detach().numpy())[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"The probability that you have a <span class="subst">&#123;label&#125;</span> tumor is <span class="subst">&#123;prob.detach().numpy()[<span class="number">0</span>]*<span class="number">100.0</span>:<span class="number">.0</span>f&#125;</span>%"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># The probability that you have a benign tumor is 90%</span></span><br></pre></td></tr></table></figure><h2 id="Unscaled-weights">Unscaled weights</h2><p>同样的，我们亦可以逆标准化我们的权重和偏差。</p><p>注意到只有$X$被标准化过<br>$$<br>\hat{y}_{unscaled} = \sum_{j=1}^k W_{scaled(j)} x_{scaled(j)} + b_{scaled}<br>$$</p><p>已知<br>$$<br>\hat{x}_{scaled} = \frac{x_{j} - \overline{x}_{j}}{\sigma_{j}}<br>$$</p><p>于是<br>$$<br>\hat{y}_{unscaled} = (b_{scaled} - \sum_{j=1}^k W_{scaled(j)} \frac{\overline{x}_j}{\sigma_{j}}) + \sum_j{\frac{W_{scaled(j)}}{\sigma_j}}x_j<br>$$</p><p>对比公式</p><p>$$<br>\hat{y}_{unscaled} = W_{unscaled} x + b_{unscaled}<br>$$</p><p>便可得知<br>$$<br>W_{unscaled} = \frac{W_{scaled(j)}}{\sigma_j}<br>$$</p><p>$$<br>b_{unscaled} = b_{scaled} - \sum_{j=1}^k W_{unscaled(j)} \overline{x}_j<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unstandardize weights</span></span><br><span class="line">W = model.fc1.weight.data.numpy()</span><br><span class="line">b = model.fc1.bias.data.numpy()</span><br><span class="line">W_unscaled = W / X_scaler.scale_</span><br><span class="line">b_unscaled = b - np.sum((W_unscaled * X_scaler.mean_))</span><br><span class="line"><span class="keyword">print</span> (W_unscaled)</span><br><span class="line"><span class="keyword">print</span> (b_unscaled)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[ 0.80800055 -1.47212977]</span></span><br><span class="line"><span class="comment">#  [-0.88854214  0.77129243]]</span></span><br><span class="line"><span class="comment"># [11.4279   13.336911]</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>到这里，我们便完成了PyTorch的逻辑回归任务的介绍。</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Logistic regression - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      使用PyTorch实现逻辑回归。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 机器学习之Logistic Regression (一)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-LogisticRegression-1/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-LogisticRegression-1/</id>
    <published>2023-05-28T11:24:37.000Z</published>
    <updated>2023-06-08T09:42:41.627Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文的目标是使用NumPy实现一个多项式逻辑回归模型 $\hat{y}$ ，以实现对给定的特征$X$预测其目标类别的概率分布。</p><p>$$<br>\hat{y} = \frac{e^{XW_y}}{\sum_j{e^{XW}}}<br>$$</p><p>逻辑回归与前篇所述的线性回归，同属于广义线性回归。皆是建模一条直线(或一个平面)。但不同的是，逻辑回归是一种分类模型。</p><h2 id="Set-up">Set up</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">np.random.seed(SEED)</span><br><span class="line">random.seed(SEED)</span><br></pre></td></tr></table></figure><h2 id="Load-data">Load data</h2><p>我们的任务是基于白细胞计数和血压来确定肿瘤是否为良性（无害）或恶性（有害）。</p><p>请注意，这是一个合成数据集，没有临床意义。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pandas.plotting <span class="keyword">import</span> scatter_matrix</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read from CSV to Pandas DataFrame</span></span><br><span class="line">url = <span class="string">"http://s3.mindex.xyz/datasets/tumors.csv"</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>) <span class="comment"># load</span></span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>) <span class="comment"># shuffle</span></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/7fdb16f4e4e0b9cc36e4c1b765fbc718.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define X and y</span></span><br><span class="line">X = df[[<span class="string">"leukocyte_count"</span>, <span class="string">"blood_pressure"</span>]].values</span><br><span class="line">y = df[<span class="string">"tumor_class"</span>].values</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot data</span></span><br><span class="line">colors = &#123;<span class="string">"benign"</span>: <span class="string">"red"</span>, <span class="string">"malignant"</span>: <span class="string">"blue"</span>&#125;</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=[colors[_y] <span class="keyword">for</span> _y <span class="keyword">in</span> y], s=<span class="number">25</span>, edgecolors=<span class="string">"k"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"leukocyte count"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"blood pressure"</span>)</span><br><span class="line">plt.legend([<span class="string">"malignant"</span>, <span class="string">"benign"</span>], loc=<span class="string">"upper right"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/57d000f09d3f8c9d9f9971618d58ffd9.png" alt=""></p><h2 id="Split-data">Split data</h2><p>我们希望将数据集分成三份，使得每个子集中的类别分布相同，以便进行适当的训练和评估。</p><p><a href="https://scikit-learn.org/stable/index.html" target="_blank" rel="noopener" title="scikit-learn">scikit-learn</a> 提供的方法 <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener" title="train_test_split">train_test_split</a> 可以很容易的做到。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_test_split</span><span class="params">(X, y, train_size)</span>:</span></span><br><span class="line">    <span class="string">"""Split dataset into data splits."""</span></span><br><span class="line">    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)</span><br><span class="line">    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (700, 2), y_train: (700,)</span></span><br><span class="line"><span class="comment"># X_val: (150, 2), y_val: (150,)</span></span><br><span class="line"><span class="comment"># X_test: (150, 2), y_test: (150,)</span></span><br><span class="line"><span class="comment"># Sample point: [18.60187909 18.37050035] → malignant</span></span><br></pre></td></tr></table></figure><p>现在让我们看一下分割后的数据集中样本分布：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Overall class distribution</span></span><br><span class="line">class_counts = dict(collections.Counter(y))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Classes: <span class="subst">&#123;class_counts&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f'm:b = <span class="subst">&#123;class_counts[<span class="string">"malignant"</span>]/class_counts[<span class="string">"benign"</span>]:<span class="number">.2</span>f&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Classes: &#123;'malignant': 611, 'benign': 389&#125;</span></span><br><span class="line"><span class="comment"># m:b = 1.57</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Per data split class distribution</span></span><br><span class="line">train_class_counts = dict(collections.Counter(y_train))</span><br><span class="line">val_class_counts = dict(collections.Counter(y_val))</span><br><span class="line">test_class_counts = dict(collections.Counter(y_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f'train m:b = <span class="subst">&#123;train_class_counts[<span class="string">"malignant"</span>]/train_class_counts[<span class="string">"benign"</span>]:<span class="number">.2</span>f&#125;</span>'</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f'val m:b = <span class="subst">&#123;val_class_counts[<span class="string">"malignant"</span>]/val_class_counts[<span class="string">"benign"</span>]:<span class="number">.2</span>f&#125;</span>'</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f'test m:b = <span class="subst">&#123;test_class_counts[<span class="string">"malignant"</span>]/test_class_counts[<span class="string">"benign"</span>]:<span class="number">.2</span>f&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train m:b = 1.57</span></span><br><span class="line"><span class="comment"># val m:b = 1.59</span></span><br><span class="line"><span class="comment"># test m:b = 1.54</span></span><br></pre></td></tr></table></figure><p>可以看出，分割后的数据集里样本分布大体是接近的</p><h2 id="Label-encoding">Label encoding</h2><p>注意到我们的分类标签是文本。我们需要将其进行编码，以便在模型中使用。</p><p>通常我们使用scikit-learn的<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder" target="_blank" rel="noopener" title="LabelEncoder">LabelEncoder</a>快速编码。</p><p>不过这里我们将编写自己的编码器，以便了解其具体的实现机制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelEncoder</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Label encoder for tag labels."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, class_to_index=&#123;&#125;)</span>:</span></span><br><span class="line">        self.class_to_index = class_to_index <span class="keyword">or</span> &#123;&#125;  <span class="comment"># mutable defaults ;)</span></span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = list(self.class_to_index.keys())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.class_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"&lt;LabelEncoder(num_classes=<span class="subst">&#123;len(self)&#125;</span>)&gt;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        classes = np.unique(y)</span><br><span class="line">        <span class="keyword">for</span> i, class_ <span class="keyword">in</span> enumerate(classes):</span><br><span class="line">            self.class_to_index[class_] = i</span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = list(self.class_to_index.keys())</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        encoded = np.zeros((len(y)), dtype=int)</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            encoded[i] = self.class_to_index[item]</span><br><span class="line">        <span class="keyword">return</span> encoded</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        classes = []</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            classes.append(self.index_to_class[item])</span><br><span class="line">        <span class="keyword">return</span> classes</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;<span class="string">'class_to_index'</span>: self.class_to_index&#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(cls, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"r"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">label_encoder.fit(y_train)</span><br><span class="line"><span class="keyword">print</span> (label_encoder.class_to_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;'benign': 0, 'malignant': 1&#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Encoder</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">y_train = label_encoder.encode(y_train)</span><br><span class="line">y_val = label_encoder.encode(y_val)</span><br><span class="line">y_test = label_encoder.encode(y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"decoded: <span class="subst">&#123;label_encoder.decode([y_train[<span class="number">0</span>]])&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: malignant</span></span><br><span class="line"><span class="comment"># y_train[0]: 1</span></span><br><span class="line"><span class="comment"># decoded: ['malignant']</span></span><br></pre></td></tr></table></figure><p>我们还想计算出类别的权重，这对于在训练期间加权损失函数非常有用。它会告诉模型需要关注样本量不足的类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> enumerate(counts)&#125;</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># counts: [272 428]</span></span><br><span class="line"><span class="comment"># weight: &#123;0: 0.003676470588235294, 1: 0.002336448598130841&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Standardize-data">Standardize data</h2><p>我们需要对数据进行标准化处理（零均值和单位方差），以便某个特定特征的大小不会影响模型学习其权重。</p><p>这里我们只需要标准化输入$X$，因为我们的输出$y$是离散标签，无须标准化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the data (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler().fit(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply scaler on training and test data (don't standardize outputs for classification)</span></span><br><span class="line">X_train = X_scaler.transform(X_train)</span><br><span class="line">X_val = X_scaler.transform(X_val)</span><br><span class="line">X_test = X_scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[0]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[1]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_test[0]: mean: -0.0, std: 1.0</span></span><br><span class="line"><span class="comment"># X_test[1]: mean: 0.1, std: 1.0</span></span><br></pre></td></tr></table></figure><h2 id="Weights">Weights</h2><p>我们的目标是学习一个逻辑回归模型 $\hat{y}$，用来建模自变量 $X$ 与 因变量 $y$ 的关系。</p><p>$$<br>\hat{y} = \frac{e^{W_yX}}{\sum_j{e^{WX}}}<br>$$</p><p>尽管我们的示例任务只涉及两个类别，但我们仍将使用多项式逻辑回归，因为softmax分类器可以推广到任意数量的类别。</p><p><strong>第一步</strong>: 随机初始化模型的权重 $W$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>] <span class="comment"># X is 2-dimensional</span></span><br><span class="line">NUM_CLASSES = len(label_encoder.classes) <span class="comment"># y has two possibilities (benign or malignant)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random weights</span></span><br><span class="line">W = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, NUM_CLASSES)</span><br><span class="line">b = np.zeros((<span class="number">1</span>, NUM_CLASSES))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"W: <span class="subst">&#123;W.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"b: <span class="subst">&#123;b.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># W: (2, 2)</span></span><br><span class="line"><span class="comment"># b: (1, 2)</span></span><br></pre></td></tr></table></figure><h2 id="Model">Model</h2><p><strong>第二步</strong> 计算输入 $X$ 的对数值（$ z = WX $）。然后执行softmax操作得到预测类别的独热编码形式。</p><p>举个例子，如果有三个类别，那么一种可能的预测概率是[0.3, 0.3, 0.4]。<br>$$<br>\hat{y} = softmax(z) = softmax(WX) = \frac{e^{W_yX}}{\sum_j{e^{WX}}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Forward pass [NX2] · [2X2] + [1,2] = [NX2]</span></span><br><span class="line">logits = np.dot(X_train, W) + b</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"logits: <span class="subst">&#123;logits.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample: <span class="subst">&#123;logits[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># logits: (700, 2)</span></span><br><span class="line"><span class="comment"># sample: [0.0151033  0.03500428]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalization via softmax to obtain class probabilities</span></span><br><span class="line">exp_logits = np.exp(logits)</span><br><span class="line">y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_hat: <span class="subst">&#123;y_hat.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample: <span class="subst">&#123;y_hat[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_hat: (700, 2)</span></span><br><span class="line"><span class="comment"># sample: [0.49502492 0.50497508]</span></span><br></pre></td></tr></table></figure><h2 id="Loss">Loss</h2><p><strong>第三步</strong> 使用代价函数比较预测值 $\hat{y}$ （比如：[0.3, 0.3, 0.4]）和目标值 $y$ （比如：[0. 0. 1]）来确定损失 $J$。</p><p>逻辑回归的常见目标函数是交叉熵损失.</p><p>$$<br>J(\theta) = - \sum_i^K{log(\hat{y}_i)} = - \sum_i^K{log(\frac{e^{W_yX_i}}{\sum_j{e^{WX_i}}})}\\<br>$$</p><p>交叉熵损失函数的目标仍然是最小化预测与实际标签之间的差距，从而让模型能够更准确地进行分类.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss</span></span><br><span class="line">correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])</span><br><span class="line">loss = np.sum(correct_class_logprobs) / len(y_train)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 0.69</span></span><br></pre></td></tr></table></figure><h2 id="Gradients">Gradients</h2><p><strong>第四步</strong> 计算损失函数 $J(\theta)$ 相对于权重的梯度。这里假设我们的类别是互斥的。<br>$$<br>\begin{split}<br>\frac{\partial{J}}{\partial{W_j}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{W_j}} = - \frac{1}{\hat{y}} \frac{\partial{\hat{y}}}{\partial{W_j}} \\<br>&amp;= - \frac{1}{\frac{e^{W_yX}}{\sum_j{e^{WX}}}} \frac{\sum_j{e^{WX}e^{W_yX}0 - e^{W_yX}e^{W_jX}X}}{(\sum_j{e^{WX}})^2} = \frac{Xe^{W_jX}}{\sum_j{e^{WX}}} = X\hat{y}<br>\end{split}<br>$$</p><p>$$<br>\begin{split}<br>\frac{\partial{J}}{\partial{W_y}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{W_y}} = - \frac{1}{\hat{y}} \frac{\partial{\hat{y}}}{\partial{W_y}} \\<br>&amp;= - \frac{1}{\frac{e^{W_yX}}{\sum_j{e^{WX}}}} \frac{\sum_j{e^{WX}e^{W_yX}X - e^{W_yX}e^{W_yX}X}}{(\sum_j{e^{WX}})^2} = \frac{1}{\hat{y}} (X\hat{y}^2 - X\hat{y}) = X(\hat{y} - 1)<br>\end{split}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Backpropagation</span></span><br><span class="line">y_hat[range(len(y_hat)), y_train] -= <span class="number">1</span></span><br><span class="line">y_hat /= len(y_train)</span><br><span class="line">dW = np.dot(X_train.T, y_hat)</span><br><span class="line">db = np.sum(y_hat, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="Update-weights">Update weights</h2><p><strong>第五步</strong> 指定一个学习率来更新权重 $W$，惩罚错误的分类奖励正确的分类。<br>$$<br>W_j = W_j - \alpha \frac{\partial{J}}{\partial{W_j}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Update weights</span></span><br><span class="line">W == -LEARNING_RATE * dW</span><br><span class="line">b += -LEARNING_RATE * db</span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><p><strong>第六步</strong>: 重复步骤 2 ~ 5，以最小化损失为目的来训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Initialize random weights</span></span><br><span class="line">W = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, NUM_CLASSES)</span><br><span class="line">b = np.zeros((<span class="number">1</span>, NUM_CLASSES))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training loop</span></span><br><span class="line">NUM_EPOCHS = <span class="number">50</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line"><span class="keyword">for</span> epoch_num <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass [NX2] · [2X2] = [NX2]</span></span><br><span class="line">    logits = np.dot(X_train, W) + b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Normalization via softmax to obtain class probabilities</span></span><br><span class="line">    exp_logits = np.exp(logits)</span><br><span class="line">    y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])</span><br><span class="line">    loss = np.sum(correct_class_logprobs) / len(y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># show progress</span></span><br><span class="line">    <span class="keyword">if</span> epoch_num%<span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Accuracy</span></span><br><span class="line">        y_pred = np.argmax(logits, axis=<span class="number">1</span>)</span><br><span class="line">        accuracy =  np.mean(np.equal(y_train, y_pred))</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch_num&#125;</span>, loss: <span class="subst">&#123;loss:<span class="number">.3</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.3</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backpropagation</span></span><br><span class="line">    y_hat[range(len(y_hat)), y_train] -= <span class="number">1</span></span><br><span class="line">    y_hat /= len(y_train)</span><br><span class="line">    dW = np.dot(X_train.T, y_hat)</span><br><span class="line">    db = np.sum(y_hat, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    W += -LEARNING_RATE * dW</span><br><span class="line">    b += -LEARNING_RATE * db</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0, loss: 0.694, accuracy: 0.093</span></span><br><span class="line"><span class="comment"># Epoch: 10, loss: 0.451, accuracy: 0.973</span></span><br><span class="line"><span class="comment"># Epoch: 20, loss: 0.353, accuracy: 0.973</span></span><br><span class="line"><span class="comment"># Epoch: 30, loss: 0.299, accuracy: 0.973</span></span><br><span class="line"><span class="comment"># Epoch: 40, loss: 0.264, accuracy: 0.976</span></span><br></pre></td></tr></table></figure><h2 id="Evaluation">Evaluation</h2><p>在测试集上评估我们训练好的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegressionFromScratch</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        logits = np.dot(x, W) + b</span><br><span class="line">        exp_logits = np.exp(logits)</span><br><span class="line">        y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> y_hat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluation</span></span><br><span class="line">model = LogisticRegressionFromScratch()</span><br><span class="line">logits_train = model.predict(X_train)</span><br><span class="line">pred_train = np.argmax(logits_train, axis=<span class="number">1</span>)</span><br><span class="line">logits_test = model.predict(X_test)</span><br><span class="line">pred_test = np.argmax(logits_test, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Training and test accuracy</span></span><br><span class="line">train_acc =  np.mean(np.equal(y_train, pred_train))</span><br><span class="line">test_acc = np.mean(np.equal(y_test, pred_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"train acc: <span class="subst">&#123;train_acc:<span class="number">.2</span>f&#125;</span>, test acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train acc: 0.98, test acc: 0.97</span></span><br></pre></td></tr></table></figure><p>可视化我们的结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiclass_decision_boundary</span><span class="params">(model, X, y, savefig_fp=None)</span>:</span></span><br><span class="line">    <span class="string">"""Plot the multiclass decision boundary for a model that accepts 2D inputs.</span></span><br><span class="line"><span class="string">    Credit: https://cs231n.github.io/neural-networks-case-study/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        model &#123;function&#125; -- trained model with function model.predict(x_in).</span></span><br><span class="line"><span class="string">        X &#123;numpy.ndarray&#125; -- 2D inputs with shape (N, 2).</span></span><br><span class="line"><span class="string">        y &#123;numpy.ndarray&#125; -- 1D outputs with shape (N,).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Axis boundaries</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].max() + <span class="number">0.1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].max() + <span class="number">0.1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.linspace(x_min, x_max, <span class="number">101</span>),</span><br><span class="line">                         np.linspace(y_min, y_max, <span class="number">101</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create predictions</span></span><br><span class="line">    x_in = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    y_pred = model.predict(x_in)</span><br><span class="line">    y_pred = np.argmax(y_pred, axis=<span class="number">1</span>).reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot decision boundary</span></span><br><span class="line">    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.RdYlBu)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot</span></span><br><span class="line">    <span class="keyword">if</span> savefig_fp:</span><br><span class="line">        plt.savefig(savefig_fp, format=<span class="string">"png"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/d6ba4a09a521c4639bf58e05962a6ebe.png" alt=""></p><h2 id="Ending">Ending</h2><p>可以看出，使用NumPy实现的代码相对复杂，下一篇我们即将看到PyTorch是如何便捷的实现逻辑回归。</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Logistic regression - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      使用NumPy实现逻辑回归。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · PyTorch的CrossEntropyLoss实现的不对？</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-CrossEntropyLoss/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-CrossEntropyLoss/</id>
    <published>2023-05-26T03:25:41.000Z</published>
    <updated>2023-05-27T13:56:31.413Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>作者在学习机器学习之逻辑回归任务时，遇到的交叉熵计算不符合预期，才发现了PyTorch的别有洞天。</p><p>于是，本文便是结合实验交代了PyTorch中交叉熵损失的真实计算过程。</p><h2 id="数据准备">数据准备</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.random.seed(seed=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">X = np.random.dirichlet(np.ones(<span class="number">3</span>), size=<span class="number">3</span>)</span><br><span class="line">Y = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (X)</span><br><span class="line"><span class="keyword">print</span> (Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[0.13807842 0.76510708 0.09681451]</span></span><br><span class="line"><span class="comment">#  [0.31698662 0.26993158 0.4130818 ]</span></span><br><span class="line"><span class="comment">#  [0.46864621 0.01320047 0.51815332]]</span></span><br><span class="line"><span class="comment"># [0 1 2]</span></span><br></pre></td></tr></table></figure><h2 id="正文">正文</h2><p>大部分博客给出的公式如下：</p><p>$$<br>H = - \sum_i{y_i log(\hat{y}_i)}<br>$$</p><p>其中 $\hat{y}_i$ 为预测值，$y_i$ 为真实值。</p><p>我们在低维空间复现此公式，注意到PyTorch可以采用<a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html" target="_blank" rel="noopener" title="CROSS_ENTROPY">class indices</a>直接取下标进行计算，这里采用同样的方式模拟。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    h = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Y)):</span><br><span class="line">        _h = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> X[i]:</span><br><span class="line">            _h += np.log(X[i][Y[i]])</span><br><span class="line">        h += - _h</span><br><span class="line">    <span class="keyword">return</span> np.around(h / len(Y), <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cross_entropy_(X, Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 3.947</span></span><br></pre></td></tr></table></figure><p>我们看一下PyTorch的计算结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">entroy = nn.CrossEntropyLoss()</span><br><span class="line">entroy(torch.from_numpy(X), torch.from_numpy(Y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor(1.1484, dtype=torch.float64)</span></span><br></pre></td></tr></table></figure><p>可以看到，结果并不相同。所以PyTorch应该是采用了另外的<a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" target="_blank" rel="noopener" title="CROSSENTROPYLOSS">实现方式</a>，而这也是大部分教程没有交代的。</p><p>$$<br>H(x, class) = - log{\frac{e^{x_{class}}}{\sum_i{e^{x_i}}}} = - x_{class} + log{\sum_i{e^{x_i}}}<br>$$</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    h = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Y)):</span><br><span class="line">        _h = sum([np.exp(j) <span class="keyword">for</span> j <span class="keyword">in</span> X[i]])</span><br><span class="line">        h += (- X[i][Y[i]] + np.log(_h))</span><br><span class="line">    <span class="keyword">return</span> np.around(h / len(Y), <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">cross_entropy(X, Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 1.1484</span></span><br></pre></td></tr></table></figure><p>如此，可以看到PyTorch的CrossEntropyLoss的真正计算过程。</p><h2 id="More">More</h2><p>事实上，我们还可以发现，nn.CrossEntropyLoss() 其实是 nn.logSoftmax() 和 nn.NLLLoss() 的整合版本。<br>$$<br>logSoftmax = log{\frac{e^x}{\sum_i{e^{x_i}}}}<br>$$</p><p>$$<br>NLLLoss(x, class) = -x[class]<br>$$</p><p>验证代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">X = np.random.dirichlet(np.ones(<span class="number">3</span>), size=<span class="number">3</span>)</span><br><span class="line">Y = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">X_ = torch.from_numpy(X)</span><br><span class="line">Y_ = torch.from_numpy(Y)</span><br><span class="line"></span><br><span class="line">entroy = nn.CrossEntropyLoss()</span><br><span class="line">print(entroy(X_, Y_))</span><br><span class="line"></span><br><span class="line">softmax = nn.LogSoftmax()</span><br><span class="line">loss = nn.NLLLoss()</span><br><span class="line">print(loss(softmax(X_), Y_))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor(1.0033, dtype=torch.float64)</span></span><br><span class="line"><span class="comment"># tensor(1.0033, dtype=torch.float64)</span></span><br></pre></td></tr></table></figure><h2 id="结论">结论</h2><ol><li><p>nn.CrossEntropyLoss() 的计算公式如下：<br>$$<br>H(x, class) = - log{\frac{e^{x_{class}}}{\sum_i{e^{x_i}}}} = - x_{class} + log{\sum_i{e^{x_i}}}<br>$$</p></li><li><p>nn.CrossEntropyLoss() 是 nn.logSoftmax() 和 nn.NLLLoss() 的整合。</p></li></ol>]]></content>
    
    <summary type="html">
    
      学习需要脚踏实地。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 机器学习之Linear Regression (二)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-LinearRegression-2/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-LinearRegression-2/</id>
    <published>2023-05-21T10:37:38.000Z</published>
    <updated>2023-05-29T06:08:40.370Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>接上篇，本文的目标是使用PyTorch实现一个线性回归模型。</p><h2 id="Generate-data">Generate data</h2><p>我们复用上篇生成的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line">NUM_SAMPLES = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate synthetic data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_data</span><span class="params">(num_samples)</span>:</span></span><br><span class="line">    <span class="string">"""Generate dummy data for linear regression."""</span></span><br><span class="line">    X = np.array(range(num_samples))</span><br><span class="line">    random_noise = np.random.uniform(<span class="number">-10</span>, <span class="number">20</span>, size=num_samples)</span><br><span class="line">    y = <span class="number">3.5</span>*X + random_noise <span class="comment"># add some noise</span></span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate random (linear) data</span></span><br><span class="line">X, y = generate_data(num_samples=NUM_SAMPLES)</span><br><span class="line">data = np.vstack([X, y]).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load into a Pandas DataFrame</span></span><br><span class="line">df = pd.DataFrame(data, columns=[<span class="string">"X"</span>, <span class="string">"y"</span>])</span><br><span class="line">X = df[[<span class="string">"X"</span>]].values</span><br><span class="line">y = df[[<span class="string">"y"</span>]].values</span><br><span class="line"></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/44b3f9578128ebd663285e3dd1ac4120.png" alt=""></p><p>我们将数据绘制成散点图，可以看到它们有很明显的线性趋势。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scatter plot</span></span><br><span class="line">plt.title(<span class="string">"Generated data"</span>)</span><br><span class="line">plt.scatter(x=df[<span class="string">"X"</span>], y=df[<span class="string">"y"</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/48c4cc75b277978a0b5e3eadf9e252e7.png" alt=""></p><h2 id="Split-data">Split data</h2><p>区别于上一篇中我们使用自定义摇骰子的方式分割数据，这里选择使用<a href="https://scikit-learn.org/stable/" target="_blank" rel="noopener" title="scikit-learn">scikit-learn</a>包里提供的<code>train_test_split</code>方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Split (train)</span></span><br><span class="line">X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"train: <span class="subst">&#123;len(X_train)&#125;</span> (<span class="subst">&#123;(len(X_train) / len(X)):<span class="number">.2</span>f&#125;</span>)\n"</span></span><br><span class="line">       <span class="string">f"remaining: <span class="subst">&#123;len(X_)&#125;</span> (<span class="subst">&#123;(len(X_) / len(X)):<span class="number">.2</span>f&#125;</span>)"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train: 35 (0.70)</span></span><br><span class="line"><span class="comment"># remaining: 15 (0.30)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Split (test)</span></span><br><span class="line">X_val, X_test, y_val, y_test = train_test_split(</span><br><span class="line">    X_, y_, train_size=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"train: <span class="subst">&#123;len(X_train)&#125;</span> (<span class="subst">&#123;len(X_train)/len(X):<span class="number">.2</span>f&#125;</span>)\n"</span></span><br><span class="line">  <span class="string">f"val: <span class="subst">&#123;len(X_val)&#125;</span> (<span class="subst">&#123;len(X_val)/len(X):<span class="number">.2</span>f&#125;</span>)\n"</span></span><br><span class="line">  <span class="string">f"test: <span class="subst">&#123;len(X_test)&#125;</span> (<span class="subst">&#123;len(X_test)/len(X):<span class="number">.2</span>f&#125;</span>)"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train: 35 (0.70)</span></span><br><span class="line"><span class="comment"># val: 7 (0.14)</span></span><br><span class="line"><span class="comment"># test: 8 (0.16)</span></span><br></pre></td></tr></table></figure><h2 id="Standardize-data">Standardize data</h2><p>同样的，我们需要对数据进行标准化处理。这里使用<code>scikit-learn</code>里提供的<code>StandardScaler</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the data (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler().fit(X_train)</span><br><span class="line">y_scaler = StandardScaler().fit(y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply scaler on training and test data</span></span><br><span class="line">X_train = X_scaler.transform(X_train)</span><br><span class="line">y_train = y_scaler.transform(y_train).ravel().reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">X_val = X_scaler.transform(X_val)</span><br><span class="line">y_val = y_scaler.transform(y_val).ravel().reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">X_test = X_scaler.transform(X_test)</span><br><span class="line">y_test = y_scaler.transform(y_test).ravel().reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;np.mean(X_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;np.mean(y_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(y_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># mean: 0.4, std: 1.0</span></span><br><span class="line"><span class="comment"># mean: 0.3, std: 0.9</span></span><br></pre></td></tr></table></figure><h2 id="Weights">Weights</h2><p>我们将使用PyTorch的<code>Linear layers</code>来实现一个没有隐含层的神经网络。<br>关于神经网络我们后面会具体学习。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">torch.manual_seed(SEED)</span><br><span class="line"></span><br><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>] <span class="comment"># X is 1-dimensional</span></span><br><span class="line">OUTPUT_DIM = y_train.shape[<span class="number">1</span>] <span class="comment"># y is 1-dimensional</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Inputs</span></span><br><span class="line">N = <span class="number">3</span> <span class="comment"># num samples</span></span><br><span class="line">x = torch.randn(N, INPUT_DIM)</span><br><span class="line"><span class="keyword">print</span> (x.shape)</span><br><span class="line"><span class="keyword">print</span> (x.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"><span class="comment"># [[-1.4836688 ]</span></span><br><span class="line"><span class="comment">#  [ 0.26714355]</span></span><br><span class="line"><span class="comment">#  [-1.8336787 ]]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Weights</span></span><br><span class="line">m = nn.Linear(INPUT_DIM, OUTPUT_DIM)</span><br><span class="line"><span class="keyword">print</span> (m)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"weights (<span class="subst">&#123;m.weight.shape&#125;</span>): <span class="subst">&#123;m.weight[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"bias (<span class="subst">&#123;m.bias.shape&#125;</span>): <span class="subst">&#123;m.bias[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Linear(in_features=1, out_features=1, bias=True)</span></span><br><span class="line"><span class="comment"># weights (torch.Size([1, 1])): -0.2795013189315796</span></span><br><span class="line"><span class="comment"># bias (torch.Size([1])): -0.7643394470214844</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Forward pass</span></span><br><span class="line">z = m(x)</span><br><span class="line"><span class="keyword">print</span> (z.shape)</span><br><span class="line"><span class="keyword">print</span> (z.detach().numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"><span class="comment"># [[-0.34965205]</span></span><br><span class="line"><span class="comment">#  [-0.8390064 ]</span></span><br><span class="line"><span class="comment">#  [-0.25182384]]</span></span><br></pre></td></tr></table></figure><h2 id="Model">Model</h2><p>$$<br>\hat{y} = WX + b<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, output_dim)</span>:</span></span><br><span class="line">        super(LinearRegression, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        y_pred = self.fc1(x_in)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = LinearRegression(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of LinearRegression(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=1, out_features=1, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Loss">Loss</h2><p>同样的，我们使用PyTorch自带的<a href="">Loss Functions</a>, 这里指定<code>MSELoss</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line">y_pred = torch.Tensor([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line">y_true =  torch.Tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>])</span><br><span class="line">loss = loss_fn(y_pred, y_true)</span><br><span class="line">print(<span class="string">"Loss: "</span>, loss.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Loss:  0.75</span></span><br></pre></td></tr></table></figure><h2 id="Optimizer">Optimizer</h2><p>上一篇中我们介绍了使用梯度下降的方法来更新我们的权重。PyTorch中有很多不同的权重更新方法，需要根据不同的场景来选择合适的。详见<a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener" title="TORCH.OPTIM">TORCH.OPTIM</a>。<br>这里我们采用适合大多数场景的<code>ADAM optimizer</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.Tensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.Tensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.Tensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line">NUM_EPOCHS = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    <span class="comment"># https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 1.11</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.10</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.04</span></span><br><span class="line"><span class="comment"># Epoch: 60 | loss: 0.03</span></span><br><span class="line"><span class="comment"># Epoch: 80 | loss: 0.03</span></span><br></pre></td></tr></table></figure><h2 id="Evaluation">Evaluation</h2><p>现在我们准备评估我们训练好的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">pred_train = model(X_train)</span><br><span class="line">pred_test = model(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">train_error = loss_fn(pred_train, y_train)</span><br><span class="line">test_error = loss_fn(pred_test, y_test)</span><br><span class="line">print(<span class="string">f"train_error: <span class="subst">&#123;train_error:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"test_error: <span class="subst">&#123;test_error:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train_error: 0.03</span></span><br><span class="line"><span class="comment"># test_error: 0.04</span></span><br></pre></td></tr></table></figure><p>由于我们只有一个特征，因此可以轻松地对模型进行可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Figure size</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot train data</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plt.scatter(X_train, y_train, label=<span class="string">"y_train"</span>)</span><br><span class="line">plt.plot(X_train, pred_train.detach().numpy(), color=<span class="string">"red"</span>, linewidth=<span class="number">1</span>, linestyle=<span class="string">"-"</span>, label=<span class="string">"model"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot test data</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plt.scatter(X_test, y_test, label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.plot(X_test, pred_test.detach().numpy(), color=<span class="string">"red"</span>, linewidth=<span class="number">1</span>, linestyle=<span class="string">"-"</span>, label=<span class="string">"model"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show plots</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/332563148a19dae3df2c54de08dafccd.png" alt=""></p><h2 id="Inference">Inference</h2><p>训练完模型后，我们可以使用它来对新数据进行预测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Feed in your own inputs</span></span><br><span class="line">sample_indices = [<span class="number">10</span>, <span class="number">15</span>, <span class="number">25</span>]</span><br><span class="line">X_infer = np.array(sample_indices, dtype=np.float32)</span><br><span class="line">X_infer = torch.Tensor(X_scaler.transform(X_infer.reshape(<span class="number">-1</span>, <span class="number">1</span>)))</span><br></pre></td></tr></table></figure><p>由于我们对数据都进行了标准化，所以对预测值需要进行逆操作。<br>$$<br>\hat{y}_{scaled} = \frac{\hat{y} - \mu_{\hat{y}}}{\sigma_{\hat{y}}}<br>$$</p><p>$$<br>\hat{y} = \hat{y}_{scaled} * \sigma_{\hat{y}} + \mu_{\hat{y}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Unstandardize predictions</span></span><br><span class="line">pred_infer = model(X_infer).detach().numpy() * np.sqrt(y_scaler.var_) + y_scaler.mean_</span><br><span class="line"><span class="keyword">for</span> i, index <span class="keyword">in</span> enumerate(sample_indices):</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">f"<span class="subst">&#123;df.iloc[index][<span class="string">'y'</span>]:<span class="number">.2</span>f&#125;</span> (actual) → <span class="subst">&#123;pred_infer[i][<span class="number">0</span>]:<span class="number">.2</span>f&#125;</span> (predicted)"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 28.10 (actual) → 40.99 (predicted)</span></span><br><span class="line"><span class="comment"># 56.45 (actual) → 58.62 (predicted)</span></span><br><span class="line"><span class="comment"># 100.83 (actual) → 93.88 (predicted)</span></span><br></pre></td></tr></table></figure><h2 id="Interpretability">Interpretability</h2><p>线性回归具有高度可解释性的巨大优势。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unstandardize coefficients</span></span><br><span class="line">W = model.fc1.weight.data.numpy()[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">b = model.fc1.bias.data.numpy()[<span class="number">0</span>]</span><br><span class="line">W_unscaled = W * (y_scaler.scale_/X_scaler.scale_)</span><br><span class="line">b_unscaled = b * y_scaler.scale_ + y_scaler.mean_ - np.sum(W_unscaled*X_scaler.mean_)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"[actual] y = 3.5X + noise"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"[model] y_hat = <span class="subst">&#123;W_unscaled[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>X + <span class="subst">&#123;b_unscaled[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [actual] y = 3.5X + noise</span></span><br><span class="line"><span class="comment"># [model] y_hat = 3.5X + 5.7</span></span><br></pre></td></tr></table></figure><h2 id="Regularization">Regularization</h2><p>正则化有助于减少过拟合。本例使用<code>L2正则化</code> (岭回归)。</p><p>通过L2正则化，我们对大的权重值进行惩罚，鼓励权重是较小值。 还有其他类型的正则化，比如L1（套索回归），它可以用于创建稀疏模型，其中一些特征系数被清零，或者结合了L1和L2惩罚的弹性正则化。</p><p>正则化不仅适用于线性回归，您可以使用它来处理任何模型的权重，包括我们将在未来学习到的模型。</p><p>$$<br>J(\theta) = \frac{1}{2} \sum_{i}(WX_i - y_i)^2 + \frac{\lambda}{2} \sum_i{W_i}^2<br>$$</p><p>$$<br>\frac{\partial(J)}{\partial(W)} = (\hat{y} - y)X + \lambda{W}<br>$$</p><p>$$<br>W = W - \alpha{\frac{\partial{J}}{\partial{W}}}<br>$$</p><p>$\lambda$: 正则化系数; $\alpha$: 学习率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">L2_LAMBDA = <span class="number">1e-2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = LinearRegression(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer (w/ L2 regularization)</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 0.67</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.06</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.03</span></span><br><span class="line"><span class="comment"># Epoch: 60 | loss: 0.03</span></span><br><span class="line"><span class="comment"># Epoch: 80 | loss: 0.03</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">pred_train = model(X_train)</span><br><span class="line">pred_test = model(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">train_error = loss_fn(pred_train, y_train)</span><br><span class="line">test_error = loss_fn(pred_test, y_test)</span><br><span class="line">print(<span class="string">f"train_error: <span class="subst">&#123;train_error:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"test_error: <span class="subst">&#123;test_error:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line">train_error: <span class="number">0.03</span></span><br><span class="line">test_error: <span class="number">0.03</span></span><br></pre></td></tr></table></figure><p>对于这个特定的例子，正则化并没有在性能上产生差异，因为我们的数据是从一个完美的线性方程生成的。但是对于大规模真实数据，正则化可以帮助我们的模型很好地泛化。</p><h2 id="Ending">Ending</h2><p>本篇基于上一篇的基础，简单介绍了如何用PyTorch实现线性回归。</p><p>Peace out.</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Linear regression - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      使用PyTorch实现线性回归。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 机器学习之Linear Regression (一)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-LinearRegression-1/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-LinearRegression-1/</id>
    <published>2023-05-18T10:31:52.000Z</published>
    <updated>2023-05-28T07:34:03.591Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文的目标是使用NumPy实现一个线性回归模型 $\hat{y}$ ，该模型通过最小化预测值和真实值之间的距离来拟合出一条最佳的直线(或一个平面)。<br>我们使用标记数据 $(X, y)$ 来训练模型，利用梯度下降的方法来学习权重 $W$ 和 偏差 $b$。</p><p>$$<br>\hat{y} = WX + b<br>$$</p><h2 id="Generate-data">Generate data</h2><p>我们会生成一些简单的数据，数据大致符合线性分布，但加上一点随机噪声以模拟现实情况( $y = 3.5 * X + 噪声$)，意味着这些数据并不完全在一条直线上。</p><p>我们的目标是使模型收敛到类似的线性方程上（由于随机噪声的加入，最终的模型结果可能会有差异）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line">NUM_SAMPLES = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate synthetic data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_data</span><span class="params">(num_samples)</span>:</span></span><br><span class="line">    <span class="string">"""Generate dummy data for linear regression."""</span></span><br><span class="line">    X = np.array(range(num_samples))</span><br><span class="line">    random_noise = np.random.uniform(<span class="number">-10</span>, <span class="number">20</span>, size=num_samples)</span><br><span class="line">    y = <span class="number">3.5</span>*X + random_noise <span class="comment"># add some noise</span></span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate random (linear) data</span></span><br><span class="line">X, y = generate_data(num_samples=NUM_SAMPLES)</span><br><span class="line">data = np.vstack([X, y]).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load into a Pandas DataFrame</span></span><br><span class="line">df = pd.DataFrame(data, columns=[<span class="string">"X"</span>, <span class="string">"y"</span>])</span><br><span class="line">X = df[[<span class="string">"X"</span>]].values</span><br><span class="line">y = df[[<span class="string">"y"</span>]].values</span><br><span class="line"></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/44b3f9578128ebd663285e3dd1ac4120.png" alt=""></p><p>我们将数据绘制成散点图，可以看到它们有很明显的线性趋势。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scatter plot</span></span><br><span class="line">plt.title(<span class="string">"Generated data"</span>)</span><br><span class="line">plt.scatter(x=df[<span class="string">"X"</span>], y=df[<span class="string">"y"</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/48c4cc75b277978a0b5e3eadf9e252e7.png" alt=""></p><h2 id="Split-data">Split data</h2><p>现在我们已经准备好了数据，接下来需要随机将数据集分成三个部分：训练集、验证集和测试集。</p><ul><li>训练集：用来训练我们的模型</li><li>验证集：在训练过程中用来检验我们模型的性能</li><li>测试集：用来检测我们最终训练好的模型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Shuffle data</span></span><br><span class="line"><span class="comment"># 不要错误的将 X 和 y 分开shuffle，务必保持自变量和因变量始终对齐</span></span><br><span class="line">indices = list(range(NUM_SAMPLES))</span><br><span class="line">np.random.shuffle(indices)</span><br><span class="line">X = X[indices]</span><br><span class="line">y = y[indices]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split indices</span></span><br><span class="line">train_start = <span class="number">0</span></span><br><span class="line">train_end = int(<span class="number">0.7</span>*NUM_SAMPLES)</span><br><span class="line">val_start = train_end</span><br><span class="line">val_end = int((TRAIN_SIZE+VAL_SIZE)*NUM_SAMPLES)</span><br><span class="line">test_start = val_end</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data</span></span><br><span class="line">X_train = X[train_start:train_end]</span><br><span class="line">y_train = y[train_start:train_end]</span><br><span class="line">X_val = X[val_start:val_end]</span><br><span class="line">y_val = y[val_start:val_end]</span><br><span class="line">X_test = X[test_start:]</span><br><span class="line">y_test = y[test_start:]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_test: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (35, 1), y_train: (35, 1)</span></span><br><span class="line"><span class="comment"># X_val: (7, 1), y_test: (7, 1)</span></span><br><span class="line"><span class="comment"># X_test: (8, 1), y_test: (8, 1)</span></span><br></pre></td></tr></table></figure><h2 id="Standardize-data">Standardize data</h2><p>我们需要对数据进行标准化处理（零均值和单位方差），这样做的目的是使不同特征之间的值具有可比性，并且能够减少不同特征之间的差异性。</p><p>$$<br>z = \frac{x_i - \mu}{\sigma}<br>$$</p><p>$z$: 标准化后的值;  $x_i$: 第i个输入;  $\mu$: 平均值;  $\sigma$: 标准差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standardize_data</span><span class="params">(data, mean, std)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (data - mean) / std</span><br></pre></td></tr></table></figure><p>需要注意的是，我们将验证集和测试集视为隐藏数据集。因此，我们只使用训练集来确定均值和标准差，以避免偏向我们的训练过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Determine means and stds</span></span><br><span class="line">X_mean = np.mean(X_train)</span><br><span class="line">X_std = np.std(X_train)</span><br><span class="line">y_mean = np.mean(y_train)</span><br><span class="line">y_std = np.std(y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize</span></span><br><span class="line">X_train = standardize_data(X_train, X_mean, X_std)</span><br><span class="line">y_train = standardize_data(y_train, y_mean, y_std)</span><br><span class="line">X_val = standardize_data(X_val, X_mean, X_std)</span><br><span class="line">y_val = standardize_data(y_val, y_mean, y_std)</span><br><span class="line">X_test = standardize_data(X_test, X_mean, X_std)</span><br><span class="line">y_test = standardize_data(y_test, y_mean, y_std)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;np.mean(X_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;np.mean(y_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(y_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># mean: 0.3, std: 1.0</span></span><br><span class="line"><span class="comment"># mean: 0.3, std: 1.0</span></span><br></pre></td></tr></table></figure><h2 id="Weights">Weights</h2><p>我们的目标是学习一个模型 $\hat{y}$ ，用权重向量 $W\in\mathbb{R}^{d}$ 和截距 $b\in\mathbb{R}$ 来表达自变量 $X\in\mathbb{R}^{d}$ 与因变量 $y\in\mathbb{R}$ 之间的线性关系:<br>$$<br>\hat{y} = XW + b\<br>$$</p><p><strong>第一步</strong>: 随机初始化模型的权重 $W$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>] <span class="comment"># X is 1-dimensional</span></span><br><span class="line">OUTPUT_DIM = y_train.shape[<span class="number">1</span>] <span class="comment"># y is 1-dimensional</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random weights</span></span><br><span class="line">W = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, OUTPUT_DIM)</span><br><span class="line">b = np.zeros((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"W: <span class="subst">&#123;W.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"b: <span class="subst">&#123;b.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># W: (1, 1)</span></span><br><span class="line"><span class="comment"># b: (1, 1)</span></span><br></pre></td></tr></table></figure><h2 id="Model">Model</h2><p><strong>第二步</strong>: 给模型喂 $X$ 以获得预测结果 $\hat{y}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Forward pass [NX1] · [1X1] = [NX1]</span></span><br><span class="line">y_pred = np.dot(X_train, W) + b</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_pred: <span class="subst">&#123;y_pred.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_pred: (35, 1)</span></span><br></pre></td></tr></table></figure><h2 id="Loss">Loss</h2><p><strong>第三步</strong>: 通过比较预测值和实际目标值来确定损失 $J$ 。线性回归常见的损失函数是<code>均方误差(MSE)</code>。<br>$$<br>J(\theta) =  \frac{1}{N} \sum_i(y_i - \hat{y}_i)^2<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss</span></span><br><span class="line">N = len(y_train)</span><br><span class="line">loss = (<span class="number">1</span>/N) * np.sum((y_train - y_pred)**<span class="number">2</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># loss: 0.99</span></span><br></pre></td></tr></table></figure><h2 id="Gradients">Gradients</h2><p><strong>第四步</strong>: 计算损失函数 $J(\theta)$ 的梯度，并更新模型的权重<br>$$<br>\rightarrow \frac{\partial(J)}{\partial(W)} = -\frac{2}{N}\sum_i(y_i - \hat{y}_i)^2 * X_i<br>$$</p><p>$$<br>\rightarrow \frac{\partial(J)}{\partial(b)} = -\frac{2}{N}\sum_i(y_i - \hat{y}_i)^2 * 1<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Backpropagation</span></span><br><span class="line">dW = -(<span class="number">2</span>/N) * np.sum((y_train - y_pred) * X_train)</span><br><span class="line">db = -(<span class="number">2</span>/N) * np.sum((y_train - y_pred) * <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>梯度是函数的导数或变化率。它是一个向量，指向函数增长最快的方向。<br>在我们这个例子里面，损失函数 $J$ 在 $W$ 的梯度告诉我们如何改变 $W$ 以最大化 $J$ 。然而我们希望最小化损失，因此我们需要从 $W$ 中减去梯度。</p><h2 id="Update-weights">Update weights</h2><p><strong>第五步</strong>: 利用一个很小的学习率 $\alpha$ 更新权重<br>$$<br>W = W - \alpha \frac{\partial(J)}{\partial(W)}<br>$$</p><p>$$<br>b = b - \alpha \frac{\partial(J)}{\partial(b)}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Update weights</span></span><br><span class="line">W += -LEARNING_RATE * dW</span><br><span class="line">b += -LEARNING_RATE * db</span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><p><strong>第六步</strong>: 重复步骤 2 ~ 5，以最小化损失为目的来训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">NUM_EPOCHS = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random weights</span></span><br><span class="line">W = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, OUTPUT_DIM)</span><br><span class="line">b = np.zeros((<span class="number">1</span>, ))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training loop</span></span><br><span class="line"><span class="keyword">for</span> epoch_num <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass [NX1] · [1X1] = [NX1]</span></span><br><span class="line">    y_pred = np.dot(X_train, W) + b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = (<span class="number">1</span>/len(y_train)) * np.sum((y_train - y_pred)**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Show progress</span></span><br><span class="line">    <span class="keyword">if</span> epoch_num%<span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch_num&#125;</span>, loss: <span class="subst">&#123;loss:<span class="number">.3</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backpropagation</span></span><br><span class="line">    dW = -(<span class="number">2</span>/N) * np.sum((y_train - y_pred) * X_train)</span><br><span class="line">    db = -(<span class="number">2</span>/N) * np.sum((y_train - y_pred) * <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    W += -LEARNING_RATE * dW</span><br><span class="line">    b += -LEARNING_RATE * db</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0, loss: 0.992</span></span><br><span class="line"><span class="comment"># Epoch: 10, loss: 0.043</span></span><br><span class="line"><span class="comment"># Epoch: 20, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 30, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 40, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 50, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 60, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 70, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 80, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 90, loss: 0.032</span></span><br></pre></td></tr></table></figure><p>为了保持简洁，我们在这里没有计算和显示每个Epoch后的验证集损失。在后续的学习会体现出，在验证集上的表现对训练进程有着至关重要的影响（学习率是否合理、什么时候停止训练等等）</p><h2 id="Evaluation">Evaluation</h2><p>接下来看一下我们训练好的模型在测试数据集上的表现如何。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">pred_train = W*X_train + b</span><br><span class="line">pred_test = W*X_test + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train and test MSE</span></span><br><span class="line">train_mse = np.mean((y_train - pred_train) ** <span class="number">2</span>)</span><br><span class="line">test_mse = np.mean((y_test - pred_test) ** <span class="number">2</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"train_MSE: <span class="subst">&#123;train_mse:<span class="number">.2</span>f&#125;</span>, test_MSE: <span class="subst">&#123;test_mse:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train_MSE: 0.03, test_MSE: 0.04</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Figure size</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot train data</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plt.scatter(X_train, y_train, label=<span class="string">"y_train"</span>)</span><br><span class="line">plt.plot(X_train, pred_train, color=<span class="string">"red"</span>, linewidth=<span class="number">1</span>, linestyle=<span class="string">"-"</span>, label=<span class="string">"model"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot test data</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plt.scatter(X_test, y_test, label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.plot(X_test, pred_test, color=<span class="string">"red"</span>, linewidth=<span class="number">1</span>, linestyle=<span class="string">"-"</span>, label=<span class="string">"model"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show plots</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/be75da05ba1c2d284e29c1e95319c6f9.png" alt=""></p><h2 id="Interpretability">Interpretability</h2><p>由于我们对输入和输出进行了标准化，因此我们的权重适配了这些标准化值。因此，我们需要将权重还原为非标准化状态，以便与真实权重（3.5）进行比较。<br>注意到 $X$ 和 $\hat{y}$ 都已经标准化过。</p><p>$$<br>\hat{y}_{scaled} = \sum_{j=1}^k W_{scaled(j)} x_{scaled(j)} + b_{scaled}<br>$$</p><p>已知<br>$$<br>\hat{y}_{scaled} = \frac{\hat{y}_{unscaled} - \overline{y}}{\sigma}<br>$$</p><p>$$<br>\hat{x}_{scaled} = \frac{x_{j} - \overline{x}_{j}}{\sigma_{j}}<br>$$</p><p>于是<br>$$<br>\frac{\hat{y}_{unscaled} - \overline{y}}{\sigma} = \sum_{j=1}^k W_{scaled(j)} (\frac{x_{j} - \overline{x}_{j}}{\sigma_{j}}) + b_{scaled}<br>$$</p><p>进一步可以得到</p><p>$$<br>\hat{y}_{unscaled} = \sum_{j=1}^k W_{scaled(j)} (\frac{\sigma_{y}}{\sigma_{j}}) x_{j} - \sum_{j=1}^k W_{scaled(j)} (\frac{\sigma_{y}}{\sigma_{j}}) \overline{x}_{j} + b_{scaled} \sigma_y + \overline{y}<br>$$</p><p>对比公式</p><p>$$<br>\hat{y}_{unscaled} = W_{unscaled} x + b_{unscaled}<br>$$</p><p>便可得知<br>$$<br>W_{unscaled} = W_{scaled} (\frac{\sigma_{y}}{\sigma_{j}})<br>$$</p><p>$$<br>b_{unscaled} = - \sum_{j=1}^k W_{unscaled(j)} \overline{x}_{j} + b_{scaled} \sigma_y + \overline{y}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unscaled weights</span></span><br><span class="line">W_unscaled = W * (y_std / X_std)</span><br><span class="line">b_unscaled = - np.sum(W_unscaled * X_mean) + b * y_std + y_mean</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"[actual] y = 3.5X + noise"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"[model] y_hat = <span class="subst">&#123;W_unscaled[<span class="number">0</span>][<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>X + <span class="subst">&#123;b_unscaled[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [actual] y = 3.5X + noise</span></span><br><span class="line"><span class="comment"># [model] y_hat = 3.4X + 7.9</span></span><br></pre></td></tr></table></figure><p>从上面的结果可以看出，我们成功的拟合出了这个线性方程。</p><h2 id="Ending">Ending</h2><p>该模型的优势：计算简单，高度可解释, 可以处理连续的和可分类的特征.  而缺点也很明显：只有当数据是线性可分的时候，该模型才能表现良好。</p><p>除了回归任务，你也可以将线性回归用于二元分类任务，其中如果预测的连续值高于阈值，则属于某个类别。 不过未来，我们会介绍更好的分类技术。</p><p>下一篇，我们将介绍如何用PyTorch实现本篇的内容。</p><p>Peace out.</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Linear regression - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      使用NumPy实现线性回归。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · PyTorch</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-PyTorch/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-PyTorch/</id>
    <published>2023-05-17T10:11:12.000Z</published>
    <updated>2023-05-27T13:56:17.775Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文简单介绍了PyTorch这个机器学习框架的必备知识。</p><h2 id="Set-up">Set up</h2><p>首先，我们将导入NumPy和PyTorch库，并设置随机种子以实现可重复性。<br>请注意，PyTorch 也需要一个种子，因为我们将生成随机张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">np.random.seed(seed=SEED)</span><br><span class="line">torch.manual_seed(SEED)</span><br></pre></td></tr></table></figure><h2 id="Basic">Basic</h2><p>下面一些 PyTorch 的基础知识，例如如何创建张量以及将常见的数据结构（列表、数组等）转换为张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating a random tensor</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># normal distribution (rand(2,3) -&gt; uniform distribution)</span></span><br><span class="line">print(<span class="string">f"Type: <span class="subst">&#123;x.type()&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Type: torch.FloatTensor</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-1.4837,  0.2671, -1.8337],</span></span><br><span class="line"><span class="comment">#         [-0.1047,  0.6002, -0.5496]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Zero and Ones tensor</span></span><br><span class="line">x = torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.]])</span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># List → Tensor</span></span><br><span class="line">x = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[1., 2., 3.],</span></span><br><span class="line"><span class="comment">#         [4., 5., 6.]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># NumPy array → Tensor</span></span><br><span class="line">x = torch.Tensor(np.random.rand(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[0.4445, 0.3168, 0.9231],</span></span><br><span class="line"><span class="comment">#         [0.4659, 0.7984, 0.1992]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Changing tensor type</span></span><br><span class="line">x = torch.Tensor(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(<span class="string">f"Type: <span class="subst">&#123;x.type()&#125;</span>"</span>)</span><br><span class="line">x = x.long()</span><br><span class="line">print(<span class="string">f"Type: <span class="subst">&#123;x.type()&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Type: torch.FloatTensor</span></span><br><span class="line"><span class="comment"># Type: torch.LongTensor</span></span><br></pre></td></tr></table></figure><h2 id="Operations">Operations</h2><p>下面探索一些张量的基本操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Addition</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">z = x + y</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;z.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;z&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-0.4446,  0.4933, -1.4847],</span></span><br><span class="line"><span class="comment">#         [ 0.8493,  0.6911, -0.3357]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dot product</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">z = torch.mm(x, y)</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;z.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;z&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 2])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[ 0.2733, -4.0392],</span></span><br><span class="line"><span class="comment">#         [ 1.6385, -4.7220]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Transpose</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line">y = torch.t(x)</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;y.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;y&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[ 0.5920, -0.6301, -0.8856],</span></span><br><span class="line"><span class="comment">#         [ 1.2261, -0.4671, -1.0279]])</span></span><br><span class="line"><span class="comment"># Size: torch.Size([3, 2])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[ 0.5920,  1.2261],</span></span><br><span class="line"><span class="comment">#         [-0.6301, -0.4671],</span></span><br><span class="line"><span class="comment">#         [-0.8856, -1.0279]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reshape</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">z = x.view(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;z.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;z&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([3, 2])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-1.0387,  0.1039],</span></span><br><span class="line"><span class="comment">#         [ 0.5989, -1.4801],</span></span><br><span class="line"><span class="comment">#         [-0.8618, -0.9181]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dangers of reshaping (unintended consequences)</span></span><br><span class="line">x = torch.tensor([</span><br><span class="line">    [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]],</span><br><span class="line">    [[<span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>], [<span class="number">20</span>, <span class="number">20</span>, <span class="number">20</span>, <span class="number">20</span>], [<span class="number">30</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">30</span>]]</span><br><span class="line">])</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"x: \n<span class="subst">&#123;x&#125;</span>\n"</span>)</span><br><span class="line"></span><br><span class="line">a = x.view(x.size(<span class="number">1</span>), <span class="number">-1</span>)</span><br><span class="line">print(<span class="string">f"\nSize: <span class="subst">&#123;a.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"a: \n<span class="subst">&#123;a&#125;</span>\n"</span>)</span><br><span class="line"></span><br><span class="line">b = x.transpose(<span class="number">0</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">print(<span class="string">f"\nSize: <span class="subst">&#123;b.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"b: \n<span class="subst">&#123;b&#125;</span>\n"</span>)</span><br><span class="line"></span><br><span class="line">c = b.view(b.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">print(<span class="string">f"\nSize: <span class="subst">&#123;c.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"c: \n<span class="subst">&#123;c&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3, 4])</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment"># tensor([[[ 1,  1,  1,  1],</span></span><br><span class="line"><span class="comment">#          [ 2,  2,  2,  2],</span></span><br><span class="line"><span class="comment">#          [ 3,  3,  3,  3]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[10, 10, 10, 10],</span></span><br><span class="line"><span class="comment">#          [20, 20, 20, 20],</span></span><br><span class="line"><span class="comment">#          [30, 30, 30, 30]]])</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Size: torch.Size([3, 8])</span></span><br><span class="line"><span class="comment"># a:</span></span><br><span class="line"><span class="comment"># tensor([[ 1,  1,  1,  1,  2,  2,  2,  2],</span></span><br><span class="line"><span class="comment">#         [ 3,  3,  3,  3, 10, 10, 10, 10],</span></span><br><span class="line"><span class="comment">#         [20, 20, 20, 20, 30, 30, 30, 30]])</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Size: torch.Size([3, 2, 4])</span></span><br><span class="line"><span class="comment"># b:</span></span><br><span class="line"><span class="comment"># tensor([[[ 1,  1,  1,  1],</span></span><br><span class="line"><span class="comment">#          [10, 10, 10, 10]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[ 2,  2,  2,  2],</span></span><br><span class="line"><span class="comment">#          [20, 20, 20, 20]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[ 3,  3,  3,  3],</span></span><br><span class="line"><span class="comment">#          [30, 30, 30, 30]]])</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Size: torch.Size([3, 8])</span></span><br><span class="line"><span class="comment"># c:</span></span><br><span class="line"><span class="comment"># tensor([[ 1,  1,  1,  1, 10, 10, 10, 10],</span></span><br><span class="line"><span class="comment">#         [ 2,  2,  2,  2, 20, 20, 20, 20],</span></span><br><span class="line"><span class="comment">#         [ 3,  3,  3,  3, 30, 30, 30, 30]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dimensional operations</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line">y = torch.sum(x, dim=<span class="number">0</span>) <span class="comment"># add each row's value for every column</span></span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;y&#125;</span>"</span>)</span><br><span class="line">z = torch.sum(x, dim=<span class="number">1</span>) <span class="comment"># add each columns's value for every row</span></span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;z&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-0.0355,  0.4145,  0.6798],</span></span><br><span class="line"><span class="comment">#         [-0.2936,  0.1872, -0.2724]])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([-0.3292,  0.6017,  0.4074])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([ 1.0588, -0.3788])</span></span><br></pre></td></tr></table></figure><h2 id="Indexing">Indexing</h2><p>可以使用索引从张量中提取指定的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"x: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"x[:1]: \n<span class="subst">&#123;x[:<span class="number">1</span>]&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"x[:1, 1:3]: \n<span class="subst">&#123;x[:<span class="number">1</span>, <span class="number">1</span>:<span class="number">3</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line">x:</span><br><span class="line"><span class="comment"># tensor([[-0.5524, -0.8358, -2.8240,  0.2564],</span></span><br><span class="line"><span class="comment">#         [ 0.5045, -1.1290,  0.7631,  1.0155],</span></span><br><span class="line"><span class="comment">#         [-1.2475, -0.0335,  0.5442,  0.4280]])</span></span><br><span class="line"><span class="comment"># x[:1]:</span></span><br><span class="line"><span class="comment"># tensor([[-0.5524, -0.8358, -2.8240,  0.2564]])</span></span><br><span class="line"><span class="comment"># x[:1, 1:3]:</span></span><br><span class="line"><span class="comment"># tensor([[-0.8358, -2.8240]])</span></span><br></pre></td></tr></table></figure><h2 id="Slicing">Slicing</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Select with dimensional indices</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">col_indices = torch.LongTensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">chosen = torch.index_select(x, dim=<span class="number">1</span>, index=col_indices) <span class="comment"># values from column 0 &amp; 2</span></span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;chosen&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">row_indices = torch.LongTensor([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">col_indices = torch.LongTensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">chosen = x[row_indices, col_indices] <span class="comment"># values from (0, 0) &amp; (1, 2)</span></span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;chosen&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-1.6357,  0.7964,  0.9450],</span></span><br><span class="line"><span class="comment">#         [-1.6535,  1.8129,  0.9162]])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-1.6357,  0.9450],</span></span><br><span class="line"><span class="comment">#         [-1.6535,  0.9162]])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([-1.6357,  0.9162])</span></span><br></pre></td></tr></table></figure><h2 id="Joining">Joining</h2><p>我们还可以使用concatenate和stack来合并张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor([[-0.7004,  0.8429,  0.8971],</span></span><br><span class="line"><span class="comment">#         [-0.0272,  0.4722,  1.1621]])</span></span><br><span class="line"><span class="comment"># torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Concatenation</span></span><br><span class="line">y = torch.cat([x, x], dim=<span class="number">0</span>) <span class="comment"># concat on a specified dimension</span></span><br><span class="line"><span class="keyword">print</span> (y)</span><br><span class="line"><span class="keyword">print</span> (y.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor([[-0.7004,  0.8429,  0.8971],</span></span><br><span class="line"><span class="comment">#         [-0.0272,  0.4722,  1.1621],</span></span><br><span class="line"><span class="comment">#         [-0.7004,  0.8429,  0.8971],</span></span><br><span class="line"><span class="comment">#         [-0.0272,  0.4722,  1.1621]])</span></span><br><span class="line"><span class="comment"># torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Stacking</span></span><br><span class="line">z = torch.stack([x, x], dim=<span class="number">0</span>) <span class="comment"># stack on new dimension</span></span><br><span class="line"><span class="keyword">print</span> (z)</span><br><span class="line"><span class="keyword">print</span> (z.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor([[[-0.7004,  0.8429,  0.8971],</span></span><br><span class="line"><span class="comment">#          [-0.0272,  0.4722,  1.1621]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[-0.7004,  0.8429,  0.8971],</span></span><br><span class="line"><span class="comment">#          [-0.0272,  0.4722,  1.1621]]])</span></span><br><span class="line"><span class="comment"># torch.Size([2, 2, 3])</span></span><br></pre></td></tr></table></figure><h2 id="Gradients-梯度">Gradients 梯度</h2><p>我们可以使用梯度追踪(gradient bookkeeping) 来计算张量相对于其组成部分的梯度（变化率）。</p><p>梯度是机器学习和深度学习中最重要的概念，没有之一。后续会进一步介绍，这里先简单示例PyTorch如何计算某个函数在某点处的梯度:</p><p>$$<br>y = 3x + 2<br>$$</p><p>$$<br>z = \sum(y/N)<br>$$</p><p>$$<br>\frac{\partial(z)}{\partial(x)} = \frac{\partial(z)}{\partial(y)} \cdot \frac{\partial(y)}{\partial(x)} = \frac{1}{N} \cdot 3 = \frac{1}{3 \cdot 4} \cdot 3 = 0.25<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tensors with gradient bookkeeping</span></span><br><span class="line">x = torch.rand(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="literal">True</span>)  <span class="comment"># requires_grad=True 表示此处需要计算梯度</span></span><br><span class="line">y = <span class="number">3</span>*x + <span class="number">2</span></span><br><span class="line">z = y.mean()</span><br><span class="line">z.backward()  <span class="comment"># 此时开始计算梯度</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"x: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"x.grad: \n<span class="subst">&#123;x.grad&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment"># tensor([[0.1154, 0.1101, 0.4831, 0.1580],</span></span><br><span class="line"><span class="comment">#         [0.4459, 0.2242, 0.9525, 0.8113],</span></span><br><span class="line"><span class="comment">#         [0.0387, 0.1512, 0.9678, 0.7512]], requires_grad=True)</span></span><br><span class="line"><span class="comment"># x.grad:</span></span><br><span class="line"><span class="comment"># tensor([[0.2500, 0.2500, 0.2500, 0.2500],</span></span><br><span class="line"><span class="comment">#         [0.2500, 0.2500, 0.2500, 0.2500],</span></span><br><span class="line"><span class="comment">#         [0.2500, 0.2500, 0.2500, 0.2500]])</span></span><br></pre></td></tr></table></figure><h2 id="CUDA">CUDA</h2><p>我们可以使用CUDA（Nvidia的并行计算平台和API）将张量加载到GPU上进行并行计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set device</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">x = torch.rand(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> (x.is_cuda)</span><br><span class="line">x = torch.rand(<span class="number">2</span>,<span class="number">3</span>).to(device)</span><br><span class="line"><span class="keyword">print</span> (x.is_cuda)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># False</span></span><br><span class="line"><span class="comment"># True</span></span><br></pre></td></tr></table></figure><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; PyTorch - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>到这里，便拥有了Way2AI路上需要的PyTorch的必备知识。我们可以发现，PyTorch的基础操作，与NumPy其实没什么太大的差别。</p><p>事实上NumPy和PyTorch可以相互转换，PyTorch提供了与NumPy兼容的接口，可以方便地将数据从NumPy数组转换为PyTorch张量，并在它们之间进行转换。这使得在使用PyTorch进行深度学习时，可以利用NumPy的强大功能进行数据预处理和后处理。</p><p>一般地，当我们需要进行常规的数值计算、数组操作和数学函数应用时，可以使用NumPy。当我们需要构建、训练和部署神经网络模型时，可以使用PyTorch。</p><p><a href="https://pytorch.org/" target="_blank" rel="noopener" title="PyTorch">PyTorch官网</a> 上有关于PyTorch的全部知识。</p>]]></content>
    
    <summary type="html">
    
      学习如何使用PyTorch这个机器学习框架。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · Pandas</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-Pandas/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-Pandas/</id>
    <published>2023-05-17T08:15:17.000Z</published>
    <updated>2023-05-27T13:56:59.688Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文简单介绍了Pandas这个数据分析处理库的必备知识。</p><h2 id="Set-up">Set up</h2><p>首先，我们将导入NumPy和Pandas库，并设置随机种子以实现可重复性。<br>我们还要下载一个数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">np.random.seed(seed=<span class="number">1024</span>)</span><br></pre></td></tr></table></figure><h2 id="Load-Data">Load Data</h2><p>我们将在 <a href="https://www.kaggle.com/c/titanic/data" target="_blank" rel="noopener" title="Titanic dataset">Titanic</a> 这个数据集上完成学习，这是一个非常常见且丰富的数据集，包含了1912年登上泰坦尼克号的人员相关信息以及他们在远航中幸存与否，非常适合使用Pandas进行探索性数据分析。</p><p>让我们将CSV文件中的数据加载到Pandas dataframe中。header=0表示第一行（索引为0）是一个标题行，其中包含了我们数据集中每个列的名称。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Read from CSV to Pandas DataFrame</span></span><br><span class="line">url = <span class="string">"http://s3.mindex.xyz/datasets/titanic.csv"</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># First few items</span></span><br><span class="line">df.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>输出如下图所示：<br><img src="//s3.mindex.xyz/blog/Courses/11a15080ea7aff309c574598cf6d1f03.png" alt=""></p><p>解释一下数据的特征列：</p><ul><li>PassengerId: ID</li><li>Survived: 存活指标（0 - died, 1 - survived）</li><li>Pclass: 票的等级</li><li>Name: 旅客的全名</li><li>Sex: 性别</li><li>Age: 年龄</li><li>SibSp: 兄弟姐妹 / 配偶</li><li>Parch: 父母 / 子女</li><li>Ticket: 票号</li><li>Fare: 票价</li><li>Cabin: 房间号</li><li>Embarked: 出发的港口</li></ul><h2 id="探索性数据分析-EDA">探索性数据分析 (EDA)</h2><p>现在我们已经加载了数据，准备开始探索以找到有用的信息。<br>我们可以使用 .describe() 方法提取数值特征的一些标准细节。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Describe features</span></span><br><span class="line">df.describe()</span><br></pre></td></tr></table></figure><p>输出如下图所示：<br><img src="//s3.mindex.xyz/blog/Courses/094c0b85aee702ed965db3005be4b6e9.png" alt=""></p><p>导入matplotlib以提供更直观的数据可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Correlation matrix</span></span><br><span class="line">plt.matshow(df.corr())</span><br><span class="line">continuous_features = df.describe().columns</span><br><span class="line">plt.xticks(range(len(continuous_features)), continuous_features, rotation=<span class="string">"45"</span>)</span><br><span class="line">plt.yticks(range(len(continuous_features)), continuous_features, rotation=<span class="string">"45"</span>)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/6429df82847c28e9a8325d674bef1b41.png" alt=""></p><p>我们还可以使用.hist()函数来查看每个特征的值的直方图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Histograms</span></span><br><span class="line">df[<span class="string">"Age"</span>].hist()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/5482837c04020c25d4148ac1c83b5530.png" alt=""></p><p>使用.unique()函数查看特征值的所有类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unique values</span></span><br><span class="line">df[<span class="string">"Embarked"</span>].unique()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># array(['S', 'C', 'Q', nan], dtype=object)</span></span><br></pre></td></tr></table></figure><h2 id="Filtering">Filtering</h2><p>我们可以按照特征甚至是特定特征中的具体值（或值范围）来过滤数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Selecting data by feature</span></span><br><span class="line">df[<span class="string">"Name"</span>].head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 0                              Braund, Mr. Owen Harris</span></span><br><span class="line"><span class="comment"># 1    Cumings, Mrs. John Bradley (Florence Briggs Th...</span></span><br><span class="line"><span class="comment"># 2                               Heikkinen, Miss. Laina</span></span><br><span class="line"><span class="comment"># 3         Futrelle, Mrs. Jacques Heath (Lily May Peel)</span></span><br><span class="line"><span class="comment"># 4                             Allen, Mr. William Henry</span></span><br><span class="line"><span class="comment"># Name: Name, dtype: object</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Filtering</span></span><br><span class="line">df[df[<span class="string">"Sex"</span>]==<span class="string">"female"</span>].head() <span class="comment"># only the female data appear</span></span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/8c221ac048730e95fec59bd917ea6248.png" alt="filtering by sex"></p><h2 id="Sorting">Sorting</h2><p>我们还可以按升序或降序对功能进行排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sorting</span></span><br><span class="line">df.sort_values(<span class="string">"Age"</span>, ascending=<span class="literal">False</span>).head()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/4d34974359deaf3f3b1a27264a4b7ee6.png" alt=""></p><h2 id="Grouping">Grouping</h2><p>我们还可以针对特定分组获取特征的统计数据。在这里，我们想根据乘客是否幸存来查看连续特征的平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Grouping</span></span><br><span class="line">survived_group = df.groupby(<span class="string">"Survived"</span>)</span><br><span class="line">survived_group.mean()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/68441798b47d6714e9ff255212a68128.png" alt=""></p><h2 id="Indexing">Indexing</h2><p>我们可以使用iloc在数据框中获取特定位置的行或列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Selecting row 0</span></span><br><span class="line">df.iloc[<span class="number">0</span>, :]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># PassengerId                          1</span></span><br><span class="line"><span class="comment"># Survived                             0</span></span><br><span class="line"><span class="comment"># Pclass                               3</span></span><br><span class="line"><span class="comment"># Name           Braund, Mr. Owen Harris</span></span><br><span class="line"><span class="comment"># Sex                               male</span></span><br><span class="line"><span class="comment"># Age                               22.0</span></span><br><span class="line"><span class="comment"># SibSp                                1</span></span><br><span class="line"><span class="comment"># Parch                                0</span></span><br><span class="line"><span class="comment"># Ticket                       A/5 21171</span></span><br><span class="line"><span class="comment"># Fare                              7.25</span></span><br><span class="line"><span class="comment"># Cabin                              NaN</span></span><br><span class="line"><span class="comment"># Embarked                             S</span></span><br><span class="line"><span class="comment"># Name: 0, dtype: object</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Selecting a specific value</span></span><br><span class="line">df.iloc[<span class="number">0</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 'Braund, Mr. Owen Harris'</span></span><br></pre></td></tr></table></figure><h2 id="Preprocessing">Preprocessing</h2><p>在探索完数据后，我们可以对数据集进行清洗和预处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Rows with at least one NaN value</span></span><br><span class="line">df[pd.isnull(df).any(axis=<span class="number">1</span>)]</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/9e922e0e75b89fa3d3104e990a5bd4b6.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Drop rows with Nan values</span></span><br><span class="line">df = df.dropna() <span class="comment"># removes rows with any NaN values</span></span><br><span class="line">df = df.reset_index() <span class="comment"># reset's row indexes in case any rows were dropped</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dropping multiple columns</span></span><br><span class="line">df = df.drop([<span class="string">"Name"</span>, <span class="string">"Cabin"</span>, <span class="string">"Ticket"</span>], axis=<span class="number">1</span>) <span class="comment"># we won't use text features for our initial basic models</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Map feature values</span></span><br><span class="line">df[<span class="string">"Sex"</span>] = df[<span class="string">"Sex"</span>].map( &#123;<span class="string">"female"</span>: <span class="number">0</span>, <span class="string">"male"</span>: <span class="number">1</span>&#125; ).astype(int)</span><br><span class="line">df[<span class="string">"Embarked"</span>] = df[<span class="string">"Embarked"</span>].dropna().map( &#123;<span class="string">"S"</span>:<span class="number">0</span>, <span class="string">"C"</span>:<span class="number">1</span>, <span class="string">"Q"</span>:<span class="number">2</span>&#125; ).astype(int)</span><br><span class="line"></span><br><span class="line">df</span><br></pre></td></tr></table></figure><p>结果如下：<br><img src="//s3.mindex.xyz/blog/Courses/ab30159a0aa58a9ca9fa1a2a3826a41a.png" alt=""></p><h2 id="Feature-Engineering-特征工程">Feature Engineering 特征工程</h2><p>我们现在要使用特征工程来创建一个名为FamilySize的列。我们将首先定义一个名为get_family_size的函数，该函数将使用父母和兄弟姐妹数量来确定家庭大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Lambda expressions to create new features</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_family_size</span><span class="params">(sibsp, parch)</span>:</span></span><br><span class="line">    family_size = sibsp + parch</span><br><span class="line">    <span class="keyword">return</span> family_size</span><br></pre></td></tr></table></figure><p>我们就可以使用lambda将该函数应用于每一行（使用每行中兄弟姐妹和父母的数量来确定每行的家庭规模）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">"FamilySize"</span>] = df[[<span class="string">"SibSp"</span>, <span class="string">"Parch"</span>]].apply(<span class="keyword">lambda</span> x: get_family_size(x[<span class="string">"SibSp"</span>], x[<span class="string">"Parch"</span>]), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reorganize headers</span></span><br><span class="line">df = df[[<span class="string">"Pclass"</span>, <span class="string">"Sex"</span>, <span class="string">"Age"</span>, <span class="string">"SibSp"</span>, <span class="string">"Parch"</span>, <span class="string">"FamilySize"</span>, <span class="string">"Fare"</span>, <span class="string">"Embarked"</span>, <span class="string">"Survived"</span>]]</span><br><span class="line"></span><br><span class="line">df</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/917c83ce4d6135c15dd0214fc82aab46.png" alt=""></p><p>特征工程可以与领域专家合作进行，他们可以指导我们在工程和使用哪些特征。</p><h2 id="Save-Data">Save Data</h2><p>最后，让我们将预处理后的数据保存到一个新的CSV文件中以备后用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Saving dataframe to CSV</span></span><br><span class="line">df.to_csv(<span class="string">"processed_titanic.csv"</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="Scaling">Scaling</h2><p>当处理非常大的数据集时，我们的Pandas DataFrames可能会变得非常庞大，对它们进行操作可能会变得非常缓慢或不可行。这就是分布式工作负载或在更高效硬件上运行的软件包派上用场的地方。</p><ul><li><a href="https://dask.org/" target="_blank" rel="noopener" title="Dask">Dask</a>: 使用并行计算来扩展Numpy、Pandas和scikit-learn等软件包在单个/多台机器上的应用。</li><li><a href="https://github.com/rapidsai/cudf" target="_blank" rel="noopener" title="cuDF">cuDF</a>: 在GPU上高效加载和计算dataframe。</li></ul><p>当然，我们可以将它们（<a href="https://github.com/rapidsai/cudf/tree/main/python/dask_cudf" target="_blank" rel="noopener" title="Dask-cuDF">Dask-cuDF</a>）结合在一起，在GPU上对dataframe分块进行操作。</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Pandas - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>到这里，便拥有了Way2AI路上需要的Pandas的必备知识。</p><p>但我们不应该止步于此。<a href="https://pandas.pydata.org/" target="_blank" rel="noopener" title="Pandas">Pandas官网</a> 上有关于Pandas的全部知识。</p>]]></content>
    
    <summary type="html">
    
      使用Pandas库进行数据操作。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · NumPy</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-NumPy/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-NumPy/</id>
    <published>2023-05-17T01:47:59.000Z</published>
    <updated>2023-05-27T13:56:52.237Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文简单介绍了NumPy这个科学计算扩展包的必备知识。</p><h2 id="Set-up">Set up</h2><p>首先我们需要导入NumPy包，做实验的时候可以设置随机种子以实现可重复性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">np.random.seed(seed=<span class="number">1024</span>)</span><br></pre></td></tr></table></figure><p>接下来分别示例 0D（标量）、1D（向量）、2D（矩阵）、3D（3维张量）。</p><p><img src="//s3.mindex.xyz/blog/Courses/49497eb3ffb5cdd0dc88c6e8e2d08270.png" alt="tensors"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scalar</span></span><br><span class="line">x = np.array(<span class="number">6</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x: "</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x ndim: "</span>, x.ndim) <span class="comment"># number of dimensions</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x shape:"</span>, x.shape) <span class="comment"># dimensions</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x size: "</span>, x.size) <span class="comment"># size of elements</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x dtype: "</span>, x.dtype) <span class="comment"># data type</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:  6</span></span><br><span class="line"><span class="comment"># x ndim:  0</span></span><br><span class="line"><span class="comment"># x shape: ()</span></span><br><span class="line"><span class="comment"># x size:  1</span></span><br><span class="line"><span class="comment"># x dtype:  int64 </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Vector</span></span><br><span class="line">x = np.array([<span class="number">1.3</span> , <span class="number">2.2</span> , <span class="number">1.7</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x: "</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x ndim: "</span>, x.ndim)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x shape:"</span>, x.shape)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x size: "</span>, x.size)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x dtype: "</span>, x.dtype) <span class="comment"># notice the float datatype</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:  [1.3 2.2 1.7]</span></span><br><span class="line"><span class="comment"># x ndim:  1</span></span><br><span class="line"><span class="comment"># x shape: (3,)</span></span><br><span class="line"><span class="comment"># x size:  3</span></span><br><span class="line"><span class="comment"># x dtype:  float64</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Matrix</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x ndim: "</span>, x.ndim)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x shape:"</span>, x.shape)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x size: "</span>, x.size)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x dtype: "</span>, x.dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[1 2]</span></span><br><span class="line"><span class="comment">#  [3 4]]</span></span><br><span class="line"><span class="comment"># x ndim:  2</span></span><br><span class="line"><span class="comment"># x shape: (2, 2)</span></span><br><span class="line"><span class="comment"># x size:  4</span></span><br><span class="line"><span class="comment"># x dtype:  int64</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3D Tensor</span></span><br><span class="line">x = np.array([[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], [[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x ndim: "</span>, x.ndim)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x shape:"</span>, x.shape)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x size: "</span>, x.size)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x dtype: "</span>, x.dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[[1 2]</span></span><br><span class="line"><span class="comment">#   [3 4]]</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#  [[5 6]</span></span><br><span class="line"><span class="comment">#   [7 8]]]</span></span><br><span class="line"><span class="comment"># x ndim:  3</span></span><br><span class="line"><span class="comment"># x shape: (2, 2, 2)</span></span><br><span class="line"><span class="comment"># x size:  8</span></span><br><span class="line"><span class="comment"># x dtype:  int64</span></span><br></pre></td></tr></table></figure><p>NumPy当然也提供了几个函数，可以快速创建张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Functions</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"np.zeros((2, 2)):\n"</span>, np.zeros((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"np.ones((2, 2)):\n"</span>, np.ones((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"np.eye((2)):\n"</span>, np.eye((<span class="number">2</span>))) <span class="comment"># identity matrix</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"np.random.random((2, 2)):\n"</span>, np.random.random((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># np.zeros((2, 2)):</span></span><br><span class="line"><span class="comment">#  [[0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0.]]</span></span><br><span class="line"><span class="comment"># np.ones((2, 2)):</span></span><br><span class="line"><span class="comment">#  [[1. 1.]</span></span><br><span class="line"><span class="comment">#  [1. 1.]]</span></span><br><span class="line"><span class="comment"># np.eye((2)):</span></span><br><span class="line"><span class="comment">#  [[1. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1.]]</span></span><br><span class="line"><span class="comment"># np.random.random((2, 2)):</span></span><br><span class="line"><span class="comment">#  [[0.64769123 0.99691358]</span></span><br><span class="line"><span class="comment">#  [0.51880326 0.65811273]]</span></span><br></pre></td></tr></table></figure><h2 id="Indexing">Indexing</h2><p>我们可以使用索引从张量中提取指定的值。<br>请记住，索引从0开始。与使用列表进行索引一样，我们也可以使用负数索引（其中-1是最后一个项目）。</p><p><img src="//s3.mindex.xyz/blog/Courses/f6ba5bef483ad3756ddb81f1d9a05576.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Indexing</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x: "</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[0]: "</span>, x[<span class="number">0</span>])</span><br><span class="line">x[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x: "</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:  [1 2 3]</span></span><br><span class="line"><span class="comment"># x[0]:  1</span></span><br><span class="line"><span class="comment"># x:  [0 2 3]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Slicing</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x column 1: "</span>, x[:, <span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x row 0: "</span>, x[<span class="number">0</span>, :])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x rows 0,1 &amp; cols 1,2: \n"</span>, x[<span class="number">0</span>:<span class="number">2</span>, <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[ 1  2  3  4]</span></span><br><span class="line"><span class="comment">#  [ 5  6  7  8]</span></span><br><span class="line"><span class="comment">#  [ 9 10 11 12]]</span></span><br><span class="line"><span class="comment"># x column 1:  [ 2  6 10]</span></span><br><span class="line"><span class="comment"># x row 0:  [1 2 3 4]</span></span><br><span class="line"><span class="comment"># x rows 0,1 &amp; cols 1,2: </span></span><br><span class="line"><span class="comment">#  [[2 3]</span></span><br><span class="line"><span class="comment">#  [6 7]]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Integer array indexing</span></span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line">rows_to_get = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"rows_to_get: "</span>, rows_to_get)</span><br><span class="line">cols_to_get = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"cols_to_get: "</span>, cols_to_get)</span><br><span class="line"><span class="comment"># Combine sequences above to get values to get</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"indexed values: "</span>, x[rows_to_get, cols_to_get]) <span class="comment"># (0, 0), (1, 2), (2, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[ 1  2  3  4]</span></span><br><span class="line"><span class="comment">#  [ 5  6  7  8]</span></span><br><span class="line"><span class="comment">#  [ 9 10 11 12]]</span></span><br><span class="line"><span class="comment"># rows_to_get:  [0 1 2]</span></span><br><span class="line"><span class="comment"># cols_to_get:  [0 2 1]</span></span><br><span class="line"><span class="comment"># indexed values:  [ 1  7 10]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Boolean array indexing</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x &gt; 2:\n"</span>, x &gt; <span class="number">2</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[x &gt; 2]:\n"</span>, x[x &gt; <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[1 2]</span></span><br><span class="line"><span class="comment">#  [3 4]</span></span><br><span class="line"><span class="comment">#  [5 6]]</span></span><br><span class="line"><span class="comment"># x &gt; 2:</span></span><br><span class="line"><span class="comment">#  [[False False]</span></span><br><span class="line"><span class="comment">#  [ True  True]</span></span><br><span class="line"><span class="comment">#  [ True  True]]</span></span><br><span class="line"><span class="comment"># x[x &gt; 2]:</span></span><br><span class="line"><span class="comment">#  [3 4 5 6]</span></span><br></pre></td></tr></table></figure><h2 id="Arithmetic-运算">Arithmetic 运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Basic math</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x + y:\n"</span>, np.add(x, y)) <span class="comment"># or x + y</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x - y:\n"</span>, np.subtract(x, y)) <span class="comment"># or x - y</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x * y:\n"</span>, np.multiply(x, y)) <span class="comment"># or x * y</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x + y:</span></span><br><span class="line"><span class="comment">#  [[2. 4.]</span></span><br><span class="line"><span class="comment">#  [6. 8.]]</span></span><br><span class="line"><span class="comment"># x - y:</span></span><br><span class="line"><span class="comment">#  [[0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0.]]</span></span><br><span class="line"><span class="comment"># x * y:</span></span><br><span class="line"><span class="comment">#  [[ 1.  4.]</span></span><br><span class="line"><span class="comment">#  [ 9. 16.]]</span></span><br></pre></td></tr></table></figure><h2 id="Dot-product-点积">Dot product 点积</h2><p>在机器学习中，我们最常使用的NumPy操作之一是使用点积进行矩阵乘法。<br>假设我们需要取一个2x3的矩阵a和一个3x2的矩阵b的点积，我们将得到矩阵a的行及矩阵b的列作为点积的输出，也就是得到一个2x2的矩阵。点积能够正确运行需要满足的条件便是内部维度匹配，即示例中，矩阵a有3列，矩阵b有3行。</p><p><img src="//s3.mindex.xyz/blog/Courses/e11a898f34e386460b7dbb42d1cff842.gif" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Dot product</span></span><br><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=np.float64) <span class="comment"># we can specify dtype</span></span><br><span class="line">b = np.array([[<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>], [<span class="number">11</span>, <span class="number">12</span>]], dtype=np.float64)</span><br><span class="line">c = a.dot(b)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"<span class="subst">&#123;a.shape&#125;</span> · <span class="subst">&#123;b.shape&#125;</span> = <span class="subst">&#123;c.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># (2, 3) · (3, 2) = (2, 2)</span></span><br><span class="line"><span class="comment"># [[ 58.  64.]</span></span><br><span class="line"><span class="comment">#  [139. 154.]]</span></span><br></pre></td></tr></table></figure><h2 id="Axis-operations">Axis operations</h2><p>我们还可以沿着特定的轴进行操作。</p><p><img src="//s3.mindex.xyz/blog/Courses/a80be7714f670936b98bb5769037e313.gif" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sum across a dimension</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sum all: "</span>, np.sum(x)) <span class="comment"># adds all elements</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sum axis=0: "</span>, np.sum(x, axis=<span class="number">0</span>)) <span class="comment"># sum across rows</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sum axis=1: "</span>, np.sum(x, axis=<span class="number">1</span>)) <span class="comment"># sum across columns</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[1 2]</span></span><br><span class="line"><span class="comment">#  [3 4]]</span></span><br><span class="line"><span class="comment"># sum all:  10</span></span><br><span class="line"><span class="comment"># sum axis=0:  [4 6]</span></span><br><span class="line"><span class="comment"># sum axis=1:  [3 7]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Min/Max</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"min: "</span>, x.min())</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"max: "</span>, x.max())</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"min axis=0: "</span>, x.min(axis=<span class="number">0</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"min axis=1: "</span>, x.min(axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># min:  1</span></span><br><span class="line"><span class="comment"># max:  6</span></span><br><span class="line"><span class="comment"># min axis=0:  [1 2 3]</span></span><br><span class="line"><span class="comment"># min axis=1:  [1 4]</span></span><br></pre></td></tr></table></figure><h2 id="Broadcast">Broadcast</h2><p>当我们尝试使用看似不兼容的张量形状进行操作时会发生什么？<br>它们的维度不兼容，但是NumPy为何仍然给出了结果？这就是广播的作用。</p><p><img src="//s3.mindex.xyz/blog/Courses/db0288a489a3c03ebdea1be427f1d963.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Broadcasting</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>]) <span class="comment"># vector</span></span><br><span class="line">y = np.array(<span class="number">3</span>) <span class="comment"># scalar</span></span><br><span class="line">z = x + y</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"z:\n"</span>, z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># z:</span></span><br><span class="line"><span class="comment">#  [4 5]</span></span><br></pre></td></tr></table></figure><h2 id="Gotchas">Gotchas</h2><p>在下面的情况中，c的值是多少，它的形状是什么？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = np.array((<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">b = np.expand_dims(a, axis=<span class="number">1</span>)</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line">a.shape <span class="comment"># (3,)</span></span><br><span class="line">b.shape <span class="comment"># (3, 1)</span></span><br><span class="line">c.shape <span class="comment"># (3, 3)</span></span><br><span class="line"><span class="keyword">print</span> (c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># array([[ 6,  7,  8],</span></span><br><span class="line"><span class="comment">#         [ 7,  8,  9],</span></span><br><span class="line"><span class="comment">#         [ 8,  9, 10]])</span></span><br></pre></td></tr></table></figure><p>如果我们不想出现意外的广播行为，就需要小心确保 矩阵a 和 矩阵b 的形状相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = a.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">a.shape <span class="comment"># (3, 1)</span></span><br><span class="line">c = a + b</span><br><span class="line">c.shape <span class="comment"># (3, 1)</span></span><br><span class="line"><span class="keyword">print</span> (c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[ 6]</span></span><br><span class="line"><span class="comment">#  [ 8]</span></span><br><span class="line"><span class="comment">#  [10]]</span></span><br></pre></td></tr></table></figure><h2 id="Transpose-转置">Transpose 转置</h2><p>我们经常需要改变张量的维度，以进行诸如点积之类的操作。如果我们需要交换两个维度，可以对张量进行转置。</p><p><img src="//s3.mindex.xyz/blog/Courses/9fe94457fbf3d53aca519eb20ebf487f.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Transposing</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x.shape: "</span>, x.shape)</span><br><span class="line">y = np.transpose(x, (<span class="number">1</span>, <span class="number">0</span>)) <span class="comment"># flip dimensions at index 0 and 1</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y:\n"</span>, y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y.shape: "</span>, y.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]]</span></span><br><span class="line"><span class="comment"># x.shape:  (2, 3)</span></span><br><span class="line"><span class="comment"># y:</span></span><br><span class="line"><span class="comment">#  [[1 4]</span></span><br><span class="line"><span class="comment">#  [2 5]</span></span><br><span class="line"><span class="comment">#  [3 6]]</span></span><br><span class="line"><span class="comment"># y.shape:  (3, 2)</span></span><br></pre></td></tr></table></figure><h2 id="Reshape">Reshape</h2><p>reshape是另一种改变张量形状的办法。<br>如下面所示，我们reshape后的张量与原始张量具有相同数量的值。我们还可以在一个维度上使用<code>-1</code>，NumPy会根据输入张量自动推断该维度的大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reshaping</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x.shape: "</span>, x.shape)</span><br><span class="line">y = np.reshape(x, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: \n"</span>, y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y.shape: "</span>, y.shape)</span><br><span class="line">z = np.reshape(x, (<span class="number">2</span>, <span class="number">-1</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"z: \n"</span>, z)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"z.shape: "</span>, z.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[1 2 3 4 5 6]]</span></span><br><span class="line"><span class="comment"># x.shape:  (1, 6)</span></span><br><span class="line"><span class="comment"># y:</span></span><br><span class="line"><span class="comment">#  [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]]</span></span><br><span class="line"><span class="comment"># y.shape:  (2, 3)</span></span><br><span class="line"><span class="comment"># z:</span></span><br><span class="line"><span class="comment">#  [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]]</span></span><br><span class="line"><span class="comment"># z.shape:  (2, 3)</span></span><br></pre></td></tr></table></figure><p>reshape函数的工作原理是查看新张量的每个维度，并将原始张量分成相应数量的单元。因此，在这里，新张量<code>index 0</code>处的维度为2，因此我们将原始张量分成2个单元，每个单元都有3个值。</p><p><img src="//s3.mindex.xyz/blog/Courses/1aaa08d5c71d13009b4e969815e1e8a8.png" alt=""></p><h2 id="Joining">Joining</h2><p>我们还可以使用concatenate和stack来合并张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.random((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[0.14950888 0.698439   0.59335256]</span></span><br><span class="line"><span class="comment">#  [0.89991535 0.44445739 0.316785  ]]</span></span><br><span class="line"><span class="comment"># (2, 3)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Concatenation</span></span><br><span class="line">y = np.concatenate([x, x], axis=<span class="number">0</span>) <span class="comment"># concat on a specified axis</span></span><br><span class="line"><span class="keyword">print</span> (y)</span><br><span class="line"><span class="keyword">print</span> (y.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[0.14950888 0.698439   0.59335256]</span></span><br><span class="line"><span class="comment">#  [0.89991535 0.44445739 0.316785  ]</span></span><br><span class="line"><span class="comment">#  [0.14950888 0.698439   0.59335256]</span></span><br><span class="line"><span class="comment">#  [0.89991535 0.44445739 0.316785  ]]</span></span><br><span class="line"><span class="comment"># (4, 3)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Stacking</span></span><br><span class="line">z = np.stack([x, x], axis=<span class="number">0</span>) <span class="comment"># stack on new axis</span></span><br><span class="line"><span class="keyword">print</span> (z)</span><br><span class="line"><span class="keyword">print</span> (z.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[[0.14950888 0.698439   0.59335256]</span></span><br><span class="line"><span class="comment">#   [0.89991535 0.44445739 0.316785  ]]</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#  [[0.14950888 0.698439   0.59335256]</span></span><br><span class="line"><span class="comment">#   [0.89991535 0.44445739 0.316785  ]]]</span></span><br><span class="line"><span class="comment"># (2, 2, 3)</span></span><br></pre></td></tr></table></figure><h2 id="Expanding-Reducing">Expanding / Reducing</h2><p>我们还可以轻松地向张量中添加和删除维度，这样做是为了使张量能够兼容某些操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Adding dimensions</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x.shape: "</span>, x.shape)</span><br><span class="line">y = np.expand_dims(x, axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: \n"</span>, y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y.shape: "</span>, y.shape)   <span class="comment"># notice extra set of brackets are added</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]]</span></span><br><span class="line"><span class="comment"># x.shape:  (2, 3)</span></span><br><span class="line"><span class="comment"># y:</span></span><br><span class="line"><span class="comment">#  [[[1 2 3]]</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#  [[4 5 6]]]</span></span><br><span class="line"><span class="comment"># y.shape:  (2, 1, 3)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Removing dimensions</span></span><br><span class="line">x = np.array([[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]], [[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x.shape: "</span>, x.shape)</span><br><span class="line">y = np.squeeze(x, axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: \n"</span>, y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y.shape: "</span>, y.shape)  <span class="comment"># notice extra set of brackets are gone</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[[1 2 3]]</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#  [[4 5 6]]]</span></span><br><span class="line"><span class="comment"># x.shape:  (2, 1, 3)</span></span><br><span class="line"><span class="comment"># y:</span></span><br><span class="line"><span class="comment">#  [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]]</span></span><br><span class="line"><span class="comment"># y.shape:  (2, 3)</span></span><br></pre></td></tr></table></figure><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; NumPy - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>到这里，便拥有了Way2AI路上需要的NumPy的必备知识。</p><p>但我们不应该止步于此。<a href="https://numpy.org/" target="_blank" rel="noopener" title="NumPy">NumPy官网</a> 上有关于NumPy的全部知识。</p>]]></content>
    
    <summary type="html">
    
      Numerical analysis with the NumPy computing package.
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · Python</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-python/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-python/</id>
    <published>2023-05-16T10:36:00.000Z</published>
    <updated>2023-05-27T13:57:05.958Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文简单介绍了Python这门语言的必备知识。</p><h2 id="Variables">Variables</h2><p>变量是用于存储数据的容器，它们由名称和值定义。<br>在我们的示例中，我们使用变量名x，但是当您处理特定任务时，请确保在创建变量（函数、类等）时指定<strong>明确含义的名称</strong>（例如first_name），以确保程序的可读性。</p><p><img src="//s3.mindex.xyz/blog/Courses/c249bf303d806ffea1f144af762cd99c.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># integer variable</span></span><br><span class="line">x = <span class="number">5</span></span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (type(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 5 </span></span><br><span class="line"><span class="comment"># &lt;class 'int'&gt;</span></span><br></pre></td></tr></table></figure><p>我们可以通过将新值赋给变量来改变它的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sring variable</span></span><br><span class="line">x = <span class="string">"hello"</span></span><br><span class="line"><span class="keyword">print</span> (x, type(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># hello &lt;class 'str'&gt;</span></span><br></pre></td></tr></table></figure><p>python有许多不同类型的变量：integers, floats, strings, boolean 等等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># int variable</span></span><br><span class="line">x = <span class="number">5</span></span><br><span class="line"><span class="keyword">print</span> (x, type(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 5 &lt;class 'int'&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># float variable</span></span><br><span class="line">x = <span class="number">5.0</span></span><br><span class="line"><span class="keyword">print</span> (x, type(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 5.0 &lt;class 'float'&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># string variable</span></span><br><span class="line">x = <span class="string">"5"</span></span><br><span class="line"><span class="keyword">print</span> (x, type(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 5 &lt;class 'str'&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># boolean variable</span></span><br><span class="line">x = <span class="literal">True</span></span><br><span class="line"><span class="keyword">print</span> (x, type(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># True &lt;class 'bool'&gt;</span></span><br></pre></td></tr></table></figure><p>我们也可以使用变量进行运算操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Variables can be used with each other</span></span><br><span class="line">a = <span class="number">1</span></span><br><span class="line">b = <span class="number">2</span></span><br><span class="line">c = a + b</span><br><span class="line"><span class="keyword">print</span> (c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 3</span></span><br></pre></td></tr></table></figure><p>我们应该始终知道我们正在处理哪种类型的变量，以便可以正确地对它们进行操作。</p><h2 id="Lists">Lists</h2><p>列表是一个有序的、可变的值集合，由方括号包围并用逗号分隔。一个列表可以包含许多不同类型的变量。下面是一个包含整数、字符串和浮点数的列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating a list</span></span><br><span class="line">x = [<span class="number">3</span>, <span class="string">"hello"</span>, <span class="number">1.2</span>]</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [3, 'hello', 1.2]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Length of a list</span></span><br><span class="line">len(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 3</span></span><br></pre></td></tr></table></figure><p>使用append方法向列表里添加元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Adding to a list</span></span><br><span class="line">x.append(<span class="number">7</span>)</span><br><span class="line"><span class="keyword">print</span> (x, len(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [3, 'hello', 1.2, 7] 4</span></span><br></pre></td></tr></table></figure><p>通过indexing修改已存在的元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Replacing items in a list</span></span><br><span class="line">x[<span class="number">1</span>] = <span class="string">"bye"</span></span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [3, 'bye', 1.2, 7]</span></span><br></pre></td></tr></table></figure><p>列表也可以进行操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Operations</span></span><br><span class="line">y = [<span class="number">2.4</span>, <span class="string">"world"</span>]</span><br><span class="line">z = x + y</span><br><span class="line"><span class="keyword">print</span> (z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [3, 'bye', 1.2, 7, 2.4, 'world']</span></span><br></pre></td></tr></table></figure><h2 id="Tuples">Tuples</h2><p>元组是有序且不可变（无法更改）的集合。我们将使用元组来存储永远不会更改的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating a tuple</span></span><br><span class="line">x = (<span class="number">3.0</span>, <span class="string">"hello"</span>) <span class="comment"># tuples start and end with ()</span></span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># (3.0, 'hello')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Adding values to a tuple</span></span><br><span class="line">x = x + (<span class="number">5.6</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># (3.0, 'hello', 5.6, 4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Try to change (it won't work and we get an error)</span></span><br><span class="line">x[<span class="number">0</span>] = <span class="number">1.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># TypeError: 'tuple' object does not support item assignment</span></span><br></pre></td></tr></table></figure><h2 id="Sets">Sets</h2><p>集合是无序且可变的。而且，集合中的每个元素必须是唯一的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sets</span></span><br><span class="line">text = <span class="string">"Learn ML with Made With ML"</span></span><br><span class="line"><span class="keyword">print</span> (set(text))</span><br><span class="line"><span class="keyword">print</span> (set(text.split(<span class="string">" "</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;'e', 'M', ' ', "r", "w", 'd', 'a', 'h', 't', 'i', 'L', 'n', "w"&#125;</span></span><br><span class="line"><span class="comment"># &#123;'with', 'Learn', 'ML', 'Made', 'With'&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Indexing">Indexing</h2><p>从列表中进行索引和切片可以让我们检索到特定的值。请注意，索引可以是正数（从0开始）或负数（-1及以下，其中-1是列表中的最后一项）。</p><p><img src="//s3.mindex.xyz/blog/Courses/72e91f603cb74286d4147011b61e07f4.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Indexing</span></span><br><span class="line">x = [<span class="number">3</span>, <span class="string">"hello"</span>, <span class="number">1.2</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[0]: "</span>, x[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[1]: "</span>, x[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[-1]: "</span>, x[<span class="number">-1</span>]) <span class="comment"># the last item</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[-2]: "</span>, x[<span class="number">-2</span>]) <span class="comment"># the second to last item</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x[0]:  3</span></span><br><span class="line"><span class="comment"># x[1]:  hello</span></span><br><span class="line"><span class="comment"># x[-1]:  1.2</span></span><br><span class="line"><span class="comment"># x[-2]:  hello</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Slicing</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[:]: "</span>, x[:]) <span class="comment"># all indices</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[1:]: "</span>, x[<span class="number">1</span>:]) <span class="comment"># index 1 to the end of the list</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[1:2]: "</span>, x[<span class="number">1</span>:<span class="number">2</span>]) <span class="comment"># index 1 to index 2 (not including index 2)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[:-1]: "</span>, x[:<span class="number">-1</span>]) <span class="comment"># index 0 to last index (not including last index)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x[:]:  [3, 'hello', 1.2</span></span><br><span class="line"><span class="comment"># x[1:]:  ['hello', 1.2]</span></span><br><span class="line"><span class="comment"># x[1:2]:  ['hello']</span></span><br><span class="line"><span class="comment"># x[:-1]:  [3, 'hello']</span></span><br></pre></td></tr></table></figure><h2 id="Dictionaries">Dictionaries</h2><p>字典是一种无序的可变键值对集合。您可以根据键检索值，而且一个字典不能有两个相同的键。</p><p><img src="//s3.mindex.xyz/blog/Courses/fb3ab2534970cdff498d3914ed172716.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating a dictionary</span></span><br><span class="line">person = &#123;<span class="string">"name"</span>: <span class="string">"Goku"</span>,</span><br><span class="line">          <span class="string">"eye_color"</span>: <span class="string">"brown"</span>&#125;</span><br><span class="line"><span class="keyword">print</span> (person)</span><br><span class="line"><span class="keyword">print</span> (person[<span class="string">"name"</span>])</span><br><span class="line"><span class="keyword">print</span> (person[<span class="string">"eye_color"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;"name": "Goku", "eye_color": "brown"&#125;</span></span><br><span class="line"><span class="comment"># Goku</span></span><br><span class="line"><span class="comment"># brown</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Adding new key-value pairs</span></span><br><span class="line">person[<span class="string">"age"</span>] = <span class="number">24</span></span><br><span class="line"><span class="keyword">print</span> (person)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;"name": "Goku", "eye_color": "green", "age": 24&#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Length of a dictionary</span></span><br><span class="line"><span class="keyword">print</span> (len(person))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 3</span></span><br></pre></td></tr></table></figure><p>当然，使用Python几乎可以实现任何事情。例如，尽管原生字典是无序的，但我们可以利用OrderedDict数据结构来改变它（如果我们想按特定顺序迭代键等，则非常有用）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line"><span class="comment"># Native dict</span></span><br><span class="line">d = &#123;&#125;</span><br><span class="line">d[<span class="string">"a"</span>] = <span class="number">2</span></span><br><span class="line">d[<span class="string">"c"</span>] = <span class="number">3</span></span><br><span class="line">d[<span class="string">"b"</span>] = <span class="number">1</span></span><br><span class="line"><span class="keyword">print</span> (d)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;'a': 2, 'c': 3, 'b': 1&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Python3.7以后，原生的字典是是按插入的顺序排序的。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dictionary items</span></span><br><span class="line"><span class="keyword">print</span> (d.items())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># dict_items([('a', 2), ('c', 3), ('b', 1)])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Order by keys</span></span><br><span class="line"><span class="keyword">print</span> (OrderedDict(sorted(d.items())))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># OrderedDict([('a', 2), ('b', 1), ('c', 3)])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Order by values</span></span><br><span class="line"><span class="keyword">print</span> (OrderedDict(sorted(d.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># OrderedDict([('b', 1), ('a', 2), ('c', 3)])</span></span><br></pre></td></tr></table></figure><h2 id="If-statements">If statements</h2><p>我们可以使用if语句来有条件地执行某些操作。条件由if、elif（代表else if）和else这几个关键字定义。我们可以拥有任意多个elif语句。每个条件下面缩进的代码是在该条件为True时将要执行的代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If statement</span></span><br><span class="line">x = <span class="number">4</span></span><br><span class="line"><span class="keyword">if</span> x &lt; <span class="number">1</span>:</span><br><span class="line">    score = <span class="string">"low"</span></span><br><span class="line"><span class="keyword">elif</span> x &lt;= <span class="number">4</span>: <span class="comment"># elif = else if</span></span><br><span class="line">    score = <span class="string">"medium"</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    score = <span class="string">"high"</span></span><br><span class="line"><span class="keyword">print</span> (score)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># medium</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># If statement with a boolean</span></span><br><span class="line">x = <span class="literal">True</span></span><br><span class="line"><span class="keyword">if</span> x:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"it worked"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># it worked</span></span><br></pre></td></tr></table></figure><h2 id="Loops">Loops</h2><h3 id="For-loops">For loops</h3><p>for循环可以迭代值的集合（列表、元组、字典等）。缩进的代码对于集合中的每个项目都会执行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For loop</span></span><br><span class="line">veggies = [<span class="string">"carrots"</span>, <span class="string">"broccoli"</span>, <span class="string">"beans"</span>]</span><br><span class="line"><span class="keyword">for</span> veggie <span class="keyword">in</span> veggies:</span><br><span class="line">    <span class="keyword">print</span> (veggie)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># carrots</span></span><br><span class="line"><span class="comment"># broccoli</span></span><br><span class="line"><span class="comment"># beans</span></span><br></pre></td></tr></table></figure><p>当循环遇到break命令时，循环将立即终止。如果列表中还有更多的项，则它们不会被处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># `break` from a for loop</span></span><br><span class="line">veggies = [<span class="string">"carrots"</span>, <span class="string">"broccoli"</span>, <span class="string">"beans"</span>]</span><br><span class="line"><span class="keyword">for</span> veggie <span class="keyword">in</span> veggies:</span><br><span class="line">    <span class="keyword">if</span> veggie == <span class="string">"broccoli"</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">print</span> (veggie)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># carrots</span></span><br></pre></td></tr></table></figure><p>当循环遇到 continue 命令时，该循环将仅跳过列表中当前项的所有其他操作。如果列表中还有更多项，则循环将正常继续。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># `continue` to the next iteration</span></span><br><span class="line">veggies = [<span class="string">"carrots"</span>, <span class="string">"broccoli"</span>, <span class="string">"beans"</span>]</span><br><span class="line"><span class="keyword">for</span> veggie <span class="keyword">in</span> veggies:</span><br><span class="line">    <span class="keyword">if</span> veggie == <span class="string">"broccoli"</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">print</span> (veggie)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># carrots</span></span><br><span class="line"><span class="comment"># beans</span></span><br></pre></td></tr></table></figure><h3 id="While-Loops">While Loops</h3><p>当条件为真时，while循环可以重复执行。我们也可以在while循环中使用continue和break命令。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># While loop</span></span><br><span class="line">x = <span class="number">3</span></span><br><span class="line"><span class="keyword">while</span> x &gt; <span class="number">0</span>:</span><br><span class="line">    x -= <span class="number">1</span> <span class="comment"># same as x = x - 1</span></span><br><span class="line">    <span class="keyword">print</span> (x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 0</span></span><br></pre></td></tr></table></figure><h2 id="List-comprehension">List comprehension</h2><p>我们可以结合列表和for循环的知识，利用<code>列表推导式</code>来创建简洁的代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For loop</span></span><br><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">y = []</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> x:</span><br><span class="line">    <span class="keyword">if</span> item &gt; <span class="number">2</span>:</span><br><span class="line">        y.append(item)</span><br><span class="line"><span class="keyword">print</span> (y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [3, 4, 5]</span></span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/b4995feadf0e5ccf8642f301ede28c44.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># List comprehension</span></span><br><span class="line">y = [item <span class="keyword">for</span> item <span class="keyword">in</span> x <span class="keyword">if</span> item &gt; <span class="number">2</span>]</span><br><span class="line"><span class="keyword">print</span> (y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [3, 4, 5]</span></span><br></pre></td></tr></table></figure><p>下面示例一个复杂的列表推导式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Nested for loops</span></span><br><span class="line">words = [[<span class="string">"Am"</span>, <span class="string">"ate"</span>, <span class="string">"ATOM"</span>, <span class="string">"apple"</span>], [<span class="string">"bE"</span>, <span class="string">"boy"</span>, <span class="string">"ball"</span>, <span class="string">"bloom"</span>]]</span><br><span class="line">small_words = []</span><br><span class="line"><span class="keyword">for</span> letter_list <span class="keyword">in</span> words:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> letter_list:</span><br><span class="line">        <span class="keyword">if</span> len(word) &lt; <span class="number">3</span>:</span><br><span class="line">            small_words.append(word.lower())</span><br><span class="line"><span class="keyword">print</span> (small_words)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># ['am', 'be']</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Nested list comprehension</span></span><br><span class="line">small_words = [word.lower() <span class="keyword">for</span> letter_list <span class="keyword">in</span> words <span class="keyword">for</span> word <span class="keyword">in</span> letter_list <span class="keyword">if</span> len(word) &lt; <span class="number">3</span>]</span><br><span class="line"><span class="keyword">print</span> (small_words)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># ['am', 'be']</span></span><br></pre></td></tr></table></figure><h2 id="Functions">Functions</h2><p>函数是一种将可重用代码模块化的方式。它们由关键字def定义，代表definition，可以包含以下组件。<br><img src="//s3.mindex.xyz/blog/Courses/3898be6c32f04d864a2586e873e4926c.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_two</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Increase x by 2."""</span></span><br><span class="line">    x += <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>以下是使用该函数时可能需要的组件。我们需要确保函数名称和输入参数与上面定义的函数匹配。<br><img src="//s3.mindex.xyz/blog/Courses/9ca41d3f8448ba4c422a4ae26b76deee.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use the function</span></span><br><span class="line">score = <span class="number">0</span></span><br><span class="line">new_score = add_two(x=score)</span><br><span class="line"><span class="keyword">print</span> (new_score)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 2</span></span><br></pre></td></tr></table></figure><p>一个函数可以有任意多个输入参数和输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Function with multiple inputs</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join_name</span><span class="params">(first_name, last_name)</span>:</span></span><br><span class="line">    <span class="string">"""Combine first name and last name."""</span></span><br><span class="line">    joined_name = first_name + <span class="string">" "</span> + last_name</span><br><span class="line">    <span class="keyword">return</span> joined_name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the function</span></span><br><span class="line">first_name = <span class="string">"Goku"</span></span><br><span class="line">last_name = <span class="string">"Mohandas"</span></span><br><span class="line">joined_name = join_name(</span><br><span class="line">    first_name=first_name, last_name=last_name)</span><br><span class="line"><span class="keyword">print</span> (joined_name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Goku Mohandas</span></span><br></pre></td></tr></table></figure><p>在使用函数时，始终使用关键字参数是一个好习惯，这样非常清楚每个输入变量属于哪个函数输入参数。相关的是，您经常会看到术语*args和**kwargs，它们代表位置参数和关键字参数。当它们传递到函数中时，可以提取它们。星号的重要性在于任何数量的位置参数和关键字参数都可以传递到函数中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">    x = args[<span class="number">0</span>]</span><br><span class="line">    y = kwargs.get(<span class="string">"y"</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">f"x: <span class="subst">&#123;x&#125;</span>, y: <span class="subst">&#123;y&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">f(<span class="number">5</span>, y=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x: 5, y: 2</span></span><br></pre></td></tr></table></figure><h2 id="Classes">Classes</h2><p>类是对象构造函数，也是Python面向对象编程的基本组成部分。它们由一组定义类及其操作的函数组成。</p><h3 id="Magic-methods">Magic methods</h3><p>可以使用像<code>__init__</code>和<code>__str__</code>这样的魔术方法来自定义类，以实现强大的操作。这些也被称为dunder方法，表示有两个前置和后置下划线。</p><p><code>__init__</code>方法用于初始化类的实例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating the class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pet</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Class object for a pet."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, species, name)</span>:</span></span><br><span class="line">        <span class="string">"""Initialize a Pet."""</span></span><br><span class="line">        self.species = species</span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Creating an instance of a class</span></span><br><span class="line">my_dog = Pet(species=<span class="string">"dog"</span>,</span><br><span class="line">             name=<span class="string">"Scooby"</span>)</span><br><span class="line"><span class="keyword">print</span> (my_dog)</span><br><span class="line"><span class="keyword">print</span> (my_dog.name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;__main__.Pet object at 0x7fe487e9c358&gt;</span></span><br><span class="line"><span class="comment"># Scooby</span></span><br></pre></td></tr></table></figure><p><code>print(my_dog)</code>命令打印了一些与我们不太相关的内容。让我们使用__str__函数来修复它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating the class</span></span><br><span class="line"><span class="comment"># Creating the class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pet</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Class object for a pet."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, species, name)</span>:</span></span><br><span class="line">        <span class="string">"""Initialize a Pet."""</span></span><br><span class="line">        self.species = species</span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Output when printing an instance of a Pet."""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"<span class="subst">&#123;self.species&#125;</span> named <span class="subst">&#123;self.name&#125;</span>"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Creating an instance of a class</span></span><br><span class="line">my_dog = Pet(species=<span class="string">"dog"</span>,</span><br><span class="line">             name=<span class="string">"Scooby"</span>)</span><br><span class="line"><span class="keyword">print</span> (my_dog)</span><br><span class="line"><span class="keyword">print</span> (my_dog.name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># dog named Scooby</span></span><br><span class="line"><span class="comment"># Scooby</span></span><br></pre></td></tr></table></figure><h3 id="Object-functions">Object functions</h3><p>除了这些魔术方法，类还可以拥有对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating the class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pet</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Class object for a pet."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, species, name)</span>:</span></span><br><span class="line">        <span class="string">"""Initialize a Pet."""</span></span><br><span class="line">        self.species = species</span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Output when printing an instance of a Pet."""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"<span class="subst">&#123;self.species&#125;</span> named <span class="subst">&#123;self.name&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">change_name</span><span class="params">(self, new_name)</span>:</span></span><br><span class="line">        <span class="string">"""Change the name of your Pet."""</span></span><br><span class="line">        self.name = new_name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Creating an instance of a class</span></span><br><span class="line">my_dog = Pet(species=<span class="string">"dog"</span>, name=<span class="string">"Scooby"</span>)</span><br><span class="line"><span class="keyword">print</span> (my_dog)</span><br><span class="line"><span class="keyword">print</span> (my_dog.name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># dog named Scooby</span></span><br><span class="line"><span class="comment"># Scooby</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Using a class's function</span></span><br><span class="line">my_dog.change_name(new_name=<span class="string">"Scrappy"</span>)</span><br><span class="line"><span class="keyword">print</span> (my_dog)</span><br><span class="line"><span class="keyword">print</span> (my_dog.name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># dog named Scrappy</span></span><br><span class="line"><span class="comment"># Scrappy</span></span><br></pre></td></tr></table></figure><h3 id="Inheritance">Inheritance</h3><p>我们还可以使用继承来构建类的层次结构，这样我们就能够从另一个类（父类）中继承所有属性和方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span><span class="params">(Pet)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, breed)</span>:</span></span><br><span class="line">        super().__init__(species=<span class="string">"dog"</span>, name=name)</span><br><span class="line">        self.breed = breed</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"A <span class="subst">&#123;self.breed&#125;</span> doggo named <span class="subst">&#123;self.name&#125;</span>"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scooby = Dog(species=<span class="string">"dog"</span>, breed=<span class="string">"Great Dane"</span>, name=<span class="string">"Scooby"</span>)</span><br><span class="line"><span class="keyword">print</span> (scooby)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># A Great Dane doggo named Scooby</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scooby.change_name(<span class="string">"Scooby Doo"</span>)</span><br><span class="line"><span class="keyword">print</span> (scooby)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># A Great Dane doggo named Scooby Doo</span></span><br></pre></td></tr></table></figure><p>注意我们从父类Pet继承了初始化变量，如species和name。我们还继承了像change_name()这样的函数。</p><h3 id="Methods">Methods</h3><p>在类方面，有两个重要的装饰器方法需要了解：@classmethod 和 @staticmethod。我们将在下一节中学习装饰器，但这些特定的方法与类相关，因此我们将在此处介绍它们。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span><span class="params">(Pet)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, breed)</span>:</span></span><br><span class="line">        super().__init__(species=<span class="string">"dog"</span>, name=name)</span><br><span class="line">        self.breed = breed</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"<span class="subst">&#123;self.breed&#125;</span> named <span class="subst">&#123;self.name&#125;</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_dict</span><span class="params">(cls, d)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(name=d[<span class="string">"name"</span>], breed=d[<span class="string">"breed"</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_cute</span><span class="params">(breed)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span>  <span class="comment"># all animals are cute!</span></span><br></pre></td></tr></table></figure><p><code>@classmethod</code>允许我们通过传递未实例化的类本身（cls）来创建类实例。这是从对象（例如字典）创建（或加载）类的好方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create instance</span></span><br><span class="line">d = &#123;<span class="string">"name"</span>: <span class="string">"Cassie"</span>, <span class="string">"breed"</span>: <span class="string">"Border Collie"</span>&#125;</span><br><span class="line">cassie = Dog.from_dict(d=d)</span><br><span class="line">print(cassie)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Border Collie named Cassie</span></span><br></pre></td></tr></table></figure><p>一个 <code>@staticmethod</code> 可以从未实例化的类对象中调用，因此我们可以这样做：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Static method</span></span><br><span class="line">res = Dog.is_cute(breed=<span class="string">"Border Collie"</span>)</span><br><span class="line"><span class="keyword">print</span> (res)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># True</span></span><br></pre></td></tr></table></figure><h2 id="Decorators">Decorators</h2><p>回想一下，函数允许我们将代码模块化并重复使用。然而，我们经常希望在主要函数执行之前或之后添加一些功能，并且可能希望对许多不同的函数进行此操作。我们可以使用装饰器来实现这个目的，而不是向原始函数添加更多代码！</p><p>装饰器：在函数之前/之后增加处理以增强该函数。装饰器包裹在主函数周围，允许我们对输入和/输出进行操作。</p><p>假设我们有一个名为operations的函数，它将输入值x增加1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">operations</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Basic operations."""</span></span><br><span class="line">    x += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">operations(x=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 2</span></span><br></pre></td></tr></table></figure><p>现在假设我们想要在操作函数执行前后将输入 x 增加 1，为了说明这个例子，让我们假设增量必须是单独的步骤。</p><p>下面是我们通过更改原始代码来完成的方案：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">operations</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Basic operations."""</span></span><br><span class="line">    x += <span class="number">1</span></span><br><span class="line">    x += <span class="number">1</span></span><br><span class="line">    x += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">operations(x=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 4</span></span><br></pre></td></tr></table></figure><p>我们已经实现了想要的功能，但是我们现在增加了操作函数的规模，如果我们想对任何其他函数进行相同的递增操作，那么我们也必须将相同的代码添加到所有这些函数中… 这不太高效。为了解决这个问题，让我们创建一个名为<code>add</code>的装饰器，在主函数f执行之前和之后将x增加1。</p><h3 id="Creating-a-decorator">Creating a decorator</h3><p>装饰器函数接受一个函数f作为参数，这个函数是我们想要包装的函数，在本例中，它是operations()。装饰器的输出是其包装器函数，该函数接收传递给函数f的参数和关键字参数。</p><p>在包装函数中，我们可以：</p><ul><li><ol><li>提取传递给函数f的输入参数</li></ol></li><li><ol start="2"><li>对函数输入进行任何更改</li></ol></li><li><ol start="3"><li>执行函数f</li></ol></li><li><ol start="4"><li>对函数输出进行任何更改</li></ol></li><li><ol start="5"><li>wrapper函数返回一些值，这就是装饰器返回的值，因为它也返回wrapper。</li></ol></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(f)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        <span class="string">"""Wrapper function for @add."""</span></span><br><span class="line">        x = kwargs.pop(<span class="string">"x"</span>) <span class="comment"># .get() if not altering x</span></span><br><span class="line">        x += <span class="number">1</span> <span class="comment"># executes before function f</span></span><br><span class="line">        x = f(*args, **kwargs, x=x)</span><br><span class="line">        x += <span class="number">1</span> <span class="comment"># executes after function f</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure><p>我们可以通过在主函数顶部添加 <code>@</code> 符号来简单地使用这个装饰器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@add</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">operations</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Basic operations."""</span></span><br><span class="line">    x += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">operations(x=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 4</span></span><br></pre></td></tr></table></figure><p>假设我们想要调试并查看实际执行operations()函数的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">operations.__name__, operations.__doc__</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># ('wrapper', 'Wrapper function for @add.')</span></span><br></pre></td></tr></table></figure><p>函数名和文档字符串并不是我们要寻找的内容，但它们出现在这里是因为执行的是包装器函数。为了解决这个问题，Python 提供了 functools.wraps 函数来保留主函数的元数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(f)</span>:</span></span><br><span class="line"><span class="meta">    @wraps(f)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrap</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        <span class="string">"""Wrapper function for @add."""</span></span><br><span class="line">        x = kwargs.pop(<span class="string">"x"</span>)</span><br><span class="line">        x += <span class="number">1</span></span><br><span class="line">        x = f(*args, **kwargs, x=x)</span><br><span class="line">        x += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">return</span> wrap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@add</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">operations</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Basic operations."""</span></span><br><span class="line">    x += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">operations.__name__, operations.__doc__</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># ('operations', 'Basic operations.')</span></span><br></pre></td></tr></table></figure><p>太棒了！我们成功地使用装饰器来实现自定义，而不必修改函数本身。我们可以将这个装饰器用于其他需要相同自定义的函数中！</p><h3 id="Callbacks">Callbacks</h3><p>装饰器允许在主函数执行前后进行自定义操作，但是中间呢？假设我们想有条件地/有场景地执行一些操作。我们可以使用回调函数，而不是编写大量的if语句并使我们的函数臃肿。</p><p>回调：在函数内的有条件/有场景地执行任务。</p><p>我们定义一个有特殊函数的类，这些函数将在主函数执行期间的各个时期执行。 函数名称由我们决定，但我们需要在主要功能中调用相同的回调函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Callback</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XTracker</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.history = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">at_start</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.history.append(x)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">at_end</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.history.append(x)</span><br></pre></td></tr></table></figure><p>我们可以传入任意数量的回调函数，因为它们具有特定命名的函数，所以它们将在适当的时间被调用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">operations</span><span class="params">(x, callbacks=[])</span>:</span></span><br><span class="line">    <span class="string">"""Basic operations."""</span></span><br><span class="line">    <span class="keyword">for</span> callback <span class="keyword">in</span> callbacks:</span><br><span class="line">        callback.at_start(x)</span><br><span class="line">    x += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> callback <span class="keyword">in</span> callbacks:</span><br><span class="line">        callback.at_end(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = <span class="number">1</span></span><br><span class="line">tracker = XTracker(x=x)</span><br><span class="line">operations(x=x, callbacks=[tracker])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tracker.history</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [1, 2]</span></span><br></pre></td></tr></table></figure><h3 id="Putting-it-all-together">Putting it all together</h3><p>decorators + callbacks = 不增加复杂度的情况下在函数执行的前、中或后执行超强的自定义任务。 我们将在未来的中使用这个组合来创建强大的机器学习/深度学习训练脚本，这些脚本具有高度可定制性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(f)</span>:</span></span><br><span class="line"><span class="meta">    @wraps(f)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrap</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        <span class="string">"""Wrapper function for @add."""</span></span><br><span class="line">        x = kwargs.pop(<span class="string">"x"</span>) <span class="comment"># .get() if not altering x</span></span><br><span class="line">        x += <span class="number">1</span> <span class="comment"># executes before function f</span></span><br><span class="line">        x = f(*args, **kwargs, x=x)</span><br><span class="line">        <span class="comment"># can do things post function f as well</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">return</span> wrap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Callback</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XTracker</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.history = [x]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">at_start</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.history.append(x)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">at_end</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.history.append(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Main function</span></span><br><span class="line"><span class="meta">@add</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">operations</span><span class="params">(x, callbacks=[])</span>:</span></span><br><span class="line">    <span class="string">"""Basic operations."""</span></span><br><span class="line">    <span class="keyword">for</span> callback <span class="keyword">in</span> callbacks:</span><br><span class="line">        callback.at_start(x)</span><br><span class="line">    x += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> callback <span class="keyword">in</span> callbacks:</span><br><span class="line">        callback.at_end(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = <span class="number">1</span></span><br><span class="line">tracker = XTracker(x=x)</span><br><span class="line">operations(x=x, callbacks=[tracker])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tracker.history</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [1, 2, 3]</span></span><br></pre></td></tr></table></figure><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Python - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>到这里，便拥有了Way2AI路上需要的Python的必备知识。</p><p>但我们不应该止步于此，应该学习更多的Python知识以面对更大的挑战。<br><a href="https://www.python.org/" target="_blank" rel="noopener" title="python">Python官网</a> 上有关于Python这门语言全部知识。</p>]]></content>
    
    <summary type="html">
    
      The fundamentals of Python programming for AI.
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>NLP的神经网络简史</title>
    <link href="https://neo1989.net/Notes/NOTE-neural-history-of-nlp/"/>
    <id>https://neo1989.net/Notes/NOTE-neural-history-of-nlp/</id>
    <published>2023-05-13T15:08:58.000Z</published>
    <updated>2023-05-15T15:22:42.770Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>本文主要内容源于NLP神经网络发展史及<a href="https://aylien.com/blog/a-review-of-the-recent-history-of-natural-language-processing#2001neurallanguagemodels" target="_blank" rel="noopener" title="A Review of the Neural History of NLP">《A Review of the Neural History of Natural Language Processing》</a>，仅供学习探讨，如有错误均是本人的，与他人无关。</p><h2 id="2000-Neural-Language-Models">2000 - Neural Language Models</h2><p>语言建模的任务是在给定前面的词的情况下预测文本中的下一个词。这可能是最简单的语言处理任务，并且有具体的实际应用，比如智能输入法和电子邮件回复建议。语言建模有着丰富的历史，经典的方法是基于n-gram的，并采用平滑处理来解决未见n-grams的问题。</p><p>第一个神经网络语言模型 <strong>前馈神经网络</strong> 首次于2000年在文章《A Neural Probabilistic Language Model》中被提出，并收录于<a href="https://proceedings.neurips.cc/paper_files/paper/2000" target="_blank" rel="noopener" title="NIPS 2000">NIPS 2000</a>。</p><p><img src="//s3.mindex.xyz/blog/Notes/7bf9f44058ca39125ca0926b43dc0bb8.png" alt="前馈神经网络"></p><p>该神经网络语言模型的主要构成如下：</p><ul><li>输入层：输入通过查询表C得到的前n个词的向量表示。也就是如今大家熟知的词嵌入。</li><li>隐藏层：将n个词嵌入连接起来，作为隐藏层的输入。以进一步学习词之间的表示。</li><li>输出层：隐藏层的输出被提供给softmax以输出概率。</li></ul><p>该模型的主要novelty在于引入了词嵌入，这比简单的one-hot编码能提供更丰富的词义信息。这些词嵌入可以体现出词与词之间的关系，从而捕捉到更复杂的上下文关系。<br>模型通过最大似然估计来训练整个网络，找到最优的词嵌入和网络参数，使得模型可以更好地预测下一个词。<br>关于该模型的更多内容，参考<a href="https://www.ruder.io/word-embeddings-1/" target="_blank" rel="noopener" title="Word Embeddings">这篇博文</a>。</p><h2 id="2008-Multi-task-learning">2008 - Multi-task learning</h2><p>多任务学习，是一种通过在多个任务上训练模型来共享参数的一般方法。在神经网络中，多任务学习可以通过共享不同层的权重来简单实现。<br>多任务学习鼓励模型学习对许多任务都有用的表征。这对于学习通用的底层表征，聚焦模型的注意力或在训练数据有限的设置中特别有用。<br>多任务学习于2008年在文章<a href="http://machinelearning.org/archive/icml2008/papers/391.pdf" target="_blank" rel="noopener" title="A Unifield Architecture for NLP">《A Unified Architecture for Natural Language Processing》</a>中首次被应用于NLP神经网络。在他们的模型中，使用共享的lookup tables（词嵌入矩阵）实现多任务学习。如下图所示</p><p><img src="//s3.mindex.xyz/blog/Notes/b9dfb3dddfad8124c4c5314841284859.png" alt="共享词嵌入矩阵"></p><p>共享词嵌入使得模型能够协作并分享词嵌入矩阵中的底层信息，这通常构成了模型中最大数量的参数。该论文不仅使用了多任务学习，而且还推动了预训练词嵌入和使用卷积神经网络（CNNs）处理文本等思想，这些思想后来来才被广泛采用。而且这篇论文获得了<a href="https://research.facebook.com/blog/2018/07/facebook-researchers-win-test-of-time-award-at-icml-2018/" target="_blank" rel="noopener" title="the test-of-time award at ICML 2018">the test-of-time award at ICML 2018</a>。</p><h2 id="2013-Word-embeddings">2013 - Word embeddings</h2><p>文本的向量表示，在自然语言处理中有着悠久的历史。早在2001年，我们就已经看到了单词的密集向量表示或词嵌入。Mikolov等人于2013年提出的主要创新是通过去除隐藏层并近似目标来使这些单词嵌入的训练更加高效。虽然这些变化在性质上很简单，但它们与高效的word2vec实现一起，使得大规模训练单词嵌入成为可能。</p><p>Word2vec有两种不同的模型，如下图所示：连续词袋模型（CBOW）和Skip-gram模型。它们的目标不同：一个是基于周围单词预测中心单词，而另一个则相反。</p><p><img src="//s3.mindex.xyz/blog/Notes/52a8eded3320bc69bed21ce7393b708a.png" alt="Continuous bag-of-words and skip-gram architectures (Mikolov et al., 2013a; 2013b)"></p><p>虽然这些嵌入在概念上与使用前馈神经网络学习的嵌入没有区别，但是在大型语料库上进行训练使它们能够捕捉到单词之间的某些关系，例如性别、动词时态和国家-首都关系，如下图所示。</p><p><img src="//s3.mindex.xyz/blog/Notes/f091661e00c40308e7af75b2b0c81fcc.png" alt="Relations captured by word2vec (Mikolov et al., 2013a; 2013b)"></p><p>这些关系及其背后的含义引发了对词嵌入的初步兴趣，许多研究探讨了这些线性关系的起源。虽然word2vec捕获的关系具有直观和近乎神奇的特质，但后来的研究表明，word2vec本身并没有什么特别之处：词嵌入也可以通过矩阵分解进行学习，并且经过适当调整，像SVD和LSA这样的经典矩阵分解方法可以实现类似的结果。</p><h2 id="2013-Neural-networks-for-NLP">2013 - Neural networks for NLP</h2><p>2013年和2014年是神经网络模型开始在自然语言处理中得到应用的时间。三种主要类型的神经网络变得最为广泛使用：循环神经网络、卷积神经网络和递归神经网络。</p><p>循环神经网络（RNNs）是处理自然语言处理中普遍存在的动态输入序列的明显选择。但传统的<a href="https://www.dlsi.ua.es/~mlf/nnafmc/papers/elman90finding.pdf" target="_blank" rel="noopener" title="Finding Structure in Time">Vanilla RNNs（Elman, 1990）</a> 很快被经典的<a href="https://ieeexplore.ieee.org/abstract/document/6795963" target="_blank" rel="noopener" title="Long Short-Term Memory">长短期记忆网络(LSTM)（Hochreiter＆Schmidhuber, 1997）</a>所取代，后者对于梯度消失和爆炸问题更具鲁棒性。</p><p>LSTM单元如下图所示，而一个<a href="https://ieeexplore.ieee.org/abstract/document/6707742" target="_blank" rel="noopener" title="Hybrid speech recognition with Deep Bidirectional LSTM">bidirectional LSTM(Graves et al., 2013)</a>通常用于处理左右两侧的上下文。</p><p><img src="//s3.mindex.xyz/blog/Notes/134bf9c41fc64fb60f29effbc4adcca4.png" alt="An LSTM network"></p><p>随着卷积神经网络（CNNs）在计算机视觉中的广泛应用，它们也开始被应用于语言领域（<a href="https://arxiv.org/abs/1404.2188" target="_blank" rel="noopener" title="A Convolutional Neural Network for Modelling Sentences">Kalchbrenner et al., 2014</a>）。文本卷积神经网络仅在两个维度上运行，并且滤波器只需要沿时间维度移动。下图显示了NLP中使用的典型CNN。</p><p><img src="//s3.mindex.xyz/blog/Notes/32da185b5fde2ca9f37171f50682e7a2.png" alt="A convolutional neural network for text (Kim, 2014)"></p><p>卷积神经网络的一个优点是它们比循环神经网络更易于并行化，因为每个时间步的状态仅取决于局部上下文（通过卷积操作），而不像RNNs中那样依赖所有过去状态。可以使用扩张卷积来扩展具有更广泛感受野的CNNs以捕获更广泛的上下文（<a href="https://arxiv.org/abs/1610.10099" target="_blank" rel="noopener" title="Neural Machine Translation in Linear Time">Kalchbrenner et al., 2016</a>）。CNNs和LSTMs也可以组合和堆叠，并且可以使用卷积来加速LSTM。</p><p>RNNs和CNNs都将语言视为序列。然而，从语言学的角度来看，语言本质上是分层的：单词组成更高级别的短语和从句，这些短语和从句可以根据一组生成规则进行递归组合。在语言学启发下，将句子视为树而不是序列的想法引出了递归神经网络，如下图所示。</p><p><img src="//s3.mindex.xyz/blog/Notes/38990eb262a258a52ca3d53b52362337.png" alt="A recursive neural network (Socher et al., 2013)"></p><p>递归神经网络从底部向上构建序列的表示，与RNNs相比，后者是从左到右或从右到左处理句子。在树的每个节点处，通过组合子节点的表示来计算新的表示。由于树也可以被视为对RNNs施加不同的处理顺序，因此LSTMs自然而然地扩展到了树结构。</p><p>于是，不只是循环神经网络和长短时记忆网络可以扩展以处理分层结构，词嵌入亦可以基于局部上下文或语法上下文进行学习（<a href="https://aclanthology.org/P14-2050.pdf" target="_blank" rel="noopener" title="Dependency-Based Word Embeddings">Levy＆Goldberg, 2014</a>）， 语言模型可以根据句法堆栈生成单词（<a href="https://www.sciencedirect.com/science/article/abs/pii/S0028393220301500" target="_blank" rel="noopener" title="Localizing syntactic predictions using recurrent neural network grammars">Dyer et al., 2016</a>）， 而且图卷积神经网络可以在树上操作（<a href="https://arxiv.org/abs/1704.04675" target="_blank" rel="noopener" title="Graph Convolutional Encoders for Syntax-aware Neural Machine Translation">Bastings et al., 2017</a>）。</p><h2 id="2014-Seq2Seq-models">2014 - Seq2Seq models</h2><p>2014年，Sutskever等人提出了seq2seq，这是一种使用神经网络将一个序列映射到另一个序列的通用框架。在这个框架中，编码器神经网络逐个符号地处理一个句子，并将其压缩成向量表示；解码器神经网络根据编码器状态逐个预测输出符号，在每一步都以先前预测的符号作为输入，如下图所示。</p><p><img src="//s3.mindex.xyz/blog/Notes/259c165e05db3259338615a284cb7bc4.png" alt="A sequence-to-sequence model (Sutskever et al., 2014)"></p><p>机器翻译成为了该框架的杀手级应用。2016年，谷歌宣布开始用神经机器翻译模型（<a href="https://arxiv.org/abs/1609.08144" target="_blank" rel="noopener" title="Google's Neural Machine Translation System">Wu et al.，2016</a>）替换其基于短语的机器翻译模型。根据Jeff Dean的说法，这意味着用一个500行的神经网络模型代替了50万行的基于短语的机器翻译代码。</p><p>由于其灵活性，这个框架现在已成为自然语言生成任务的首选框架，不同的模型扮演编码器和解码器的角色。重要的是，解码器模型不仅可以基于序列进行条件化，还可以基于任意表示进行条件化。例如，在图像上生成标题（<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Vinyals_Show_and_Tell_2015_CVPR_paper.html" target="_blank" rel="noopener" title="A Neural Image Caption Generator">Vinyals et al., 2015</a>）（如下图所示），根据表格生成文本（<a href="https://arxiv.org/abs/1603.07771" target="_blank" rel="noopener" title="Neural Text Generation from Structured Data with Application to the Biography Domain">Lebret et al., 2016</a>），以及根据源代码更改生成描述（<a href="https://arxiv.org/abs/1704.04856" target="_blank" rel="noopener" title="A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes">Loyola et al., 2017</a>）等许多其他应用。</p><p><img src="//s3.mindex.xyz/blog/Notes/83398534fc191b613e45801767e23402.png" alt="Generating a caption based on an image (Vinyals et al., 2015)"></p><p>序列到序列学习甚至可以应用于NLP中常见的结构化预测任务，其中输出具有特定的结构。如下图简单的成分句法分析示例，可以看出输出被线性化了。神经网络已经证明了在给定足够数量的成分句法解析（<a href="https://arxiv.org/abs/1412.7449" target="_blank" rel="noopener" title="Grammar as a Foreign Language">Vinyals et al., 2015</a>）和命名实体识别（<a href="https://arxiv.org/abs/1512.00103" target="_blank" rel="noopener" title="Multilingual Language Processing From Bytes">Gillick et al., 2016</a>）训练数据的情况下直接学习生成这种线性化输出的能力。</p><p><img src="//s3.mindex.xyz/blog/Notes/1ec99e613705027ba8bb0ff275f0a45b.png" alt="Linearizing a constituency parse tree (Vinyals et al., 2015)"></p><h2 id="2015-Attention">2015 - Attention</h2><p>注意力机制（<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener" title="Neural Machine Translation by Jointly Learning to Align and Translate">Bahdanau et al., 2015</a>）是神经机器翻译（NMT）中的核心创新之一，也是使NMT模型胜过传统基于短语的机器翻译系统的关键思想。序列到序列学习的主要瓶颈在于需要将源序列的整个内容压缩成固定大小的向量。注意力通过允许解码器回顾源序列隐藏状态来缓解这个问题，然后将其作为加权平均值提供给解码器作为附加输入，如下图所示。</p><p><img src="//s3.mindex.xyz/blog/Notes/f45ebf52e4a5d70c6847755553774912.png" alt="Attention (Bahdanau et al., 2015)"></p><p>注意力机制广泛适用于任何需要基于输入的某些部分做出决策的任务，并具有潜在的实用价值。它已被应用于成分句法分析（<a href="https://arxiv.org/abs/1412.7449" target="_blank" rel="noopener" title="Grammar as a Foreign Language">Vinyals et al., 2015</a>）、阅读理解（<a href="https://arxiv.org/abs/1506.03340" target="_blank" rel="noopener" title="Teaching Machines to Read and Comprehend">Hermann et al., 2015</a>）和一次性学习（<a href="https://arxiv.org/abs/1606.04080" target="_blank" rel="noopener" title="Matching Networks for One Shot Learning">Vinyals et al., 2016</a>）等许多领域。输入甚至不需要是一个序列，而可以由其他表示形式组成，如下图所示的图像字幕生成（<a href="https://proceedings.mlr.press/v37/xuc15.html" target="_blank" rel="noopener" title="Show, Attend and Tell: Neural Image Caption Generation with Visual Attention">Xu et al., 2015</a>）。注意力机制的一个有用的副作用是通过检查注意权重来确定哪些输入部分与特定输出相关联，从而提供了对模型内部工作方式的一瞥。</p><p><img src="//s3.mindex.xyz/blog/Notes/2fad1cf2fd8986f0b2689aad1807eb75.png" alt="Visual attention in an image captioning model indicating what the model is attending to when generating the word &quot;frisbee&quot;. (Xu et al., 2015)"></p><p>注意力不仅限于查看输入序列；<strong>自注意力</strong>可以用来查看句子或文档中周围的单词，以获得更具上下文敏感性的单词表示。多层自注意是Transformer架构（<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener" title="Attention Is All You Need">Vaswani et al.,2017</a>）的核心，这是当前最先进的NMT模型。</p><h2 id="2015-Memory-based-networks">2015 - Memory-based networks</h2><p>注意力可以被看作是一种模糊记忆形式，其中记忆由模型的过去隐藏状态组成，而模型选择从记忆中检索什么。关于注意力及其与记忆的联系的更详细概述，请查看<a href="https://dennybritz.com/posts/wildml/attention-and-memory-in-deep-learning-and-nlp/" target="_blank" rel="noopener" title="Attention and Memory in Deep Learning and NLP">此文章</a>。已经提出了许多具有更明确内存的模型。它们有不同的变体，例如神经图灵机（<a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener" title="Neural Turing Machines">Graves et al., 2014</a>），记忆网络（<a href="https://arxiv.org/abs/1410.3916" target="_blank" rel="noopener" title="Memory Networks">Weston et al., 2015</a>）和端到端内存网络（<a href="https://arxiv.org/abs/1503.08895" target="_blank" rel="noopener" title="End-To-End Memory Networks">Sukhbaatar et al., 2015</a>），动态内存网络（<a href="https://arxiv.org/abs/1506.07285" target="_blank" rel="noopener" title="Ask Me Anythins: Dynamic Memory Networks for Natural Language Processing">Kumar et al., 2015</a>），神经可微分计算机（<a href="https://www.nature.com/articles/nature20101" target="_blank" rel="noopener" title="Hybrid computing using a neural network with dynamic external memory">Graves et al., 2016</a>）以及循环实体网络（<a href="https://arxiv.org/abs/1612.03969" target="_blank" rel="noopener" title="Tracking the World State with Recurrent Entity Networks">Henaff et al., 2017</a>）。</p><p>记忆通常是基于与当前状态的相似性进行访问，类似于注意力，并且通常可以进行写入和读取。模型在实现和利用内存方面存在差异。例如，端到端记忆网络多次处理输入并更新内存以实现多步推理。神经图灵机还具有基于位置的寻址功能，使它们能够学习简单的计算机程序，如排序。基于记忆的模型通常应用于需要保留信息更长时间跨度可能会有帮助的任务中，例如语言建模和阅读理解。记忆概念非常灵活：知识库或表格可以作为一种内存功能，而内存也可以根据整个输入或特定部分来填充。</p><h2 id="2018-Pretrained-language-models">2018 - Pretrained language models</h2><p>预训练的词嵌入是与上下文无关的，仅用于初始化我们模型中的第一层。后来，一系列监督任务已被用来预训练神经网络（<a href="https://arxiv.org/abs/1705.02364" target="_blank" rel="noopener" title="Supervised Learning of Universal Sentence Representations from Natural Language Inference Data">Conneau et al., 2017</a>; <a href="https://proceedings.neurips.cc/paper_files/paper/2017/hash/20c86a628232a67e7bd46f76fba7ce12-Abstract.html" target="_blank" rel="noopener" title="Learned in Translation: Contextualized Word Vectors">McCann et al., 2017</a>; <a href="https://arxiv.org/abs/1804.00079" target="_blank" rel="noopener" title="Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning">Subramanian et al., 2018</a>）。相比之下，语言模型只需要未标记的文本；因此可以扩展到数十亿个标记、新领域和新语言。<br>预先训练的语言模型最初是在2015年提出的（<a href="https://proceedings.neurips.cc/paper_files/paper/2015/hash/7137debd45ae4d0ab9aa953017286b20-Abstract.html" target="_blank" rel="noopener" title="Semi-supervised Sequence Learning">Dai＆Le, 2015</a>）；直到18年才显示它们对各种任务都有益处。<br>语言模型嵌入可以用作目标模型（<a href="https://arxiv.org/abs/1809.09795" target="_blank" rel="noopener" title="Deep contextualized word representations">Peters et al., 2018</a>）中的特征或者可以在目标任务数据上微调语言模型（<a href="https://arxiv.org/abs/1611.02683" target="_blank" rel="noopener" title="Unsupervised Pretraining for Sequence to Sequence Learning">Ramachandran et al., 2017</a>; <a href="https://arxiv.org/abs/1801.06146" target="_blank" rel="noopener" title="Universal Language Model Fine-tuning for Text Classification">Howard＆Ruder, 2018</a>）。<br>如下图所示，在许多不同任务中添加语言模型嵌入可大幅改善现有技术水平。</p><p><img src="//s3.mindex.xyz/blog/Notes/94b544d5bf32bd5298c52e477b3bcdf2.png" alt="Improvements with language model embeddings over the state-of-the-art (Peters et al., 2018)"></p><p>预训练语言模型被证明可以在使用更少数据的情况下进行学习。由于语言模型只需要无标签数据，因此它们对于标注数据稀缺的低资源语言尤其有益。</p><h2 id="2018-Large-language-models">2018 - Large language models</h2><p>如你所知，整个NLP领域发展到了LLMs时代。参考阅读<a href="https://mp.weixin.qq.com/s/_pDxwIy7Z8punWMKoJiJKQ" target="_blank" rel="noopener" title="ChatGPT核心技术的进化之路">《ChatGPT核心技术的进化之路》</a></p><h2 id="结尾">结尾</h2><p>GPT的提出标志着NLP技术的飞速发展，但对新手来说也增加了学习难度。要从头学习NLP，Claude给出了以下建议：</p><ul><li>了解NLP基础知识。包括语言建模，词嵌入，神经网络，LSTM，Transformer等基础模型和技术。这些知识可以通过阅读《自然语言处理综述》等资料得到。</li><li>实现基本NLP任务的模型。如语言建模，文本分类，问答系统等。可以从简单的RNN和LSTM模型实现，了解基本思路和步骤。</li><li>理解Transformer和GPT等新模型。阅读论文《Attention is All You Need》和《Improving Language Understanding by Generative Pre-Training》，理解其模型结构，训练方法和创新点。</li><li>学习PyTorch和TensorFlow等深度学习框架。这些框架可以让你更容易实现复杂的NLP模型，对学习NLP模型很有帮助。</li><li>了解BERT，XLNet，RoBERTa等预训练语言模型。这些模型的使用和微调技巧需要理解，可以带来很大便利。</li><li>跟进NLP最新进展。阅读ACL，EMNLP，NAACL等顶会论文，了解最新的模型，技术和数据集。这可以让你了解NLP最新研究热点和前沿方向。</li><li>多练习和实现。上手实践是学习NLP最重要的方式之一。可以实现论文中的模型，优化和改进这些模型，并在各种数据集上进行试验。这可以加深理解和提高实践能力。</li><li>多阅读优质学习资料。像《神经网络与深度学习》《动手学深度学习》等书籍，全面且深入地介绍了深度学习和NLP相关知识。这些资料对学习NLP大有帮助。</li></ul><p>总之，要系统和全面地学习NLP，需要理论与实践相结合。理解基础知识和最新研究进展，跟进理论前沿；并且要大量动手实践，通过实现和优化相关模型来加深理解。同时，也要利用优质学习资料，能够更全面和深入地学习这个领域的知识。持之以恒，NLP的学习之路才能走得更加通透。</p>]]></content>
    
    <summary type="html">
    
      了解历史背景可以帮助我们更好地理解其概念，同时也有助于掌握其未来发展方向和趋势。
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="NLP" scheme="https://neo1989.net/tags/NLP/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>LangChain | 快速释放LLMs的能力 (三)</title>
    <link href="https://neo1989.net/Notes/NOTE-langchain-3/"/>
    <id>https://neo1989.net/Notes/NOTE-langchain-3/</id>
    <published>2023-05-12T04:49:09.000Z</published>
    <updated>2023-05-27T13:58:33.795Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>本文介绍了一种方法，让LLM拥有了上网的能力。</p><h2 id="结果展示">结果展示</h2><p>如下图，问了它两个股票的价格现价，都能够正确的给出答案。</p><p><img src="//s3.mindex.xyz/blog/Notes/3c6908b694ec7ccaf268899e3e33c208.png" alt=""></p><h2 id="如何实现">如何实现</h2><p>在没有大模型之前，这个问题如何实现？</p><p>其实也简单，一点点NLP技能就可以解决。</p><p>NLP伪代码:</p><ul><li>抽取问题里的词槽，上述示例中词槽便是 stock_name: 科大讯飞</li><li>执行对应的action，也就是调用方法get_current_price(stockname=“科大讯飞”)</li><li>格式化输出</li></ul><p>那这里的问题是，在抽取阶段，你需要写很多的规则，或者自己训练一个信息抽取模型。</p><p>如今，交给LLMs就行了。</p><p>来看看LangChain是如何简化这件事情的。这里就需要用到开篇里介绍的<a href="https://python.langchain.com/en/latest/modules/agents.html" target="_blank" rel="noopener" title="LangChain Agents">Agents</a>组件了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sina <span class="keyword">import</span> Sina</span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel, Field</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain.chat_models <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> AgentType, initialize_agent</span><br><span class="line"><span class="keyword">from</span> langchain.tools <span class="keyword">import</span> BaseTool</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Type, Optional</span><br><span class="line"><span class="keyword">from</span> langchain.callbacks.manager <span class="keyword">import</span> CallbackManagerForToolRun, AsyncCallbackManagerForToolRun</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StockQueryInput</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    stock_name: str = Field()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StockPriceQueryTool</span><span class="params">(BaseTool)</span>:</span></span><br><span class="line">    name = <span class="string">"StockPriceQuery"</span></span><br><span class="line">    description = <span class="string">"useful for when you need to answer questions about the price of a stock code"</span></span><br><span class="line">    args_schema: Type[BaseModel] = StockQueryInput</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run</span><span class="params">(self, stock_name: str, run_manager: Optional[CallbackManagerForToolRun] = None)</span> -&gt; str:</span></span><br><span class="line"></span><br><span class="line">        stock_code = StockCodeMapping.get(stock_name)</span><br><span class="line">        sina = Sina(stock_code)</span><br><span class="line">        res = sina.get_current_price()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"当前时间<span class="subst">&#123;res[<span class="string">'date'</span>]&#125;</span> <span class="subst">&#123;res[<span class="string">'time'</span>]&#125;</span>，<span class="subst">&#123;res[<span class="string">'name'</span>]&#125;</span>的价格是<span class="subst">&#123;res[<span class="string">'new'</span>]&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">_arun</span><span class="params">(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None)</span> -&gt; str:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">"StockPriceQueryTool does not support async"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">tools = [StockPriceQueryTool(), ]</span><br><span class="line">agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=<span class="literal">True</span>)</span><br><span class="line">agent.run(<span class="string">"科大讯飞现在的股价是多少，什么时间的数据？"</span>)</span><br></pre></td></tr></table></figure><p>代码非常简单，核心就是继承BaseTool这个父类，以实现自定义。</p><p>打开调试模式，我们看看LangChain都干了些什么，见下图：</p><p><img src="//s3.mindex.xyz/blog/Notes/7455c3613be0d6748f24b30814cd3b7c.png" alt=""></p><p>LangChain大概的心理活动可能是：</p><ul><li>哦，任务来了，我需要提供科大讯飞的当前股价</li><li>我该找谁问呢？哦，股票的事情要问StockPriceQueryTool</li><li>StockPriceQueryTool.run(“科大讯飞”)</li><li>哦，答案来了，“当前时间2023-05-12 14:30:48，科大讯飞的价格是63.930”</li><li>这个答案完美，我组织一下语言</li></ul><p>是不是很酷？</p><h2 id="尾声">尾声</h2><p>其实笔者在LLM爆发前夜，实现了一个小型的ChatBot，利用的是NLP组合技，感兴趣的朋友看<a href="http://neo1989.net/HandMades/HANDMADE-build-a-cool-chatbot-step-by-step/" title="如何构建自己的ChatBot">这里</a></p><p>LangChain到这里就介绍完了。会不会不重要，重要的是它背后的思想。</p><p>猴子变成人，是因为学会了使用工具。既然新的工具出现了，要不要变一下？</p><p>Peace out。</p>]]></content>
    
    <summary type="html">
    
      学习使我快乐。
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>LangChain | 快速释放LLMs的能力 (二)</title>
    <link href="https://neo1989.net/Notes/NOTE-langchain-2/"/>
    <id>https://neo1989.net/Notes/NOTE-langchain-2/</id>
    <published>2023-05-09T15:18:31.000Z</published>
    <updated>2023-05-27T13:58:27.222Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>本文介绍了一种方法，让LLM拥有了学习新知识的能力。</p><h2 id="结果展示">结果展示</h2><p>LLM的训练是通过大量已有的数据进行学习，由于训练的难度和高额的成本，通常情况它是无法知道最新的时事的，于是它会胡编乱造，也就是是所谓的“幻觉”。如下图所示：<br><img src="//s3.mindex.xyz/blog/Notes/cd9244074883f119f2e21f77745dc193.png" alt="胡编乱造，但像模像样"></p><p>而通过提供资料让它学习，比如作者提供了以下两篇关于<code>星火大模型</code>的新闻稿给它学习:</p><ul><li><a href="https://new.qq.com/rain/a/20230508A01E8200" target="_blank" rel="noopener" title="腾讯网关于星火大模型的报道">腾讯网：星火认知大模型发布，科大讯飞入场科技巨头AI大战？</a></li><li><a href="https://finance.stockstar.com/IG2023050900014747.shtml" target="_blank" rel="noopener" title="证券之星关于星火大模型的报道">证券之星：科大讯飞星火认知大模型发布 AI 星火营生态计划同步开启</a></li></ul><p>于是它便能很好的回答相关的问题了。如下图所示：</p><p><img src="//s3.mindex.xyz/blog/Notes/fd3e43f90ecfa7ea380c69331c93bcfa.png" alt="它是真的在学习"></p><h2 id="如何实现">如何实现</h2><p>上一篇文章已经介绍了利用LangChain的基本操作实现了一个对任意文本进行总结的小应用。<br>本篇便是在前一篇的基础上引入了<code>embedding</code> 和 <code>vectorstore</code> 这两个核心能力来实现上述能力。</p><p>先说一下伪代码，流程如下图所示:</p><ul><li>加载文档</li><li>将文档分割成文本块</li><li>对文本块进行 <code>Embedding</code></li><li>将上述结果，也就是<code>Vectors</code>，存到向量数据库中</li><li>对用户的 <code>Query</code> 进行 <code>Embedding</code>, 生成 <code>Query Vector</code></li><li>在向量数据库中，利用向量similarity查询出 相关的文本块</li><li>将上述文本块，套上 <code>Prompt Template</code>，生成最终的<code>Prompt</code></li><li>丢给<code>LLM</code>，得到回答</li></ul><p><img src="//s3.mindex.xyz/blog/Notes/cc18b036c98679aafe101d7c47a529d5.png" alt=""></p><p>接下来看看LangChain是如何便捷的实现上述功能的。核心代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> SpacyTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings.openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> Chroma</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores.base <span class="keyword">import</span> VectorStoreRetriever</span><br><span class="line"></span><br><span class="line">doc = <span class="string">""</span></span><br><span class="line"></span><br><span class="line">text_splitter = SpacyTextSplitter(separator=<span class="string">"\n\n"</span>, pipeline=<span class="string">"zh_core_web_sm"</span>, chunk_size=<span class="number">200</span>)</span><br><span class="line">docs = text_splitter.create_documents([doc])</span><br><span class="line"></span><br><span class="line">embeddings = OpenAIEmbeddings(openai_api_version=<span class="string">'2020-11-07'</span>, model=<span class="string">"text-davinci-003"</span>)</span><br><span class="line">vectorstore = Chroma.from_documents(docs, embeddings, persist_directory=<span class="string">"vector_store"</span>)</span><br><span class="line"></span><br><span class="line">retriever = VectorStoreRetriever(vectorstore=vectorstore)</span><br><span class="line">qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=<span class="string">"refine"</span>, retriever=retriever)</span><br><span class="line"></span><br><span class="line">result = qa(&#123;<span class="string">"query"</span>: <span class="string">"星火认知大模型 是什么"</span>&#125;)</span><br><span class="line">result</span><br></pre></td></tr></table></figure><p>不超过10行的核心代码，是不是有手就会？</p><h2 id="什么是Embedding">什么是Embedding</h2><p>这里涉及到一点NLP的基础知识。</p><p>对于给定的一段自然语言文本，第一件事就是将其token化。所谓token就是分割出的一个字或一个词（中文场景）。</p><ul><li>给定：星火认知大模型是什么</li><li>按字：星/火/认/知/大/模/型/是/什/么</li><li>按词：星火/认知/大模型/是/什么</li></ul><p>当然还有其它的分割算法，这里按下不表。</p><p>Token化之后，如何表示呢，我们知道计算机只能处理数字，所以聪明的先驱们发明了利用一个包含很多小数的数组（也就是一组稠密向量）来表示文本。 Embedding 就是把文本变成向量的过程。</p><p>参考<a href="https://platform.openai.com/docs/guides/embeddings/what-are-embeddings" target="_blank" rel="noopener" title="OpenAI's text embeddings">OpenAI官方文档</a> ，Embeddings通常被用于以下场景：</p><ul><li>搜索（对结果集进行相关性排序）</li><li>聚类（对文本按照相关性进行分组）</li><li>分类（对文本按照标签相似度进行分类）</li><li>推荐（优先推荐相似度高的item）</li><li>异常检测（识别出相似度小的异常）</li><li>多样性衡量（对相似度的分布进行分析）</li></ul><p>那相似度是如何衡量的？就是计算两个向量之间的距离。距离近相关度高，距离远相关度低，就是这样。</p><h2 id="什么是VectorStore">什么是VectorStore</h2><p>顾名思义，向量数据库就是用来存储，检索，分析向量的数据库。</p><p>LangChain提供了非常多的选择，具体请<a href="https://python.langchain.com/en/latest/modules/indexes/vectorstores.html" target="_blank" rel="noopener" title="LangChain VectorStores">参考文档</a>。<br>本文示例中的<a href="https://www.trychroma.com/" target="_blank" rel="noopener" title="Chroma">Chroma</a>便是其中一种轻量级的，最近拿了$18M的种子轮。</p><h2 id="尾声">尾声</h2><p>另一个简单的示例，看起来已经可以拿去干点什么了。但LangChain的能力不止如此，我们下一篇再看。</p><p>Peace out。</p>]]></content>
    
    <summary type="html">
    
      学习使我快乐。
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>LangChain | 快速释放LLM的能力 (一)</title>
    <link href="https://neo1989.net/Notes/NOTE-langchain-1/"/>
    <id>https://neo1989.net/Notes/NOTE-langchain-1/</id>
    <published>2023-05-08T05:56:52.000Z</published>
    <updated>2023-05-27T13:58:21.131Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是LangChain">什么是LangChain</h2><p>其实就是用于开发基于LLM应用程序的开发框架。<br>一个强大的应用程序肯定不是只去调用LLM的API，于是LangChain便提供了两个核心的能力：</p><ul><li>有数据意识：将LLM与其他数据源连接起来</li><li>掌握主动权：允许LLM同运行环境交互</li></ul><p><img src="http://s3.mindex.xyz/blog/Notes/c151520a5f0042caff5ecb39519eed17.png" alt=""></p><h2 id="LangChain的组件">LangChain的组件</h2><p>LangChain提供了模块化抽象，用于处理与语言模型相关的组件。LangChain还拥有所有这些抽象的实现集合。这些组件被设计成易于使用，无论您是否使用LangChain框架的其余部分。</p><h3 id="Models">Models</h3><p>LangChain提供了各种不同类型的模型，方便集成和使用。如 LLMs、Chat Models、Text Embedding Models。</p><h3 id="Prompts">Prompts</h3><p>如你所知，跟模型打交道的方式便是prompt。通常prompt很少是硬编码的，而是通过多个组件搭建起来的。LangChain提供了多个类和方法，使的结合PromptTemplate构建prompts变得容易。</p><h3 id="Chains">Chains</h3><p>对于一些简单的应用来说，孤立地使用LLM是没有问题的，但许多更复杂的应用需要将LLM串联起来。LangChain提供了标准的接口，可以轻松的创建和管理链，以便更好的控制模型的输出。</p><h3 id="Agents">Agents</h3><p>有些应用不仅需要一个预先确定的调用LLM/其他工具的链，而且可能需要一个取决于用户输入的未知链。在这些类型的链中，有一个 “代理”，它可以访问一整套的工具。根据用户的输入，代理可以决定调用这些工具中的哪个（如果有的话）。</p><p><img src="http://s3.mindex.xyz/blog/Notes/bb20e5c953c69766556b713226d80125.png" alt=""></p><h3 id="Memory">Memory</h3><p>默认情况下，链和代理是无状态的，这意味着它们会独立处理每个传入的查询（正如底层的LLM和聊天模型）。在一些应用中（聊天机器人就是一个很好的例子），记住以前的互动是非常重要的，无论是短期的还是长期的。LangChain提供了方便的Memory组件和简单的方法以集成到应用程序中。</p><h3 id="Indexes">Indexes</h3><p>索引指的是结构化文件的方法，以便LLM能够与它们进行最好的交互。<br>使用索引通常是在检索这一步发生的。即返回与用户的输入最相关的文档。LangChain支持的主要索引和检索类型是围绕着向量数据库进行的。<br>这部分包含的模块有：</p><ul><li>Document Loaders： 文档加载器，可以从各种源头加载文档</li><li>Text Splitters: 文本分割</li><li>VectorStores：向量存储</li><li>Retrievers：检索</li></ul><h2 id="LangChain实战">LangChain实战</h2><p>基本的介绍如上，详细内容请参考<a href="https://python.langchain.com/en/latest/" target="_blank" rel="noopener" title="LangChain">官方文档</a>，LearnByDoing才是正经事。</p><p>一个很简单的需求，对任意文章进行总结并输出。核心代码及部分注释如下, 10行代码搞定长文本总结：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 相关依赖</span></span><br><span class="line"><span class="comment"># tiktoken==0.4.0</span></span><br><span class="line"><span class="comment"># openai==0.27.6</span></span><br><span class="line"><span class="comment"># langchain==0.0.161</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> SpacyTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.chains.summarize <span class="keyword">import</span> load_summarize_chain</span><br><span class="line"></span><br><span class="line">doc = <span class="string">""</span>  <span class="comment"># 文章内容</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定文本分割器，使用Spacy的中文模型</span></span><br><span class="line">text_splitter = SpacyTextSplitter(separator=<span class="string">"\n\n"</span>, pipeline=<span class="string">"zh_core_web_sm"</span>, chunk_size=<span class="number">200</span>)</span><br><span class="line">texts = text_splitter.create_documents([doc])</span><br><span class="line"></span><br><span class="line">llm = OpenAI(model_name=<span class="string">"text-davinci-003"</span>, max_tokens=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">tokens_num = llm.get_num_tokens(doc)  <span class="comment"># 计算文档需要多少token</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建总结链</span></span><br><span class="line">chain = load_summarize_chain(llm, chain_type=<span class="string">"map_reduce"</span>, verbose=<span class="literal">True</span>)</span><br><span class="line">output = chain.run(texts)</span><br><span class="line"></span><br><span class="line">output</span><br></pre></td></tr></table></figure><p>于是便可以很方便的集成进任意应用程序了。<br><img src="http://s3.mindex.xyz/blog/Notes/769492ddeaa20d8f26395bbe2ce9ffd3.png" alt="愚苏记示例"></p><h2 id="结尾">结尾</h2><p>一个简单的示例，但足以体现LLM的强大以及LangChain的便捷。接下来笔者将尝试在更复杂的应用场景下使用LangChain解决更复杂的问题，Peace out。</p>]]></content>
    
    <summary type="html">
    
      打不过就加入。
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT核心技术的进化之路</title>
    <link href="https://neo1989.net/Notes/NOTE-gpt-history/"/>
    <id>https://neo1989.net/Notes/NOTE-gpt-history/</id>
    <published>2023-04-23T08:25:31.000Z</published>
    <updated>2023-04-25T07:52:45.493Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>我们面临这样一个时代的机会。它既是机会，也是挑战。</p></blockquote><h2 id="TL-DR">TL;DR</h2><p>本文简单梳理了GPT系列的核心工作，从GPT-1到InstructGPT，帮助理解GPT的发展过程。</p><h2 id="GPT-1-2018">GPT-1 | 2018</h2><p><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener" title="GPT-1">GPT-1</a> 这篇论文提出了“Unsupervised pre-training + Supervised fine-tuning”核心架构，即首先使用大量未标记的文本数据来训练语言模型，然后使用标记数据来微调模型，以便更好地完成特定的判别性任务。</p><p>在 GPT 出现之前，NLP 的无监督预训练方法确实很长一段时间都是基于 WordVec 的模型，如 CBOW 和 SkipGram 等。这些模型主要的目的是通过在上下文中共现的词汇之间建立近似的距离关系，从而对自然语言进行表示和分析。</p><p>但词向量的模型的主要局限是一个词的向量表示没有考虑在不同的上下文环境里词的含义不同。 因此后续进化出了模型如 ELMo，使用双向 LSTM 对句子建模，得到前向和后向两种特征表示，然后将这两种特征表示拼接起来，作为最终的词向量表示，从而能够较好的处理多义词问题。</p><p>然而 ELMo 模型存在计算开销大，对上下文建模和长程依赖较弱，并且本身只能获得句子语义表示等一定的缺陷与局限性。 于是 Transformer 崛起，BERT 和 GPT 便是基于 Transformer 的杰出改进成果。</p><p>BERT 在训练过程中采用了 Masked Language Model(MLM) 和 Next Sentence Prediction(NSP) 任务使得两个方向的上下文信息保持一致，以更好的捕捉全局的语义信息。</p><p>区别于 BERT，GPT 的无监督预训练阶段，使用的是多层 Transformer Decoder，在对某个位置 i 的 token 进行预测时，利用 i 之前的所有 token 的信息输入 Transformer 进行编码，输出一个条件概率分布来预测 i 位置的 token.</p><p><img src="http://s3.mindex.xyz/blog/Notes/9cfe2357f08d51c4e2fe95f113ecefd0.png" alt="unsupervised pre-training | GPT"></p><p>在完成 GPT 的预训练，就进入有监督微调阶段。在特定的任务中使用标记数据微调模型，见下图右.</p><p><img src="http://s3.mindex.xyz/blog/Notes/cf4aa7093b4248108d916c90254f981f.png" alt="GPT summary"></p><p>概括下来，GPT-1 的核心是通过在大量未标记文本语料库上对语言模型进行生成式预训练(generative pre-training)，然后对每个特定任务进行判别性微调(discriminative fine-tuning)，来提高自然语言理解能力。</p><h2 id="GPT-2-2019">GPT-2 | 2019</h2><p><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener" title="GPT-2">GPT-2</a> 展现出的能力是在特定的数据集上训练时，能够在没有明确监督的情况下学习特定的任务，即 Zero-shot learning。GPT-2 表明在极限情况下，预训练技术能够学习直接执行任务，而不需要监督适应或者修改。</p><p>下图表明 GPT-2 在多个任务上进行了实验的结果，表明 GPT-2 只有在具有足够的容量时才能够在许多典型的任务上取得良好的零样本表现。<br><img src="http://s3.mindex.xyz/blog/Notes/45a1d879b6ffd17674d83c06e15556a7.png" alt="performance | GPT-2"></p><p>虽然 GPT-2 的零样本表现为很多任务建立了 baseline，但并不清楚 fine-tuning 的上限。于是沿着 GPT-2 的思路，诞生了 GPT-3.</p><h2 id="GPT-3-2020">GPT-3 | 2020</h2><p><a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener" title="GPT-3">GPT-3</a> 便沿着 GPT-2 的思路，思考如何让下游任务更好的适配预训练语言模型。<br>首先他们在无监督预训练阶段，加入了&quot;in-context learning&quot;。简单讲就是模型在不更新自身参数的情况下，通过在模型输入中代入新任务的描述和少量样本，就能让模型“学习”的新任务的特征。<br><img src="http://s3.mindex.xyz/blog/Notes/c74565286e6ebba3d5ebaed295769bb1.png" alt=""></p><p>GPT-3 没有使用 fine-tuning，而是在 in-context learning 阶段尝试了三种设定：Zero-sot，One-shot 以及 Few-shot。Prompt 便是在这个阶段加入的。<br><img src="http://s3.mindex.xyz/blog/Notes/25962a81939bed0c6b631ddefab0a633.png" alt=""></p><p>其次，他们使用了更多、更高质量的语料，训练出了更大的模型。<br><img src="http://s3.mindex.xyz/blog/Notes/f67772aa8935bf2630356cdcf701d30c.png" alt=""></p><p>GPT-3 一经发布，便引起了学术界产业界爱好者们前沿的广泛讨论, 列举几篇</p><ul><li><a href="https://www.huxiu.com/article/385064.html" target="_blank" rel="noopener" title="虎嗅的：与 GPT-3 对话：它的回答令人细思极恐">虎嗅的：与 GPT-3 对话：它的回答令人细思极恐</a></li><li><a href="https://zhuanlan.zhihu.com/p/334340996" target="_blank" rel="noopener" title="AI TIME：地表最强的 GPT-3，是在推理，还是胡言乱语？">AI TIME：地表最强的 GPT-3，是在推理，还是胡言乱语？</a></li><li><a href="https://mp.weixin.qq.com/s/Lp93p0sYJcw42JPG6ItEYg" target="_blank" rel="noopener" title="新智元：GPT-3 真是人工智能「核武器」吗？花 1200 万美元训练却没能通过图灵测试">新智元：GPT-3 真是人工智能「核武器」吗？花 1200 万美元训练却没能通过图灵测试</a></li></ul><p>研究者们也在论文中提出了一些关于 GPT-3 的担忧。</p><ul><li>语言模型的滥用</li><li>公平性、偏见上的挑战</li><li>资源消耗巨大</li></ul><p>当然，困难是不会打倒开拓者们的。</p><h2 id="InstructGPT-2022">InstructGPT | 2022</h2><p><a href="https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf" target="_blank" rel="noopener" title="InstructGPT">InstructGPT</a> 的核心是如何让 GPT 生成的回答更符合人类的需求。而该模型引入了一种名为 <a href="https://arxiv.org/abs/2204.05862" target="_blank" rel="noopener" title="RLHF">RLHF</a> 的方法来进行微调。结果表明，InstructGPT 模型的输出比 GPT-3 更受欢迎，而且更真实，更少的有毒输出。利用 RLHF 微调，是使语言模型与人类意图保持一致的一个有前途的方向。</p><p>概括起来就三个步骤</p><ul><li>预训练一个语言模型 (LM)，并在高质量 prompt 数据集上进行有监督微调</li><li>聚合对比数据并训练一个奖励模型 (Reward Model，RM)</li><li>用强化学习 (RL) 方式微调 LM</li></ul><p><img src="http://s3.mindex.xyz/blog/Notes/73e84a5630105ccd9d034ce8a24d943b.png" alt=""></p><p>于是，GPT 便朝着人类期望的方向不断的进化了。</p><h2 id="ChatGPT-2022-末">ChatGPT | 2022 末</h2><p>2022 年 11 月 30 日，OpenAI 发布了一个通过由 GPT-3.5 系列 LLM 微调而成的全新对话式 AI 工具 ChatGPT，掀起了人工智能的热潮。</p><h2 id="拐点已至-2023">拐点已至 | 2023</h2><ul><li><p>OpenAI 获得微软投资 100 亿美元</p></li><li><p><a href="https://www.microsoft.com/en-gb/bing" target="_blank" rel="noopener" title="New Bing 发布">Feb 7, New Bing 发布</a></p></li><li><p><a href="https://openai.com/research/gpt-4" target="_blank" rel="noopener" title="GPT-4 发布">Mar 14, GPT-4 发布</a></p></li><li><p><a href="https://www.anthropic.com/index/introducing-claude" target="_blank" rel="noopener" title="Claude 发布">Mar 14, Claude 发布</a></p></li><li><p><a href="https://github.com/features/preview/copilot-x" target="_blank" rel="noopener" title="Copilot-X 发布">Mar 22, Copilot-X 发布</a></p></li><li><p><a href="https://openai.com/blog/chatgpt-plugins" target="_blank" rel="noopener" title="ChatGPT-plugins 发布">Mar 23, ChatGPT-plugins 发布</a></p></li></ul><h2 id="One-More-Thing">One More Thing</h2><p>文章开头引用来自于陆奇最新演讲<a href="https://mp.weixin.qq.com/s/_ZvyxRpgIA4L4pqfcQtPTQ" target="_blank" rel="noopener" title="我的大模型世界观">《我的大模型世界观》</a> ，利用AI总结了以下几点take-away:</p><ul><li>大模型时代的发展速度非常快，甚至他自己都跟不上</li><li>OpenAI在大模型领域处于领先地位，并且未来有可能比Google更大</li><li>未来是一个模型无处不在的时代</li><li>大模型时代对每个人都将产生深远和系统性影响，每个人很快将有副驾驶员</li><li>陆奇认为，创业公司基本上有三类：数字化技术、满足人类需求和改变世界</li></ul><p>如果您还未能将使用ChatGPT等AI作为习惯，我感到非常遗憾。</p><p>如果您还没有使用 ChatGPT 的渠道，关注这个公众号👇并打招呼，可以获取使用入口。</p>]]></content>
    
    <summary type="html">
    
      打不过就加入。
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="ChatGPT" scheme="https://neo1989.net/tags/ChatGPT/"/>
    
  </entry>
  
  <entry>
    <title>日落收集</title>
    <link href="https://neo1989.net/SeizeTheDay/COLLECTION-sunsets/"/>
    <id>https://neo1989.net/SeizeTheDay/COLLECTION-sunsets/</id>
    <published>2023-03-04T14:34:15.000Z</published>
    <updated>2023-06-08T15:40:34.403Z</updated>
    
    <content type="html"><![CDATA[<h3 id="May-20-2023">May 20, 2023</h3><p><img src="//s3.mindex.xyz/blog/Notes/a8dcfd31636d678b6bc786345a7342a9.png" alt="西湖·太子湾公园 | 浙江"></p><h3 id="May-13-2023">May 13, 2023</h3><p><img src="//s3.mindex.xyz/blog/Notes/02be63a08b4d3f50fe9f022c13f21291.png" alt="清水路·环湖大道 | 苏州"></p><h3 id="Apr-15-2023">Apr 15, 2023</h3><p><img src="//s3.mindex.xyz/blog/Notes/97765437415756031a4b064691e3fc5e.png" alt="清水路·环湖大道 | 苏州"></p><h3 id="Mar-11-2023">Mar 11, 2023</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/d1c296dc199be5df7087034673e4267a.png" alt="On the Rock @ NamPhrae | Thailand"></p><h3 id="Mar-10-2023">Mar 10, 2023</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/3454d0d46192236de9293e1261e6a65c.png" alt="Route 107 | Thailand"></p><h3 id="Mar-4-2023">Mar 4, 2023</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/7d8d79718363def5e1f35fc64c131589.png" alt="米堆山 | 苏州"></p><h3 id="Jan-30-2023">Jan 30, 2023</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/cb9d279f773c498fb81893d84e3801a1.png" alt="滨江中路 | 上海"></p><h3 id="Nov-8-2022">Nov 8, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/2e7efec1c357cc31f2d9d9111263fcf6.png" alt="自家楼顶 | 上海"></p><h3 id="Oct-29-2022">Oct 29, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/480778a07a8ad62f4667758d7b9f1838.png" alt="自家阳台 | 上海"></p><h3 id="Oct-21-2022">Oct 21, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/60a5da12bed4e3378b8b06b131961ada.png" alt="望金路 | 苏州"></p><h3 id="Oct-12-2022">Oct 12, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/ea79b3cb30dbbd25b2a8f50cdc6e787e.png" alt="隔离酒店 | 上海"></p><h3 id="Oct-3-2022">Oct 3, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/a64244c75a29eb7f5229965d1019dd9f.png" alt="新巴尔虎左旗 | 呼伦贝尔"></p><h3 id="Sept-9-2022">Sept 9, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/588fe20a7f83e4ebea9248bffc3c02cd.png" alt="正大立方大厦 | 上海"></p><h3 id="Aug-12-2022">Aug 12, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/256ec6bd4be58d507b83a7674bbf3e5d.png" alt="陆家嘴 | 上海"></p><h3 id="Jul-2-2022">Jul 2, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/5b3a941292b14a6ed57e5d8472e4d166.png" alt="淀峰村 | 上海"></p><h3 id="Feb-26-2022">Feb 26, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/ed0aed30bd1f5eb7e812f6b910e2d272.png" alt="广富林郊野公园 | 上海"></p><h3 id="Feb-5-2022">Feb 5, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/252a930e286996cb70257dc33cdfd179.png" alt="申嘉湖高速 | 嘉兴"></p><h3 id="Dec-19-2021">Dec 19, 2021</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/20658e52339f61f6fa2cacc6378f1597.png" alt="望金路 | 苏州"></p><h3 id="Oct-23-2021">Oct 23, 2021</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/096adcde8f85c1e11e0935ec5c1c33b8.png" alt="广富林郊野公园 | 上海"></p><h3 id="Oct-5-2021">Oct 5, 2021</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/431a40cf0e17f31eca88b6d183bf0977.png" alt="小柴旦湖 | 海西"></p><h3 id="Oct-2-2021">Oct 2, 2021</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/c0d17cf3a1e1d0ce86aef91b63b49166.png" alt="黑独山 | 海西"></p><h3 id="Sept-21-2021">Sept 21, 2021</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/8150b4294afbf75dce10bd0e35c53328.png" alt="望金路 | 苏州"></p><h3 id="Sept-5-2021">Sept 5, 2021</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/cfe9e81ec7bb7732d7c37059e7719356.png" alt="望金路 | 苏州"></p><h3 id="Oct-5-2020">Oct 5, 2020</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/d9332c6fae74c9383402562a60efd961.png" alt="石头公园 | 文昌"></p><h3 id="Aug-30-2020">Aug 30, 2020</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/bf9d8d69f72adfa310a5e43609ffe447.png" alt="家附近 | 上海"></p><h3 id="Jul-28-2019">Jul 28, 2019</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/74b15296714f1663e085455cf6203a36.png" alt="欢乐谷 | 上海"></p><h3 id="Jun-16-2019">Jun 16, 2019</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/6c4c12c624df285655f099d06dc25497.png" alt="中谭路 | 上海"></p><h3 id="Apr-27-2019">Apr 27, 2019</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/69ab3d05f731687f45ddbfb7a101b883.png" alt="直岛 | 日本"></p><h3 id="Sept-9-2018">Sept 9, 2018</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/39920b32a2d354c61351c98100d5d341.png" alt="松江新城 | 上海"></p><h3 id="Jul-14-2018">Jul 14, 2018</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/1d03b9cc2dad2f2a729a619a34d9ea14.png" alt="中谭路 | 上海"></p><h3 id="May-11-2017">May 11, 2017</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/5722cafa310b4aeb6aedcf4dbbe07543.png" alt="Baiyoke Tower | Bankok"></p><h3 id="May-4-2017">May 4, 2017</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/75728df9ad538b27fdaf54abf50d6534.png" alt="Lanta | Krabi"></p><h3 id="May-3-2017">May 3, 2017</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/a90e458e957d32e3bcc8a54fe7870ccc.png" alt="Lanta | Krabi"></p><h3 id="May-1-2017">May 1, 2017</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/6bcd6f83a888c1beb19625c29e47f7d3.png" alt="Patong | Phuket"></p><h3 id="Apr-22-2017">Apr 22, 2017</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/15dc820477de2bc560c2f4495f8f913e.png" alt="陆家嘴金融广场 | 上海"></p><h3 id="Oct-5-2016">Oct 5, 2016</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/70da8fd8fa5ef68c87b457bc1165b69e.png" alt="家楼顶 | 官港"></p><h3 id="Mar-31-2016">Mar 31, 2016</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/bd759acc6ffdcad5637434f94911b3ea.png" alt="曾厝垵 | 厦门"></p><h3 id="Mar-6-2016">Mar 6, 2016</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/eb64a0ff6ac66640838a0eea11eaa59d.png" alt="康桥 | 上海"></p><h3 id="Feb-20-2016">Feb 20, 2016</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/dbeaec79d916ce1f60a75f3afef56d69.png" alt="淮海中路 | 上海"></p><h3 id="Jan-25-2016">Jan 25, 2016</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/27e5876049c7553a7cd3062d692679e0.png" alt="浦软 | 上海"></p><h3 id="Sept-24-2015">Sept 24, 2015</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/1e9de7867582054def65e0a6e12c337c.png" alt="浦软 | 上海"></p><h3 id="Jan-15-2014">Jan 15, 2014</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/a87b05e7aeeb54bc490bc283fbec81af.png" alt="创智天地 | 上海"></p><h3 id="Oct-3-2013">Oct 3, 2013</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/3fd71e4380cd0024aa7967dcbe0b3c1a.png" alt="阳朔 | 桂林"></p><h3 id="May-11-2013">May 11, 2013</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/825ec397bfc4623828ed3eb7566ead05.png" alt="广储门 | 扬州"></p><h3 id="Mar-3-2013">Mar 3, 2013</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/9614bee33966b93edfdaebd9ba865da8.png" alt="世纪公园 | 上海"></p>]]></content>
    
    <summary type="html">
    
      日落尤其温柔，人间皆是浪漫。
    
    </summary>
    
    
      <category term="SeizeTheDay" scheme="https://neo1989.net/categories/SeizeTheDay/"/>
    
    
  </entry>
  
  <entry>
    <title>A Demo of the CNN LSTM</title>
    <link href="https://neo1989.net/HandMades/HANDMADE-CNN-LSTM-try/"/>
    <id>https://neo1989.net/HandMades/HANDMADE-CNN-LSTM-try/</id>
    <published>2023-02-15T02:00:17.000Z</published>
    <updated>2023-04-25T07:55:44.420Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>The moving square video prediction problem is contrived to demonstrate the CNN LSTM. The problem involves the generation of a sequence of frames. In each image a line is drawn from left to right or right to left. Each frame shows the extension of the line by one pixel. The task is for the model to classify whether the line moved left or right in the sequence of frames.Technically, the problem is a sequence classification problem framed with a many-to-one prediction model.</p></blockquote><p>&quot;Moving Square Video Prediction&quot;是<a href="https://machinelearningmastery.com/lstms-with-python/" target="_blank" rel="noopener">《Long Short-Term Memory Networks With Python》</a> 这本书里的一个示例。我在这里做了一下扩展，将其变成一个多分类问题。</p><h3 id="The-Problem">The Problem</h3><p>问题定义为一个帧序列，从边缘开始每多一帧就增加一个像素点，以展示朝某个方向延伸的一条线（从上到下，从下到上，从左到右，从右到左）。<br>模型的任务就是预测这条线是如何运动的。<br>很明显，这就是一个many-to-one的分类任务。输入帧序列，输出单个标签（<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">图片来源</a>）。</p><p><img src="//s3.mindex.xyz/blog/Notes/faf00c2f0f3277b8c0c8e95a7d504552.png" alt="Recurrent Neural Networks"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 代码示例</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint, random, choice</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"></span><br><span class="line">directions = [<span class="string">"Up"</span>, <span class="string">"Down"</span>, <span class="string">"Left"</span>, <span class="string">"Right"</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_frame</span><span class="params">(last_step, last_frame, row=None, col=None)</span>:</span></span><br><span class="line">    lower = max(<span class="number">0</span>, last_step<span class="number">-1</span>)</span><br><span class="line">    upper = min(last_frame.shape[<span class="number">0</span>]<span class="number">-1</span>, last_step+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    step = randint(lower, upper)</span><br><span class="line"></span><br><span class="line">    frame = last_frame.copy()</span><br><span class="line">    <span class="keyword">if</span> row <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        frame[row, step] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> col <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        frame[step, col] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> frame, step</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_frames</span><span class="params">(size)</span>:</span></span><br><span class="line">    frames = list()</span><br><span class="line"></span><br><span class="line">    frame = np.zeros((size, size))</span><br><span class="line">    step = randint(<span class="number">0</span>, size<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    towards = choice(directions)</span><br><span class="line">    <span class="keyword">if</span> towards <span class="keyword">in</span> [<span class="string">"Up"</span>, <span class="string">"Down"</span>]:</span><br><span class="line"></span><br><span class="line">        down = <span class="number">1</span> <span class="keyword">if</span> towards == <span class="string">"Down"</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        row = <span class="number">0</span> <span class="keyword">if</span> down <span class="keyword">else</span> size<span class="number">-1</span></span><br><span class="line">        frame[row, step] = <span class="number">1</span></span><br><span class="line">        frames.append(frame)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, size):</span><br><span class="line">            row = i <span class="keyword">if</span> down <span class="keyword">else</span> size<span class="number">-1</span>-i</span><br><span class="line">            frame, step = next_frame(step, frame, row=row)</span><br><span class="line">            frames.append(frame)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        right = <span class="number">1</span> <span class="keyword">if</span> towards == <span class="string">"Right"</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        col = <span class="number">0</span> <span class="keyword">if</span> right <span class="keyword">else</span> size<span class="number">-1</span></span><br><span class="line">        frame[step, col] = <span class="number">1</span></span><br><span class="line">        frames.append(frame)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, size):</span><br><span class="line">            col = i <span class="keyword">if</span> right <span class="keyword">else</span> size<span class="number">-1</span>-i</span><br><span class="line">            frame, step = next_frame(step, frame, col=col)</span><br><span class="line">            frames.append(frame)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> frames, towards</span><br><span class="line"></span><br><span class="line">size = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">frames, towards = build_frames(size)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"Towards: <span class="subst">&#123;towards&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">pyplot.figure(figsize=[<span class="number">8</span>, <span class="number">8</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(size):</span><br><span class="line">    pyplot.subplot(size // <span class="number">10</span>, <span class="number">10</span>, i+<span class="number">1</span>)</span><br><span class="line">    pyplot.imshow(frames[i], cmap=<span class="string">'Greys'</span>)</span><br><span class="line">    ax = pyplot.gca()</span><br><span class="line">    ax.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">    ax.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Notes/973e4cf02597baebd977e10eca8a6ce9.png" alt="显示运动方向为向上 (从左上到右下逐帧)"></p><h3 id="CNN-LSTM">CNN LSTM</h3><p>使用CNN层对输入数据进行特征提取, 使用LSTM来做序列预测。<br>这种架构还被用于语音识别和自然语言处理问题，其中CNN被用作音频和文本输入数据上的特征提取器，以供LSTM使用。<br>此架构适用于以下问题：</p><ul><li>在其输入中具有空间结构，例如 2D 结构或图像中的像素或句子，段落或文档中的单词的一维结构。</li><li>在其输入中具有时间结构，诸如视频中的图像的顺序或文本中的单词，或者需要生成具有时间结构的输出，诸如文本描述中的单词。</li></ul><p><img src="//s3.mindex.xyz/blog/Notes/e018c5595051507361a05e2ecb6aa01d.png" alt="卷积神经网络长短期记忆网络架构"></p><h3 id="generate-examples">generate_examples</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder, OneHotEncoder</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># label encode</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">label_encoded = label_encoder.fit_transform(directions)</span><br><span class="line">label_encoded = label_encoded.reshape(len(directions), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># one hot encode</span></span><br><span class="line">onehot_encoder = OneHotEncoder(sparse=<span class="literal">False</span>)</span><br><span class="line">onehot_encoder.fit(label_encoded)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_examples</span><span class="params">(size, n_patterns)</span>:</span></span><br><span class="line">    X, y = list(), list()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_patterns):</span><br><span class="line">        frames, towards = build_frames(size)</span><br><span class="line">        X.append(frames)</span><br><span class="line">        y.append(towards)</span><br><span class="line">    X = np.array(X).reshape(n_patterns, size, size, size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    label_encoded = label_encoder.transform(np.array(y))</span><br><span class="line">    y = onehot_encoder.transform(label_encoded.reshape(len(label_encoded), <span class="number">1</span>)) </span><br><span class="line">    <span class="keyword">return</span> X, y</span><br></pre></td></tr></table></figure><h3 id="CNN-Model">CNN Model</h3><p>The Conv2D will interpret snapshots of the image (e.g. small squares) and the pooling layers will consolidate or abstract the interpretation.<br>We will define a Conv2D as an input layer with 2 filters and a 2 × 2 kernel to pass across the input images. The use of 2 filters was found with some experimentation and it is convention to use small kernel sizes. The Conv2D will output 2 49 × 49 pixel impressions of the input.</p><p>Convolutional layers are often immediately followed by a pooling layer. Here we use a MaxPooling2D pooling layer with a pool size of 2 × 2, which will in effect halve the size of each filter output from the previous layer, in turn outputting two 24 × 24 maps.</p><p>The pooling layer is followed by a Flatten layer to transform the [24,24,2] 3D output from the MaxPooling2D layer into a one-dimensional 1,152 element vector…</p><p>We want to apply the CNN model to each input image and pass on the output of each input image to the LSTM as a single time step.<br>We can achieve this by wrapping the entire CNN input model (one layer or more) in a TimeDistributed layer.</p><p>Next, we can define the LSTM elements of the model. We will use a single LSTM layer with 50 memory cells, configured after a little trial and error. The use of a TimeDistributed wrapper around the whole CNN model means that the LSTM will see 50 time steps, with each time step presenting a 1,152 element vector as input.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> MaxPooling2D</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Flatten</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> TimeDistributed</span><br><span class="line"></span><br><span class="line">size = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># define the model</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(TimeDistributed(Conv2D(<span class="number">2</span>, (<span class="number">2</span>, <span class="number">2</span>), activation=<span class="string">'relu'</span>), input_shape=(<span class="literal">None</span>, size, size, <span class="number">1</span>)))</span><br><span class="line">model.add(TimeDistributed(MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>))))</span><br><span class="line">model.add(TimeDistributed(Flatten()))</span><br><span class="line">model.add(LSTM(<span class="number">50</span>))</span><br><span class="line">model.add(Dense(len(directions), activation=<span class="string">'softmax'</span>))</span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'acc'</span>])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Notes/22c2c8bfed7763a934fe6044eefac0e8.png" alt="Model Summary"></p><h3 id="fit-and-evaluate-the-model">fit and evaluate the model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fit model</span></span><br><span class="line">X, y = generate_examples(size, <span class="number">5000</span>)</span><br><span class="line">model.fit(X, y, batch_size=<span class="number">32</span>, epochs=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate model</span></span><br><span class="line">X, y = generate_examples(size, <span class="number">100</span>)</span><br><span class="line">loss, acc = model.evaluate(X, y, verbose=<span class="number">0</span>)</span><br><span class="line">print(<span class="string">f"loss: <span class="subst">&#123;loss:<span class="number">.10</span>f&#125;</span> acc: <span class="subst">&#123;acc:<span class="number">.10</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Notes/f09f8a2296dd060ce612753e82a28c1f.png" alt="fit &amp; evaluate"></p><h3 id="prediction">prediction</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prediction on new data</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    X, y = generate_examples(size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    yhat = model.predict(X, verbose=<span class="number">0</span>)</span><br><span class="line">    print(<span class="string">f"predict_i: <span class="subst">&#123;i&#125;</span>"</span>)</span><br><span class="line">    print(label_encoder.inverse_transform([np.argmax(y[<span class="number">0</span>, :])]), y)</span><br><span class="line">    print(label_encoder.inverse_transform(np.array([np.argmax(yhat[<span class="number">0</span>, :])])), yhat)</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Notes/f757cfdf47d2ecf0da2a4fe6c9a09e38.png" alt="predictions"></p><h3 id="Further-Reading">Further Reading</h3><ul><li><a href="https://keras.io/api" target="_blank" rel="noopener">Keras API.</a></li><li><a href="https://arxiv.org/abs/1411.4389" target="_blank" rel="noopener">Long-term Recurrent Convolutional Networks for Visual Recognition and Description, 2015.</a></li><li><a href="https://arxiv.org/abs/1411.4555" target="_blank" rel="noopener">Show and Tell: A Neural Image Caption Generator, 2015.</a></li><li><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43455.pdf" target="_blank" rel="noopener">Convolutional, Long Short-Term Memory, fully connected Deep Neural Networks, 2015.</a></li><li><a href="https://arxiv.org/abs/1508.06615" target="_blank" rel="noopener">Character-Aware Neural Language Models, 2015.</a></li><li><a href="https://arxiv.org/abs/1506.04214" target="_blank" rel="noopener">Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting, 2015.</a></li></ul>]]></content>
    
    <summary type="html">
    
      “只要学不死，就往死里学。”
    
    </summary>
    
    
      <category term="HandMades" scheme="https://neo1989.net/categories/HandMades/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="DL" scheme="https://neo1989.net/tags/DL/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>《为什么没人早点告诉我》</title>
    <link href="https://neo1989.net/Notes/NOTE-why-has-nobody-told-me-this-before/"/>
    <id>https://neo1989.net/Notes/NOTE-why-has-nobody-told-me-this-before/</id>
    <published>2023-02-13T01:59:49.000Z</published>
    <updated>2023-02-14T01:28:06.952Z</updated>
    
    <content type="html"><![CDATA[<h3 id="就是开心不起来怎么办">就是开心不起来怎么办</h3><h4 id="如何看待情绪低落">如何看待情绪低落</h4><ul><li>情绪有起伏波动是狠正常的，没有人能一直开兴。但我们不能被情绪控制，而应该去做一些有帮助的事情。</li><li>情绪低落并不是大脑出了问题，更可能是因为需求没有得到满足。</li><li>我们生活中的每一刻都可以拆解成体验的不同方面。</li><li>这些方面相互影响，向我们展示了我们是如何陷入情绪低落（甚至是抑郁）的恶性循环的。</li><li>我们的情绪是通过那些我们能影响的事情构建的。</li><li>情绪没有开关，我们也无法选择情绪，但我们可以利用可控的东西来改变自己的感受。</li><li>使用十字概念化这个工具（如下图所示）来培养觉察能力，注意哪些因素会影响我们的情绪，让我们深陷其中。</li></ul><p><img src="//s3.mindex.xyz/blog/Notes/87e1ba72e2112f5c0b54a8843a4b41a7.png" alt="纠结于消极的想法很可能会让人情绪低落，而情绪低落又会催生出更多消极的想法。这张图很好地说明了我们是如何陷入情绪低落的恶性循环，同时也告诉我们，如何才能走出困境。"></p><h4 id="当心情绪陷阱">当心情绪陷阱</h4><ul><li>思维偏差是不可避免的，但对其负面影响，我们并不是无能为力。</li><li>我们会自然而然地寻找证据来证实自己的看法，并坚定地相信它，尽管有很多其他证据表明这种看法并不正确。</li><li>情绪低落是由什么引起的，都会让我们将注意力集中在威胁与消极因素上。</li><li>如果我们持续关注这些因素，并把它们当作事实，那么这种负面偏见就会中庸到我们身上，加剧情绪低落。</li><li>对抗这种恶性循环的策略就是要弄明白，感受并不能作为证据，它不能证明你的想法就是事实。</li><li>另一种策略是保持好奇、探究的态度。</li><li>通过了解常见的思维偏差类型，和这些想法保持距离，注意它们可能会在什么时候出现，时刻记住它们只是偏见，不是事实。</li></ul><p><img src="//s3.mindex.xyz/blog/Notes/ee25c3fe40343a79901e40c4edefbebe.png" alt="思维偏差的种类及实例"></p><h4 id="怎么做才有用">怎么做才有用</h4><ul><li>我们无法控制突然出现在脑海中的想法，但我们可以控制注意力的聚焦点。</li><li>试图不去想某件事，只会让你想得更多。</li><li>允许所有想法的存在，但要确定哪些想法是值得投入时间和精力的，这对我们的情感体验有很大的影响。</li><li>正念练习与感恩练习能够训练我们转移注意力的能力。</li><li>当我们专注与一个问题的时候，也要关注我们前进的方向，以及我们想要如何感受，如何行动。</li><li>想法不是事实，想法只是大脑给出的意见，帮助我们理解这个世界。</li><li>一个想法对我们能有多大影响，取决于我们在多大程度上相信它是事实。</li><li>要从想法中汲取力量，就需要后退异步，与它保持一定的距离（元认知策略：注意到你的脑海中出现了哪些想法，并观察它们给你带来的感受的过程。），看清它的真实面目。</li></ul><h4 id="如何把糟糕的一天变成美好的一天">如何把糟糕的一天变成美好的一天</h4><ul><li>我们应该专注于做出好的决定，而不是完美的决定。以“足够好”为标准，会引导你做出真正的改变。完美主义会导致你瞻前顾后，难以做出选择，而要想改善情绪，你必须做出决定，采取行动。</li><li>改变可以从小事做起，要持之以恒。</li><li>别人情绪低落时，我们通常会表现得友好、体贴，因为我们知道这正是它们所需要的。所以当我们努力调节情绪和心理状态时，也应该练习自我关怀。</li><li>明白了这些，你就能以此为出发点，找到自己想要的方向，并专注于脚下的路。</li></ul><h4 id="防御，让你不被打倒的力量">防御，让你不被打倒的力量</h4><ul><li>守护心理健康的“守门员”为身心健康打下了基础。如果你每天都能照顾好它们，它们一定会给你丰厚的回报。</li><li>如果你今天只想给自己安排一件事，那就去运动吧。选择你喜欢的运动，这样更容易坚持。</li><li>睡眠与心理健康是相互作用的。高质量的睡眠对心理健康有益，改变心理状态也会促进睡眠。</li><li>你给大脑充电的方式会影响你的感受。研究表明，传统的<a href="https://zh.wikipedia.org/zh-hans/%E5%9C%B0%E4%B8%AD%E6%B5%B7%E9%A3%B2%E9%A3%9F" target="_blank" rel="noopener">地中海饮食</a>、<a href="https://zh.wikipedia.org/zh-hans/%E6%97%A5%E6%9C%AC%E6%96%99%E7%90%86" target="_blank" rel="noopener">日本饮食</a>与<a href="https://zh.wikipedia.org/zh-hans/%E6%8C%AA%E5%A8%81%E9%A3%B2%E9%A3%259https://zh.wikipedia.org/zh-hans/%E6%8C%AA%E5%A8%81%E9%A3%B2%E9%A3%9FF" target="_blank" rel="noopener">挪威饮食</a>对心理健康有益。</li><li>人与人的连接是强化复原力的有力工具。人际关系会改变你的身心状态。</li></ul><h3 id="做事提不起精神，没有动力怎么办">做事提不起精神，没有动力怎么办</h3><h4 id="理解驱动力">理解驱动力</h4><ul><li>驱动力并不是与生俱来的。</li><li>那种充满动力，想要去做某事的感觉不会一直存在，所以你不能依赖它。</li><li>要掌控你的驱动力，就是培养这样一种能力：无论你有多么不想做，你也会自动去做那些对你最重要的事。</li><li>拖延症通常是为了逃避压力和不适感。</li><li>快感缺失指的是我们现在无法从过去喜欢做的事中找到乐趣，通常和情绪低落、抑郁症有关。</li><li>如果有些事对你很重要，同时又对你的健康有益，那么现在就去做，不要等到你想做的时候才行动。</li></ul><h4 id="如何培养动机感">如何培养动机感</h4><ul><li>虽然我们无法控制动机感，但我们可以做一些事情来让自己更多地体验到动机感。</li><li>要培养动机感，得先让身体动起来。哪怕是少量的运动，也比完全不运动好，能让你充满动力。</li><li>与目标保持连接有助于持续激发驱动力。</li><li>从微小而持续的行动开始。</li><li>在压力环境中学会休息，补充能量，能最大程度地锻炼意志力。</li><li>羞耻感并不像你想的那样，能够让你产生驱动力。你要做的是改变你与失败的关系。</li></ul><h4 id="不想做一件事时，怎样才能让自己去做呢">不想做一件事时，怎样才能让自己去做呢</h4><ul><li>驱动力不是永远存在的。</li><li>我们可以练习与冲动相反的行为，我们要按照自己的价值观做事，而不是根据当下的感受。</li><li>只要重复的次数足够多，一个新的行为就能成为习惯。</li><li>要想实现远大的目标，在前进的道路上就必须休息、充电，就像优秀的运动员那样，这非常重要。</li><li>在实现目标的过程中，不断给自己小小的奖励。</li></ul><p><img src="//s3.mindex.xyz/blog/Notes/31cfde9c9f710898c3d6388cf1fd70f3.png" alt="做出改变与维持现状的利弊"></p><h4 id="重大的人生改变，应该从哪里开始">重大的人生改变，应该从哪里开始</h4><ul><li>我们有时候并不清楚应该改变什么以及如何去改变。</li><li>没有对自我的理解，就无法改变。</li><li>彻底了解你的问题所在，才能更容易确定下一步应该怎么做。</li><li>事情发生后，先反思一下。</li><li>准备好诚实地面对：你是如何导致问题的出现，又是如何让自己陷入困境的。</li><li>心理治疗的过程能为你提供支持，如果你没机会看心理医生，也可以从记日记开始。</li></ul><h3 id="陷入痛苦情绪怎么办">陷入痛苦情绪怎么办</h3><h4 id="让情绪全部消失">让情绪全部消失</h4><ul><li>情绪既不是敌人，也不是朋友。</li><li>我们对自己的情绪状态的影响力笔我们以为的要大的很多。</li><li>抗拒情绪只会带来更多问题，我们不如接纳情绪，顺其自然。</li><li>情绪不是事实，只是一个可能的视角。</li><li>如果你现在有痛苦的情绪，保持好奇心，提出问题——情绪能告诉你什么？</li></ul><h4 id="如何处理情绪">如何处理情绪</h4><ul><li>感受不能代表你，你也不能等同于你的感受。</li><li>情绪的感受就是经由你身体的体验。</li><li>每种情绪都能给你提供信息，但这些信息并不全面。</li><li>情绪的作用就是告诉你，你需要什么。</li><li>当你感受到情绪时，就给它起个名字。情绪不仅包括快乐或悲伤，还应该有更细致的分类方式。</li><li>我们应该接纳情绪，而不是抗拒情绪。要学会自我安抚。</li></ul><h4 id="如何利用语言的力量">如何利用语言的力量</h4><ul><li>我们所使用的语言极大地影响这我们对世界的体验。</li><li>描述自己感受的词汇越多越好。</li><li>想不出合适的词语时，可以参考感受圆盘。</li><li>注意别人是如何用词的，你可以通过读书、看电影等多种凡是来扩大自己的情绪词汇量。</li></ul><p><img src="//s3.mindex.xyz/blog/Notes/df5eae13d31c1027bd7bc01863922a4a.png" alt="在感受圆盘中找到能描述自己感受的词汇"></p><h4 id="当你关心的人陷入痛苦时">当你关心的人陷入痛苦时</h4><ul><li>当别人出现心理问题时，我们想去帮助他，但又觉得不知所措、力不从心，这是很正常的。</li><li>看到别人遭受痛苦，你想支持他，又怕自己说错话，这回让你很有压力，但一定不要因此回避他。</li><li>强有力的支持并不意味着要解决所有问题。</li><li>要照顾好自己，别让自己心力交瘁。维护自己的利益，设定清晰的界限。</li><li>永远不要低估倾听的力量。</li></ul><h3 id="无法走出悲伤怎么办">无法走出悲伤怎么办</h3><h4 id="理解悲伤">理解悲伤</h4><ul><li>那些对我们来说意义重大的事物的结束，都会引发悲伤——这种结束不一定是死亡。</li><li>悲伤是人类情感体验中很正常、自然的一部分。</li><li>痛苦可以是情绪上的，也可以是身体上的。</li><li>有帮助的事情并不会让痛苦小事，也不会迫使你放手。</li><li>完全回避悲伤可能会导致更深层次的问题。</li></ul><h4 id="悲伤的阶段">悲伤的阶段</h4><ul><li>否认能帮助我们承受住悲伤带来的痛苦。否认消退后，新的情绪会付出水面。</li><li>如果你无法控制愤怒的情绪，可以去做运动，利用生理唤醒，让身体暂时恢复平静。</li><li>反复思考“假如…现在会怎样”，很容易把自己引入自责的陷阱。</li><li>抑郁是失去亲人后的正常反应。</li><li>接受并不意味这你喜欢或认可现状。</li></ul><h4 id="哀悼的任务">哀悼的任务</h4><ul><li>治愈悲伤的第一步是允许自己感受痛苦。</li><li>我们需要时间来适应亲人已不复存在的生活。</li><li>我们需要找到一种方法，即使亲人不在了，也能继续和他们保持连接。</li><li>接受新的现实，我们才能继续去做那些对我们很重要的事。无论你有什么样的感受，都是正常的。</li><li>不要低估每一小步的作用，稳步前进。</li></ul><h4 id="力量的支柱">力量的支柱</h4><ul><li>时间、努力与坚持能帮合租你重建失去亲人后的生活。</li><li>通过新的方式与逝去的人保持连接，比如去你们一起去过的、有特殊意义的地方，或者去墓地悼念。</li><li>在整个过程中要尽可能地倾听自己的需求。</li><li>表达悲伤的方式无所谓对错。</li><li>不要规定自己必须多长时间走出悲伤。</li></ul><h3 id="低自尊人格，经常自我怀疑怎么办">低自尊人格，经常自我怀疑怎么办</h3><h4 id="如何看待别人的批评与否定">如何看待别人的批评与否定</h4><ul><li>学习正确地看待批评与否定，这是一项重要的生活技能。</li><li>我们天生就在乎别人对我们的看法，说“我不在乎任何人的想法”的人都不是真心的。</li><li>取悦别人绝不是表面上的与人为善，而是在任何情况下都把别人的需求置于自己的需求之上，甚至不惜损害自己的健康和幸福。</li><li>要理解为什么有些人总是吹毛求疵，这对你很有帮助。</li><li>你可以培养自尊感与羞耻感复原力，这能改变你的人生。</li></ul><h4 id="建立信心的关键">建立信心的关键</h4><ul><li>一个人只有在缺乏自信的情况下才会增加自信。</li><li>想要建立自信，就要走出舒适区。每天重复这样做，你的自信心会与日自增。</li><li>自信会根据情境而改变，当情境发生变化时，你要相信自己能克服畏惧，从而增强自信。</li><li>你不需要把自己置于最恶劣的环境中，可以从小的改变开始。</li><li>在建立自信的过程中，要做好自己的教练，而不是最苛刻的批评者。</li><li>先有勇气，再有自信。</li></ul><h4 id="你的错误不能代表你这个人">你的错误不能代表你这个人</h4><ul><li>大多数自我怀疑都与我们和失败的关系有关。</li><li>别人如何看待你的失败并不能说明你的个性，也不代表你作为人的价值。</li><li>失败带来的痛苦会驱使我们麻痹自己，自我封闭。就算你一开始没有觉察到自己的感受，你也可以觉察自己是通过哪些行为来屏蔽情绪的。</li><li>做自己的教练，把失败变成学习的机会，你才能不断进步，朝着你觉得最重要的方向努力。</li><li>失败会引发巨大的情绪反应，所以不要着急。</li></ul><h4 id="对自己更“狠”一些">对自己更“狠”一些</h4><ul><li>有一种误解，认为自我接纳会让人变得懒惰、自满、缺乏动力。</li><li>研究表明，那些能学着自我接纳、自我关怀的人不太可能惧怕失败，他们也更愿意再次尝试。</li><li>自我接纳不是被动地接受失败。</li><li>自我关怀也包括选择那条更难走但对你更有利的路。</li></ul><h3 id="极度焦虑，整天忧心忡忡怎么办">极度焦虑，整天忧心忡忡怎么办</h3><h4 id="消除焦虑">消除焦虑</h4><ul><li>人们希望消除焦虑是可以理解的，因为焦虑会令人不适。</li><li>要战胜恐惧，首先必须愿意面对它。</li><li>逃避只能短期缓解焦虑，长期来看，它只会加重焦虑。</li><li>我们为了控制和消除恐惧而做出的努力，正支配着我们的一举一动。</li><li>威胁应对系统会快速行动，你还来不及仔细思考，它就拉响了警报。</li></ul><h4 id="哪些做法会加重焦虑">哪些做法会加重焦虑</h4><ul><li>人在焦虑时最自然而然的反应就是逃避。</li><li>但逃避不会让焦虑消失。</li><li>仅仅告诉大脑某些东西是安全的还不够，你必须亲身体验，才能真正相信。</li><li>你需要一遍一遍重复这种行为，大脑才会被说服。</li><li>你做的最多的失去会成为你的舒适区。</li><li><strong>要想减轻对某件事的焦虑，就反复做这件事</strong>。</li></ul><h4 id="如何平复当下的焦虑">如何平复当下的焦虑</h4><ul><li>焦虑时，呼吸会变得急促、短浅。</li><li>想要让身体平静下来，请慢慢地深呼吸。</li><li>试着让呼气时间比吸气时间更长，呼气比吸气更有力。</li><li>要给它一些时间，焦虑反应会开始消退。</li></ul><h4 id="如何处理焦虑的想法">如何处理焦虑的想法</h4><ul><li>发现偏差并确定它是哪种类型的，这样才能与焦虑的想法保持距离。</li><li>记住，即使你还是会持续关注焦虑的想法，但你可以控制关注的焦点。</li><li>善意能减轻威胁反应，无论是来自他人的善意，还是我们对自己的善意。</li><li>将威胁重新定义为挑战，能让我们充满勇气。</li><li>行动要与价值观一致，你做决定应该根据什么对你最重要，而不是处于恐惧。</li></ul><h4 id="对不可避免的事情的恐惧">对不可避免的事情的恐惧</h4><ul><li>我们都惧怕死亡，包括它的已知和未知。</li><li>对于有些人来说，接近死亡会让他们成长，也会带来积极的生活转变。</li><li>接受死亡并不意味着放弃生命，而是恰恰相反。</li><li>接受死亡才能让我们赋予生命意义。</li></ul><h3 id="压力大到濒临崩溃怎么办">压力大到濒临崩溃怎么办</h3><h4 id="压力和焦虑有什么不同吗">压力和焦虑有什么不同吗</h4><ul><li>人们常把“压力”和“焦虑”这两个词混用</li><li>当我们的生理状态能够与外部需求相匹配时，即便有压力，我们也会把它解读为积极的感受。</li><li>当我们的大脑在为我们要做的事做准备时，我们就会感到压力。</li><li>大脑会释放能量，以提高警觉性，让我们对环境做出反应。</li><li>我们常把焦虑理解为基于恐惧而做出反应，实际上它是为了满足你的需求而产生的一种压力反应。</li></ul><h4 id="为什么减压不是唯一的答案">为什么减压不是唯一的答案</h4><ul><li>压力并不总是敌人，它也是最有价值的工具。</li><li>要学着在承受压力后充实身心，与试图消除压力相比，这个做法更实际。</li><li>压力能帮助你表现得更出色，也能驱使你做最重要的事，但我们不能一直处于压力状态。</li><li>适度的压力会让生活充满乐趣和挑战，但压力太大会让人无法感受到生活的美好。</li></ul><h4 id="当有益的压力变得有害">当有益的压力变得有害</h4><ul><li>短期的压力反应能激发出最好的状态。</li><li>长期压力就像在高速公路上开车挂二挡，用不了多长时间，就会出事故。</li><li>倦怠不仅仅是工作造成的。</li><li>没有万能的灵丹妙药。对其他人保持平衡有用的方法对你也许没有作用。</li><li>如果你出现倦怠的迹象，你要倾听它们并及时回应，要学着满足自己的需求。</li></ul><h4 id="把压力变成动力">把压力变成动力</h4><ul><li>改变一些简单的行为模式，比如呼吸模式，能改变你的压力水平。</li><li>科学表明，冥想对大脑和我们处理问题的方式有显著影响。</li><li>与他人建立连接能帮助我们从压力中恢复。社交隔离会让人身心俱疲，承受巨大压力。</li><li>以做出贡献为目标，而不是把竞争当成目标，这样在面对压力时，我们才有动力和毅力。</li><li>寻找能让自己产生敬畏之心的体验，以改变视角。</li></ul><h4 id="如何处理必须面对的压力">如何处理必须面对的压力</h4><ul><li>我们对压力的看法会影响我们在压力状态下的表现。</li><li>把压力看作一种财富，可以帮助你实现目标，你不需要花太多精力去摆脱压力，而是要专注于你被要求做到的事。</li><li>关注应该做什么，而不是不该做什么。</li><li>调整你的关注点可以改变压力水平。</li><li>改变你与失败的关系，培养羞耻感复原力，能帮助你应对高压环境。</li></ul><h3 id="觉得人生没有意义怎么办">觉得人生没有意义怎么办</h3><h4 id="关于“我只想要幸福”的问题">关于“我只想要幸福”的问题</h4><ul><li>我们经常被灌输的观念史，幸福才是常态，一个人要是觉得不幸福，那他肯定是有心理问题的。</li><li>有时我们会觉得不幸福，这恰恰是因为我们是人，而且，人生本就艰难。</li><li>让生命有价值的人或是带给我们的不仅仅是幸福感，也混杂了爱、快乐、恐惧、羞愧和伤害。</li><li>要弄清楚自己的价值观，因为它能引导我们去设定人生目标，而实现目标的过程是有意义、有价值的。</li><li>要把价值观放在第一位，因为它能帮助我们熬过人生的痛苦时刻，让我们知道自己走在正确的路上。</li></ul><h4 id="找到最重要的事">找到最重要的事</h4><ul><li>你可以通过一些简单的练习来弄清楚自己现阶段的价值观。</li><li>价值观会随着时间的推移而改变，我们的生活与价值观的一致程度也会改变，因此，有必要经常审视自己的价值观。</li><li>我们可以根据价值观进行大的目标设定，并确立日常的小目标。</li><li>重点不在于你希望会发生什么，而在于你想成为什么样的人，你想做出什么样的贡献，以及无论发生什么，你都会如何面对生活。</li></ul><p><img src="//s3.mindex.xyz/blog/Notes/5ee2743c032d16c193da31c6c6daae4b.png" alt="价值观——请圈出对你而言最重要、最有意义的价值观"><br><img src="//s3.mindex.xyz/blog/Notes/ba2d9900c52c6ccd737d32a3680d4b23.png" alt="这张表格中的示例能说明价值观与目标的区别，应该如何让目标与价值观一致，并转化为日常行为。"></p><h4 id="如何创造有意义的人生">如何创造有意义的人生</h4><ul><li>在下定决心要做出改变时，我们往往会给自己设立一个宏大的、激进的新目标。</li><li>只有一个目标并不足以保证我们能做出改变，更不能保证长久的改变。</li><li>花时间思考并想象你要成为怎样的人，并把这些想法转化为具体的、可持续的行动，这样你会觉得自己的努力更有意义。</li><li>将你行动的初衷与身份认同练习起来，这样最初的目标实现后，新的行为习惯也会持续下去。</li></ul><h4 id="关系">关系</h4><ul><li>当我们谈到幸福生活时，关系要比金钱、名誉、社会阶层、基因以及所有我们被告知要尽力争取的东西都要重要。</li><li>我们的关系以及我们在关系中感受到的幸福程度与我们的整体健康密不可分，关系是幸福和健康的核心。</li><li>改善自我有助于改善关系，而改善关系又有助于改善自我。</li><li>童年时期所形成的依恋关系通常会体现在成年后的关系中。</li></ul><h4 id="何时该寻求帮助">何时该寻求帮助</h4><ul><li>只要你关心自己的心理健康，你随时可以去寻求帮助。</li><li>如果你不确定自己需要多少帮助，专业人士可以帮助你做出决策。</li><li>在理想的世界，无论是谁，只要需要，就能得到专业的心理治疗。但理想的世界并不存在。</li><li>如果没有条件获得专业服务，那就抓住一切机会去了解关于心理疗愈的只是，并向你信任的人寻求支持。</li></ul>]]></content>
    
    <summary type="html">
    
      人生中的事分两类，你能控制的，你不能控制的。
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Health" scheme="https://neo1989.net/tags/Health/"/>
    
  </entry>
  
</feed>
