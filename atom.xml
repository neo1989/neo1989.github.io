<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>愚苏记</title>
  
  <subtitle>To no avail but try.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://neo1989.net/"/>
  <updated>2024-01-12T06:46:47.028Z</updated>
  <id>https://neo1989.net/</id>
  
  <author>
    <name>Neo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Way2AI · 语义搜索</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-Semantic-Search/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-Semantic-Search/</id>
    <published>2024-01-10T04:21:28.000Z</published>
    <updated>2024-01-12T06:46:47.028Z</updated>
    
    <content type="html"><![CDATA[<p>语义搜索通过理解搜索查询的内容来提高搜索准确性。与传统搜索引擎不同，传统搜索引擎仅根据词法匹配查找文档，而语义搜索还可以找到同义词。</p><h3 id="背景">背景</h3><p>语义搜索的基本思想是将语料库中的所有条目（无论是句子、段落还是文档）都嵌入到一个向量空间中。</p><p>在搜索时，查询会被嵌入到相同的向量空间中，然后从语料库中找到与查询最接近的嵌入。这些嵌入应该与查询具有高度语义重叠。</p><p><img src="//s3.mindex.xyz/blog/Courses/a88a6ad41612242bacf9371252618da4.png" alt=""></p><h3 id="对称与非对称">对称与非对称</h3><p>对称语义搜索是指你的查询和语料库中的条目长度大致相同，并且具有相同数量的内容。一个例子是: 通过搜索类似的问题 “如何在线学习 Python？” ，你想找到一个像“如何在网络上学习 Python？”这样的条目。对于对称任务，你可能可以在语料库中翻找到查询和对应的条目。</p><p>非对称语义搜索是指你通常有一个简短的查询（例如一个问题或一些关键词），你想找到一个更长的段落来回答查询。像 “什么是 Python” 的查询，你想找到段落“Python 是一种解释型、高级且通用的编程语言。Python 的设计理念是……”。对于非对称任务，在语料库中翻找通常没有意义。</p><p>选择适合任务类型的模型非常重要。</p><p>适合对称语义搜索的模型：<a href="https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models" target="_blank" rel="noopener">Pre-Trained Sentence Embedding Models</a></p><p>适合非对称语义搜索的模型：<a href="https://www.sbert.net/docs/pretrained-models/msmarco-v3.html" target="_blank" rel="noopener">Pre-Trained MS MARCO Models</a></p><h3 id="Python">Python</h3><p>在数据量不大的语料库中（条目数量最多大约100万），我们有能力计算出搜索词与语料库内每一个条目之间的余弦相似度。</p><p>在接下来的示例中，我们创建了一个包括几个样本句子的小型语料库，并为这个语料库以及我们的搜索词分别计算了它们的嵌入向量。</p><p>接着，我们运用 <code>sentence_transformers.util.cos_sim()</code> 函数来测量搜索词与语料库中所有条目之间的余弦相似性。</p><p>面对庞大的语料库，对所有评分逐一排序实在是效率太低。所以，我们采用了 <code>torch.topk</code> 函数来直接提取得分最高的前 k 个条目。</p><p>下面是一个简单的示例；参见 <a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search.py" target="_blank" rel="noopener">semantic_search.py</a>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">This is a simple application for sentence embeddings: semantic search</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">We have a corpus with various sentences. Then, for a given query sentence,</span></span><br><span class="line"><span class="string">we want to find the most similar sentence in this corpus.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This script outputs for various queries the top 5 most similar sentences in the corpus.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer, util</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">embedder = SentenceTransformer(<span class="string">'all-MiniLM-L6-v2'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Corpus with example sentences</span></span><br><span class="line">corpus = [</span><br><span class="line">    <span class="string">'A man is eating food.'</span>,</span><br><span class="line">    <span class="string">'A man is eating a piece of bread.'</span>,</span><br><span class="line">    <span class="string">'The girl is carrying a baby.'</span>,</span><br><span class="line">    <span class="string">'A man is riding a horse.'</span>,</span><br><span class="line">    <span class="string">'A woman is playing violin.'</span>,</span><br><span class="line">    <span class="string">'Two men pushed carts through the woods.'</span>,</span><br><span class="line">    <span class="string">'A man is riding a white horse on an enclosed ground.'</span>,</span><br><span class="line">    <span class="string">'A monkey is playing drums.'</span>,</span><br><span class="line">    <span class="string">'A cheetah is running behind its prey.'</span></span><br><span class="line">]</span><br><span class="line">corpus_embeddings = embedder.encode(corpus, convert_to_tensor=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Query sentences:</span></span><br><span class="line">queries = [<span class="string">'A man is eating pasta.'</span>, <span class="string">'Someone in a gorilla costume is playing a set of drums.'</span>, <span class="string">'A cheetah chases prey on across a field.'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity</span></span><br><span class="line">top_k = min(<span class="number">5</span>, len(corpus))</span><br><span class="line"><span class="keyword">for</span> query <span class="keyword">in</span> queries:</span><br><span class="line">    query_embedding = embedder.encode(query, convert_to_tensor=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We use cosine-similarity and torch.topk to find the highest 5 scores</span></span><br><span class="line">    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[<span class="number">0</span>]</span><br><span class="line">    top_results = torch.topk(cos_scores, k=top_k)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"\n\n======================\n\n"</span>)</span><br><span class="line">    print(<span class="string">"Query:"</span>, query)</span><br><span class="line">    print(<span class="string">"\nTop 5 most similar sentences in corpus:"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> score, idx <span class="keyword">in</span> zip(top_results[<span class="number">0</span>], top_results[<span class="number">1</span>]):</span><br><span class="line">        print(corpus[idx], <span class="string">"(Score: &#123;:.4f&#125;)"</span>.format(score))</span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk</span></span><br><span class="line"><span class="string">    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)</span></span><br><span class="line"><span class="string">    hits = hits[0]      #Get the hits for the first query</span></span><br><span class="line"><span class="string">    for hit in hits:</span></span><br><span class="line"><span class="string">        print(corpus[hit['corpus_id']], "(Score: &#123;:.4f&#125;)".format(hit['score']))</span></span><br><span class="line"><span class="string">    """</span></span><br></pre></td></tr></table></figure><h3 id="速度优化">速度优化</h3><p>要想让 <code>sentence_transformers.util.cos_sim()</code> 方法运行得更快，最好的做法是将 <code>query_embeddings</code> 和 <code>corpus_embeddings</code> 存在同一块 GPU 设备上。这样做可以明显提升处理性能。</p><p>另外，我们还可以对语料库嵌入进行标准化处理，使每个语料库嵌入的长度都为 1。这样，我们就可以通过点积运算来计算得分了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus_embeddings = corpus_embeddings.to(<span class="string">'cuda'</span>)</span><br><span class="line">corpus_embeddings = util.normalize_embeddings(corpus_embeddings)</span><br><span class="line"></span><br><span class="line">query_embeddings = query_embeddings.to(<span class="string">'cuda'</span>)</span><br><span class="line">query_embeddings = util.normalize_embeddings(query_embeddings)</span><br><span class="line">hits = util.semantic_search(query_embeddings, corpus_embeddings, score_function=util.dot_score)</span><br></pre></td></tr></table></figure><h3 id="ElasticSearch">ElasticSearch</h3><p>从 7.3 版本开始，<a href="https://www.elastic.co/elasticsearch/" target="_blank" rel="noopener">ElasticSearch</a> 推出了一个新功能，即能够索引密集向量 (dense vectors)，并将其用于对文档进行评分。所以，我们可以利用 ElasticSearch 对文档以及嵌入向量（embeddings）进行索引，以此在搜索时使用对应的嵌入向量寻找相关的文档信息。</p><p>ElasticSearch的一个优点是，它便于向索引中添加新的文档，而且能够和我们的向量一起存储其他数据。但缺点是它的性能较慢，这是因为它需要将搜索的嵌入内容和每一个已经存储的嵌入内容进行比较。这种操作的时间成本是线性的，对于大规模（超过 100k）的数据集来说，可能会慢得无法接受。</p><p>更多详细信息，请参见 <a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_elasticsearch.py" target="_blank" rel="noopener">semantic_search_quora_elasticsearch.py</a>。</p><h3 id="近似最近邻点">近似最近邻点</h3><p>如果使用精确的最近邻搜索方法（如 <code>sentence_transformers.util.semantic_search</code> 所采用的方式），在一个巨大的语料库中进行查找，特别是这个语料库中包含数百万个嵌入，可能会花费大量的时间。</p><p>在这种情况下，近似最近邻（Approximate Nearest Neighor，ANN）可能会很有帮助。这里，数据被划分为相似的嵌入小部分。利用索引可以有效地进行搜索，甚至在有数百万的向量时也能在毫秒内检索到最高相似性的嵌入（即最近的邻居）。</p><p>不过，结果未必都是精确的。可能有些具有高度相似性的向量被遗漏了。这就是我们称它为“近似最近邻居”的原因。</p><p>所有的人工神经网络（ANN）方法都通常需要调整一到多个参数，以达到召回率与搜索速度的权衡。如果你追求极高的搜索速度，可能会错过一些重要的搜索结果。反之，如果你期望得到高召回率，搜索的速度就可能会变慢。</p><p>近似最近邻搜索库中，<a href="https://github.com/spotify/annoy" target="_blank" rel="noopener" title="Annoy">Annoy</a>、<a href="https://github.com/facebookresearch/faiss" target="_blank" rel="noopener" title="FAISS">FAISS</a> 和 <a href="https://github.com/nmslib/hnswlib/" target="_blank" rel="noopener" title="hnswlib">hnswlib</a> 都很热门。但是，我个人更偏向 <code>hnswlib</code>，因为它不仅使用起来十分简单，性能卓越，而且包含了许多在实际应用中至关重要的特色功能。</p><p>示例：</p><ul><li><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_hnswlib.py" target="_blank" rel="noopener" title="semantic_search_quora_hnswlib.py">semantic_search_quora_hnswlib.py</a></li><li><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_annoy.py" target="_blank" rel="noopener" title="semantic_search_quora_annoy.py">semantic_search_quora_annoy.py</a></li><li><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_faiss.py" target="_blank" rel="noopener" title="semantic_search_quora_faiss.py">semantic_search_quora_faiss.py</a></li></ul><h3 id="召回和重排">召回和重排</h3><p>对于复杂的语义搜索场景，建议使用「召回和重排」流程：</p><p><img src="//s3.mindex.xyz/blog/Courses/3d66117e5374e1f95d858d7d422fc22e.png" alt=""></p><p>当我们有一个搜索请求时，我们会首先使用一个检索系统，这个系统能够找出大约 100 个可能的结果，这些结果可能与我们的搜索请求相关。在进行检索时，我们可以选择使用词汇搜索，比如说使用 ElasticSearch 这样的工具，或者我们也可以选择使用双向编码器进行深度检索。</p><p>但是，这个检索系统可能会找到一些与搜索请求并不太相关的文档。因此，在第二步，我们会使用一个基于交叉编码器的重新排序系统，这个系统会评估所有候选结果与搜索请求的相关性。</p><p>最终，我们将得到一个排名的结果列表，这个列表可以直接呈现给用户。</p><h4 id="召回-Bi-Encoder">召回: Bi-Encoder</h4><p>在寻找候选结果集的过程中，我们可以选择使用词汇搜索（比如 ElasticSearch），或者我们也可以选择使用在这个代码库中实现的双向编码器。</p><h3 id="完整示例">完整示例</h3><h4 id="相似问题检索">相似问题检索</h4><p><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_pytorch.py" target="_blank" rel="noopener">semantic_search_quora_pytorch.py</a> [<a href="https://colab.research.google.com/drive/11GunvCqJuebfeTlgbJWkIMT0xJH6PWF1?usp=sharing" target="_blank" rel="noopener">Colab Version</a>] 是一个基于Quora重复问题数据集的应用案例。通过它，用户可以输入任何问题，然后代码会运用 <code>sentence_transformers.util.semantic_search</code> 方法从数据集中找出与输入问题最相近的问题。模型是 distilbert-multilingual-nli-stsb-quora-ranking，它的主要任务是去识别类似的问题，并且它支持超过50种语言。所以，无论用户用这50多种语言中的何种来提问，都可以得到有效的答案。这是一个对称的搜索任务，因为搜索查询的长度和内容与语料库中的问题相同。</p><h4 id="相似出版物检索">相似出版物检索</h4><p><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_publications.py" target="_blank" rel="noopener">semantic_search_publications.py</a> [<a href="https://colab.research.google.com/drive/12hfBveGHRsxhPIUMmJYrll2lFU4fOX06?usp=sharing" target="_blank" rel="noopener">Colab Version</a>] 这个示例演示了如何找到与某篇科学论文相似的其他论文。我们的语料库由在 EMNLP 2016 - 2018 会议上发表的所有论文组成。在搜索过程中，我们会输入最近发表的论文的标题和摘要，然后在我们的语料库中寻找相关的论文。我们使用的是 <a href="https://arxiv.org/abs/2004.07180" target="_blank" rel="noopener">SPECTER</a> 模型。这个搜索任务是对称的，因为我们的语料库中的论文和我们搜索的内容都是由标题和摘要组成。</p><h4 id="问答检索">问答检索</h4><p><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_wikipedia_qa.py" target="_blank" rel="noopener">semantic_search_wikipedia_qa.py</a> [<a href="https://colab.research.google.com/drive/11GunvCqJuebfeTlgbJWkIMT0xJH6PWF1?usp=sharing" target="_blank" rel="noopener">Colab Version</a>]：这个例子展示了一个在 <a href="https://ai.google.com/research/NaturalQuestions/" target="_blank" rel="noopener">Natural Questions dataset</a> 数据集上进行训练的模型。这个数据集包含了大约十万条真实的 Google 搜索请求，以及从维基百科获取并附带注解的段落，这些段落提供了问题的答案。这是一个非对称搜索任务的典型例子。在这个例子中，我们使用了体积较小的 <a href="https://simple.wikipedia.org/wiki/Main_Page" target="_blank" rel="noopener">Simple English Wikipedia</a> 作为语料库，这样它就可以轻松地加载到内存中。</p><p><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.py" target="_blank" rel="noopener">retrieve_rerank_simple_wikipedia.py</a> [<a href="https://colab.research.google.com/github/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb" target="_blank" rel="noopener">Colab Version </a>]：这个脚本采用了 <strong>召回和重排</strong> 的策略，是一个非对称搜索任务的典型例子。我们把所有维基百科的文章切分成各个段落，并用双向编码器进行编码处理。当有新的查询或问题输入时，我们也用同样的双向编码器进行编码，然后找出与之余弦相似度最高的段落。然后，我们用一个交叉编码器对找到的候选段落进行重新排序，最终将得分最高的5个段落展示给用户。我们使用的模型是在 <a href="https://github.com/microsoft/MSMARCO-Passage-Ranking/" target="_blank" rel="noopener">MS Marco Passage Reranking datase</a> 数据集上进行训练的，这个数据集包含了大约 500k 来自 Bing 搜索的真实查询。</p><h3 id="Reference">Reference</h3><p><a href="https://www.sbert.net/examples/applications/semantic-search/README.html" target="_blank" rel="noopener">Semantic Search</a></p>]]></content>
    
    <summary type="html">
    
      语义搜索通过理解搜索查询的内容来提高搜索准确性。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>《我就是你啊》</title>
    <link href="https://neo1989.net/Notes/NOTE-Sept-graines-de-lumiere-dans-le-coeur-des-guerriers/"/>
    <id>https://neo1989.net/Notes/NOTE-Sept-graines-de-lumiere-dans-le-coeur-des-guerriers/</id>
    <published>2024-01-06T12:20:47.000Z</published>
    <updated>2024-01-08T13:35:04.775Z</updated>
    
    <content type="html"><![CDATA[<p>必要前提：记住“武力对抗”会适得其反。在整个余人交流的过程中，都要避免陷入<strong>解释</strong>、<strong>威胁</strong>和<strong>人身攻击</strong>的怪圈当中。</p><h3 id="第一步：平复自己的情绪">第一步：平复自己的情绪</h3><p>当我们感觉内心出现了想要攻击对方的冲动时，我们需要尽力控制住它。比如，我们可以通过以下几种方法做到这一点：</p><ul><li>纠正对事实的误判；</li><li>通过深呼吸来分散注意力；</li><li>或只需收住自己想要伸出的拳头；</li><li>或其他任何一种可能奏效的方法。</li></ul><p>在与人交流的过程中，每当你感到内心再次燃起了这种冲动，就需要在脑海里回顾这一步。完成这一步只需要几秒钟的时间，这大概是最难完成，也是最重要的一步。</p><h3 id="第二步：平复他人的情绪">第二步：平复他人的情绪</h3><p>如果与你对话的人能够保持冷静，那就好说了，你就可以直接跳过这一步。但是如果对方不冷静呢，你又该怎么做？答案是 “什么也不要做”，或者说 “几乎什么也不要做”。尤其不要和对方说“你冷静点儿！”或者“你生气是没有用的！”。此时你应该遵循“<strong>不唱反调，不做评判</strong>”的原则。具体如何做到这一点呢？你可以用 “同意” “好的” “是的” “没错” 等来回复对方。这些字眼可以向对方传达一种信息：“你这样说以及你选择以这样的方式说，自然有你的道理。我愿意与你探讨这个问题。”这样做你会收获到惊人的效果：对方的情绪起初会有些波动，但随后便会逐渐稳定，直到最后慢慢平复。实现这一步，只需要你说出 “<strong>我同意</strong>” 的一刹那就够了。 如果你成功控制住了自己的情绪，又成功平复了他人的情绪，那就可以进入下一步了。</p><h3 id="第三步：试着理解他人而非让他人被理解">第三步：试着理解他人而非让他人被理解</h3><p>如何做到这一步呢？最简单的方法就是 “<strong>向对方提问</strong>”。最有效的问题就是：“你为什么步同意我的观点呢？”之后便要倾听他的回答，并试着站在他们的角度看问题，甚至是设身处地地为他人的利益着想。努力去理解和接受别人的观点，而非将自己的观点强加于人。</p><p>你要学会从他们在做解释时所说的话语中寻找双方的“共识”，并欣然接受对方的观点。这是双方达成“共识”的先决条件。这时候你多一些对别人的“私心”：让自己多为对方的利益考虑！。</p><p>一旦你理解了他人的想法，解决方法便会自己现身，分歧也就迎刃而解了。但通常来说，做到这一点还不够，你需要继续完成下一步…</p><h3 id="第四步：通过复述别人的话来让对方明白“你以及理解了对方的观点”">第四步：通过复述别人的话来让对方明白“你以及理解了对方的观点”</h3><p>如果你想让别人倾听你的观点，你需要先让对方发言。然后再用自己的话将你所理解的对方的观点讲一遍。之后问对方 “我说得对吗？”。这样你会收获意想不到的神奇效果——对方会眉头上扬，露出满意的笑容，大赞一声“对啊！”。而后他便会闭上嘴，听你说话。复述有两个好处：第一，你可以检验一下自己是否真的理解了对方的观点；第二，让你的对话者知道自己被人理解了，进而打消继续争论的念头。</p><p>但实现这一步有一个前提条件——不要因为用错了一个词，让你之前的努力都付之东流。这个词就是接下来这一步的关键词。这个词的使用也是一门艺术。</p><h3 id="第五步：使用表并列的词汇提出自己的观点，而非将双方观点对立">第五步：使用表并列的词汇提出自己的观点，而非将双方观点对立</h3><p>如何做到这一点呢？你可以运用以下这些字眼，比如：就我而言、对我来说、与此同时、从我的角度出发…而不要用极其生硬的 “没错，但是…”。然后等到双方都明确了对方的观点之后，问问你自己 “如何才能让对方的诉求得到满足，同时又能达到自己的目的呢？”。如此一来，你就可以发挥两个人的聪明材质，来寻求解决方案。接下来，我们开始进入第六步。</p><h3 id="第六步：提出解决方案">第六步：提出解决方案</h3><p>采用可能的双赢方案。如果我们自己找不到可行的方案，试着问问别人有没有好主意吧。让拿回我们可以一起针对其进行讨论。</p><p>如果找不到任何双赢的方法，我们可以采用妥协折中的方式解决问题。人们往往会接受折中的方案，因为这样至少恩纳锅够建立友好写作的关系。</p><p>那么，如果连折中的方案也不存在呢？这种情况非常罕见，但并非绝不可能发生。此时，可以与对方协商，再花点时间一起探讨可能的出路。尽管最终不一定能找到解决方案，但至少维持了良好的关系。而这种良性关系对于未来协作的达成至关重要。</p><p><img src="https://s3.mindex.xyz/blog/Notes/bf65eb5d1adba66e1f624e828eaf5b7f.png" alt=""></p><h3 id="Reference">Reference</h3><p>[1] <a href="https://book.douban.com/subject/35445960/" target="_blank" rel="noopener">我就是你啊 : 走进他人内心的7项修炼</a></p>]]></content>
    
    <summary type="html">
    
      化解一场纷争需要经历哪些重要的步骤？
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="SoftSkills" scheme="https://neo1989.net/tags/SoftSkills/"/>
    
  </entry>
  
  <entry>
    <title>高效做事的底层逻辑</title>
    <link href="https://neo1989.net/Notes/NOTE-work-efficiently/"/>
    <id>https://neo1989.net/Notes/NOTE-work-efficiently/</id>
    <published>2023-12-15T09:26:25.000Z</published>
    <updated>2023-12-17T06:03:20.899Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第一步：WOOP模型，让你对目标动力十足">第一步：WOOP模型，让你对目标动力十足</h2><p><strong>W</strong>ish: 明确愿望和目标是什么？<br><strong>O</strong>utcome: 实现愿望后有什么收获？<br><strong>O</strong>bstacle: 追求目标过程中有哪些障碍？<br><strong>P</strong>lan: 制定什么样的计划来克服那些障碍？</p><h2 id="第二步：SMART法则，设定更科学的目标">第二步：SMART法则，设定更科学的目标</h2><p><strong>S</strong>pecific: 目标设定必须明确具体，很多目标之所以不能实现，就是因为设定之初模棱两可。<br><strong>M</strong>easureable: 所谓可衡量，就是要有一组明确的数据。用数据作为衡量目标是否实现的标准。<br><strong>A</strong>ttainable: 设定的目标要在自己的能力范围之内。<br><strong>R</strong>elevant: 所设立ide目标要与其他目标相关联。<br><strong>T</strong>ime-bound: 目标是要有时间限制的。设定一个目标完成的期限，促进目标的达成。</p><h2 id="第三步：用GRAI定期复盘">第三步：用GRAI定期复盘</h2><p><strong>G</strong>oal: 当初立了哪些flag。期望的结果是什么？<br><strong>R</strong>esult: 对照目标，完成的怎么样了？<br><strong>A</strong>nalysis: 成功或失败的关键原因是什么？<br><strong>I</strong>nsight: 得失的体会是什么？是否有规律性的东西值得思考并指导下一次行动计划？</p><h2 id="第四步：用PDCA不断优化">第四步：用PDCA不断优化</h2><p><strong>P</strong>lan:<br>- 分析现状，找出问题<br>- 分析产生问题的影响因素<br>- 找出主要因素<br>- 设定目标，指定计划</p><p><strong>D</strong>o: 按照预订计划，努力实现预期目标</p><p><strong>C</strong>heck: 评估结果，检查效果，分析原因</p><p><strong>A</strong>ction:<br>- 将成功经验进行标准化及流程制定，作为下次行动的参考<br>- 总结经验和问题，为开展新一轮PDCA提供依据</p>]]></content>
    
    <summary type="html">
    
      4个步骤让你效率暴涨。
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="SoftSkills" scheme="https://neo1989.net/tags/SoftSkills/"/>
    
  </entry>
  
  <entry>
    <title>查理·芒格 语录</title>
    <link href="https://neo1989.net/Notes/NOTE-Charles-Thomas-Munger/"/>
    <id>https://neo1989.net/Notes/NOTE-Charles-Thomas-Munger/</id>
    <published>2023-11-29T07:36:36.000Z</published>
    <updated>2023-12-17T09:21:12.799Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于认知">关于认知</h2><ul><li>变化总在发生，你不去迎接进步的变化，就会等到退步的变化。</li><li>我们能取得今时今日的成就，不是因为我们的能力比别人高出多少，而是我们比别人更清楚自己能力的大小。</li><li>决定结果的主要有两个因素：一个是形势，一个是人。形势太强，任凭你有多大能力，都无济于事。</li><li>一个人，手里拿着锤子，看什么都像钉子。</li><li>只要做好准备，在人生中抓住几个机会。迅速地采取适当的行动，去做简单而合乎逻辑的事情，这辈子的财富就会得到极大的增长。</li><li>我们很清楚自己的不足，很清楚有很多试我们做不到，所以我们谨小慎微地留在我们的“能力圈”中。“能力圈”是沃伦提出的概念。沃伦和我都认为，我们的“能力圈”是一个非常小的圆圈。</li><li>在显示世界中，学会看透本质，我们能活得更从容。探究本质并非朝夕之功，必须有板凳要坐十年冷的精神。</li><li>如果你想要说服别人，要诉诸利益，而非诉诸理性。</li><li>卓越的人很少，有机会追随他们，和他们走到一起，或许值得付出溢价，将来可能获得丰厚的回报。</li><li>保持理性是一种道德律令。在不该犯傻的时候千万别犯傻。</li><li>和被人比是比不过来的，无论做什么，都是一山更比一山高，强中自有强中手。</li><li>所有人都看好的机会，最容易发生踩踏，造成的损失最惨烈。</li><li>每一天都追求比醒来的那一刻多增长一分智慧；每一天都追求有能力承担更大的责任；每一天都追求尽善尽美地完成所有工作。日复一日，年复一年，你终将出人头地。</li><li>按照我的经验，解决问题的最佳方法是不让问题出现。</li><li>所有人的潜意识里都有这样的偏见：给别人提建议时，以为是在为别人考虑，其实是从自己的利益出发。</li><li>我们能成功，不是因为我们善于解决难题，而是因为我们善于远离难题。我们只是找简单的事做而已。</li><li>反过来想，总是反过来想。</li><li>如何才能成功？严格自律、遵守道德，找到志同道合的人，抓住难得的大机会，说出来都是些很简单的道理。</li><li>我不会因为人性而感到意外，也不会花太多时间感受背叛，我总是低下头调整自己去适应这类事情，我不喜欢任何成为受害者的感受。我不是受害者，我是幸存者。</li><li>人类都试图变得精明，而我只想证明自己并不是在做傻事，但这比许多人想想的要困难的多。</li></ul><h2 id="关于成长">关于成长</h2><ul><li>想要得到一件东西，最稳妥的方法就是让自己配得上它。</li><li>找出你最擅长的事情，然后持之以恒，乐此不疲地去把它做好。</li><li>承认无知是智慧的开始。</li><li>一个不能从别人的经验汇总学习的人，一辈子注定一只摔跟头。</li><li>把问题彻底想明白，问题就解决了一半。</li><li>我不会质疑过去，而是从过去中学习，为未来做决定。</li><li>如果你真想成功，真想取得别人无法取得的成就，就甘坐冷板凳，日复一日地阅读。如果你想拥有良好的认知能力，如果你想比别人更具智慧，能在艰难时刻有更好的表现，除了拿出大量时间思考，别无他法。</li><li>我这辈子遇到的聪明人没有不每天阅读的——一个都没有。</li><li>独学而无友，则孤陋寡闻。所有人都需要找到志同道合的人，相互切磋、共同进步。</li><li>我们成功源自我的长期专注。</li><li>在过去的任何一年，如果你一次都没有推翻过自己最中意的想法，那么这一年就算浪费了。</li><li>我喜欢能够坦然承认自己很愚蠢的人。我知道，如果正面承认自己的错误，我会表现得更好。这是一个非常棒的学习窍门。</li><li>我见识过很多取得很大成就的人。虽然他们既不是最聪明的人，甚至也不是最勤奋的人，但他们都是很善于学习的人。</li><li>极度专业化才是成功之道。比起理解整个世界来说，大多数人更加擅长专攻一个方面。</li><li>我做过很多傻事，我一直在和自己的成见做斗争。消除错误的想法是一件好事，我把消除错误的想法作为自己的一种追求。</li><li>进步不总是肉眼可见，而是往往出现在不经意间，但进步总是源于长期坚持，源于每一天的努力。</li><li>如果不终身学习，你们将不会取得很高的成就。光靠已有的知识，你们在生活中走不了多远。</li><li>只要能达到正确的终点，路途再颠簸，我都受得了。</li><li>脚踏实地，一步一个脚印，坚持不懈地长期努力，这是我的成功之道。</li></ul><h2 id="关于人生">关于人生</h2><ul><li>生活的铁律就是，只有20%的人能够取得比其他80%的人优秀的成绩。</li><li>不要同一头猪摔跤，因为这样你会把全身弄脏，而对方却乐此不疲。</li><li>你不必非常出色，只要在很长、很长的时间内保持比其他人聪明一点点就够了。</li><li>如果你的生活方式是正确的，那么到了晚年只会比年轻时更加幸福。</li><li>我会尽我所能逆流而上，而不是去预测潮汐何时到来。</li><li>如果你专注的时间周期足够长，你不断地为解决难题而努力，你就会跌跌撞撞地得到一个答案。这是人生的半个秘方。</li><li>任何人都会有错过机会的时候，这是命中注定的。我一直认为，改变不了的事情，不要太纠结。<strong>牢骚和抱怨是人生中的大忌。</strong></li><li>在生活中，很多人抱残守缺，他们满脑子的旧思想，新思想根本进不去。有句德国谚语说得好：“我们总是老的太快，聪明的太迟”。所有人都有这个问题。</li><li>身处逆境的时候，你要有一股咬紧牙关、埋头苦干的拼劲。怨天尤人、牢骚不断，只能越来越苦、越来越难。</li><li>想要什么，就立刻要得到，这样的人不但一事无成，还可能坠入深渊。</li><li>抵抗衰老的最佳措施就是在老之前好好生活。</li><li>生活总是以某种防护死伤害某人，又以某种方式帮助他人。面对生活中的打击，每个人都应该积极应对。</li><li>人生的困难一个接一个，每个困难都是对我们的一次考验，都是我们表现自己的机会。</li><li>很多人总是一味地逃避，不愿承受短期的痛苦。自找苦吃、主动吃眼前的苦，这才是正确的处世态度。</li><li>当逆境不期而至时，我们应该敢于迎难而上，这才是一种积极向上的人生态度。整天哼哼唧唧地怨天尤人，谁都救不了你。</li><li>坚持做有意义的事；坚持做有价值的人；坚持追求理智、正直、诚信，总有一天，一定能获得成功。</li><li>要想幸福，第一条，降低自己的预期。这一点是你自己能掌控的。</li><li>我觉得犯嫉妒这种罪的人最蠢，得不到一丁点快乐，整个人都被痛苦包围着，何必遭这份罪呢？</li><li>托马斯·卡莱尔有一句名言：“与其为朦胧的未来而烦恼忧虑，不如脚踏实地地、做好眼前的事。” 这句话说的很对。大多数时候，我们应该把眼前的事做好，尽人事，听天命。</li></ul><h2 id="关于商业管理">关于商业管理</h2><ul><li>最理想的公司，每年创造的现金高于净利润，能为所有者提供大量可自由支配的现金。</li><li>充分认清客观条件的限制，充分认识自身能力的限制，谨小慎微地在限制范围内活动，这是赚钱的诀窍。这个诀窍，与其说是“谦卑”，不如说是“有克制的贪婪”。</li><li>一家公司建立好了文化之后，就能走上良性循环的轨道。</li><li>为了防范风险，我们制定的规矩，恰恰是不赚最后一个铜板。</li><li>任何一家高杠杆的金融机构，无论管理者多么尽职尽责，都可能遭遇意外的损失。关键是遭遇意外之后，能否第一时间解决问题。在问题暴露出来以后，很多公司首先想到的是如何隐瞒，如何用会计手段蒙混过去。我们认为，应该不遮不掩、立刻解决。</li><li>即使拥有诚实守信的优良传统，时间久了，制度漏洞还是会毁掉优良传统。</li><li>裁员成本是一项巨大的隐形负债。很多公司因为裁员而支付的成本高达几亿、几十亿美金。公司明明要为缩小规模而付出成本，但这项成本并没有在资产负债表上体现出来。</li><li>我们从不签署允许我们懒惰的合同，以免我们走向堕落。</li><li>纵观商业史，很多公司辉煌过，赚过大钱，但是当它们被新的科技浪潮淘汰后，它们的家底很快就会耗光，最终走向消亡。</li><li>在服务业，只有全力以赴，为客户消除所有痛点，才能超越竞争对手。</li><li>经营一家公司，你懂的延迟满足，能把公司经营的越来越好。在人生中懂得延迟满足，你死的时候能很风光。</li><li>在沃伦眼中，优秀的管理者是这样的：你把他从火车上扔下去，扔到一个偏僻的小镇，不给他钱，他在这个小镇上诚实本分地经营，用不了多长时间，又发家致富了。</li><li>在与别人合作的过程中，沃伦和我都是首先以高标准要求自己。因为有优秀的人与我们一道努力，我们才能取得今天的成绩。</li><li>要找到优秀的伴侣，只有一个办法，就是自己得配得上。同样的道理，要找到优秀的人共事，你自己首先要是一个优秀的人。</li><li>我们不懂具体的软件业务，那我们怎么领导每日期刊公司呢？我们主要靠知人善任。</li><li>在做管理工作的过程中，最容易犯的错误是，已经发现该换人了，但迟迟下不了决心，拖了很长时间，才把不合适的人换掉。即使是有着多年管理经验的人，也很容易犯这个错误。</li><li>公司越大，越难建立起正确的文化。大公司特别容易患上官僚主义这个通病。</li><li>人们钻空子，肯定是因为激励制度有漏洞。</li><li>职业生涯的三条规则：不要销售你自己都不愿意买的东西；不要为你不尊重和不欣赏的人工作；只和你喜欢的人一起工作。</li><li>我们很少换人，不是因为我们软弱或愚蠢，而是因为我们一开始就把人选对了。</li><li>我愿意和优秀的人共事，不愿意和平庸的人为伍。</li><li>信任是你自己赢得的。你自己做事总是很靠谱，时间久了，别人自然会信任你。</li><li>我觉得在面对难题的时候，列一张清单非常有用。在单子上一列，所有问题一目了然，能把问题考虑得更周全，不会有什么遗漏。</li><li>凡是往简单处想，往认真处行。</li></ul><h2 id="关于投资理念">关于投资理念</h2><ul><li>真正做收购是好事多磨，要熬过辛苦的等待，经历反复的波折。</li><li>大多数时候，我们什么都不做。我们出手的时候很少。即使是出手的时候，我们也是如履薄冰，对可能承担的风险感到不安。</li><li>有些人收集邮票，而我收集疯狂和荒谬，然后避开它们。</li><li>钱多机会少，总比钱少机会多强。</li><li>我们只在很少的时候，能看透重大的机会。</li><li>我们始终把眼前所有的投资机会进行比较，力求找到当下最合理的投资逻辑，这才是重中之重。找到了最合理的同欧字逻辑之后，无论周期波动如何剧烈，是顺境还是逆境，我们都泰然自若。这就是我们的投资之道。</li><li>投资要选容错率高的好生意。有点管理问题，有点困难，有点错误，好生意照样还是好生意。</li><li>按我们这种方式投资，必须准确判断一家公司的前景。也就是说，你不但要能看出来，一家公司现在的生意是好生意，而且要能看出来，它在将来的很长时间里仍然是好生意。</li><li>买入好生意长期持有才是正道。</li><li>真正的好公司，现在的价格，大家可能觉得很贵，其实不贵。</li><li>格雷厄姆提出了安全边际的原则，这个概念永不过时。“市场是我们的仆人，不是我们的老师”。</li><li>做投资，一个是必须等大眼睛等待机会出现，另一个是机会出现时，必须果断出手。</li><li>钓鱼的第一条原则是，在有鱼的地方钓。钓鱼的第二条规则是，记住第一条规则。投资是同样的道理。</li><li>归根结底，投资只有价值投资一种。为什么这么说？因为我们每做一笔投资，把钱投进去，都是为了将来能获得更多的价值。</li><li>首先，要找自己能看懂的机会，不做自己看不懂的投资。然后，要踏踏实实地去做大量实际的工作。</li><li>成功的投资即需要进取心又需要耐心，而且还需要准备好在机会出现时抓住它，因为在这个世界上，机会不会持续很久。</li><li>我们能够成功，不是因为我们善于解决难题，而是因为我们善于远离难题。我们只是找到了容易做的事情。</li></ul>]]></content>
    
    <summary type="html">
    
      卓越的人很少，有机会追随他们，和他们走到一起...
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Wisdom" scheme="https://neo1989.net/tags/Wisdom/"/>
    
  </entry>
  
  <entry>
    <title>SQ3R阅读法</title>
    <link href="https://neo1989.net/Notes/NOTE-SQ3R/"/>
    <id>https://neo1989.net/Notes/NOTE-SQ3R/</id>
    <published>2023-07-11T11:33:19.000Z</published>
    <updated>2023-07-11T11:39:47.879Z</updated>
    
    <content type="html"><![CDATA[<p><img src="//s3.mindex.xyz/tmp/fc643b514ad76d94a6f205de37e747f8.png" alt=""></p><h4 id="Survey">Survey</h4><p>快速扫描章节小标题，识别出来几个关键点，如果有章节小结的，重点阅读。</p><h4 id="Question">Question</h4><p>把章节的标题换成一个问题，阅读这章节的目的就是为了回答这个问题。</p><h4 id="Read">Read</h4><p>带着问题去阅读，阅读过程中始终记得为这个问题寻找答案</p><h4 id="Recite">Recite</h4><p>阅读完之后，用自己的话尝试解答这个问题，如果回答不出来，就重复以上四个步骤，直到回答出来位置。</p><h4 id="Review">Review</h4><p>最后再次回顾，并用自己的话来复述整本书的主要观点。</p>]]></content>
    
    <summary type="html">
    
      方法
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="SoftSkills" scheme="https://neo1989.net/tags/SoftSkills/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · Embeddings （下）</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-Embeddings-2/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-Embeddings-2/</id>
    <published>2023-07-06T05:02:57.000Z</published>
    <updated>2023-07-10T14:17:47.993Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>上篇文章简单介绍了Embeddings，以及Glove。本篇将简单介绍加入Embedding层的CNN。</p><p>注意，所有的前置工作与<a href="https://neo1989.net/Way2AI/Way2AI-CNN/" title="卷积神经网络">《Way2AI · 卷积神经网络》</a>这篇文章里的介绍没有太大区别，最大的区别在于建模的时候加入了Embeddings层。</p><h2 id="Set-up">Set up</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip install numpy==1.21.2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_seeds</span><span class="params">(seed=<span class="number">1024</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Set seeds for reproducibility."""</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)  <span class="comment"># multi-GPU</span></span><br><span class="line"></span><br><span class="line">set_seeds(seed=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">cuda = <span class="literal">True</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span>(torch.cuda.is_available() <span class="keyword">and</span> cuda) <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">torch.set_default_tensor_type(&#123;<span class="string">"cuda"</span>: <span class="string">"torch.cuda.FloatTensor"</span>, <span class="string">"cpu"</span>: <span class="string">"torch.FloatTensor"</span>&#125;.get(str(device)))</span><br></pre></td></tr></table></figure><h2 id="Load-data">Load data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">"https://s3.mindex.xyz/datasets/news.csv"</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>)</span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df[<span class="string">"title"</span>][:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 0     Israel announces West Bank housing plan; barri...</span></span><br><span class="line"><span class="comment"># 1     Red Sox #39;s Feat: As far back as I can remember</span></span><br><span class="line"><span class="comment"># 2     J.P. Morgan Cancels IBM Outsourcing Deal (Reut...</span></span><br><span class="line"><span class="comment"># 3                          Intel Names Otellini New CEO</span></span><br><span class="line"><span class="comment"># 4     Branson Launches Virgin Atlantic Flights to Au...</span></span><br><span class="line"><span class="comment">#                             ...</span></span><br><span class="line"><span class="comment"># 95    Yahoo Profit Surges on Sales of Ads, Google Stock</span></span><br><span class="line"><span class="comment"># 96                                     DirecT Touchdown</span></span><br><span class="line"><span class="comment"># 97         Struggling Bucs Best Dismal Bears, 19-7 (AP)</span></span><br><span class="line"><span class="comment"># 98    Romania PM, Bucharest Mayor Battle for Preside...</span></span><br><span class="line"><span class="comment"># 99                      Glazer Quest for United Falters</span></span><br><span class="line"><span class="comment"># Name: title, Length: 100, dtype: object</span></span><br></pre></td></tr></table></figure><h2 id="Processing">Processing</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> PorterStemmer</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">nltk.download(<span class="string">"stopwords"</span>)</span><br><span class="line">STOPWORDS = stopwords.words(<span class="string">"english"</span>)</span><br><span class="line"><span class="keyword">print</span> (STOPWORDS[:<span class="number">5</span>])</span><br><span class="line">porter = PorterStemmer()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(text, stopwords=STOPWORDS)</span>:</span></span><br><span class="line">    <span class="string">"""Conditional preprocessing on our text unique to our task."""</span></span><br><span class="line">    <span class="comment"># Lower</span></span><br><span class="line">    text = text.lower()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove stopwords</span></span><br><span class="line">    pattern = re.compile(<span class="string">r"\b("</span> + <span class="string">r"|"</span>.join(stopwords) + <span class="string">r")\b\s*"</span>)</span><br><span class="line">    text = pattern.sub(<span class="string">""</span>, text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove words in parenthesis</span></span><br><span class="line">    text = re.sub(<span class="string">r"\([^)]*\)"</span>, <span class="string">""</span>, text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Spacing and filters</span></span><br><span class="line">    text = re.sub(<span class="string">r"([-;;.,!?&lt;=&gt;])"</span>, <span class="string">r" \1 "</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">"[^A-Za-z0-9]+"</span>, <span class="string">" "</span>, text) <span class="comment"># remove non alphanumeric chars</span></span><br><span class="line">    text = re.sub(<span class="string">" +"</span>, <span class="string">" "</span>, text)  <span class="comment"># remove multiple spaces</span></span><br><span class="line">    text = text.strip()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply to dataframe</span></span><br><span class="line">preprocessed_df = df.copy()</span><br><span class="line">preprocessed_df.title = preprocessed_df.title.apply(preprocess)</span><br></pre></td></tr></table></figure><h2 id="Split-data">Split data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_test_split</span><span class="params">(X, y, train_size)</span>:</span></span><br><span class="line">    <span class="string">"""Split dataset into data splits."""</span></span><br><span class="line">    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)</span><br><span class="line">    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Data</span></span><br><span class="line">X = preprocessed_df[<span class="string">"title"</span>].values</span><br><span class="line">y = preprocessed_df[<span class="string">"category"</span>].values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (84000,), y_train: (84000,)</span></span><br><span class="line"><span class="comment"># X_val: (18000,), y_val: (18000,)</span></span><br><span class="line"><span class="comment"># X_test: (18000,), y_test: (18000,)</span></span><br><span class="line"><span class="comment"># Sample point: ibm wins time talks pension case → Sci/Tech</span></span><br></pre></td></tr></table></figure><h2 id="Label-encoding">Label encoding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelEncoder</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Label encoder for tag labels."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, class_to_index=&#123;&#125;)</span>:</span></span><br><span class="line">        self.class_to_index = class_to_index <span class="keyword">or</span> &#123;&#125;  <span class="comment"># mutable defaults ;)</span></span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = list(self.class_to_index.keys())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.class_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"&lt;LabelEncoder(num_classes=<span class="subst">&#123;len(self)&#125;</span>)&gt;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        classes = np.unique(y)</span><br><span class="line">        <span class="keyword">for</span> i, class_ <span class="keyword">in</span> enumerate(classes):</span><br><span class="line">            self.class_to_index[class_] = i</span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = list(self.class_to_index.keys())</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        encoded = np.zeros((len(y)), dtype=int)</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            encoded[i] = self.class_to_index[item]</span><br><span class="line">        <span class="keyword">return</span> encoded</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        classes = []</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            classes.append(self.index_to_class[item])</span><br><span class="line">        <span class="keyword">return</span> classes</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;<span class="string">'class_to_index'</span>: self.class_to_index&#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(cls, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"r"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Encode</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">label_encoder.fit(y_train)</span><br><span class="line">NUM_CLASSES = len(label_encoder)</span><br><span class="line">print(label_encoder.class_to_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert labels to tokens</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">y_train = label_encoder.encode(y_train)</span><br><span class="line">y_val = label_encoder.encode(y_val)</span><br><span class="line">y_test = label_encoder.encode(y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: Sci/Tech</span></span><br><span class="line"><span class="comment"># y_train[0]: 1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> enumerate(counts)&#125;</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># counts: [21000 21000 21000 21000]</span></span><br><span class="line"><span class="comment"># weights: &#123;0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Tokenizer">Tokenizer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> more_itertools <span class="keyword">import</span> take</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, char_level, num_tokens=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 pad_token=<span class="string">"&lt;PAD&gt;"</span>, oov_token=<span class="string">"&lt;UNK&gt;"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 token_to_index=None)</span>:</span></span><br><span class="line">        self.char_level = char_level</span><br><span class="line">        self.separator = <span class="string">""</span> <span class="keyword">if</span> self.char_level <span class="keyword">else</span> <span class="string">" "</span></span><br><span class="line">        <span class="keyword">if</span> num_tokens: num_tokens -= <span class="number">2</span> <span class="comment"># pad + unk tokens</span></span><br><span class="line">        self.num_tokens = num_tokens</span><br><span class="line">        self.pad_token = pad_token</span><br><span class="line">        self.oov_token = oov_token</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> token_to_index:</span><br><span class="line">            token_to_index = &#123;pad_token: <span class="number">0</span>, oov_token: <span class="number">1</span>&#125;</span><br><span class="line">        self.token_to_index = token_to_index</span><br><span class="line">        self.index_to_token = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.token_to_index.items()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.token_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"&lt;Tokenizer(num_tokens=<span class="subst">&#123;len(self)&#125;</span>)&gt;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_on_texts</span><span class="params">(self, texts)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.char_level:</span><br><span class="line">            texts = [text.split(<span class="string">" "</span>) <span class="keyword">for</span> text <span class="keyword">in</span> texts]</span><br><span class="line">        all_tokens = [token <span class="keyword">for</span> text <span class="keyword">in</span> texts <span class="keyword">for</span> token <span class="keyword">in</span> text]</span><br><span class="line">        counts = Counter(all_tokens).most_common(self.num_tokens)</span><br><span class="line">        self.min_token_freq = counts[<span class="number">-1</span>][<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> token, count <span class="keyword">in</span> counts:</span><br><span class="line">            index = len(self)</span><br><span class="line">            self.token_to_index[token] = index</span><br><span class="line">            self.index_to_token[index] = token</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">texts_to_sequences</span><span class="params">(self, texts)</span>:</span></span><br><span class="line">        sequences = []</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> texts:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.char_level:</span><br><span class="line">                text = text.split(<span class="string">" "</span>)</span><br><span class="line">            sequence = []</span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> text:</span><br><span class="line">                sequence.append(self.token_to_index.get(</span><br><span class="line">                    token, self.token_to_index[self.oov_token]))</span><br><span class="line">            sequences.append(np.asarray(sequence))</span><br><span class="line">        <span class="keyword">return</span> sequences</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sequences_to_texts</span><span class="params">(self, sequences)</span>:</span></span><br><span class="line">        texts = []</span><br><span class="line">        <span class="keyword">for</span> sequence <span class="keyword">in</span> sequences:</span><br><span class="line">            text = []</span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> sequence:</span><br><span class="line">                text.append(self.index_to_token.get(index, self.oov_token))</span><br><span class="line">            texts.append(self.separator.join([token <span class="keyword">for</span> token <span class="keyword">in</span> text]))</span><br><span class="line">        <span class="keyword">return</span> texts</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;</span><br><span class="line">                <span class="string">"char_level"</span>: self.char_level,</span><br><span class="line">                <span class="string">"oov_token"</span>: self.oov_token,</span><br><span class="line">                <span class="string">"token_to_index"</span>: self.token_to_index</span><br><span class="line">            &#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(cls, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"r"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tokenize</span></span><br><span class="line">tokenizer = Tokenizer(char_level=<span class="literal">False</span>, num_tokens=<span class="number">5000</span>)</span><br><span class="line">tokenizer.fit_on_texts(texts=X_train)</span><br><span class="line">VOCAB_SIZE = len(tokenizer)</span><br><span class="line"><span class="keyword">print</span> (tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output </span></span><br><span class="line"><span class="comment"># &lt;Tokenizer(num_tokens=5000)&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample of tokens</span></span><br><span class="line"><span class="keyword">print</span> (take(<span class="number">5</span>, tokenizer.token_to_index.items()))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"least freq token's freq: <span class="subst">&#123;tokenizer.min_token_freq&#125;</span>"</span>) <span class="comment"># use this to adjust num_tokens</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [('&lt;PAD&gt;', 0), ('&lt;UNK&gt;', 1), ('39', 2), ('b', 3), ('gt', 4)]</span></span><br><span class="line"><span class="comment"># least freq token's freq: 14</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert texts to sequences of indices</span></span><br><span class="line">X_train = tokenizer.texts_to_sequences(X_train)</span><br><span class="line">X_val = tokenizer.texts_to_sequences(X_val)</span><br><span class="line">X_test = tokenizer.texts_to_sequences(X_test)</span><br><span class="line">preprocessed_text = tokenizer.sequences_to_texts([X_train[<span class="number">0</span>]])[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Text to indices:\n"</span></span><br><span class="line">    <span class="string">f"  (preprocessed) → <span class="subst">&#123;preprocessed_text&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  (tokenized) → <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Text to indices:</span></span><br><span class="line"><span class="comment">#   (preprocessed) → ibm wins time talks pension case</span></span><br><span class="line"><span class="comment">#   (tokenized) → [ 31  32  69  26 715 100]</span></span><br></pre></td></tr></table></figure><h2 id="Padding">Padding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_sequences</span><span class="params">(sequences, max_seq_len=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Pad sequences to max length in sequence."""</span></span><br><span class="line">    max_seq_len = max(max_seq_len, max(len(sequence) <span class="keyword">for</span> sequence <span class="keyword">in</span> sequences))</span><br><span class="line">    padded_sequences = np.zeros((len(sequences), max_seq_len))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        padded_sequences[i][:len(sequence)] = sequence</span><br><span class="line">    <span class="keyword">return</span> padded_sequences</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2D sequences</span></span><br><span class="line">padded = pad_sequences(X_train[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> (padded.shape)</span><br><span class="line"><span class="keyword">print</span> (padded)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># (3, 8)</span></span><br><span class="line"><span class="comment"># [[3.100e+01 3.200e+01 6.900e+01 2.600e+01 7.150e+02 1.000e+02 0.000e+00</span></span><br><span class="line"><span class="comment">#   0.000e+00]</span></span><br><span class="line"><span class="comment">#  [3.568e+03 9.000e+00 4.520e+03 2.000e+00 1.000e+00 2.396e+03 7.760e+02</span></span><br><span class="line"><span class="comment">#   1.500e+01]</span></span><br><span class="line"><span class="comment">#  [1.000e+01 1.094e+03 7.600e+01 5.960e+02 5.740e+02 8.000e+02 0.000e+00</span></span><br><span class="line"><span class="comment">#   0.000e+00]]</span></span><br></pre></td></tr></table></figure><h2 id="Dataset">Dataset</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">FILTER_SIZES = list(range(<span class="number">2</span>, <span class="number">5</span>)) <span class="comment"># bi, tri and 4 grams</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, X, y, max_filter_size)</span>:</span></span><br><span class="line">        self.X = X</span><br><span class="line">        self.y = y</span><br><span class="line">        self.max_filter_size = max_filter_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"&lt;Dataset(N=<span class="subst">&#123;len(self)&#125;</span>)&gt;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        X = self.X[index]</span><br><span class="line">        y = self.y[index]</span><br><span class="line">        <span class="keyword">return</span> [X, y]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(self, batch)</span>:</span></span><br><span class="line">        <span class="string">"""Processing on a batch."""</span></span><br><span class="line">        <span class="comment"># Get inputs</span></span><br><span class="line">        batch = np.array(batch)</span><br><span class="line">        X = batch[:, <span class="number">0</span>]</span><br><span class="line">        y = batch[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pad sequences</span></span><br><span class="line">        X = pad_sequences(X)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cast</span></span><br><span class="line">        X = torch.LongTensor(X.astype(np.int32))</span><br><span class="line">        y = torch.LongTensor(y.astype(np.int32))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_dataloader</span><span class="params">(self, batch_size, shuffle=False, drop_last=False)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.utils.data.DataLoader(</span><br><span class="line">            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,</span><br><span class="line">            shuffle=shuffle, drop_last=drop_last, pin_memory=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create datasets</span></span><br><span class="line">max_filter_size = max(FILTER_SIZES)</span><br><span class="line">train_dataset = Dataset(X=X_train, y=y_train, max_filter_size=max_filter_size)</span><br><span class="line">val_dataset = Dataset(X=X_val, y=y_val, max_filter_size=max_filter_size)</span><br><span class="line">test_dataset = Dataset(X=X_test, y=y_test, max_filter_size=max_filter_size)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Datasets:\n"</span></span><br><span class="line">    <span class="string">f"  Train dataset:<span class="subst">&#123;train_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  Val dataset: <span class="subst">&#123;val_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  Test dataset: <span class="subst">&#123;test_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">    <span class="string">"Sample point:\n"</span></span><br><span class="line">    <span class="string">f"  X: <span class="subst">&#123;train_dataset[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  y: <span class="subst">&#123;train_dataset[<span class="number">0</span>][<span class="number">1</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Datasets:</span></span><br><span class="line"><span class="comment">#   Train dataset:&lt;Dataset(N=84000)&gt;</span></span><br><span class="line"><span class="comment">#   Val dataset: &lt;Dataset(N=18000)&gt;</span></span><br><span class="line"><span class="comment">#   Test dataset: &lt;Dataset(N=18000)&gt;</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: [ 31  32  69  26 715 100]</span></span><br><span class="line"><span class="comment">#   y: 1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create dataloaders</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_dataloader = train_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">val_dataloader = val_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">test_dataloader = test_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">batch_X, batch_y = next(iter(train_dataloader))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Sample batch:\n"</span></span><br><span class="line">    <span class="string">f"  X: <span class="subst">&#123;list(batch_X.size())&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  y: <span class="subst">&#123;list(batch_y.size())&#125;</span>\n"</span></span><br><span class="line">    <span class="string">"Sample point:\n"</span></span><br><span class="line">    <span class="string">f"  X: <span class="subst">&#123;batch_X[<span class="number">0</span>]&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  y: <span class="subst">&#123;batch_y[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Sample batch:</span></span><br><span class="line"><span class="comment">#   X: [64, 10]</span></span><br><span class="line"><span class="comment">#   y: [64]</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: tensor([ 31,  32,  69,  26, 715, 100,   0,   0,   0,   0])</span></span><br><span class="line"><span class="comment">#   y: 1</span></span><br></pre></td></tr></table></figure><h2 id="Model">Model</h2><p>可视化一下模型的前向传播.</p><ul><li>首先对输入tokenizer化 (batch_size, max_seq_len)</li><li>然后我们对tokenizered输入进行embed (batch_size, max_seq_len, embedding_dim)</li><li>接下来，使用filters（filter_size, vocab_size, num_filter)进行卷积，然后批归一化。我们讲使用三个不同size的filter（2, 3 和 4）分别充当bi-gram, tri-gram 和 4-gram 特征提取器。</li><li>紧跟着，应用一维max polling，从特征图中提取最相关信息以做出决策</li><li>再接一个含dropout的全连接层</li><li>最后再使用一个softmax全连接层以输出最终的类别概率</li></ul><p><img src="//s3.mindex.xyz/tmp/89231298b192a7831de7c18f7c52f6ad.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">HIDDEN_DIM = <span class="number">100</span></span><br><span class="line">DROPOUT_P = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_dim, vocab_size, num_filters,</span></span></span><br><span class="line"><span class="function"><span class="params">                 filter_sizes, hidden_dim, dropout_p, num_classes,</span></span></span><br><span class="line"><span class="function"><span class="params">                 pretrained_embeddings=None, freeze_embeddings=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 padding_idx=<span class="number">0</span>)</span>:</span></span><br><span class="line">        super(CNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Filter sizes</span></span><br><span class="line">        self.filter_sizes = filter_sizes</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize embeddings</span></span><br><span class="line">        <span class="keyword">if</span> pretrained_embeddings <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.embeddings = nn.Embedding(</span><br><span class="line">                embedding_dim=embedding_dim, num_embeddings=vocab_size,</span><br><span class="line">                padding_idx=padding_idx)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()</span><br><span class="line">            self.embeddings = nn.Embedding(</span><br><span class="line">                embedding_dim=embedding_dim, num_embeddings=vocab_size,</span><br><span class="line">                padding_idx=padding_idx, _weight=pretrained_embeddings)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Freeze embeddings or not</span></span><br><span class="line">        <span class="keyword">if</span> freeze_embeddings:</span><br><span class="line">            self.embeddings.weight.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Conv weights</span></span><br><span class="line">        self.conv = nn.ModuleList(</span><br><span class="line">            [nn.Conv1d(in_channels=embedding_dim,</span><br><span class="line">                       out_channels=num_filters,</span><br><span class="line">                       kernel_size=f) <span class="keyword">for</span> f <span class="keyword">in</span> filter_sizes])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FC weights</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout_p)</span><br><span class="line">        self.fc1 = nn.Linear(num_filters*len(filter_sizes), hidden_dim)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, channel_first=False)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Embed</span></span><br><span class="line">        x_in, = inputs</span><br><span class="line">        x_in = self.embeddings(x_in)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Rearrange input so num_channels is in dim 1 (N, C, L)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> channel_first:</span><br><span class="line">            x_in = x_in.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Conv outputs</span></span><br><span class="line">        z = []</span><br><span class="line">        max_seq_len = x_in.shape[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">for</span> i, f <span class="keyword">in</span> enumerate(self.filter_sizes):</span><br><span class="line">            <span class="comment"># `SAME` padding</span></span><br><span class="line">            padding_left = int((self.conv[i].stride[<span class="number">0</span>]*(max_seq_len<span class="number">-1</span>) - max_seq_len + self.filter_sizes[i])/<span class="number">2</span>)</span><br><span class="line">            padding_right = int(math.ceil((self.conv[i].stride[<span class="number">0</span>]*(max_seq_len<span class="number">-1</span>) - max_seq_len + self.filter_sizes[i])/<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Conv + pool</span></span><br><span class="line">            _z = self.conv[i](F.pad(x_in, (padding_left, padding_right)))</span><br><span class="line">            _z = F.max_pool1d(_z, _z.size(<span class="number">2</span>)).squeeze(<span class="number">2</span>)</span><br><span class="line">            z.append(_z)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Concat conv outputs</span></span><br><span class="line">        z = torch.cat(z, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FC layers</span></span><br><span class="line">        z = self.fc1(z)</span><br><span class="line">        z = self.dropout(z)</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure><h2 id="Using-GloVe">Using GloVe</h2><p>先实现一些方便能够将预训练的GloVe加载到我们的模型中的公共方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_glove_embeddings</span><span class="params">(embeddings_file)</span>:</span></span><br><span class="line">    <span class="string">"""Load embeddings from a file."""</span></span><br><span class="line">    embeddings = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> open(embeddings_file, <span class="string">"r"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        <span class="keyword">for</span> index, line <span class="keyword">in</span> enumerate(fp):</span><br><span class="line">            values = line.split()</span><br><span class="line">            word = values[<span class="number">0</span>]</span><br><span class="line">            embedding = np.asarray(values[<span class="number">1</span>:], dtype=<span class="string">'float32'</span>)</span><br><span class="line">            embeddings[word] = embedding</span><br><span class="line">    <span class="keyword">return</span> embeddings</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_embeddings_matrix</span><span class="params">(embeddings, word_index, embedding_dim)</span>:</span></span><br><span class="line">    <span class="string">"""Create embeddings matrix to use in Embedding layer."""</span></span><br><span class="line">    embedding_matrix = np.zeros((len(word_index), embedding_dim))</span><br><span class="line">    <span class="keyword">for</span> word, i <span class="keyword">in</span> word_index.items():</span><br><span class="line">        embedding_vector = embeddings.get(word)</span><br><span class="line">        <span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            embedding_matrix[i] = embedding_vector</span><br><span class="line">    <span class="keyword">return</span> embedding_matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create embeddings</span></span><br><span class="line">embeddings_file = <span class="string">'glove.6B.&#123;0&#125;d.txt'</span>.format(EMBEDDING_DIM)</span><br><span class="line">glove_embeddings = load_glove_embeddings(embeddings_file=embeddings_file)</span><br><span class="line">embedding_matrix = make_embeddings_matrix(</span><br><span class="line">    embeddings=glove_embeddings, word_index=tokenizer.token_to_index,</span><br><span class="line">    embedding_dim=EMBEDDING_DIM)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"&lt;Embeddings(words=<span class="subst">&#123;embedding_matrix.shape[<span class="number">0</span>]&#125;</span>, dim=<span class="subst">&#123;embedding_matrix.shape[<span class="number">1</span>]&#125;</span>)&gt;"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;Embeddings(words=5000, dim=100)&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Experiments">Experiments</h2><p>接下来，我们将进行三个实验：</p><ul><li>随机初始化的embeddings (fine-tuned)</li><li>GloVe embeddings (frozen)</li><li>GloVe embeddings (fine-tuned)</li></ul><p>先定义我们的Trainer</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">NUM_FILTERS = <span class="number">50</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-3</span></span><br><span class="line">PATIENCE = <span class="number">5</span></span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, device, loss_fn=None, optimizer=None, scheduler=None)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set params</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.device = device</span><br><span class="line">        self.loss_fn = loss_fn</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self.scheduler = scheduler</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Train step."""</span></span><br><span class="line">        <span class="comment"># Set model to train mode</span></span><br><span class="line">        self.model.train()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over train batches</span></span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Step</span></span><br><span class="line">            batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">            inputs, targets = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">            self.optimizer.zero_grad()  <span class="comment"># Reset gradients</span></span><br><span class="line">            z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">            J = self.loss_fn(z, targets)  <span class="comment"># Define loss</span></span><br><span class="line">            J.backward()  <span class="comment"># Backward pass</span></span><br><span class="line">            self.optimizer.step()  <span class="comment"># Update weights</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Cumulative Metrics</span></span><br><span class="line">            loss += (J.detach().item() - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Validation or test step."""</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.eval()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line">        y_trues, y_probs = [], []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Step</span></span><br><span class="line">                batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">                inputs, y_true = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">                z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">                J = self.loss_fn(z, y_true).item()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Cumulative Metrics</span></span><br><span class="line">                loss += (J - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line">                y_trues.extend(y_true.cpu().numpy())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, np.vstack(y_trues), np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Prediction step."""</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.eval()</span><br><span class="line">        y_probs = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Forward pass w/ inputs</span></span><br><span class="line">                inputs, targets = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">                z = self.model(inputs)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, num_epochs, patience, train_dataloader, val_dataloader)</span>:</span></span><br><span class="line">        best_val_loss = np.inf</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">            <span class="comment"># Steps</span></span><br><span class="line">            train_loss = self.train_step(dataloader=train_dataloader)</span><br><span class="line">            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)</span><br><span class="line">            self.scheduler.step(val_loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Early stopping</span></span><br><span class="line">            <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">                best_val_loss = val_loss</span><br><span class="line">                best_model = self.model</span><br><span class="line">                _patience = patience  <span class="comment"># reset _patience</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                _patience -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> _patience:  <span class="comment"># 0</span></span><br><span class="line">                print(<span class="string">"Stopping early!"</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Logging</span></span><br><span class="line">            print(</span><br><span class="line">                <span class="string">f"Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> | "</span></span><br><span class="line">                <span class="string">f"train_loss: <span class="subst">&#123;train_loss:<span class="number">.5</span>f&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"val_loss: <span class="subst">&#123;val_loss:<span class="number">.5</span>f&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"lr: <span class="subst">&#123;self.optimizer.param_groups[<span class="number">0</span>][<span class="string">'lr'</span>]:<span class="number">.2</span>E&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"_patience: <span class="subst">&#123;_patience&#125;</span>"</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> best_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(y_true, y_pred, classes)</span>:</span></span><br><span class="line">    <span class="string">"""Per-class performance metrics."""</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">"overall"</span>: &#123;&#125;, <span class="string">"class"</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">"weighted"</span>)</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"precision"</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"recall"</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"f1"</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"num_samples"</span>] = np.float64(len(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">        performance[<span class="string">"class"</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">"precision"</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">"recall"</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">"f1"</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">"num_samples"</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br></pre></td></tr></table></figure><h3 id="Random-initialization">Random initialization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">PRETRAINED_EMBEDDINGS = <span class="literal">None</span></span><br><span class="line">FREEZE_EMBEDDINGS = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = CNN(</span><br><span class="line">    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,</span><br><span class="line">    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,</span><br><span class="line">    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,</span><br><span class="line">    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)</span><br><span class="line">model = model.to(device) <span class="comment"># set device</span></span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of CNN(</span></span><br><span class="line"><span class="comment">#   (embeddings): Embedding(5000, 100, padding_idx=0)</span></span><br><span class="line"><span class="comment">#   (conv): ModuleList(</span></span><br><span class="line"><span class="comment">#     (0): Conv1d(100, 50, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (1): Conv1d(100, 50, kernel_size=(3,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (2): Conv1d(100, 50, kernel_size=(4,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   )</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=150, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer &amp; scheduler</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">    optimizer, mode=<span class="string">"min"</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trainer module</span></span><br><span class="line">trainer = Trainer(model=model, device=device, loss_fn=loss_fn,</span><br><span class="line">                  optimizer=optimizer, scheduler=scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">best_model = trainer.train(NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 1 | train_loss: 0.78800, val_loss: 0.64168, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 2 | train_loss: 0.49324, val_loss: 0.60757, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 3 | train_loss: 0.38917, val_loss: 0.63572, lr: 1.00E-03, _patience: 4</span></span><br><span class="line"><span class="comment"># Epoch: 4 | train_loss: 0.31891, val_loss: 0.70638, lr: 1.00E-03, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 5 | train_loss: 0.26606, val_loss: 0.76403, lr: 1.00E-03, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 6 | train_loss: 0.22631, val_loss: 0.79747, lr: 1.00E-04, _patience: 1</span></span><br><span class="line"><span class="comment"># Stopping early!</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions</span></span><br><span class="line">test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Determine performance</span></span><br><span class="line">performance = get_metrics(</span><br><span class="line">    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance[<span class="string">"overall"</span>], indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "precision": 0.8065551302331581,</span></span><br><span class="line"><span class="comment">#   "recall": 0.8066666666666666,</span></span><br><span class="line"><span class="comment">#   "f1": 0.8062901077799052,</span></span><br><span class="line"><span class="comment">#   "num_samples": 18000.0</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><h3 id="Glove-frozen">Glove (frozen)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">PRETRAINED_EMBEDDINGS = embedding_matrix</span><br><span class="line">FREEZE_EMBEDDINGS = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = CNN(</span><br><span class="line">    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,</span><br><span class="line">    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,</span><br><span class="line">    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,</span><br><span class="line">    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)</span><br><span class="line">model = model.to(device) <span class="comment"># set device</span></span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of CNN(</span></span><br><span class="line"><span class="comment">#   (embeddings): Embedding(5000, 100, padding_idx=0)</span></span><br><span class="line"><span class="comment">#   (conv): ModuleList(</span></span><br><span class="line"><span class="comment">#     (0): Conv1d(100, 50, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (1): Conv1d(100, 50, kernel_size=(3,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (2): Conv1d(100, 50, kernel_size=(4,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   )</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=150, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer &amp; scheduler</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">    optimizer, mode=<span class="string">"min"</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trainer module</span></span><br><span class="line">trainer = Trainer(model=model, device=device, loss_fn=loss_fn,</span><br><span class="line">                  optimizer=optimizer, scheduler=scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">best_model = trainer.train(NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 1 | train_loss: 0.51462, val_loss: 0.49800, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 2 | train_loss: 0.43604, val_loss: 0.49792, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 3 | train_loss: 0.39698, val_loss: 0.50526, lr: 1.00E-03, _patience: 4</span></span><br><span class="line"><span class="comment"># Epoch: 4 | train_loss: 0.36507, val_loss: 0.51659, lr: 1.00E-03, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 5 | train_loss: 0.33745, val_loss: 0.53612, lr: 1.00E-03, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 6 | train_loss: 0.31418, val_loss: 0.56722, lr: 1.00E-04, _patience: 1</span></span><br><span class="line"><span class="comment"># Stopping early!</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions</span></span><br><span class="line">test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Determine performance</span></span><br><span class="line">performance = get_metrics(</span><br><span class="line">    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance[<span class="string">"overall"</span>], indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "precision": 0.8264024010717701,</span></span><br><span class="line"><span class="comment">#   "recall": 0.8269444444444445,</span></span><br><span class="line"><span class="comment">#   "f1": 0.8263287754212785,</span></span><br><span class="line"><span class="comment">#   "num_samples": 18000.0</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><h3 id="GloVe-fine-tuned">GloVe (fine-tuned)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">PRETRAINED_EMBEDDINGS = embedding_matrix</span><br><span class="line">FREEZE_EMBEDDINGS = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = CNN(</span><br><span class="line">    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,</span><br><span class="line">    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,</span><br><span class="line">    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,</span><br><span class="line">    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)</span><br><span class="line">model = model.to(device) <span class="comment"># set device</span></span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of CNN(</span></span><br><span class="line"><span class="comment">#   (embeddings): Embedding(5000, 100, padding_idx=0)</span></span><br><span class="line"><span class="comment">#   (conv): ModuleList(</span></span><br><span class="line"><span class="comment">#     (0): Conv1d(100, 50, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (1): Conv1d(100, 50, kernel_size=(3,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (2): Conv1d(100, 50, kernel_size=(4,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   )</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=150, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Lossclass_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer &amp; scheduler</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">    optimizer, mode=<span class="string">"min"</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trainer module</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, device=device, loss_fn=loss_fn,</span><br><span class="line">    optimizer=optimizer, scheduler=scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">best_model = trainer.train(</span><br><span class="line">    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 1 | train_loss: 0.48751, val_loss: 0.45729, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 2 | train_loss: 0.38391, val_loss: 0.45669, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 3 | train_loss: 0.33045, val_loss: 0.47826, lr: 1.00E-03, _patience: 4</span></span><br><span class="line"><span class="comment"># Epoch: 4 | train_loss: 0.27825, val_loss: 0.52608, lr: 1.00E-03, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 5 | train_loss: 0.22646, val_loss: 0.60470, lr: 1.00E-03, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 6 | train_loss: 0.18130, val_loss: 0.70291, lr: 1.00E-04, _patience: 1</span></span><br><span class="line"><span class="comment"># Stopping early!</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions</span></span><br><span class="line">test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Determine performance</span></span><br><span class="line">performance = get_metrics(</span><br><span class="line">    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance[<span class="string">"overall"</span>], indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "precision": 0.8246875013006352,</span></span><br><span class="line"><span class="comment">#   "recall": 0.8251666666666667,</span></span><br><span class="line"><span class="comment">#   "f1": 0.8248028697657125,</span></span><br><span class="line"><span class="comment">#   "num_samples": 18000.0</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><p>Ok, 保存一些必要的模型数据，以供后续能够完整的加载和使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save artifacts</span></span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line">dir = Path(<span class="string">"cnn"</span>)</span><br><span class="line">dir.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">label_encoder.save(fp=Path(dir, <span class="string">"label_encoder.json"</span>))</span><br><span class="line">tokenizer.save(fp=Path(dir, <span class="string">"tokenizer.json"</span>))</span><br><span class="line">torch.save(best_model.state_dict(), Path(dir, <span class="string">"model.pt"</span>))</span><br><span class="line"><span class="keyword">with</span> open(Path(dir, <span class="string">"performance.json"</span>), <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    json.dump(performance, indent=<span class="number">2</span>, sort_keys=<span class="literal">False</span>, fp=fp)</span><br></pre></td></tr></table></figure><h2 id="Inference">Inference</h2><p>接下来看看如何利用模型进行推理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_probability_distribution</span><span class="params">(y_prob, classes)</span>:</span></span><br><span class="line">    <span class="string">"""Create a dict of class probabilities from an array."""</span></span><br><span class="line">    results = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, class_ <span class="keyword">in</span> enumerate(classes):</span><br><span class="line">        results[class_] = np.float64(y_prob[i])</span><br><span class="line">    sorted_results = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> sorted(</span><br><span class="line">        results.items(), key=<span class="keyword">lambda</span> item: item[<span class="number">1</span>], reverse=<span class="literal">True</span>)&#125;</span><br><span class="line">    <span class="keyword">return</span> sorted_results</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load artifacts</span></span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line">label_encoder = LabelEncoder.load(fp=Path(dir, <span class="string">"label_encoder.json"</span>))</span><br><span class="line">tokenizer = Tokenizer.load(fp=Path(dir, <span class="string">"tokenizer.json"</span>))</span><br><span class="line">model = CNN(</span><br><span class="line">    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,</span><br><span class="line">    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,</span><br><span class="line">    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,</span><br><span class="line">    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)</span><br><span class="line">model.load_state_dict(torch.load(Path(dir, <span class="string">"model.pt"</span>), map_location=device))</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># CNN(</span></span><br><span class="line"><span class="comment">#   (embeddings): Embedding(5000, 100, padding_idx=0)</span></span><br><span class="line"><span class="comment">#   (conv): ModuleList(</span></span><br><span class="line"><span class="comment">#     (0): Conv1d(100, 50, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (1): Conv1d(100, 50, kernel_size=(3,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (2): Conv1d(100, 50, kernel_size=(4,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   )</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=150, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize trainer</span></span><br><span class="line">trainer = Trainer(model=model, device=device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dataloader</span></span><br><span class="line">text = <span class="string">"The final tennis tournament starts next week."</span></span><br><span class="line">X = tokenizer.texts_to_sequences([preprocess(text)])</span><br><span class="line"><span class="keyword">print</span> (tokenizer.sequences_to_texts(X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># ['final tennis tournament starts next week']</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y_filler = label_encoder.encode([label_encoder.classes[<span class="number">0</span>]]*len(X))</span><br><span class="line">dataset = Dataset(X=X, y=y_filler, max_filter_size=max_filter_size)</span><br><span class="line">dataloader = dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line">y_prob = trainer.predict_step(dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line">label_encoder.decode(y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Class distributions</span></span><br><span class="line">prob_dist = get_probability_distribution(y_prob=y_prob[<span class="number">0</span>], classes=label_encoder.classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(prob_dist, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "Sports": 1.0,</span></span><br><span class="line"><span class="comment">#   "World": 7.881690092248483e-12,</span></span><br><span class="line"><span class="comment">#   "Sci/Tech": 1.270132816196673e-13,</span></span><br><span class="line"><span class="comment">#   "Business": 2.3282168800871726e-18</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><p>推理结果是 “The final tennis tournament starts next week.” 这篇文章属于 “Sports” 这个分类。</p><p>我们可以看看不同的n-gram提取器，在最大池化层里提取都是什么。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">sample_index = <span class="number">0</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Original text:\n<span class="subst">&#123;text&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"\nPreprocessed text:\n<span class="subst">&#123;tokenizer.sequences_to_texts(X)[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"\nMost important n-grams:"</span>)</span><br><span class="line"><span class="comment"># Process conv outputs for each unique filter size</span></span><br><span class="line"><span class="keyword">for</span> i, filter_size <span class="keyword">in</span> enumerate(FILTER_SIZES):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Identify most important n-gram (excluding last token)</span></span><br><span class="line">    popular_indices = collections.Counter([np.argmax(conv_output) \</span><br><span class="line">            <span class="keyword">for</span> conv_output <span class="keyword">in</span> conv_outputs[i]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get corresponding text</span></span><br><span class="line">    start = popular_indices.most_common(<span class="number">1</span>)[<span class="number">-1</span>][<span class="number">0</span>]</span><br><span class="line">    n_gram = <span class="string">" "</span>.join([token <span class="keyword">for</span> token <span class="keyword">in</span> tokens[start:start+filter_size]])</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">f"[<span class="subst">&#123;filter_size&#125;</span>-gram]: <span class="subst">&#123;n_gram&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Original text:</span></span><br><span class="line"><span class="comment"># The final tennis tournament starts next week.</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Preprocessed text:</span></span><br><span class="line"><span class="comment"># final tennis tournament starts next week</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Most important n-grams:</span></span><br><span class="line"><span class="comment"># [2-gram]: tennis tournament</span></span><br><span class="line"><span class="comment"># [3-gram]: final tennis tournament</span></span><br><span class="line"><span class="comment"># [4-gram]: final tennis tournament starts</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>如你所见，加入Embedding层的卷积神经网络模型的表现，相较于只有one-hot编码的模型，性能上有了很大的提升。</p>]]></content>
    
    <summary type="html">
    
      Explore and motivate the need for representation via embeddings.
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · Embeddings （上）</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-Embeddings-1/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-Embeddings-1/</id>
    <published>2023-07-04T07:28:36.000Z</published>
    <updated>2023-07-04T15:35:49.798Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>虽然One-Hot编码能够将离散变量表示为二进制向量，且能保留结构化信息，但它有两个主要的缺点：</p><ul><li>线性依赖词表的大小。这在语料库很大的情况下会带来问题如维数巨大且稀疏</li><li>单个token的表示，不保留其相对于其它token的关系</li></ul><p>本文将简单介绍embeddings，及它是如何解决one-hot编码的所有缺点。</p><h2 id="Learning-embeddings">Learning embeddings</h2><p>我们将通过使用PyTorch建模来学习embeddings，不过首先，我们学习一下专门用于嵌入和主题建模的库<a href="https://radimrehurek.com/gensim/" target="_blank" rel="noopener" title="Gensim">Gensim</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.download(<span class="string">"punkt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split text into sentence</span></span><br><span class="line">tokenizer = nltk.data.load(<span class="string">"tokenizers/punkt/english.pickle"</span>)</span><br><span class="line">book = requests.get(<span class="string">"https://s3.mindex.xyz/datasets/harrypotter.txt"</span>).content</span><br><span class="line">sentences = tokenizer.tokenize(str(book))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"<span class="subst">&#123;len(sentences)&#125;</span> sentences"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 12449 sentences</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="string">"""Conditional preprocessing on our text."""</span></span><br><span class="line">    <span class="comment"># Lower</span></span><br><span class="line">    text = text.lower()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Spacing and filters</span></span><br><span class="line">    text = re.sub(<span class="string">r"([-;;.,!?&lt;=&gt;])"</span>, <span class="string">r" \1 "</span>, text)  <span class="comment"># separate punctuation tied to words</span></span><br><span class="line">    text = re.sub(<span class="string">"[^A-Za-z0-9]+"</span>, <span class="string">" "</span>, text)  <span class="comment"># remove non alphanumeric chars</span></span><br><span class="line">    text = re.sub(<span class="string">" +"</span>, <span class="string">" "</span>, text)  <span class="comment"># remove multiple spaces</span></span><br><span class="line">    text = text.strip()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Separate into word tokens</span></span><br><span class="line">    text = text.split(<span class="string">" "</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocess sentences</span></span><br><span class="line"><span class="keyword">print</span> (sentences[<span class="number">11</span>])</span><br><span class="line">sentences = [preprocess(s) <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line"><span class="keyword">print</span> (sentences[<span class="number">11</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Snape nodded, but did not elaborate.</span></span><br><span class="line"><span class="comment"># ['snape', 'nodded', 'but', 'did', 'not', 'elaborate']</span></span><br></pre></td></tr></table></figure><p>embeddings的核心就是单词表示，且这种表示不只是依赖单词本身，而且依赖它的上下文。我们有几种不同的方法可以实现这一目标：</p><ul><li>给定上下文中的单词，预测目标单词（CBOW )</li><li>给定目标词，预测上下文词（skip-gram)</li><li>给定一个文本序列，预测下一个单词（ LM ）</li></ul><p>上面这些方法都涉及到创建数据来训练模型。句子中的每个单词都成为目标单词，上下文由窗口决定。</p><p>如下图（skip-gram），窗口大小为2。我们对语料库中的每个句子重复此操作，以产生用于无监督任务的训练数据。这个任务的核心逻辑是，相似的词会出现在相似的上下文中，我们可以通过反复的使用这种(target, context)文本对来学习这种关系。</p><p><img src="//s3.mindex.xyz/tmp/7bf17eefb4ff89642692d20685c9cb1a.webp" alt=""></p><p>我们可以使用上述任何一种方法来应用Embeddings。在任务中到底选择哪种方案，可能更多的需要依靠在监督任务上的表现来做选择。</p><h3 id="Word2Vec">Word2Vec</h3><p>当我们有大量的词汇表需要应用Embeddings时，事情会变得复杂。回想一下在反向传播中使用softmax更新正确的和不正确的分类权重，这种情况下每一次反向传播都意味着一个巨大的计算。因此解决方案是使用<a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" target="_blank" rel="noopener" title="Word2Vec Negative Sampling">负采样</a>，它只更新正确的类和随机一部分不正确的类（NEGATIVE_SAMPLING = 20）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> KeyedVectors</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"></span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">WINDOW = <span class="number">5</span></span><br><span class="line">MIN_COUNT = <span class="number">3</span></span><br><span class="line">SKIP_GRAM = <span class="number">1</span></span><br><span class="line">NEGATIVE_SAMPLING = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">w2v = Word2Vec(</span><br><span class="line">    sentences=sentences, vector_size=EMBEDDING_DIM,</span><br><span class="line">    window=WINDOW, min_count=MIN_COUNT,</span><br><span class="line">    sg=SKIP_GRAM, negative=NEGATIVE_SAMPLING)</span><br><span class="line"><span class="keyword">print</span> (w2v)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Word2Vec&lt;vocab=4937, vector_size=100, alpha=0.025&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Vector for each word</span></span><br><span class="line">w2v.wv.get_vector(<span class="string">"potter"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># array([ 0.04592679,  0.26393083, -0.29759625, -0.51007414,  0.02860732,</span></span><br><span class="line"><span class="comment">#        -0.01302573,  0.3703193 ,  0.14425582, -0.4187037 ,  0.04296769,</span></span><br><span class="line"><span class="comment">#        -0.13030362, -0.30441925, -0.14958233,  0.04964258,  0.14798391,</span></span><br><span class="line"><span class="comment">#        -0.18539314,  0.51730794,  0.01598365, -0.11325987, -0.6307836 ,</span></span><br><span class="line"><span class="comment">#         0.39244524,  0.25232184,  0.29555508, -0.22162063, -0.29100868,</span></span><br><span class="line"><span class="comment">#        -0.22083738, -0.52918744, -0.68654346, -0.09764519,  0.05514489,</span></span><br><span class="line"><span class="comment">#         0.06108054,  0.3587375 , -0.01166064, -0.42530054, -0.05000629,</span></span><br><span class="line"><span class="comment">#         0.45623606, -0.29811206, -0.09037815, -0.0024387 , -0.41930553,</span></span><br><span class="line"><span class="comment">#         0.12495753, -0.1773121 ,  0.19551197,  0.02754493,  0.25369856,</span></span><br><span class="line"><span class="comment">#         0.10022393, -0.38912103, -0.10274333, -0.24544689,  0.00851442,</span></span><br><span class="line"><span class="comment">#         0.26698554, -0.03026148,  0.12343717, -0.07433262,  0.0162609 ,</span></span><br><span class="line"><span class="comment">#         0.15033086,  0.09943663,  0.28371716, -0.26024884, -0.05571229,</span></span><br><span class="line"><span class="comment">#         0.0938114 , -0.00562614, -0.11472147,  0.21217017,  0.12490374,</span></span><br><span class="line"><span class="comment">#         0.34131378,  0.10346038,  0.38650215, -0.44265935, -0.02233333,</span></span><br><span class="line"><span class="comment">#        -0.47005087, -0.28585035,  0.06968105,  0.08989634,  0.22004889,</span></span><br><span class="line"><span class="comment">#        -0.22940454, -0.06248426,  0.089827  , -0.35011858,  0.11977731,</span></span><br><span class="line"><span class="comment">#        -0.06323916,  0.0940324 , -0.31842625,  0.53730965,  0.17043817,</span></span><br><span class="line"><span class="comment">#         0.15869781,  0.40275395,  0.04705542,  0.35397893,  0.00738561,</span></span><br><span class="line"><span class="comment">#         0.21539825,  0.14310665,  0.13341616, -0.0660746 ,  0.42496106,</span></span><br><span class="line"><span class="comment">#         0.09145384,  0.47487733, -0.23636843,  0.00715503,  0.05220298],</span></span><br><span class="line"><span class="comment">#       dtype=float32)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get nearest neighbors (excluding itself)</span></span><br><span class="line">w2v.wv.most_similar(positive=<span class="string">"scar"</span>, topn=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [('forehead', 0.9045635461807251),</span></span><br><span class="line"><span class="comment">#  ('pain', 0.9014869928359985),</span></span><br><span class="line"><span class="comment">#  ('mouth', 0.8918080925941467),</span></span><br><span class="line"><span class="comment">#  ('prickling', 0.890386164188385),</span></span><br><span class="line"><span class="comment">#  ('throat', 0.8795480728149414)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Saving and loading</span></span><br><span class="line">w2v.wv.save_word2vec_format(<span class="string">"w2v.bin"</span>, binary=<span class="literal">True</span>)</span><br><span class="line">wv = KeyedVectors.load_word2vec_format(<span class="string">"w2v.bin"</span>, binary=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="FastText">FastText</h3><p>当一个词在我们的词汇表中不存在时会发生什么？我们可以分配一个 UNK 标识来表示为未登录词，或者使用<a href="https://radimrehurek.com/gensim/models/fasttext.html" target="_blank" rel="noopener" title="FastText">FastText</a>，它使用字符级的n-grams算法来embed单词，这样有助于处理罕见词,拼错的词，以及语料库中不存在但相似的词。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> FastText</span><br><span class="line"></span><br><span class="line"><span class="comment"># Super fast because of optimized C code under the hood</span></span><br><span class="line">ft = FastText(sentences=sentences, vector_size=EMBEDDING_DIM,</span><br><span class="line">              window=WINDOW, min_count=MIN_COUNT,</span><br><span class="line">              sg=SKIP_GRAM, negative=NEGATIVE_SAMPLING)</span><br><span class="line"><span class="keyword">print</span> (ft)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># FastText&lt;vocab=4937, vector_size=100, alpha=0.025&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This word doesn't exist so the word2vec model will error out</span></span><br><span class="line">wv.most_similar(positive=<span class="string">'scarring'</span>, topn=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># KeyError: "Key 'scarring' not present in vocabulary"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># FastText will use n-grams to embed an OOV word</span></span><br><span class="line">ft.wv.most_similar(positive=<span class="string">'scarring'</span>, topn=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [('swimming', 0.9938331246376038),</span></span><br><span class="line"><span class="comment">#  ('howling', 0.9927006959915161),</span></span><br><span class="line"><span class="comment">#  ('dabbing', 0.9923058748245239),</span></span><br><span class="line"><span class="comment">#  ('wriggling', 0.9921060800552368),</span></span><br><span class="line"><span class="comment">#  ('bulging', 0.9919766783714294)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Save and loading</span></span><br><span class="line">ft.wv.save(<span class="string">"ft.bin"</span>)</span><br><span class="line">ftwv = KeyedVectors.load(<span class="string">"ft.bin"</span>)</span><br></pre></td></tr></table></figure><h2 id="Pretrained-embeddings">Pretrained embeddings</h2><p>我们可以利用上述方法从头开始应用embeddings，也可以利用已经在百万文档上训练过的预训练embeddings。流行的包括<a href="https://www.tensorflow.org/tutorials/text/word2vec" target="_blank" rel="noopener" title="Word2Vec">Word2Vec</a>、<a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener" title="GloVe">GloVe</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Preview of the GloVe embeddings file</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"glove.6B.100d.txt"</span>, <span class="string">"r"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    line = next(fp)</span><br><span class="line">    values = line.split()</span><br><span class="line">    word = values[<span class="number">0</span>]</span><br><span class="line">    embedding = np.asarray(values[<span class="number">1</span>:], dtype=<span class="string">'float32'</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">f"word: <span class="subst">&#123;word&#125;</span>"</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">f"embedding:\n<span class="subst">&#123;embedding&#125;</span>"</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">f"embedding dim: <span class="subst">&#123;len(embedding)&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># word: the</span></span><br><span class="line"><span class="comment"># embedding:</span></span><br><span class="line"><span class="comment"># [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141</span></span><br><span class="line"><span class="comment">#   0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384</span></span><br><span class="line"><span class="comment">#  -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464</span></span><br><span class="line"><span class="comment">#  -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155</span></span><br><span class="line"><span class="comment">#  -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021</span></span><br><span class="line"><span class="comment">#   0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531</span></span><br><span class="line"><span class="comment">#   0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559</span></span><br><span class="line"><span class="comment">#  -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243</span></span><br><span class="line"><span class="comment">#   0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514</span></span><br><span class="line"><span class="comment">#   0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044</span></span><br><span class="line"><span class="comment">#   0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212</span></span><br><span class="line"><span class="comment">#  -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148</span></span><br><span class="line"><span class="comment">#  -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215</span></span><br><span class="line"><span class="comment">#  -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459</span></span><br><span class="line"><span class="comment">#   0.8278    0.27062 ]</span></span><br><span class="line"><span class="comment"># embedding dim: 100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load embeddings (may take a minute)</span></span><br><span class="line">glove = KeyedVectors.load_word2vec_format(<span class="string">"glove.6B.100d.txt"</span>, binary=<span class="literal">False</span>, no_header=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># (king - man) + woman = ?</span></span><br><span class="line"><span class="comment"># king - man = ? -  woman</span></span><br><span class="line">glove.most_similar(positive=[<span class="string">"woman"</span>, <span class="string">"king"</span>], negative=[<span class="string">"man"</span>], topn=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [('queen', 0.7698540687561035),</span></span><br><span class="line"><span class="comment">#  ('monarch', 0.6843381524085999),</span></span><br><span class="line"><span class="comment">#  ('throne', 0.6755736470222473),</span></span><br><span class="line"><span class="comment">#  ('daughter', 0.6594556570053101),</span></span><br><span class="line"><span class="comment">#  ('princess', 0.6520534157752991)]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get nearest neighbors (excluding itself)</span></span><br><span class="line">glove.most_similar(positive=<span class="string">"goku"</span>, topn=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [('gohan', 0.7246542572975159),</span></span><br><span class="line"><span class="comment">#  ('bulma', 0.6497020125389099),</span></span><br><span class="line"><span class="comment">#  ('raistlin', 0.644360363483429),</span></span><br><span class="line"><span class="comment">#  ('skaar', 0.6316742897033691),</span></span><br><span class="line"><span class="comment">#  ('guybrush', 0.6231325268745422)]</span></span><br></pre></td></tr></table></figure><p>我们可视化一下 king, queen, man, woman 这四个单词的位置关系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reduce dimensionality for plotting</span></span><br><span class="line">X = glove[glove.index_to_key]</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pca_results = pca.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_embeddings</span><span class="params">(words, embeddings, pca_results)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        idx = embeddings.key_to_index[word]</span><br><span class="line">        plt.scatter(pca_results[idx, <span class="number">0</span>], pca_results[idx, <span class="number">1</span>])</span><br><span class="line">        plt.annotate(word, xy=(pca_results[idx, <span class="number">0</span>], pca_results[idx, <span class="number">1</span>]))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize</span></span><br><span class="line">plot_embeddings(</span><br><span class="line">    words=[<span class="string">"king"</span>, <span class="string">"queen"</span>, <span class="string">"man"</span>, <span class="string">"woman"</span>], embeddings=glove,</span><br><span class="line">    pca_results=pca_results)</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/tmp/6c1b85445e28b27bfed73fc0d1f7d3ec.png" alt=""></p><p>再看一下，离woman和doctor近，但离man远的词有哪些</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Bias in embeddings</span></span><br><span class="line">glove.most_similar(positive=[<span class="string">"woman"</span>, <span class="string">"doctor"</span>], negative=[<span class="string">"man"</span>], topn=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [('nurse', 0.7735227942466736),</span></span><br><span class="line"><span class="comment">#  ('physician', 0.7189430594444275),</span></span><br><span class="line"><span class="comment">#  ('doctors', 0.6824328303337097),</span></span><br><span class="line"><span class="comment">#  ('patient', 0.6750683188438416),</span></span><br><span class="line"><span class="comment">#  ('dentist', 0.6726033091545105)]</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>下一篇，我们将进一步介绍Embeddings如何提升我们前篇介绍的CNN分类模型。</p>]]></content>
    
    <summary type="html">
    
      Explore and motivate the need for representation via embeddings.
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 卷积神经网络</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-CNN/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-CNN/</id>
    <published>2023-06-27T09:30:43.000Z</published>
    <updated>2023-07-04T15:10:06.945Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>本文简单示范了如何利用CNN处理NLP任务。</p><p>CNNs的核心就是利用卷积（滑动）操作来提取数据特征的卷积核（aka kernels, filters,weights, etc.)。它们随机初始化但通过参数共享来提取特征。</p><p><img src="//s3.mindex.xyz/tmp/1a58a0c1e58cd3f543995ecee0eb71d4.gif" alt=""></p><h2 id="Set-up">Set up</h2><p>复用<a href="http://neo1989.net/Way2AI/Way2AI-utilities/" title="PyTorch实现神经网络的基本套路">《PyTorch实现神经网络的基本套路》</a> 里介绍的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_seeds</span><span class="params">(seed=<span class="number">1024</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Set seeds for reproducibility."""</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    touch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)  <span class="comment"># multi-GPU</span></span><br><span class="line"></span><br><span class="line">set_seeds(seed=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">cuda = <span class="literal">True</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span>(torch.cuda.is_available() <span class="keyword">and</span> cuda) <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">torch.set_default_tensor_type(&#123;<span class="string">"cuda"</span>: <span class="string">"torch.cuda.FloatTensor"</span>, <span class="string">"cpu"</span>: <span class="string">"torch.FloatTensor"</span>&#125;.get(str(device)))</span><br></pre></td></tr></table></figure><h3 id="Load-data">Load data</h3><p>我们将在<a href="https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset" target="_blank" rel="noopener" title="AG News Classification Dataset">AGNews dataset</a> 这个数据集上完成本次学习任务。这是一份来自4个不同新闻分类120k条新闻标题样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load data</span></span><br><span class="line">url = <span class="string">"https://s3.mindex.xyz/datasets/news.csv"</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>)</span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/tmp/5820e01cf3a5f9f93ce85ba8d488647e.png" alt=""></p><h3 id="Preprocessing">Preprocessing</h3><p>首先要做的，是对这些数据进行预处理，手段包括删除停用词、字母小写（英文）、词形还原词干提取、中文分词、正则处理等。</p><p>由于我们的任务是纯英文数据，这里使用英文的通用处理方法。中文任务以后再表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> PorterStemmer</span><br><span class="line"></span><br><span class="line">nltk.download(<span class="string">"stopwords"</span>)</span><br><span class="line">STOPWORDS = stopwords.words(<span class="string">"english"</span>)</span><br><span class="line"><span class="keyword">print</span> (STOPWORDS[:<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># ['i', 'me', 'my', 'myself', 'we']</span></span><br><span class="line"></span><br><span class="line">porter = PorterStemmer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(text, stopwords=STOPWORDS)</span>:</span></span><br><span class="line">    <span class="string">"""Conditional preprocessing on our text unique to our task."""</span></span><br><span class="line">    <span class="comment"># Lower</span></span><br><span class="line">    text = text.lower()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove stopwords</span></span><br><span class="line">    pattern = re.compile(<span class="string">r"\b("</span> + <span class="string">r"|"</span>.join(stopwords) + <span class="string">r")\b\s*"</span>)</span><br><span class="line">    text = pattern.sub(<span class="string">""</span>, text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove words in parenthesis</span></span><br><span class="line">    text = re.sub(<span class="string">r"\([^)]*\)"</span>, <span class="string">""</span>, text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Spacing and filters</span></span><br><span class="line">    text = re.sub(<span class="string">r"([-;;.,!?&lt;=&gt;])"</span>, <span class="string">r" \1 "</span>, text)  <span class="comment"># separate punctuation tied to words</span></span><br><span class="line">    text = re.sub(<span class="string">"[^A-Za-z0-9]+"</span>, <span class="string">" "</span>, text)  <span class="comment"># remove non alphanumeric chars</span></span><br><span class="line">    text = re.sub(<span class="string">" +"</span>, <span class="string">" "</span>, text)  <span class="comment"># remove multiple spaces</span></span><br><span class="line">    text = text.strip()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply to dataframe</span></span><br><span class="line">preprocessed_df = df.copy()</span><br><span class="line">preprocessed_df.title = preprocessed_df.title.apply(preprocess)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"<span class="subst">&#123;df.title.values[<span class="number">-1</span>]&#125;</span>\n\n<span class="subst">&#123;preprocessed_df.title.values[<span class="number">-1</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Oil Slips Under \$55 a Barrel</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># oil slips 55 barrel</span></span><br></pre></td></tr></table></figure><h3 id="Split-data">Split data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_test_split</span><span class="params">(X, y, train_size)</span>:</span></span><br><span class="line">    X_train, X_, y_train,y_ = train_test_split(X, y, train_size=train_size, stratify=y)</span><br><span class="line">    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Data</span></span><br><span class="line">X = preprocessed_df[<span class="string">"title"</span>].values</span><br><span class="line">y = preprocessed_df[<span class="string">"category"</span>].values</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (84000,), y_train: (84000,)</span></span><br><span class="line"><span class="comment"># X_val: (18000,), y_val: (18000,)</span></span><br><span class="line"><span class="comment"># X_test: (18000,), y_test: (18000,)</span></span><br><span class="line"><span class="comment"># Sample point: wenger plans buy new keeper → Sports</span></span><br></pre></td></tr></table></figure><h2 id="Label-encoding">Label encoding</h2><p>复用<a href="http://neo1989.net/Way2AI/Way2AI-utilities/" title="PyTorch实现神经网络的基本套路">《PyTorch实现神经网络的基本套路》</a> 里介绍的 LabelEncoder</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Encode</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">label_encoder.fit(y_train)</span><br><span class="line">NUM_CLASSES = len(label_encoder)</span><br><span class="line"><span class="keyword">print</span> (label_encoder.class_to_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert labels to tokens</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">y_train = label_encoder.encode(y_train)</span><br><span class="line">y_val = label_encoder.encode(y_val)</span><br><span class="line">y_test = label_encoder.encode(y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: Sports</span></span><br><span class="line"><span class="comment"># y_train[0]: 2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> enumerate(counts)&#125;</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># counts: [21000 21000 21000 21000]</span></span><br><span class="line"><span class="comment"># weights: &#123;0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Tokenizer">Tokenizer</h2><p>由于任务要处理的是文本，无法直接送给模型。因此我们定义一个Tokenizer来处理文本数据，目的是将文本序列转化成离散的标记（tokens)，以便后续的处理和分析。这意味着每个token可以映射到一个唯一的索引，这样我们就可以用一个索引数组（向量）来表示文本序列。而一个token可以是一个字符、一个单词、一个词组等等。</p><p>下面是一个示例实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> more_itertools <span class="keyword">import</span> take</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, char_level, num_tokens=None, pad_token=<span class="string">"&lt;PAD&gt;"</span>, oov_token=<span class="string">"&lt;UNK&gt;"</span>, token_to_index=None)</span>:</span></span><br><span class="line">        self.char_level = char_level</span><br><span class="line">        self.separator = <span class="string">""</span> <span class="keyword">if</span> self.char_level <span class="keyword">else</span> <span class="string">" "</span></span><br><span class="line">        <span class="keyword">if</span> num_tokens:</span><br><span class="line">            num_tokens -= <span class="number">2</span> <span class="comment"># pad + unk tokens</span></span><br><span class="line">        self.num_tokens = num_tokens</span><br><span class="line">        self.pad_token = pad_token</span><br><span class="line">        self.oov_token = oov_token</span><br><span class="line">        self.token_to_index = token_to_index <span class="keyword">if</span> token_to_index <span class="keyword">else</span> &#123;pad_token: <span class="number">0</span>, oov_token: <span class="number">1</span>&#125;</span><br><span class="line">        self.index_to_token = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.token_to_index.items()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.token_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"&lt;Tokenizer(num_tokens=<span class="subst">&#123;len(self)&#125;</span>)&gt;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_on_texts</span><span class="params">(self, texts)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.char_level:</span><br><span class="line">            texts = [text.split(<span class="string">" "</span>) <span class="keyword">for</span> text <span class="keyword">in</span> texts]</span><br><span class="line">        all_tokens = [token <span class="keyword">for</span> text <span class="keyword">in</span> texts <span class="keyword">for</span> token <span class="keyword">in</span> text]</span><br><span class="line">        counts = Counter(all_tokens).most_common(self.num_tokens)</span><br><span class="line">        self.min_token_freq = counts[<span class="number">-1</span>][<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> token, count <span class="keyword">in</span> counts:</span><br><span class="line">            index = len(self)</span><br><span class="line">            self.token_to_index[token] = index</span><br><span class="line">            self.index_to_token[index] = token</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">texts_to_sequences</span><span class="params">(self, texts)</span>:</span></span><br><span class="line">        sequences = []</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> texts:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.char_level:</span><br><span class="line">                text = text.split(<span class="string">" "</span>)</span><br><span class="line">            sequence = []</span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> text:</span><br><span class="line">                sequence.append(self.token_to_index.get(</span><br><span class="line">                    token, self.token_to_index[self.oov_token]))</span><br><span class="line">            sequences.append(np.asarray(sequence))</span><br><span class="line">        <span class="keyword">return</span> sequences</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sequences_to_texts</span><span class="params">(self, sequences)</span>:</span></span><br><span class="line">        texts = []</span><br><span class="line">        <span class="keyword">for</span> sequence <span class="keyword">in</span> sequences:</span><br><span class="line">            text = []</span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> sequence:</span><br><span class="line">                text.append(self.index_to_token.get(index, self.oov_token))</span><br><span class="line">            texts.append(self.separator.join([token <span class="keyword">for</span> token <span class="keyword">in</span> text]))</span><br><span class="line">        <span class="keyword">return</span> texts</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;</span><br><span class="line">                <span class="string">"char_level"</span>: self.char_level,</span><br><span class="line">                <span class="string">"oov_token"</span>: self.oov_token,</span><br><span class="line">                <span class="string">"token_to_index"</span>: self.token_to_index</span><br><span class="line">            &#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(cls, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"r"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br></pre></td></tr></table></figure><p>本次实验我们限制tokens的数量为500个(停用词已删除)，其中包括两个占位的。如果您的计算资源足够，可以使用更大的tokens数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tokenize</span></span><br><span class="line">tokenizer = Tokenizer(char_level=<span class="literal">False</span>, num_tokens=<span class="number">500</span>)</span><br><span class="line">tokenizer.fit_on_texts(texts=X_train)</span><br><span class="line">VOCAB_SIZE = len(tokenizer)</span><br><span class="line"><span class="keyword">print</span> (tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;Tokenizer(num_tokens=500)&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample of tokens</span></span><br><span class="line"><span class="keyword">print</span> (take(<span class="number">10</span>, tokenizer.token_to_index.items()))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"least freq token's freq: <span class="subst">&#123;tokenizer.min_token_freq&#125;</span>"</span>) <span class="comment"># use this to adjust num_tokens</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [('&lt;PAD&gt;', 0), ('&lt;UNK&gt;', 1), ('39', 2), ('b', 3), ('gt', 4), ('lt', 5), ('us', 6), ('new', 7), ('oil', 8), ('microsoft', 9)]</span></span><br><span class="line"><span class="comment"># least freq token's freq: 166</span></span><br></pre></td></tr></table></figure><p>Ok，接下来将我们文本数据全部token化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert texts to sequences of indices</span></span><br><span class="line">X_train = tokenizer.texts_to_sequences(X_train)</span><br><span class="line">X_val = tokenizer.texts_to_sequences(X_val)</span><br><span class="line">X_test = tokenizer.texts_to_sequences(X_test)</span><br><span class="line">preprocessed_text = tokenizer.sequences_to_texts([X_train[<span class="number">0</span>]])[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Text to indices:\n"</span></span><br><span class="line">    <span class="string">f"  (preprocessed) → <span class="subst">&#123;preprocessed_text&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  (tokenized) → <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Text to indices:</span></span><br><span class="line"><span class="comment">#   (preprocessed) → ibm wins time talks &lt;UNK&gt; case</span></span><br><span class="line"><span class="comment">#   (tokenized) → [ 31  32  69  26   1 100]</span></span><br></pre></td></tr></table></figure><h2 id="One-hot-encoding">One-hot encoding</h2><p>One-hot编码是一种将离散变量表示为二进制向量的技术。它允许我们以一种模型可以理解的方式来表示数据，并且不受token的实际值的影响。</p><p>举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设我们有个只含5个字符的词表：</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"a"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"e"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"i"</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">"o"</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">"u"</span>: <span class="number">4</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 那么文本 aou 就会表示成一个二维矩阵：</span></span><br><span class="line"><span class="comment"># 列对应着词表，而每一行表示单个token的二进制向量（只在词表对应位置置为1，其他位置为0）</span></span><br><span class="line">[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]]</span><br></pre></td></tr></table></figure><p>我们手动实现一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_categorical</span><span class="params">(seq, num_classes)</span>:</span></span><br><span class="line">    <span class="string">"""One-hot encode a sequence of tokens."""</span></span><br><span class="line">    one_hot = np.zeros((len(seq), num_classes))</span><br><span class="line">    <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(seq):</span><br><span class="line">        one_hot[i, item] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> one_hot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># One-hot encoding</span></span><br><span class="line"><span class="keyword">print</span> (X_train[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">print</span> (len(X_train[<span class="number">0</span>]))</span><br><span class="line">cat = to_categorical(seq=X_train[<span class="number">0</span>], num_classes=len(tokenizer))</span><br><span class="line"><span class="keyword">print</span> (cat)</span><br><span class="line"><span class="keyword">print</span> (cat.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [ 31  32  69  26   1 100]</span></span><br><span class="line"><span class="comment"># 6</span></span><br><span class="line"><span class="comment"># [[0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 0.]]</span></span><br><span class="line"><span class="comment"># (6, 500)</span></span><br></pre></td></tr></table></figure><p>接下来需要将我们的数据进行one-hot编码处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert tokens to one-hot</span></span><br><span class="line">vocab_size = len(tokenizer)</span><br><span class="line">X_train = [to_categorical(seq, num_classes=vocab_size) <span class="keyword">for</span> seq <span class="keyword">in</span> X_train]</span><br><span class="line">X_val = [to_categorical(seq, num_classes=vocab_size) <span class="keyword">for</span> seq <span class="keyword">in</span> X_val]</span><br><span class="line">X_test = [to_categorical(seq, num_classes=vocab_size) <span class="keyword">for</span> seq <span class="keyword">in</span> X_test]</span><br></pre></td></tr></table></figure><h2 id="Padding">Padding</h2><p>由于我们的数据是不定长的新闻标题，而模型能够处理的是相同形状的数据，所以引入padding来预处理数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_sequences</span><span class="params">(sequences, max_seq_len=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Pad sequences to max length in sequence."""</span></span><br><span class="line">    max_seq_len = max(max_seq_len, max(len(sequence) <span class="keyword">for</span> sequence <span class="keyword">in</span> sequences))</span><br><span class="line">    num_classes = sequences[<span class="number">0</span>].shape[<span class="number">-1</span>]</span><br><span class="line">    padded_sequences = np.zeros((len(sequences), max_seq_len, num_classes))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        padded_sequences[i][:len(sequence)] = sequence</span><br><span class="line">    <span class="keyword">return</span> padded_sequences</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3D sequences</span></span><br><span class="line"><span class="keyword">print</span> (X_train[<span class="number">0</span>].shape, X_train[<span class="number">1</span>].shape, X_train[<span class="number">2</span>].shape)</span><br><span class="line">padded = pad_sequences(X_train[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> (padded.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># (6, 500) (8, 500) (6, 500)</span></span><br><span class="line"><span class="comment"># (3, 8, 500)</span></span><br></pre></td></tr></table></figure><h2 id="Dataset">Dataset</h2><p>一如上篇文章里介绍的，我们需要把数据放在 Dataset 中，并使用 DataLoader 来有效地创建用于训练和验证的批次。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">FILTER_SIZE = <span class="number">1</span> <span class="comment"># unigram</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, X, y, max_filter_size)</span>:</span></span><br><span class="line">        self.X = X</span><br><span class="line">        self.y = y</span><br><span class="line">        self.max_filter_size = max_filter_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"&lt;Dataset(N=<span class="subst">&#123;len(self)&#125;</span>)&gt;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        X = self.X[index]</span><br><span class="line">        y = self.y[index]</span><br><span class="line">        <span class="keyword">return</span> [X, y]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(self, batch)</span>:</span></span><br><span class="line">        <span class="string">"""Processing on a batch."""</span></span><br><span class="line">        <span class="comment"># Get inputs</span></span><br><span class="line">        batch = np.array(batch)</span><br><span class="line">        X = batch[:, <span class="number">0</span>]</span><br><span class="line">        y = batch[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pad sequences</span></span><br><span class="line">        X = pad_sequences(X, max_seq_len=self.max_filter_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cast</span></span><br><span class="line">        X = torch.FloatTensor(X.astype(np.int32))</span><br><span class="line">        y = torch.LongTensor(y.astype(np.int32))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_dataloader</span><span class="params">(self, batch_size, shuffle=False, drop_last=False)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.utils.data.DataLoader(</span><br><span class="line">            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,</span><br><span class="line">            shuffle=shuffle, drop_last=drop_last, pin_memory=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create datasets for embedding</span></span><br><span class="line">train_dataset = Dataset(X=X_train, y=y_train, max_filter_size=FILTER_SIZE)</span><br><span class="line">val_dataset = Dataset(X=X_val, y=y_val, max_filter_size=FILTER_SIZE)</span><br><span class="line">test_dataset = Dataset(X=X_test, y=y_test, max_filter_size=FILTER_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Datasets:\n"</span></span><br><span class="line">    <span class="string">f"  Train dataset:<span class="subst">&#123;train_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  Val dataset: <span class="subst">&#123;val_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  Test dataset: <span class="subst">&#123;test_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">    <span class="string">"Sample point:\n"</span></span><br><span class="line">    <span class="string">f"  X: <span class="subst">&#123;test_dataset[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  y: <span class="subst">&#123;test_dataset[<span class="number">0</span>][<span class="number">1</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Datasets:</span></span><br><span class="line"><span class="comment">#   Train dataset:&lt;Dataset(N=84000)&gt;</span></span><br><span class="line"><span class="comment">#   Val dataset: &lt;Dataset(N=18000)&gt;</span></span><br><span class="line"><span class="comment">#   Test dataset: &lt;Dataset(N=18000)&gt;</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: [[0. 1. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 0.]]</span></span><br><span class="line"><span class="comment">#   y: 1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create dataloaders</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_dataloader = train_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">val_dataloader = val_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">test_dataloader = test_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">batch_X, batch_y = next(iter(test_dataloader))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Sample batch:\n"</span></span><br><span class="line">    <span class="string">f"  X: <span class="subst">&#123;list(batch_X.size())&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  y: <span class="subst">&#123;list(batch_y.size())&#125;</span>\n"</span></span><br><span class="line">    <span class="string">"Sample point:\n"</span></span><br><span class="line">    <span class="string">f"  X: <span class="subst">&#123;batch_X[<span class="number">0</span>]&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  y: <span class="subst">&#123;batch_y[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Sample batch:</span></span><br><span class="line"><span class="comment">#   X: [64, 15, 500]</span></span><br><span class="line"><span class="comment">#   y: [64]</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: tensor([[0., 1., 0.,  ..., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 1., 0.,  ..., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.,  ..., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         ...,</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.,  ..., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.,  ..., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.,  ..., 0., 0., 0.]])</span></span><br><span class="line"><span class="comment">#   y: 1</span></span><br><span class="line"><span class="comment">#</span></span><br></pre></td></tr></table></figure><h2 id="CNN">CNN</h2><p>接下来呢要进入本篇的重点，CNN了。</p><h3 id="Inputs">Inputs</h3><p>下面这个简单的示例里，我们随机给出了N个样本，每个样本有8个token，而我们的词表大小是10个。</p><p>也就意味着，我们inputs的形状是 (N, 8, 10)</p><p>但需要注意的是，当我使用PyTorch处理CNN时，通道数需要在第二个维度，也就意味着，在这个例子里，我们的inputs的形状得是 (N, 10, 8)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume all our inputs are padded to have the same num of tokens.</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">max_seq_len = <span class="number">8</span>  <span class="comment"># tokens per input</span></span><br><span class="line">vocab_size = <span class="number">10</span>  <span class="comment"># one-hot size</span></span><br><span class="line">x = torch.randn(batch_size, max_seq_len, vocab_size)</span><br><span class="line">print(<span class="string">f"X: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">x = x.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">print(<span class="string">f"X: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X: torch.Size([64, 8, 10])</span></span><br><span class="line"><span class="comment"># X: torch.Size([64, 10, 8])</span></span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/tmp/fea49de2fb514aafde1b3545c5163058.png" alt=""></p><h3 id="Filters">Filters</h3><p>在下面的动画中，我们将卷积核和输入简化成2D，以便于可视化，而且实际上值并不总是是0或1，而是任意的浮点数。</p><p><img src="//s3.mindex.xyz/tmp/1a58a0c1e58cd3f543995ecee0eb71d4.gif" alt=""></p><p>现在回到我们的示例数据，单个样本的形状是(8, 10) [max_seq_len, vocab_size]，然后我们考虑用50个形状是(1, 3)的一维卷积来提取数据的特征，由于我们的数据的通道数是10 （num_channels = vocab_size = one_hot_size = 10）, 这边意味着这个卷积核的形状便是 (3, 10, 50) [kernel_size, vocab_size, num_filters]</p><p><img src="//s3.mindex.xyz/tmp/18c83d441855deaf7671fea9f9f26bdc.png" alt=""></p><p>这里有两个关键的概念，步长(stride) 和 填充(padding). 详见下图</p><p><img src="//s3.mindex.xyz/tmp/49dc51e7b89f5215577c01e74a17ce73.png" alt=""></p><p>这里采用一维卷积<a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d" target="_blank" rel="noopener" title="Conv1d">Conv1D</a>来处理示例数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convolutional filters (VALID padding)</span></span><br><span class="line">num_filters = <span class="number">50</span> <span class="comment"># num filters</span></span><br><span class="line">filter_size = <span class="number">3</span></span><br><span class="line">stride = <span class="number">1</span></span><br><span class="line">padding = <span class="number">0</span>  <span class="comment"># valid padding (no padding)</span></span><br><span class="line">conv1 = nn.Conv1d(in_channels=vocab_size, out_channels=num_filters,</span><br><span class="line">                  kernel_size=filter_size, stride=stride,</span><br><span class="line">                  padding=padding, padding_mode=<span class="string">"zeros"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"conv: <span class="subst">&#123;conv1.weight.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># conv: torch.Size([50, 10, 3])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Forward pass</span></span><br><span class="line">z = conv1(x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"z: <span class="subst">&#123;z.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># z: torch.Size([64, 50, 6])</span></span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/tmp/0fcb44386fcdfdd5c7e56e79be18515a.png" alt=""></p><p>如你所见，我们输入数据max_seq_len=8，而经过卷积后的output的长度却是6。如果需要保证长度一致，那么就需要引入padding了。<br>$$<br>\begin{split}<br>W = \frac{W - F + 2P}{S} + 1 \\<br>P = \frac{S(W - 1) - W + F}{2}<br>\end{split}<br>$$</p><p>如果P不是一个整数，考虑向上取整(math.ceil)在右侧填充。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convolutional filters (SAME padding)</span></span><br><span class="line">num_filters = <span class="number">50</span> <span class="comment"># num filters</span></span><br><span class="line">filter_size = <span class="number">3</span></span><br><span class="line">stride = <span class="number">1</span></span><br><span class="line">padding = <span class="number">0</span>  <span class="comment"># valid padding (no padding)</span></span><br><span class="line">conv = nn.Conv1d(in_channels=vocab_size, out_channels=num_filters,</span><br><span class="line">                 kernel_size=filter_size, stride=stride,</span><br><span class="line">                 padding=padding, padding_mode=<span class="string">"zeros"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"conv: <span class="subst">&#123;conv.weight.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># conv: torch.Size([50, 10, 3])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># `SAME` padding</span></span><br><span class="line">padding_left = int((conv.stride[<span class="number">0</span>]*(max_seq_len<span class="number">-1</span>) - max_seq_len + filter_size) / <span class="number">2</span>)</span><br><span class="line">padding_right =int(math.ceil((conv.stride[<span class="number">0</span>]*(max_seq_len<span class="number">-1</span>) - max_seq_len + filter_size) / <span class="number">2</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"padding: <span class="subst">&#123;(padding_left, padding_right)&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># padding: (1, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Forward pass</span></span><br><span class="line">z = conv(F.pad(x, (padding_left, padding_right)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"z: <span class="subst">&#123;z.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># z: torch.Size([64, 50, 8])</span></span><br></pre></td></tr></table></figure><p>未来我们会探索更高维度的卷积层。包括使用Conv2D来处理3D数据（图像、字符级别文本等），使用Conv3D来处理4D数据（视频、时间序列数据等）</p><h3 id="Pooling">Pooling</h3><p>池化是一种用于简化下游计算的方法，通过将高维特征图总结为较低维特征图来减少冗余信息。在卷积滤波器对输入进行处理后产生的特征映射中，由于卷积和重叠的性质，会存在大量的冗余信息。池化操作可以采用最大值或平均值等方式。下面是一个池化的示例：假设来自卷积层的输出是4x4的特征图，我们使用2x2的最大池化过滤器进行处理。</p><p><img src="//s3.mindex.xyz/tmp/16910ad2e084e2b567c1cfadffcabab4.png" alt=""></p><p>在这个例子里，我们使用<a href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d" target="_blank" rel="noopener" title="MaxPool1d">MaxPool1D</a>取一个max值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Max pooling</span></span><br><span class="line">pool_output = F.max_pool1d(z, z.size(<span class="number">2</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Size: <span class="subst">&#123;pool_output.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([64, 50, 1])</span></span><br></pre></td></tr></table></figure><h3 id="Batch-normalization">Batch normalization</h3><p>在构建模型前，需要讨论的最后一个主题便是<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener" title="batch normalization">batch normalization</a>.  它是一种对来自前一层激活的标准化操作，使其均值为0，标准差为1。</p><p>在以前的笔记本中，我们对输入进行标准化，以便模型能够更快地进行优化，并提高学习率。这里采用相同的概念，但我们在重复的前向传递过程中保持标准化的值，以进一步帮助优化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Batch normalization</span></span><br><span class="line">batch_norm = nn.BatchNorm1d(num_features=num_filters)</span><br><span class="line">z = batch_norm(conv(x)) <span class="comment"># applied to activations (after conv layer &amp; before pooling)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"z: <span class="subst">&#123;z.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># z: torch.Size([64, 50, 6])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Mean and std before batchnorm</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;torch.mean(conv(x)):<span class="number">.2</span>f&#125;</span>, std: <span class="subst">&#123;torch.std(conv(x)):<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># mean: -0.00, std: 0.59</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Mean and std after batchnorm</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;torch.mean(z):<span class="number">.2</span>f&#125;</span>, std: <span class="subst">&#123;torch.std(z):<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># mean: -0.00, std: 1.00</span></span><br></pre></td></tr></table></figure><h2 id="Modling">Modling</h2><h3 id="Model">Model</h3><p>可视化一下模型的前向传播.</p><ul><li>首先对输入tokenizer化 (batch_size, max_seq_len)</li><li>然后，one-hot编码 (batch_size, max_seq_len, vocab_size)</li><li>接下来，使用filters（filter_size, vocab_size, num_filter)进行卷积，然后批归一化。这里我们的filters相当于一个n-gram检测器。</li><li>紧跟着，应用max polling，从特征图中提取最相关信息</li><li>再接一个含dropout的全连接层</li><li>最后再一个softmax全连接层以输出最终的类别概率</li></ul><p><img src="//s3.mindex.xyz/tmp/3984d8e8ac31581c8b9525fd221e275c.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">NUM_FILTERS = <span class="number">50</span></span><br><span class="line">HIDDEN_DIM = <span class="number">100</span></span><br><span class="line">DROPOUT_P = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, num_filters, filter_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_dim, dropout_p, num_classes)</span>:</span></span><br><span class="line">        super(CNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># COnvolutional filters</span></span><br><span class="line">        self.filter_size = filter_size</span><br><span class="line">        self.conv = nn.Conv1d(</span><br><span class="line">            in_channels=vocab_size, out_channels=num_filters,</span><br><span class="line">            kernel_size=filter_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, padding_mode=<span class="string">'zeros'</span>)</span><br><span class="line">        self.batch_norm = nn.BatchNorm1d(num_features=num_filters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FC layers</span></span><br><span class="line">        self.fc1 = nn.Linear(num_filters, hidden_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, channel_first=False,)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Rearrange input so num_channels is in dim 1 (N, C, L)</span></span><br><span class="line">        x_in, = inputs</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> channel_first:</span><br><span class="line">            x_in = x_in.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Padding for `SAME` padding</span></span><br><span class="line">        max_seq_len = x_in.shape[<span class="number">2</span>]</span><br><span class="line">        padding_left = int((self.conv.stride[<span class="number">0</span>]*(max_seq_len<span class="number">-1</span>) - max_seq_len + self.filter_size)/<span class="number">2</span>)</span><br><span class="line">        padding_right = int(math.ceil((self.conv.stride[<span class="number">0</span>]*(max_seq_len<span class="number">-1</span>) - max_seq_len + self.filter_size)/<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Conv outputs</span></span><br><span class="line">        z = self.conv(F.pad(x_in, (padding_left, padding_right)))</span><br><span class="line">        z = F.max_pool1d(z, z.size(<span class="number">2</span>)).squeeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FC layer</span></span><br><span class="line">        z = self.fc1(z)</span><br><span class="line">        z = self.dropout(z)</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = CNN(vocab_size=VOCAB_SIZE, num_filters=NUM_FILTERS, filter_size=FILTER_SIZE,</span><br><span class="line">            hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of CNN(</span></span><br><span class="line"><span class="comment">#   (conv): Conv1d(500, 50, kernel_size=(1,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=50, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h3 id="Training">Training</h3><p>接下来，利用到<a href="http://neo1989.net/Way2AI/Way2AI-utilities/" title="PyTorch实现神经网络的基本套路">《PyTorch实现神经网络的基本套路》</a> 里介绍到Trainer类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">LEARNING_RATE = <span class="number">1e-3</span></span><br><span class="line">PATIENCE = <span class="number">5</span></span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, device, loss_fn=None, optimizer=None, scheduler=None)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set params</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.device = device</span><br><span class="line">        self.loss_fn = loss_fn</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self.scheduler = scheduler</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Train step."""</span></span><br><span class="line">        <span class="comment"># Set model to train mode</span></span><br><span class="line">        self.model.train()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over train batches</span></span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Step</span></span><br><span class="line">            batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">            inputs, targets = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">            self.optimizer.zero_grad()  <span class="comment"># Reset gradients</span></span><br><span class="line">            z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">            J = self.loss_fn(z, targets)  <span class="comment"># Define loss</span></span><br><span class="line">            J.backward()  <span class="comment"># Backward pass</span></span><br><span class="line">            self.optimizer.step()  <span class="comment"># Update weights</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Cumulative Metrics</span></span><br><span class="line">            loss += (J.detach().item() - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Validation or test step."""</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.eval()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line">        y_trues, y_probs = [], []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Step</span></span><br><span class="line">                batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">                inputs, y_true = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">                z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">                J = self.loss_fn(z, y_true).item()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Cumulative Metrics</span></span><br><span class="line">                loss += (J - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line">                y_trues.extend(y_true.cpu().numpy())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, np.vstack(y_trues), np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Prediction step."""</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.eval()</span><br><span class="line">        y_probs = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Forward pass w/ inputs</span></span><br><span class="line">                inputs, targets = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">                z = self.model(inputs)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, num_epochs, patience, train_dataloader, val_dataloader)</span>:</span></span><br><span class="line">        best_val_loss = np.inf</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">            <span class="comment"># Steps</span></span><br><span class="line">            train_loss = self.train_step(dataloader=train_dataloader)</span><br><span class="line">            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)</span><br><span class="line">            self.scheduler.step(val_loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Early stopping</span></span><br><span class="line">            <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">                best_val_loss = val_loss</span><br><span class="line">                best_model = self.model</span><br><span class="line">                _patience = patience  <span class="comment"># reset _patience</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                _patience -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> _patience:  <span class="comment"># 0</span></span><br><span class="line">                print(<span class="string">"Stopping early!"</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Logging</span></span><br><span class="line">            print(</span><br><span class="line">                <span class="string">f"Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> | "</span></span><br><span class="line">                <span class="string">f"train_loss: <span class="subst">&#123;train_loss:<span class="number">.5</span>f&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"val_loss: <span class="subst">&#123;val_loss:<span class="number">.5</span>f&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"lr: <span class="subst">&#123;self.optimizer.param_groups[<span class="number">0</span>][<span class="string">'lr'</span>]:<span class="number">.2</span>E&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"_patience: <span class="subst">&#123;_patience&#125;</span>"</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> best_model</span><br></pre></td></tr></table></figure><p>定义必要的组件，然后开始训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Defince Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer &amp; scheduler</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">    optimizer, mode=<span class="string">"min"</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train module</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, device=device, loss_fn=loss_fn,</span><br><span class="line">    optimizer=optimizer, scheduler=scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">best_model = trainer.train(</span><br><span class="line">    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 1 | train_loss: 0.86713, val_loss: 0.79795, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 2 | train_loss: 0.77799, val_loss: 0.79238, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 3 | train_loss: 0.77053, val_loss: 0.78976, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 4 | train_loss: 0.76625, val_loss: 0.78882, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 5 | train_loss: 0.76305, val_loss: 0.78799, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 6 | train_loss: 0.76027, val_loss: 0.78786, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 7 | train_loss: 0.75813, val_loss: 0.78810, lr: 1.00E-03, _patience: 4</span></span><br><span class="line"><span class="comment"># Epoch: 8 | train_loss: 0.75588, val_loss: 0.78725, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 9 | train_loss: 0.75429, val_loss: 0.78740, lr: 1.00E-03, _patience: 4</span></span><br><span class="line"><span class="comment"># Epoch: 10 | train_loss: 0.75270, val_loss: 0.78747, lr: 1.00E-03, _patience: 3</span></span><br></pre></td></tr></table></figure><h3 id="Evaluaton">Evaluaton</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(y_true, y_pred, classes)</span>:</span></span><br><span class="line">    <span class="string">"""Per-class performance metrics."""</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">"overall"</span>: &#123;&#125;, <span class="string">"class"</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">"weighted"</span>)</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"precision"</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"recall"</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"f1"</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"num_samples"</span>] = np.float64(len(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">        performance[<span class="string">"class"</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">"precision"</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">"recall"</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">"f1"</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">"num_samples"</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions</span></span><br><span class="line">test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Determine performance</span></span><br><span class="line">performance = get_metrics(</span><br><span class="line">    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance[<span class="string">"overall"</span>], indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "precision": 0.7074944756886696,</span></span><br><span class="line"><span class="comment">#   "recall": 0.6868333333333333,</span></span><br><span class="line"><span class="comment">#   "f1": 0.6866617275444412,</span></span><br><span class="line"><span class="comment">#   "num_samples": 18000.0</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><p>保存一些必要的模型数据，以供后续能够完整的加载和使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save artifacts</span></span><br><span class="line">dir = Path(<span class="string">"cnn"</span>)</span><br><span class="line">dir.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">label_encoder.save(fp=Path(dir, <span class="string">"label_encoder.json"</span>))</span><br><span class="line">tokenizer.save(fp=Path(dir, <span class="string">'tokenizer.json'</span>))</span><br><span class="line">torch.save(best_model.state_dict(), Path(dir, <span class="string">"model.pt"</span>))</span><br><span class="line"><span class="keyword">with</span> open(Path(dir, <span class="string">'performance.json'</span>), <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    json.dump(performance, indent=<span class="number">2</span>, sort_keys=<span class="literal">False</span>, fp=fp)</span><br></pre></td></tr></table></figure><h3 id="Inference">Inference</h3><p>接下来看看如何利用模型进行新的推理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load artifacts</span></span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line">label_encoder = LabelEncoder.load(fp=Path(dir, <span class="string">"label_encoder.json"</span>))</span><br><span class="line">tokenizer = Tokenizer.load(fp=Path(dir, <span class="string">'tokenizer.json'</span>))</span><br><span class="line">model = CNN(</span><br><span class="line">    vocab_size=VOCAB_SIZE, num_filters=NUM_FILTERS, filter_size=FILTER_SIZE,</span><br><span class="line">    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line">model.load_state_dict(torch.load(Path(dir, <span class="string">"model.pt"</span>), map_location=device))</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># CNN(</span></span><br><span class="line"><span class="comment">#   (conv): Conv1d(500, 50, kernel_size=(1,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=50, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize trainer</span></span><br><span class="line">trainer = Trainer(model=model, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dataloader</span></span><br><span class="line">text = <span class="string">"China’s economic recovery fades as services, factory activity show weakness"</span></span><br><span class="line">sequences = tokenizer.texts_to_sequences([preprocess(text)])</span><br><span class="line"><span class="keyword">print</span> (tokenizer.sequences_to_texts(sequences))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># ['china economic &lt;UNK&gt; &lt;UNK&gt; services &lt;UNK&gt; &lt;UNK&gt; show &lt;UNK&gt;']</span></span><br><span class="line"></span><br><span class="line">X = [to_categorical(seq, num_classes=len(tokenizer)) <span class="keyword">for</span> seq <span class="keyword">in</span> sequences]</span><br><span class="line">y_filler = label_encoder.encode([label_encoder.classes[<span class="number">0</span>]]*len(X))</span><br><span class="line">dataset = Dataset(X=X, y=y_filler, max_filter_size=FILTER_SIZE)</span><br><span class="line">dataloader = dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line">y_prob = trainer.predict_step(dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">print</span> (label_encoder.decode(y_pred))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># ['Business']</span></span><br></pre></td></tr></table></figure><p>推理结果是 “China’s economic recovery fades as services, factory activity show weakness” 这篇文章属于 “Business” 这个分类，符合预期。</p><p>我们来看一下这个case的具体概率分布</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_probability_distribution</span><span class="params">(y_prob, classes)</span>:</span></span><br><span class="line">    <span class="string">"""Create a dict of class probabilities from an array."""</span></span><br><span class="line">    results = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, class_ <span class="keyword">in</span> enumerate(classes):</span><br><span class="line">        results[class_] = np.float64(y_prob[i])</span><br><span class="line">    sorted_results = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> sorted(</span><br><span class="line">        results.items(), key=<span class="keyword">lambda</span> item: item[<span class="number">1</span>], reverse=<span class="literal">True</span>)&#125;</span><br><span class="line">    <span class="keyword">return</span> sorted_results</span><br><span class="line"></span><br><span class="line"><span class="comment"># Class distributions</span></span><br><span class="line">prob_dist = get_probability_distribution(y_prob=y_prob[<span class="number">0</span>], classes=label_encoder.classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(prob_dist, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "Business": 0.7551461458206177,</span></span><br><span class="line"><span class="comment">#   "World": 0.23087970912456512,</span></span><br><span class="line"><span class="comment">#   "Sci/Tech": 0.01362547930330038,</span></span><br><span class="line"><span class="comment">#   "Sports": 0.0003486045461613685</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>本篇给出了一个使用CNN对文本进行分类的完整示例，有很多细节需要深入学习。</p><p>但无论如何，先跑起来再说，在战斗中学习战斗。</p>]]></content>
    
    <summary type="html">
    
      先上手再说。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT开放函数调用能力 · 好用到震惊！</title>
    <link href="https://neo1989.net/Notes/NOTE-openai-function-calling/"/>
    <id>https://neo1989.net/Notes/NOTE-openai-function-calling/</id>
    <published>2023-06-16T04:06:39.000Z</published>
    <updated>2023-06-16T05:41:43.132Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>ChatGPT已经自带函数调用能力了，本文给了一个简单的示例。</p><h2 id="回顾">回顾</h2><p>笔者曾经在<a href="http://neo1989.net/Notes/NOTE-langchain-3/" title="LangChain | 快速释放LLMs的能力">LangChain</a>系列文章里交代利用LangChain赋予ChatGPT上网的能力。</p><p>然而OpenAI官方在<a href="https://openai.com/blog/function-calling-and-other-api-updates" target="_blank" rel="noopener" title="function-calling-and-other-api-updates">June 13, 2023</a>的更新里提出了function calling的能力，可以说在这个方向上直接灭掉了LangChain。</p><p>先看一下官方有哪些更新。<br><img src="//s3.mindex.xyz/blog/Courses/68e512444d547a4c6727d27d049050f4.png" alt=""></p><ul><li>在Chat Completions API中提供了新的函数调用能力</li><li><code>gpt-4</code> 和 <code>gpt-3.5-turbo</code> 模型的小版本迭代</li><li><code>gpt-3.5-turbo</code> 扩展到了4倍（16k）的上下文的能力</li><li>SOTA embeddings 模型降价 75%</li><li><code>gpt-3.5-turbo</code> 降价25%</li><li><code>gpt-3.5-turbo-0301</code> 和 <code>gpt-4-0314</code> 的退役时间</li></ul><p>而最令人激动的，实属 <code>function calling</code></p><h2 id="一个示范">一个示范</h2><p>如下图，依然是用人话要股票信息，能够直接给出df数据（完成函数调用）。<br><img src="//s3.mindex.xyz/blog/Courses/ec1319f72a0c54c58a3d8080a9231041.png" alt=""></p><h2 id="如何实现">如何实现</h2><p>简单到令人发指。</p><p>首先，只需定义functions manifest，如下示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">functions = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"get_stock_a"</span>,</span><br><span class="line">        <span class="string">"description"</span>: <span class="string">"获取指定A股股票一段时间内的量价信息"</span>,</span><br><span class="line">        <span class="string">"parameters"</span>: &#123;</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">            <span class="string">"properties"</span>: &#123;</span><br><span class="line">                <span class="string">"stock_name"</span>: &#123;</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"具体的股票名称或代号，如贵州茅台、中国移动"</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">"start_date"</span>: &#123;</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"开始日期，格式为2023-01-01"</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                 <span class="string">"end_date"</span>: &#123;</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"结束日期，格式为2023-01-01"</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">"required"</span>: [<span class="string">"stock_name"</span>, <span class="string">"start_date"</span>, <span class="string">"end_date"</span>],</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>当然这个是不能乱来的，需要遵循一定的规则，具体需要参考官方<a href="https://swagger.io/specification/" target="_blank" rel="noopener" title="OpenAPI Specification">specification</a></p><p>然后，实现你的自定义方法，这个示例就是实现 <code>get_stock_a</code> 方法，以获取指定A股股票的量价数据。这里我不给出具体实现，有兴趣私聊。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stock_a</span><span class="params">(stock_name, start_date, end_date)</span>:</span></span><br><span class="line">    <span class="string">""" 获取指定A股股票的量价数据 """</span></span><br><span class="line">    print(<span class="string">f"stock_name: <span class="subst">&#123;stock_name&#125;</span>, start_date: <span class="subst">&#123;start_date&#125;</span>, end_date: <span class="subst">&#123;end_date&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><p>接着，只需在调用Chat Completions API时候，把functions带上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">requests.post(<span class="string">f"<span class="subst">&#123;openai.api_base&#125;</span>/chat/completions"</span>, headers=headers,</span><br><span class="line">    json=&#123;</span><br><span class="line">        <span class="string">"model"</span>: GPT_MODEL,  <span class="comment"># 'gpt-3.5-turbo-0613' or 'gpt-4-0613'</span></span><br><span class="line">        <span class="string">"messages"</span>: messages,</span><br><span class="line">        <span class="string">"functions"</span>: functions</span><br><span class="line">    &#125;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>最后，你会看到类似下面这样的返回，完成一点解析和调用的动作，这个事就成了。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"role"</span>: <span class="string">"assistant"</span>,</span><br><span class="line">    <span class="attr">"content"</span>: <span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"function_call"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"get_stock_a"</span>,</span><br><span class="line">        <span class="attr">"arguments"</span>: <span class="string">"&#123;\n  \"stock_name\": \"招商银行\",\n  \"start_date\": \"2023-05-01\",\n  \"end_date\": \"2023-06-01\"\n&#125;"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="完整示例">完整示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">GPT_MODEL = <span class="string">"gpt-3.5-turbo-0613"</span></span><br><span class="line"></span><br><span class="line">openai.api_key = <span class="string">""</span>  <span class="comment"># 你的密钥</span></span><br><span class="line">openai.api_base = <span class="string">"https://api.openai.com/v1"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(messages, functions)</span>:</span></span><br><span class="line">    headers = &#123;<span class="string">"Content-Type"</span>: <span class="string">"application/json"</span>, <span class="string">"Authorization"</span>: <span class="string">f"Bearer <span class="subst">&#123;openai.api_key&#125;</span>"</span>&#125;</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.post(<span class="string">f"<span class="subst">&#123;openai.api_base&#125;</span>/chat/completions"</span>, headers=headers,</span><br><span class="line">            json=&#123;</span><br><span class="line">                <span class="string">"model"</span>: GPT_MODEL,</span><br><span class="line">                <span class="string">"messages"</span>: messages,</span><br><span class="line">                <span class="string">"functions"</span>: functions</span><br><span class="line">            &#125;,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> response.json()[<span class="string">"choices"</span>][<span class="number">0</span>][<span class="string">"message"</span>]</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">"Unable to generate ChatCompletion response"</span>)</span><br><span class="line">        print(<span class="string">f"Exception: <span class="subst">&#123;e&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">functions = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"get_stock_a"</span>,</span><br><span class="line">        <span class="string">"description"</span>: <span class="string">"获取指定A股股票一段时间内的量价信息"</span>,</span><br><span class="line">        <span class="string">"parameters"</span>: &#123;</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">            <span class="string">"properties"</span>: &#123;</span><br><span class="line">                <span class="string">"stock_name"</span>: &#123;</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"具体的股票名称或代号，如贵州茅台、中国移动"</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">"start_date"</span>: &#123;</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"开始日期，格式为2023-01-01"</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                 <span class="string">"end_date"</span>: &#123;</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"结束日期，格式为2023-01-01"</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">"required"</span>: [<span class="string">"stock_name"</span>, <span class="string">"start_date"</span>, <span class="string">"end_date"</span>],</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stock_a</span><span class="params">(stock_name, start_date, end_date)</span>:</span></span><br><span class="line">    print(<span class="string">f"stock_name: <span class="subst">&#123;stock_name&#125;</span>, start_date: <span class="subst">&#123;start_date&#125;</span>, end_date: <span class="subst">&#123;end_date&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chat_call</span><span class="params">(message)</span>:</span></span><br><span class="line">    messages = [&#123;<span class="string">"role"</span>: <span class="string">"system"</span>, <span class="string">"content"</span>: <span class="string">"不要对函数中应该填入的数值作出自作主张的假设。如果用户的要求不够明确，要求澄清。"</span>&#125;]</span><br><span class="line">    messages.append(&#123;<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: message&#125;)</span><br><span class="line">    r = call(messages, functions)</span><br><span class="line"></span><br><span class="line">    fcall = r[<span class="string">"function_call"</span>]</span><br><span class="line">    <span class="keyword">return</span> eval(fcall[<span class="string">"name"</span>])(**json.loads(fcall[<span class="string">"arguments"</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">chat_call(<span class="string">"2023年5月1日到6月1日，招商银行的A股量价给我一份"</span>)</span><br></pre></td></tr></table></figure><p>如果你足够幸运，你将看到这样一串文本: “stock_name: 招商银行, start_date: 2023-05-01, end_date: 2023-06-01”</p><h2 id="Ending">Ending</h2><p>不多说了，黄老板已经说过了，“跑起来掠食，或是努力奔跑免得成为掠食者的食物”。</p>]]></content>
    
    <summary type="html">
    
      Function calling capability in the Chat Completions API.
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · PyTorch实现神经网络的基本套路</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-utilities/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-utilities/</id>
    <published>2023-06-15T05:53:07.000Z</published>
    <updated>2023-06-27T11:03:55.252Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>本文简单交代了神经网络的基本套路以及部分实用组件，以简化开发过程。</p><h2 id="Set-up">Set up</h2><p>通常我们需要为重复实验设置很多seed，所以我们可以将其打包到一个函数里。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_seeds</span><span class="params">(seed=<span class="number">1024</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Set seeds for reproducibility."""</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    touch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)  <span class="comment"># multi-GPU</span></span><br><span class="line"></span><br><span class="line">set_seeds(seed=<span class="number">1024</span>)</span><br></pre></td></tr></table></figure><h2 id="Device">Device</h2><p>当我们有大型数据集和更大的模型要训练时，我们可以通过在 GPU 上并行化张量操作来加速。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cuda = <span class="literal">True</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span>(torch.cuda.is_available() <span class="keyword">and</span> cuda) <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">torch.set_default_tensor_type(&#123;<span class="string">"cuda"</span>: <span class="string">"torch.cuda.FloatTensor"</span>, <span class="string">"cpu"</span>: <span class="string">"torch.FloatTensor"</span>&#125;.get(str(device)))</span><br></pre></td></tr></table></figure><h2 id="Load-data">Load data</h2><p>这里依然使用前文引入的螺旋数据作为演示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">url = <span class="string">"http://s3.mindex.xyz/datasets/9378f64fc8dd2817e4c92be0a3bae8e7.csv"</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>) <span class="comment"># load</span></span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>) <span class="comment"># shuffle</span></span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Data shapes</span></span><br><span class="line">X = df[[<span class="string">"X1"</span>, <span class="string">"X2"</span>]].values</span><br><span class="line">y = df[<span class="string">"color"</span>].values</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X: "</span>, np.shape(X))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: "</span>, np.shape(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X:  (1500, 2)</span></span><br><span class="line"><span class="comment"># y:  (1500,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize data</span></span><br><span class="line">plt.title(<span class="string">"Generated non-linear data"</span>)</span><br><span class="line">colors = &#123;<span class="string">"c1"</span>: <span class="string">"red"</span>, <span class="string">"c2"</span>: <span class="string">"yellow"</span>, <span class="string">"c3"</span>: <span class="string">"blue"</span>&#125;</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=[colors[_y] <span class="keyword">for</span> _y <span class="keyword">in</span> y], edgecolors=<span class="string">"k"</span>, s=<span class="number">25</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/147e784e6ecae3fd226abce4f3905550.png" alt=""></p><h2 id="Split-data">Split data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_test_split</span><span class="params">(X, y, train_size)</span>:</span></span><br><span class="line">    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)</span><br><span class="line">    X_test, X_val, y_test, y_val = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (1050, 2), y_train: (1050,)</span></span><br><span class="line"><span class="comment"># X_val: (225, 2), y_val: (225,)</span></span><br><span class="line"><span class="comment"># X_test: (225, 2), y_test: (225,)</span></span><br><span class="line"><span class="comment"># Sample point: [0.17003003 0.63079261] → c3</span></span><br></pre></td></tr></table></figure><h2 id="Label-encoding">Label encoding</h2><p>接下来定义一个 LabelEncoder 来将文本标签编码成唯一的索引。</p><p>这里不再使用 scikit-learn 的 LabelEncoder，因为我们希望能够以我们想要的方式保存和加载我们的实例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelEncoder</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Label encoder for tag labels."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, class_to_index=None)</span>:</span></span><br><span class="line">        self.class_to_index = class_to_index <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = list(self.class_to_index.keys())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.class_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"&lt;LabelEncoder(num_classes=<span class="subst">&#123;len(self)&#125;</span>&gt;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        classes = np.unique(y)</span><br><span class="line">        <span class="keyword">for</span> i, class_ <span class="keyword">in</span> enumerate(classes):</span><br><span class="line">            self.class_to_index[class_] = i</span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = list(self.class_to_index.keys())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        encoded = np.zeros((len(y)), dtype=int)</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            encoded[i] = self.class_to_index[item]</span><br><span class="line">        <span class="keyword">return</span> encoded</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        classes = []</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            classes.append(self.index_to_class[item])</span><br><span class="line">        <span class="keyword">return</span> classes</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;<span class="string">'class_to_index'</span>: self.class_to_index&#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(cls, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"r"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Encode</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">label_encoder.fit(y_train)</span><br><span class="line">label_encoder.class_to_index</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert labels to tokens</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">y_train = label_encoder.encode(y_train)</span><br><span class="line">y_val = label_encoder.encode(y_val)</span><br><span class="line">y_test = label_encoder.encode(y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> enumerate(counts)&#125;</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: c3</span></span><br><span class="line"><span class="comment"># y_train[0]: 2</span></span><br><span class="line"><span class="comment"># counts: [350 350 350]</span></span><br><span class="line"><span class="comment"># weights: &#123;0: 0.002857142857142857, 1: 0.002857142857142857, 2: 0.002857142857142857&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Standardize-data">Standardize data</h2><p>我们需要标准化我们的数据（零均值和单位方差），这样特定特征的大小就不会影响模型学习其权重的方式。</p><p>我们只对输入X进行标准化，因为我们的输出y是类值。</p><p>我们将编写自己的 StandardScaler 类，以便在推理过程中轻松保存和加载它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StandardScaler</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mean=None, std=None)</span>:</span></span><br><span class="line">        self.mean = np.array(mean)</span><br><span class="line">        self.std = np.array(std)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        self.mean = np.mean(X_train, axis=<span class="number">0</span>)</span><br><span class="line">        self.std = np.std(X_train, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scale</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (X - self.mean) / self.std</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">unscale</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (X * self.std) + self.mean</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;<span class="string">"mean"</span>: self.mean.tolist(), <span class="string">"std"</span>: self.std.tolist()&#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(cls, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"r"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the data (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler()</span><br><span class="line">X_scaler.fit(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[0]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[1]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_test[0]: mean: 0.0, std: 1.0</span></span><br><span class="line"><span class="comment"># X_test[1]: mean: -0.0, std: 1.0</span></span><br></pre></td></tr></table></figure><h2 id="DataLoader">DataLoader</h2><p>我们将把数据放在 Dataset 中，并使用 DataLoader 来有效地创建用于训练和验证的批次。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        self.X = X</span><br><span class="line">        self.y = y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"&lt;Dataset(N=<span class="subst">&#123;len(self)&#125;</span>)&gt;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        X = self.X[index]</span><br><span class="line">        y = self.y[index]</span><br><span class="line">        <span class="keyword">return</span> [X, y]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(self, batch)</span>:</span></span><br><span class="line">        <span class="string">"""Processing on a batch."""</span></span><br><span class="line">        <span class="comment"># Get inputs</span></span><br><span class="line">        batch = np.array(batch)</span><br><span class="line">        X = np.stack(batch[:, <span class="number">0</span>], axis=<span class="number">0</span>)</span><br><span class="line">        y = batch[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cast</span></span><br><span class="line">        X = torch.FloatTensor(X.astype(np.float32))</span><br><span class="line">        y = torch.LongTensor(y.astype(np.int32))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_dataloader</span><span class="params">(self, batch_size, shuffle=False, drop_last=False)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.utils.data.DataLoader(</span><br><span class="line">            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,</span><br><span class="line">            shuffle=shuffle, drop_last=drop_last, pin_memory=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>事实上我们并不需要 collate_fn ，但我们可以让它透明（无副作用），因为当我想要对批处理做一些处理的时候，需要用到这个方法。(如：数据padding）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create datasets</span></span><br><span class="line">train_dataset = Dataset(X=X_train, y=y_train)</span><br><span class="line">val_dataset = Dataset(X=X_val, y=y_val)</span><br><span class="line">test_dataset = Dataset(X=X_test, y=y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Datasets:\n"</span></span><br><span class="line">       <span class="string">f"  Train dataset:<span class="subst">&#123;train_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">       <span class="string">f"  Val dataset: <span class="subst">&#123;val_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">       <span class="string">f"  Test dataset: <span class="subst">&#123;test_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">       <span class="string">"Sample point:\n"</span></span><br><span class="line">       <span class="string">f"  X: <span class="subst">&#123;train_dataset[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>\n"</span></span><br><span class="line">       <span class="string">f"  y: <span class="subst">&#123;train_dataset[<span class="number">0</span>][<span class="number">1</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Datasets:</span></span><br><span class="line"><span class="comment">#   Train dataset: &lt;Dataset(N=1050)&gt;</span></span><br><span class="line"><span class="comment">#   Val dataset: &lt;Dataset(N=225)&gt;</span></span><br><span class="line"><span class="comment">#   Test dataset: &lt;Dataset(N=225)&gt;</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: [-1.47355106 -1.67417243]</span></span><br><span class="line"><span class="comment">#   y: 0</span></span><br></pre></td></tr></table></figure><p>之前的文章中都是利用全部的数据进行梯度计算，然而更标准的做法是 <strong>mini-batch</strong> 随机梯度下降，也就是将样本分成多个只有 n(BATCH_SIZE) 个样本的 mini-batch。这就是 Dataloader 派上用场的地方。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create dataloaders</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_dataloader = train_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">val_dataloader = val_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">test_dataloader = test_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">batch_X, batch_y = next(iter(train_dataloader))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Sample batch:\n"</span></span><br><span class="line">       <span class="string">f"  X: <span class="subst">&#123;list(batch_X.size())&#125;</span>\n"</span></span><br><span class="line">       <span class="string">f"  y: <span class="subst">&#123;list(batch_y.size())&#125;</span>\n"</span></span><br><span class="line">       <span class="string">"Sample point:\n"</span></span><br><span class="line">       <span class="string">f"  X: <span class="subst">&#123;batch_X[<span class="number">0</span>]&#125;</span>\n"</span></span><br><span class="line">       <span class="string">f"  y: <span class="subst">&#123;batch_y[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Sample batch:</span></span><br><span class="line"><span class="comment">#   X: [64, 2]</span></span><br><span class="line"><span class="comment">#   y: [64]</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: tensor([ 0.4535, -0.3570], dtype=torch.float64)</span></span><br><span class="line"><span class="comment">#   y: 0</span></span><br></pre></td></tr></table></figure><h2 id="Model">Model</h2><p>我们需要定义一个模型，以便继续给出训练阶段的实用组件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>]  <span class="comment"># 2D</span></span><br><span class="line">HIDDEN_DIM = <span class="number">100</span></span><br><span class="line">DROPOUT_P = <span class="number">.01</span></span><br><span class="line">NUM_CLASSES = len(label_encoder.classes)</span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, dropout_p, num_classes)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = F.relu(self.fc1(x_in))</span><br><span class="line">        z = self.dropout(z)</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span class="line">            dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of MLP(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (droput): Dropout(p=0.01, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=3, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Trainer">Trainer</h2><p>之前的文章，我们一直在编写只使用循环来训练分割后的训练数据，然后在测试集上评估。</p><p>但实际工作中，我们会遵循下面这个过程：</p><ul><li>使用mini-batches进行训练</li><li>在验证集上评估损失，并更新超参</li><li>训练结束后，在测试集上评估模型</li></ul><p>所以我们需要创建 Trainer 类来组织这些过程。</p><p>首先，train_step 用来执行小批量数据训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">    self.model.train()</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">        batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]</span><br><span class="line">        inputs, targets = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">        self.optimizer.zero_grad()  <span class="comment"># reset gradients</span></span><br><span class="line">        z = self.model(inputs)  <span class="comment"># forward pass</span></span><br><span class="line">        J = self.loss_fn(z, targets)</span><br><span class="line">        J.backward()  <span class="comment"># backward pass</span></span><br><span class="line">        self.optimizer.step()  <span class="comment"># Update weights</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cumulative Metrics</span></span><br><span class="line">        loss += (j.detach().item() - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>然后 eval_step，用于验证</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">    self.model.eval()</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    y_trues, x_probs = [], []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.inference_model():</span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">            batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]</span><br><span class="line">            inputs, y_trye = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">            z = self.model(inputs)</span><br><span class="line">            J = self.loss_fn(z, y_true).item()</span><br><span class="line"></span><br><span class="line">            loss += (J - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Store outputs</span></span><br><span class="line">            y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">            y_probs.extend(y_prob)</span><br><span class="line">            y_trues.extend(y_true.cpu().numpy())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, np.vstack(y_trues), np.vstack(y_probs)</span><br></pre></td></tr></table></figure><p>最后 predict_step, 只是用来对数据进行预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">    self.model.eval()</span><br><span class="line">    y_prods = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.inference_model():</span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">            inputs, y_trye = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">            z = self.model(inputs)</span><br><span class="line">            y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">            y_probs.extend(y_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.vstack(y_probs)</span><br></pre></td></tr></table></figure><h2 id="LR-scheduler">LR scheduler</h2><p>我们将向优化器添加一个学习率调度器，以在训练期间调整我们的学习率。</p><p>有许多<a href="%22https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate%22" title="How to adjust learning rate">调度器</a>可供选择，但最受欢迎的是 ReduceLROnPlateau ，它在指标（例如：验证损失）停止改进的时候，减少学习率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the LR scheduler</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="string">"min"</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS * <span class="number">10</span>):</span><br><span class="line">    ...</span><br><span class="line">    train_loss = trainer.train_step(dataloader=train_dataloader)</span><br><span class="line">    val_loss, _, _ = trainer.eval_step(dataloader=val_dataloader)</span><br><span class="line">    scheduler.step(val_loss)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><h2 id="Early-stopping">Early stopping</h2><p>我们不应该拍脑袋训练足够多的epoch，而是应该有个明确的停止标准。</p><p>常见的停止标准，是模型达到一个期望的性能时，即停止训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Early stopping</span></span><br><span class="line"><span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">    best_val_loss = val_loss</span><br><span class="line">    best_model = trainer.model</span><br><span class="line">    _patience = patience  <span class="comment"># reset _patience</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    _patience -= <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> _patience:  <span class="comment"># 0</span></span><br><span class="line">    print(<span class="string">"Stopping early!"</span>)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><p>现在把上面这些放到一起</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">LEARNING_RATE = <span class="number">1e-2</span></span><br><span class="line">NUM_EPOCHS = <span class="number">100</span></span><br><span class="line">PATIENCE = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer &amp; scheduler</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">    optimizer, mode=<span class="string">"min"</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, device, loss_fn=None, optimizer=None, scheduler=None)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set params</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.device = device</span><br><span class="line">        self.loss_fn = loss_fn</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self.scheduler = scheduler</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Train step."""</span></span><br><span class="line">        <span class="comment"># Set model to train mode</span></span><br><span class="line">        self.model.train()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over train batches</span></span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Step</span></span><br><span class="line">            batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">            inputs, targets = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">            self.optimizer.zero_grad()  <span class="comment"># Reset gradients</span></span><br><span class="line">            z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">            J = self.loss_fn(z, targets)  <span class="comment"># Define loss</span></span><br><span class="line">            J.backward()  <span class="comment"># Backward pass</span></span><br><span class="line">            self.optimizer.step()  <span class="comment"># Update weights</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Cumulative Metrics</span></span><br><span class="line">            loss += (J.detach().item() - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Validation or test step."""</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.eval()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line">        y_trues, y_probs = [], []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Step</span></span><br><span class="line">                batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">                inputs, y_true = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">                z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">                J = self.loss_fn(z, y_true).item()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Cumulative Metrics</span></span><br><span class="line">                loss += (J - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line">                y_trues.extend(y_true.cpu().numpy())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, np.vstack(y_trues), np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Prediction step."""</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.eval()</span><br><span class="line">        y_probs = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Forward pass w/ inputs</span></span><br><span class="line">                inputs, targets = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">                z = self.model(inputs)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, num_epochs, patience, train_dataloader, val_dataloader)</span>:</span></span><br><span class="line">        best_val_loss = np.inf</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">            <span class="comment"># Steps</span></span><br><span class="line">            train_loss = self.train_step(dataloader=train_dataloader)</span><br><span class="line">            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)</span><br><span class="line">            self.scheduler.step(val_loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Early stopping</span></span><br><span class="line">            <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">                best_val_loss = val_loss</span><br><span class="line">                best_model = self.model</span><br><span class="line">                _patience = patience  <span class="comment"># reset _patience</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                _patience -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> _patience:  <span class="comment"># 0</span></span><br><span class="line">                print(<span class="string">"Stopping early!"</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Logging</span></span><br><span class="line">            print(</span><br><span class="line">                <span class="string">f"Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> | "</span></span><br><span class="line">                <span class="string">f"train_loss: <span class="subst">&#123;train_loss:<span class="number">.5</span>f&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"val_loss: <span class="subst">&#123;val_loss:<span class="number">.5</span>f&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"lr: <span class="subst">&#123;self.optimizer.param_groups[<span class="number">0</span>][<span class="string">'lr'</span>]:<span class="number">.2</span>E&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"_patience: <span class="subst">&#123;_patience&#125;</span>"</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> best_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trainer module</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, device=device, loss_fn=loss_fn,</span><br><span class="line">    optimizer=optimizer, scheduler=scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">best_model = trainer.train(</span><br><span class="line">    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 1 | train_loss: 0.87488, val_loss: 0.66353, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 2 | train_loss: 0.66368, val_loss: 0.55748, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># Epoch: 67 | train_loss: 0.03002, val_loss: 0.02305, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 68 | train_loss: 0.03011, val_loss: 0.02309, lr: 1.00E-02, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 69 | train_loss: 0.02544, val_loss: 0.02227, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 70 | train_loss: 0.02680, val_loss: 0.02154, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 71 | train_loss: 0.02897, val_loss: 0.02162, lr: 1.00E-02, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 72 | train_loss: 0.02737, val_loss: 0.02190, lr: 1.00E-02, _patience: 1</span></span><br><span class="line"><span class="comment"># Stopping early!</span></span><br></pre></td></tr></table></figure><h2 id="Evaluation">Evaluation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(y_true, y_pred, classes)</span>:</span></span><br><span class="line">    <span class="string">"""Per-class performance metrics."""</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">"overall"</span>: &#123;&#125;, <span class="string">"class"</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">"weighted"</span>)</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"precision"</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"recall"</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"f1"</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"num_samples"</span>] = np.float64(len(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">        performance[<span class="string">"class"</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">"precision"</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">"recall"</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">"f1"</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">"num_samples"</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions</span></span><br><span class="line">test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Determine performance</span></span><br><span class="line">performance = get_metrics(</span><br><span class="line">    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance[<span class="string">"overall"</span>], indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "precision": 0.9956140350877192,</span></span><br><span class="line"><span class="comment">#   "recall": 0.9955555555555555,</span></span><br><span class="line"><span class="comment">#   "f1": 0.9955553580159118,</span></span><br><span class="line"><span class="comment">#   "num_samples": 225.0</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><h2 id="Saving-loading">Saving &amp; loading</h2><p>我们需要保存一些必要的模型数据，以供后续能够完整的加载和使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Save artifacts</span></span><br><span class="line">dir = Path(<span class="string">"mlp"</span>)</span><br><span class="line">dir.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">label_encoder.save(fp=Path(dir, <span class="string">"label_encoder.json"</span>))</span><br><span class="line">X_scaler.save(fp=Path(dir, <span class="string">"X_scaler.json"</span>))</span><br><span class="line">torch.save(best_model.state_dict(), Path(dir, <span class="string">"model.pt"</span>))</span><br><span class="line"><span class="keyword">with</span> open(Path(dir, <span class="string">'performance.json'</span>), <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    json.dump(performance, indent=<span class="number">2</span>, sort_keys=<span class="literal">False</span>, fp=fp)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load artifacts</span></span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line">label_encoder = LabelEncoder.load(fp=Path(dir, <span class="string">"label_encoder.json"</span>))</span><br><span class="line">X_scaler = StandardScaler.load(fp=Path(dir, <span class="string">"X_scaler.json"</span>))</span><br><span class="line">model = MLP(</span><br><span class="line">    input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span class="line">    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line">model.load_state_dict(torch.load(Path(dir, <span class="string">"model.pt"</span>), map_location=device))</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize trainer</span></span><br><span class="line">trainer = Trainer(model=model, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dataloader</span></span><br><span class="line">sample = [[<span class="number">0.106737</span>, <span class="number">0.114197</span>]] <span class="comment"># c1</span></span><br><span class="line">X = X_scaler.scale(sample)</span><br><span class="line">y_filler = label_encoder.encode([label_encoder.classes[<span class="number">0</span>]]*len(X))</span><br><span class="line">dataset = Dataset(X=X, y=y_filler)</span><br><span class="line">dataloader = dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line">y_prob = trainer.predict_step(dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line">label_encoder.decode(y_pred)</span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>本文给出了一个机器学习项目的基本组件， 事实上，还有一些其他的重要组成没有覆盖到。比如：</p><ul><li>文本序列化的Tokenizers</li><li>表征数据的Encoders</li><li>数据padding</li><li>实验跟踪及可视化结果</li><li>超惨优化</li><li>等等</li></ul><p>后续我们会继续学习，至少到这里，我们有了入门深度学习的基础了。</p>]]></content>
    
    <summary type="html">
    
      先上手再说。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 神经网络 (二)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-neural-networks-2/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-neural-networks-2/</id>
    <published>2023-06-12T14:56:13.000Z</published>
    <updated>2023-06-14T05:21:05.688Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>接上篇，本文使用PyTorch实现一个相同的神经网络模型。</p><h2 id="Model">Model</h2><p>我们将使用两个线性连接层，并在前向传播中添加ReLU激活函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, num_classes)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = F.relu(self.fc1(x_in))</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initalize model</span></span><br><span class="line">model = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)</span><br><span class="line">print(model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of MLP(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=3, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><p>训练模型的代码跟之前学到的逻辑回归几乎没有区别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">LEARNING_RATE = <span class="number">1e-2</span></span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values()))</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_fn</span><span class="params">(y_pred, y_true)</span>:</span></span><br><span class="line">    n_correct = torch.eq(y_pred, y_true).sum().item()</span><br><span class="line">    accuarcy = (n_correct / len(y_pred)) * <span class="number">100</span></span><br><span class="line">    <span class="keyword">return</span> accuarcy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.LongTensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS * <span class="number">10</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        predictions = y_pred.max(dim=<span class="number">1</span>)[<span class="number">1</span>]  <span class="comment"># class</span></span><br><span class="line">        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 0.09, accuracy: 98.6</span></span><br><span class="line"><span class="comment"># Epoch: 10 | loss: 0.06, accuracy: 99.0</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.05, accuracy: 99.2</span></span><br><span class="line"><span class="comment"># Epoch: 30 | loss: 0.04, accuracy: 99.6</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.03, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 50 | loss: 0.03, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 60 | loss: 0.03, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 70 | loss: 0.02, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 80 | loss: 0.02, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 90 | loss: 0.02, accuracy: 99.7</span></span><br></pre></td></tr></table></figure><h2 id="Evaluation">Evaluation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predictiions</span></span><br><span class="line">y_prob = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line">y_pred = y_prob.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 1.0,</span></span><br><span class="line"><span class="comment">#     "recall": 1.0,</span></span><br><span class="line"><span class="comment">#     "f1": 1.0,</span></span><br><span class="line"><span class="comment">#     "num_samples": 225.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "c1": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 1.0,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c2": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 1.0,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c3": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 1.0,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/3ac0940bf9a117acde72a4d36241c2b8.png" alt=""></p><p>如你所见，PyTorch的直观和易用性能让我的学习曲线相对平缓。</p><p>需要我们编写的核心代码，只集中在定义模型、定义损失函数和优化器、定义训练循环、验证和测试这个四个部分。</p><p>当然，还有许多细节需要考虑，比如说数据预处理、模型的保存和加载、使用GPU等。</p><h2 id="Initializing-weights">Initializing weights</h2><p>到目前为止，我们都是使用了一个很小的随机值初始化权重，这其实不是让模型在训练阶段能够收敛的最佳方式。</p><p>我们的目标是初始化一个合适的权重，使得我们激活的输出不会消失或者爆炸，因为这两种情况都会阻碍模型收敛。事实上我们可以<a href="https://pytorch.org/docs/stable/nn.init.html" target="_blank" rel="noopener" title="nn.init">自定义权重初始化</a>方法。目前比较常用的是<a href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_normal_" target="_blank" rel="noopener" title="nn.init.xavier_normal_">Xavier初始化方法</a>和<a href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_" target="_blank" rel="noopener" title="nn.init.kaiming_normal_">He初始化方法</a>。</p><p>事实上PyTorch的Linear类默认使用了kaiming_uniform_初始化方法，相关源代码看<a href="https://github.com/pytorch/pytorch/blob/af7dc23124a6e3e7b8af0637e3b027f3a8b3fb76/torch/nn/modules/linear.py#L101" target="_blank" rel="noopener" title="Linear源码">这里</a>，后续我们会学习到更高级的优化收敛的策略如batch normalization。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, num_classes)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        init.xavier_normal_(self.fc1.weight, gain=init.calculate_gain(<span class="string">"relu"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = F.relu(self.fc1(x_in))</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure><h2 id="Dropout">Dropout</h2><p>能够让我们的模型表现的好的最好的技术是增加数据，但这并不总是一个可选项。幸运的是，还有有一些帮助模型更健壮的其他办法，如正则化、dropout等。</p><p>Dropout是在训练过程中允许我们将神经元的输出置0的技术。由于我们每批次都会丢弃一组不同的神经元，所以Dropout可以作为一种采样策略，防止过拟合。</p><p><img src="//s3.mindex.xyz/blog/Courses/2c301aaf51dcbdc7fb1556b1cf547228.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">DROPOUT_P = <span class="number">0.1</span> <span class="comment"># percentage of weights that are dropped each pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, dropout_p, num_classes)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p) <span class="comment"># dropout</span></span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        init.xavier_normal(self.fc1.weight, gain=init.calculate_gain(<span class="string">"relu"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = F.relu(self.fc1(x_in))</span><br><span class="line">        z = self.dropout(z) <span class="comment"># dropout</span></span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span class="line">            dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of MLP(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=3, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Overfitting">Overfitting</h2><p>虽然神经网络很擅长捕捉非线性关系，但它们非常容易对训练数据进行过度拟合，且无法对测试数据进行归纳。</p><p>看看下面的例子，我们使用完全随机的数据，并试图拟合含 $2 * N * C + D $ (其中N=样本数，C=标签，D表示输入纬度) 隐藏神经元的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line">NUM_EPOCHS = <span class="number">500</span></span><br><span class="line">NUM_SAMPLES_PER_CLASS = <span class="number">50</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line">HIDDEN_DIM = <span class="number">2</span> * NUM_SAMPLES_PER_CLASS * NUM_CLASSES + INPUT_DIM <span class="comment"># 2*N*C + D</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate random data</span></span><br><span class="line">X = np.random.rand(NUM_SAMPLES_PER_CLASS * NUM_CLASSES, INPUT_DIM)</span><br><span class="line">y = np.array([[i] * NUM_SAMPLES_PER_CLASS <span class="keyword">for</span> i <span class="keyword">in</span> range(NUM_CLASSES)]).reshape(<span class="number">-1</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X: "</span>, format(np.shape(X)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: "</span>, format(np.shape(y)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X:  (150, 2)</span></span><br><span class="line"><span class="comment"># y:  (150,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (105, 2), y_train: (105,)</span></span><br><span class="line"><span class="comment"># X_val: (23, 2), y_val: (23,)</span></span><br><span class="line"><span class="comment"># X_test: (22, 2), y_test: (22,)</span></span><br><span class="line"><span class="comment"># Sample point: [0.51102894 0.55377194] → 2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the inputs (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler().fit(X_train)</span><br><span class="line">X_train = X_scaler.transform(X_train)</span><br><span class="line">X_val = X_scaler.transform(X_val)</span><br><span class="line">X_test = X_scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.LongTensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span class="line">            dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of MLP(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=302, bias=True)</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=302, out_features=3, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">50</span>==<span class="number">0</span>:</span><br><span class="line">        predictions = y_pred.max(dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># class</span></span><br><span class="line">        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 1.07, accuracy: 43.8</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.94, accuracy: 52.4</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.89, accuracy: 55.2</span></span><br><span class="line"><span class="comment"># Epoch: 60 | loss: 0.87, accuracy: 49.5</span></span><br><span class="line"><span class="comment"># Epoch: 80 | loss: 0.82, accuracy: 63.8</span></span><br><span class="line"><span class="comment"># Epoch: 100 | loss: 0.84, accuracy: 62.9</span></span><br><span class="line"><span class="comment"># Epoch: 120 | loss: 0.75, accuracy: 63.8</span></span><br><span class="line"><span class="comment"># Epoch: 140 | loss: 0.77, accuracy: 60.0</span></span><br><span class="line"><span class="comment"># Epoch: 160 | loss: 0.75, accuracy: 60.0</span></span><br><span class="line"><span class="comment"># Epoch: 180 | loss: 0.75, accuracy: 66.7</span></span><br><span class="line"><span class="comment"># Epoch: 200 | loss: 0.75, accuracy: 67.6</span></span><br><span class="line"><span class="comment"># Epoch: 220 | loss: 0.69, accuracy: 68.6</span></span><br><span class="line"><span class="comment"># Epoch: 240 | loss: 0.75, accuracy: 65.7</span></span><br><span class="line"><span class="comment"># Epoch: 260 | loss: 0.73, accuracy: 71.4</span></span><br><span class="line"><span class="comment"># Epoch: 280 | loss: 0.73, accuracy: 69.5</span></span><br><span class="line"><span class="comment"># Epoch: 300 | loss: 0.71, accuracy: 62.9</span></span><br><span class="line"><span class="comment"># Epoch: 320 | loss: 0.68, accuracy: 69.5</span></span><br><span class="line"><span class="comment"># Epoch: 340 | loss: 0.74, accuracy: 65.7</span></span><br><span class="line"><span class="comment"># Epoch: 360 | loss: 0.68, accuracy: 71.4</span></span><br><span class="line"><span class="comment"># Epoch: 380 | loss: 0.78, accuracy: 63.8</span></span><br><span class="line"><span class="comment"># Epoch: 400 | loss: 0.69, accuracy: 66.7</span></span><br><span class="line"><span class="comment"># Epoch: 420 | loss: 0.75, accuracy: 67.6</span></span><br><span class="line"><span class="comment"># Epoch: 440 | loss: 0.76, accuracy: 69.5</span></span><br><span class="line"><span class="comment"># Epoch: 460 | loss: 0.71, accuracy: 67.6</span></span><br><span class="line"><span class="comment"># Epoch: 480 | loss: 0.66, accuracy: 66.7</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">y_prob = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line">y_pred = y_prob.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 0.45959595959595956,</span></span><br><span class="line"><span class="comment">#     "recall": 0.45454545454545453,</span></span><br><span class="line"><span class="comment">#     "f1": 0.4512987012987013,</span></span><br><span class="line"><span class="comment">#     "num_samples": 22.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "c1": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.5,</span></span><br><span class="line"><span class="comment">#       "recall": 0.375,</span></span><br><span class="line"><span class="comment">#       "f1": 0.42857142857142855,</span></span><br><span class="line"><span class="comment">#       "num_samples": 8.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c2": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.4444444444444444,</span></span><br><span class="line"><span class="comment">#       "recall": 0.5714285714285714,</span></span><br><span class="line"><span class="comment">#       "f1": 0.5,</span></span><br><span class="line"><span class="comment">#       "num_samples": 7.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c3": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.42857142857142855,</span></span><br><span class="line"><span class="comment">#       "recall": 0.42857142857142855,</span></span><br><span class="line"><span class="comment">#       "f1": 0.42857142857142855,</span></span><br><span class="line"><span class="comment">#       "num_samples": 7.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/8b43366da170668bdeaee171c279362d.png" alt=""></p><p>正如你所见，虽然模型在训练集上做到了接近70%的准确率，但模型在测试集上的表现并不能令人满意。</p><p>重要的是我们需要进行实验，从不合适（高偏差）的简单模型开始，并试图改进到良好的拟合，以及避免过拟合。</p><p><img src="//s3.mindex.xyz/blog/Courses/9a3b5a8d871020ccda41430ca7958bc1.png" alt=""></p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Neural networks - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      PyTorch实现一个神经网络。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 神经网络 (一)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-neural-networks-1/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-neural-networks-1/</id>
    <published>2023-06-01T08:16:24.000Z</published>
    <updated>2023-06-08T15:33:37.054Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>本章的目标依然是学习一种模型 $\hat{y}$，能准确对输入 $X$ 及对应的输出 $y$ 进行建模。</p><p>你会注意到神经网络只是我们迄今为止看到的广义线性方法的扩展，但具有非线性激活函数，因为我们的数据是高度非线性的。</p><p><img src="//s3.mindex.xyz/blog/Courses/908c174b73d8c8bb1c1ec3ba9e4cf885.png" alt=""></p><p>$$<br>z_1 = XW_1<br>$$</p><p>$$<br>a_1 = f(z_1)<br>$$</p><p>$$<br>z_2 = a_1W_2<br>$$</p><p>$$<br>\hat{y} = softmax(x)<br>$$</p><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">解释</th></tr></thead><tbody><tr><td style="text-align:center">$N$</td><td style="text-align:center">样本数</td></tr><tr><td style="text-align:center">$D$</td><td style="text-align:center">特征数</td></tr><tr><td style="text-align:center">$H$</td><td style="text-align:center">隐藏神经元</td></tr><tr><td style="text-align:center">$C$</td><td style="text-align:center">标签数</td></tr><tr><td style="text-align:center">$W_1$</td><td style="text-align:center">第一层的权重</td></tr><tr><td style="text-align:center">$z_1$</td><td style="text-align:center">第一层的输出</td></tr><tr><td style="text-align:center">$f$</td><td style="text-align:center">非线性激活函数</td></tr><tr><td style="text-align:center">$a_1$</td><td style="text-align:center">第一层的激活值</td></tr><tr><td style="text-align:center">$W_2$</td><td style="text-align:center">第二层的权重</td></tr><tr><td style="text-align:center">$z_2$</td><td style="text-align:center">第二层的输出</td></tr></tbody></table><h2 id="Set-up">Set up</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">np.random.seed(SEED)</span><br><span class="line">random.seed(SEED)</span><br></pre></td></tr></table></figure><h3 id="Load-data">Load data</h3><p>这里准备了一份非线性可分的螺旋数据来学习。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">url = <span class="string">"http://s3.mindex.xyz/datasets/9378f64fc8dd2817e4c92be0a3bae8e7.csv"</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>) <span class="comment"># load</span></span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>) <span class="comment"># shuffle</span></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/4022744de8e63b11599cdd95aab6ac62.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Data shapes</span></span><br><span class="line">X = df[[<span class="string">"X1"</span>, <span class="string">"X2"</span>]].values</span><br><span class="line">y = df[<span class="string">"color"</span>].values</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X: "</span>, np.shape(X))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: "</span>, np.shape(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X:  (1500, 2)</span></span><br><span class="line"><span class="comment"># y:  (1500,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize data</span></span><br><span class="line">plt.title(<span class="string">"Generated non-linear data"</span>)</span><br><span class="line">colors = &#123;<span class="string">"c1"</span>: <span class="string">"red"</span>, <span class="string">"c2"</span>: <span class="string">"yellow"</span>, <span class="string">"c3"</span>: <span class="string">"blue"</span>&#125;</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=[colors[_y] <span class="keyword">for</span> _y <span class="keyword">in</span> y], edgecolors=<span class="string">"k"</span>, s=<span class="number">25</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/147e784e6ecae3fd226abce4f3905550.png" alt=""></p><h3 id="Split-data">Split data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_test_split</span><span class="params">(X, y, train_size)</span>:</span></span><br><span class="line">    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)</span><br><span class="line">    X_test, X_val, y_test, y_val = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (1050, 2), y_train: (1050,)</span></span><br><span class="line"><span class="comment"># X_val: (225, 2), y_val: (225,)</span></span><br><span class="line"><span class="comment"># X_test: (225, 2), y_test: (225,)</span></span><br><span class="line"><span class="comment"># Sample point: [0.17003003 0.63079261] → c3</span></span><br></pre></td></tr></table></figure><h3 id="Label-encoding">Label encoding</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output vectorizer</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line"></span><br><span class="line"><span class="comment"># FIt on train date</span></span><br><span class="line">label_encoder = label_encoder.fit(y_train)</span><br><span class="line">classes = list(label_encoder.classes_)</span><br><span class="line">print(<span class="string">f"classes: <span class="subst">&#123;classes&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># classes: ['c1', 'c2', 'c3']</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert labels to tokens</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">y_train = label_encoder.transform(y_train)</span><br><span class="line">y_val = label_encoder.transform(y_val)</span><br><span class="line">y_test = label_encoder.transform(y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: c3</span></span><br><span class="line"><span class="comment"># y_train[0]: 2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> enumerate(counts)&#125;</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># counts: [350 350 350]</span></span><br><span class="line"><span class="comment"># weights: &#123;0: 0.002857142857142857, 1: 0.002857142857142857, 2: 0.002857142857142857&#125;</span></span><br></pre></td></tr></table></figure><h3 id="Standardize-data">Standardize data</h3><p>因为 $y$ 是类别值，所以我们只标准化 $X$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the data (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler().fit(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply scaler on training and test data (don't standardize outputs for classification)</span></span><br><span class="line">X_train = X_scaler.transform(X_train)</span><br><span class="line">X_val = X_scaler.transform(X_val)</span><br><span class="line">X_test = X_scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[0]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[1]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_test[0]: mean: 0.0, std: 1.0</span></span><br><span class="line"><span class="comment"># X_test[1]: mean: -0.0, std: 1.0</span></span><br></pre></td></tr></table></figure><h2 id="Linear-model">Linear model</h2><p>在尝试使用神经网络之前，为了解释激活函数，我们先用前面学到的逻辑回归模型来学习我们的数据。</p><p>你会发现一个用线性激活函数的线性模型对我们的数据来说并不是合适的。</p><h3 id="Model-Train">Model &amp; Train</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>] <span class="comment"># X is 2-dimensional</span></span><br><span class="line">HIDDEN_DIM = <span class="number">100</span></span><br><span class="line">NUM_CLASSES = len(classes) <span class="comment"># 3 classes</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, num_classes)</span>:</span></span><br><span class="line">        super(LinearModel, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = self.fc1(x_in) <span class="comment"># linear activation</span></span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = LinearModel(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)</span><br><span class="line"></span><br><span class="line">LEARNING_RATE = <span class="number">1e-2</span></span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values()))</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_fn</span><span class="params">(y_pred, y_true)</span>:</span></span><br><span class="line">    n_correct = torch.eq(y_pred, y_true).sum().item()</span><br><span class="line">    accuracy = (n_correct / len(y_pred)) * <span class="number">100</span></span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.LongTensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">1</span>==<span class="number">0</span>:</span><br><span class="line">        predictions = y_pred.max(dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># class</span></span><br><span class="line">        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 1.18, accuracy: 43.7</span></span><br><span class="line"><span class="comment"># Epoch: 1 | loss: 0.92, accuracy: 55.6</span></span><br><span class="line"><span class="comment"># Epoch: 2 | loss: 0.79, accuracy: 54.5</span></span><br><span class="line"><span class="comment"># Epoch: 3 | loss: 0.74, accuracy: 54.4</span></span><br><span class="line"><span class="comment"># Epoch: 4 | loss: 0.73, accuracy: 53.9</span></span><br><span class="line"><span class="comment"># Epoch: 5 | loss: 0.73, accuracy: 53.9</span></span><br><span class="line"><span class="comment"># Epoch: 6 | loss: 0.74, accuracy: 55.0</span></span><br><span class="line"><span class="comment"># Epoch: 7 | loss: 0.75, accuracy: 55.8</span></span><br><span class="line"><span class="comment"># Epoch: 8 | loss: 0.76, accuracy: 56.2</span></span><br><span class="line"><span class="comment"># Epoch: 9 | loss: 0.77, accuracy: 56.7</span></span><br></pre></td></tr></table></figure><h3 id="Evaluation">Evaluation</h3><p>我们来看一下这个线性模型在螺旋数据上的表现如何。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(y_true, y_pred, classes)</span>:</span></span><br><span class="line">    <span class="string">"""Per-class performance metrics."""</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">"overall"</span>: &#123;&#125;, <span class="string">"class"</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">"weighted"</span>)</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"precision"</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"recall"</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"f1"</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"num_samples"</span>] = np.float64(len(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">        performance[<span class="string">"class"</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">"precision"</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">"recall"</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">"f1"</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">"num_samples"</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">y_prob = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample probability: <span class="subst">&#123;y_prob[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">y_pred = y_prob.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample class: <span class="subst">&#123;y_pred[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># sample probability: tensor([0.3424, 0.0918, 0.5659], grad_fn=&lt;SelectBackward0&gt;)</span></span><br><span class="line"><span class="comment"># sample class: 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 0.5174825174825175,</span></span><br><span class="line"><span class="comment">#     "recall": 0.5155555555555555,</span></span><br><span class="line"><span class="comment">#     "f1": 0.5162093875662788,</span></span><br><span class="line"><span class="comment">#     "num_samples": 225.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "c1": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.5194805194805194,</span></span><br><span class="line"><span class="comment">#       "recall": 0.5333333333333333,</span></span><br><span class="line"><span class="comment">#       "f1": 0.5263157894736841,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c2": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.46153846153846156,</span></span><br><span class="line"><span class="comment">#       "recall": 0.48,</span></span><br><span class="line"><span class="comment">#       "f1": 0.47058823529411764,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c3": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.5714285714285714,</span></span><br><span class="line"><span class="comment">#       "recall": 0.5333333333333333,</span></span><br><span class="line"><span class="comment">#       "f1": 0.5517241379310344,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiclass_decision_boundary</span><span class="params">(model, X, y)</span>:</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].max() + <span class="number">0.1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].max() + <span class="number">0.1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.linspace(x_min, x_max, <span class="number">101</span>), np.linspace(y_min, y_max, <span class="number">101</span>))</span><br><span class="line">    cmap = plt.cm.Spectral</span><br><span class="line"></span><br><span class="line">    X_test = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()</span><br><span class="line">    y_pred = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line">    _, y_pred = y_pred.max(dim=<span class="number">1</span>)</span><br><span class="line">    y_pred = y_pred.reshape(xx.shape)</span><br><span class="line">    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.RdYlBu)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/e722233774a0fd9b91f4d77a3068287d.png" alt=""></p><h2 id="Activation-functions">Activation functions</h2><p>使用广义的线性方法产生了较差的结果，因为我们试图用线性激活函数去学习非线性数据。</p><p>所以我们需要一个可以能让模型学习到数据中的非线性的激活函数。有几种不同的选择，我们稍微探索一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fig size</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Data</span></span><br><span class="line">x = torch.arange(<span class="number">-5.</span>, <span class="number">5.</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sigmoid activation (constrain a value between 0 and 1.)</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Sigmoid activation"</span>)</span><br><span class="line">y = torch.sigmoid(x)</span><br><span class="line">plt.plot(x.numpy(), y.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tanh activation (constrain a value between -1 and 1.)</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">y = torch.tanh(x)</span><br><span class="line">plt.title(<span class="string">"Tanh activation"</span>)</span><br><span class="line">plt.plot(x.numpy(), y.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Relu (clip the negative values to 0)</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">y = F.relu(x)</span><br><span class="line">plt.title(<span class="string">"ReLU activation"</span>)</span><br><span class="line">plt.plot(x.numpy(), y.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show plots</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/499e55bee0461d4b7471247d91ec78b1.png" alt=""></p><p>ReLU激活函数$(max(0, z))$ 是目前为止用的最广泛的激活函数。但每个激活函数都有自己适用场景。比如：如果我们需要输出在0和1之间，那么sigmoid是合适的选择。</p><p>*（在某些情况下，ReLU函数也是不够的。例如，当神经元的输出大多为负时，激活函数的输出为0，这将导致神经元“死去”。为了减轻这种影响，我们可以降低学习率活着使用<a href="https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7" target="_blank" rel="noopener" title="ReLU">“ReLU变种”</a>。 如 Leaky ReLU 或 PRelu，它们会适当倾斜于神经元的负输出。）</p><h2 id="NumPy">NumPy</h2><p>现在，我们创建一个与逻辑回归模型完全相似的多层感知机，但包含一个学习数据中非线性的激活函数。</p><h3 id="Initialize-weights">Initialize weights</h3><p><strong>第一步</strong>: 随机初始化模型的权重$W$。（后面会介绍更有效的初始化策略）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize first layer's weights</span></span><br><span class="line">W1 = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, HIDDEN_DIM)</span><br><span class="line">b1 = np.zeros((<span class="number">1</span>, HIDDEN_DIM))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"W1: <span class="subst">&#123;W1.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"b1: <span class="subst">&#123;b1.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># W1: (2, 100)</span></span><br><span class="line"><span class="comment"># b1: (1, 100)</span></span><br></pre></td></tr></table></figure><h3 id="Model">Model</h3><p><strong>第二步</strong>: 讲输入 $X$ 送到模型中进行前向传播以得到网络的输出。</p><p>首先，我们将输入传给第一层。<br>$$<br>z_1 = XW_1<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># z1 = [NX2] · [2X100] + [1X100] = [NX100]</span></span><br><span class="line">z1 = np.dot(X_train, W1) + b1</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"z1: <span class="subst">&#123;z1.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># z1: (1050, 100)</span></span><br></pre></td></tr></table></figure><p>接下来，我们应用非线性激活函数Relu。<br>$$<br>a_1 = f(z_1)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply activation function</span></span><br><span class="line">a1 = np.maximum(<span class="number">0</span>, z1) <span class="comment"># ReLU</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"a_1: <span class="subst">&#123;a1.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># a_1: (1050, 100)</span></span><br></pre></td></tr></table></figure><p>然着我们将激活函数的输出传给第二层，以获得logit。<br>$$<br>z_2 = a_1W_2<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize second layer's weights</span></span><br><span class="line">W2 = <span class="number">0.01</span> * np.random.randn(HIDDEN_DIM, NUM_CLASSES)</span><br><span class="line">b2 = np.zeros((<span class="number">1</span>, NUM_CLASSES))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"W2: <span class="subst">&#123;W2.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"b2: <span class="subst">&#123;b2.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># W2: (100, 3)</span></span><br><span class="line"><span class="comment"># b2: (1, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># z2 = logits = [NX100] · [100X3] + [1X3] = [NX3]</span></span><br><span class="line">logits = np.dot(a1, W2) + b2</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"logits: <span class="subst">&#123;logits.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample: <span class="subst">&#123;logits[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># logits: (1050, 3)</span></span><br><span class="line"><span class="comment"># sample: [ 0.00017606 -0.0023457   0.00035913]</span></span><br></pre></td></tr></table></figure><p>之后，我们将应用softmax来获得网络的概率输出。</p><p>$$<br>\hat{y} = softmax(z_2)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normalization via softmax to obtain class probabilities</span></span><br><span class="line">exp_logits = np.exp(logits)</span><br><span class="line">y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_hat: <span class="subst">&#123;y_hat.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample: <span class="subst">&#123;y_hat[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_hat: (1050, 3)</span></span><br><span class="line"><span class="comment"># sample: [0.33359304 0.33275285 0.33365411]</span></span><br></pre></td></tr></table></figure><h3 id="Loss">Loss</h3><p><strong>第三步</strong>： 利用交叉熵计算我们分类任务的损失。<br>$$<br>J(\theta) = - \sum_i^K{log(\hat{y}_i)} = - \sum_i^K{log(\frac{e^{W_yX_i}}{\sum_j{e^{WX_i}}})}\\<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss</span></span><br><span class="line">correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])</span><br><span class="line">loss = np.sum(correct_class_logprobs) / len(y_train)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># loss: 1.10</span></span><br></pre></td></tr></table></figure><h3 id="Gradients">Gradients</h3><p><strong>第四步</strong> 计算损失函数 $J(\theta)$ 相对于权重的梯度。</p><p>对于$W_2$的梯度，与前篇逻辑回归的梯度相同，因为 $\hat{y} = softmax(z_2)$</p><p>$$<br>\begin{split}<br>\frac{\partial{J}}{\partial{W_{2j}}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{W_{2j}}}  \\<br>&amp;= - \frac{1}{\hat{y}} \frac{\partial{\hat{y}}}{\partial{W_{2j}}} \\<br>&amp;= - \frac{1}{\frac{e^{a_1 W_{2y}}}{\sum_j{e^{a_1 W}}}} \frac{\sum_j{e^{a_1 W}e^{a_1 W_{2y}} 0 - e^{a_1 W_{2y}} e^{a_1 W_{2j}} a_1 }}{(\sum_j{e^{a_1 W}})^2} \\<br>&amp;= \frac{a_1 e^{a_1 W_{2j}}}{\sum_j{e^{a_1 W}}} \\<br>&amp;= a_1 \hat{y}<br>\end{split}<br>$$</p><p>$$<br>\begin{split}<br>\frac{\partial{J}}{\partial{W_{2y}}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{W_{2y}}}  \\<br>&amp;= - \frac{1}{\hat{y}} \frac{\partial{\hat{y}}}{\partial{W_{2y}}} \\<br>&amp;= - \frac{1}{\frac{e^{a_1 W_{2y}}}{\sum_j{e^{a_1 W}}}} \frac{\sum_j{e^{a_1 W}e^{a_1 W_{2y}} a_1 - e^{W_{2y} a_1}e^{a_1 W_{2y}} a_1}}{(\sum_j{e^{a_1 W}})^2} = \frac{1}{\hat{y}} (a_1 \hat{y}^2 - a_1 \hat{y}) \\<br>&amp;= a_1 (\hat{y} - 1)<br>\end{split}<br>$$</p><p>对于 $W_1$ 的梯度计算有点棘手，因为我们必须要通过两组权重进行反向传播。</p><p>$$<br>\begin{split}<br>\frac{\partial{J}}{\partial{W_1}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{X}} \frac{\partial{X}}{\partial{z_1}}  \frac{\partial{z_1}}{\partial{W_1}} \\<br>&amp;= W_2 (\partial{\hat{y}})(\partial{ReLU})X<br>\end{split}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dJ/dW2</span></span><br><span class="line">dscores = y_hat</span><br><span class="line">dscores[range(len(y_hat)), y_train] -= <span class="number">1</span></span><br><span class="line">dscores /= len(y_train)</span><br><span class="line">dW2 = np.dot(a1.T, dscores)</span><br><span class="line">db2 = np.sum(dscores, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dJ/dW1</span></span><br><span class="line">dhidden = np.dot(dscores, W2.T)</span><br><span class="line">dhidden[a1 &lt;= <span class="number">0</span>] = <span class="number">0</span> <span class="comment"># ReLu backprop</span></span><br><span class="line">dW1 = np.dot(X_train.T, dhidden)</span><br><span class="line">db1 = np.sum(dhidden, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="Update-weights">Update weights</h3><p><strong>第五步</strong> 指定一个学习率来更新权重 $W$，惩罚错误的分类奖励正确的分类。<br>$$<br>W_i = W_i - \alpha \frac{\partial{J}}{\partial{W_i}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Update weights</span></span><br><span class="line">W1 += -LEARNING_RATE * dW1</span><br><span class="line">b1 += -LEARNING_RATE * db1</span><br><span class="line">W2 += -LEARNING_RATE * dW2</span><br><span class="line">b2 += -LEARNING_RATE * db2</span><br></pre></td></tr></table></figure><h3 id="Training">Training</h3><p><strong>第六步</strong>: 重复步骤 2 ~ 5，以最小化损失为目的来训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert tensors to NumPy arrays</span></span><br><span class="line">X_train = X_train.numpy()</span><br><span class="line">y_train = y_train.numpy()</span><br><span class="line">X_val = X_val.numpy()</span><br><span class="line">y_val = y_val.numpy()</span><br><span class="line">X_test = X_test.numpy()</span><br><span class="line">y_test = y_test.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random weights</span></span><br><span class="line">W1 = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, HIDDEN_DIM)</span><br><span class="line">b1 = np.zeros((<span class="number">1</span>, HIDDEN_DIM))</span><br><span class="line">W2 = <span class="number">0.01</span> * np.random.randn(HIDDEN_DIM, NUM_CLASSES)</span><br><span class="line">b2 = np.zeros((<span class="number">1</span>, NUM_CLASSES))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training loop</span></span><br><span class="line"><span class="keyword">for</span> epoch_num <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># First layer forward pass [NX2] · [2X100] = [NX100]</span></span><br><span class="line">    z1 = np.dot(X_train, W1) + b1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply activation function</span></span><br><span class="line">    a1 = np.maximum(<span class="number">0</span>, z1) <span class="comment"># ReLU</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># z2 = logits = [NX100] · [100X3] = [NX3]</span></span><br><span class="line">    logits = np.dot(a1, W2) + b2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Normalization via softmax to obtain class probabilities</span></span><br><span class="line">    exp_logits = np.exp(logits)</span><br><span class="line">    y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])</span><br><span class="line">    loss = np.sum(correct_class_logprobs) / len(y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># show progress</span></span><br><span class="line">    <span class="keyword">if</span> epoch_num%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Accuracy</span></span><br><span class="line">        y_pred = np.argmax(logits, axis=<span class="number">1</span>)</span><br><span class="line">        accuracy =  np.mean(np.equal(y_train, y_pred))</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch_num&#125;</span>, loss: <span class="subst">&#123;loss:<span class="number">.3</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.3</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dJ/dW2</span></span><br><span class="line">    dscores = y_hat</span><br><span class="line">    dscores[range(len(y_hat)), y_train] -= <span class="number">1</span></span><br><span class="line">    dscores /= len(y_train)</span><br><span class="line">    dW2 = np.dot(a1.T, dscores)</span><br><span class="line">    db2 = np.sum(dscores, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dJ/dW1</span></span><br><span class="line">    dhidden = np.dot(dscores, W2.T)</span><br><span class="line">    dhidden[a1 &lt;= <span class="number">0</span>] = <span class="number">0</span> <span class="comment"># ReLu backprop</span></span><br><span class="line">    dW1 = np.dot(X_train.T, dhidden)</span><br><span class="line">    db1 = np.sum(dhidden, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    W1 += <span class="number">-1e0</span> * dW1</span><br><span class="line">    b1 += <span class="number">-1e0</span> * db1</span><br><span class="line">    W2 += <span class="number">-1e0</span> * dW2</span><br><span class="line">    b2 += <span class="number">-1e0</span> * db2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0, loss: 1.098, accuracy: 0.519</span></span><br><span class="line"><span class="comment"># Epoch: 100, loss: 0.541, accuracy: 0.680</span></span><br><span class="line"><span class="comment"># Epoch: 200, loss: 0.305, accuracy: 0.893</span></span><br><span class="line"><span class="comment"># Epoch: 300, loss: 0.135, accuracy: 0.951</span></span><br><span class="line"><span class="comment"># Epoch: 400, loss: 0.091, accuracy: 0.976</span></span><br><span class="line"><span class="comment"># Epoch: 500, loss: 0.069, accuracy: 0.984</span></span><br><span class="line"><span class="comment"># Epoch: 600, loss: 0.056, accuracy: 0.989</span></span><br><span class="line"><span class="comment"># Epoch: 700, loss: 0.048, accuracy: 0.991</span></span><br><span class="line"><span class="comment"># Epoch: 800, loss: 0.043, accuracy: 0.994</span></span><br><span class="line"><span class="comment"># Epoch: 900, loss: 0.039, accuracy: 0.994</span></span><br></pre></td></tr></table></figure><h3 id="Evaluation-2">Evaluation</h3><p>在测试集上评估这个模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLPFromScratch</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        z1 = np.dot(x, W1) + b1</span><br><span class="line">        a1 = np.maximum(<span class="number">0</span>, z1)</span><br><span class="line">        logits = np.dot(a1, W2) + b2</span><br><span class="line">        exp_logits = np.exp(logits)</span><br><span class="line">        y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> y_hat</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluation</span></span><br><span class="line">model = MLPFromScratch()</span><br><span class="line">y_prob = model.predict(X_test)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 0.9826749826749827,</span></span><br><span class="line"><span class="comment">#     "recall": 0.9822222222222222,</span></span><br><span class="line"><span class="comment">#     "f1": 0.9822481383871041,</span></span><br><span class="line"><span class="comment">#     "num_samples": 225.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "c1": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 0.9733333333333334,</span></span><br><span class="line"><span class="comment">#       "f1": 0.9864864864864865,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c2": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.9615384615384616,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 0.9803921568627451,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c3": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.9864864864864865,</span></span><br><span class="line"><span class="comment">#       "recall": 0.9733333333333334,</span></span><br><span class="line"><span class="comment">#       "f1": 0.9798657718120806,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiclass_decision_boundary_numpy</span><span class="params">(model, X, y, savefig_fp=None)</span>:</span></span><br><span class="line">    <span class="string">"""Plot the multiclass decision boundary for a model that accepts 2D inputs.</span></span><br><span class="line"><span class="string">    Credit: https://cs231n.github.io/neural-networks-case-study/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        model &#123;function&#125; -- trained model with function model.predict(x_in).</span></span><br><span class="line"><span class="string">        X &#123;numpy.ndarray&#125; -- 2D inputs with shape (N, 2).</span></span><br><span class="line"><span class="string">        y &#123;numpy.ndarray&#125; -- 1D outputs with shape (N,).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Axis boundaries</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].max() + <span class="number">0.1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].max() + <span class="number">0.1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.linspace(x_min, x_max, <span class="number">101</span>),</span><br><span class="line">                         np.linspace(y_min, y_max, <span class="number">101</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create predictions</span></span><br><span class="line">    x_in = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    y_pred = model.predict(x_in)</span><br><span class="line">    y_pred = np.argmax(y_pred, axis=<span class="number">1</span>).reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot decision boundary</span></span><br><span class="line">    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.RdYlBu)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot</span></span><br><span class="line">    <span class="keyword">if</span> savefig_fp:</span><br><span class="line">        plt.savefig(savefig_fp, format=<span class="string">"png"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary_numpy(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary_numpy(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/0878670f7a00ef84e2037f39161b6fa5.png" alt=""></p><h2 id="Ending">Ending</h2><p>神经网络是机器学习和人工智能领域的基础，我们必须彻底掌握。</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Neural networks - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Numpy实现一个神经网络。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 机器学习之Logistic Regression (二)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-LogisticRegression-2/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-LogisticRegression-2/</id>
    <published>2023-05-29T01:51:29.000Z</published>
    <updated>2023-05-29T06:34:55.041Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>接上篇，本文使用PyTorch实现一个简单的逻辑回归。</p><h2 id="Get-ready">Get ready</h2><p>复用前篇的数据准备及预处理工作，这里直接建模。</p><h2 id="Model">Model</h2><p>我们使用PyTorch的<a href="https://pytorch.org/docs/stable/nn.html#linear-layers" target="_blank" rel="noopener" title="Linear layers">Linear layers</a> 来构建与前篇相同的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, num_classes)</span>:</span></span><br><span class="line">        super(LogisticRegression, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = self.fc1(x_in)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = LogisticRegression(input_dim=INPUT_DIM, num_classes=NUM_CLASSES)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of LogisticRegression(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=2, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Loss">Loss</h2><p>这里使用<a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss" target="_blank" rel="noopener" title="nn.CrossEntropyLoss">交叉熵损失</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">y_pred = torch.randn(<span class="number">3</span>, NUM_CLASSES, requires_grad=<span class="literal">False</span>)</span><br><span class="line">y_true = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(NUM_CLASSES)</span><br><span class="line"><span class="keyword">print</span> (y_true)</span><br><span class="line">loss = loss_fn(y_pred, y_true)</span><br><span class="line">print(<span class="string">f"Loss: <span class="subst">&#123;loss.numpy()&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor([0, 1, 1])</span></span><br><span class="line"><span class="comment"># Loss: 1.0754622220993042</span></span><br></pre></td></tr></table></figure><p>在这个任务中，我们将数据的类别权重纳入到损失函数中，以对抗样本的类别不平衡。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values()))</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br></pre></td></tr></table></figure><h2 id="Metrics">Metrics</h2><p>我们将在训练模型时引入准确度来衡量模型的性能，因为仅查看损失值并不是非常直观。</p><p>后面的章节会介绍相关指标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_fn</span><span class="params">(y_pred, y_true)</span>:</span></span><br><span class="line">    n_correct = torch.eq(y_pred, y_true).sum().item()</span><br><span class="line">    accuracy = (n_correct / len(y_pred)) * <span class="number">100</span></span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line">y_pred = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">y_true = torch.Tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">"Accuracy: &#123;accuracy_fn(y_pred, y_true):.1f&#125;"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Accuracy: &#123;accuracy_fn(y_pred, y_true):.1f&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Optimizer">Optimizer</h2><p>与之前介绍的线性回归一样，这里同样使用Adam优化器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.LongTensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line">NUM_EPOCHS = <span class="number">50</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">        predictions = y_pred.max(dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># class</span></span><br><span class="line">        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 0.71, accuracy: 49.6</span></span><br><span class="line"><span class="comment"># Epoch: 10 | loss: 0.23, accuracy: 93.1</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.14, accuracy: 97.4</span></span><br><span class="line"><span class="comment"># Epoch: 30 | loss: 0.11, accuracy: 98.3</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.09, accuracy: 98.0</span></span><br></pre></td></tr></table></figure><h2 id="Evaluation">Evaluation</h2><p>首先，我们看看一下测试集的准确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">pred_train = F.softmax(model(X_train), dim=<span class="number">1</span>)</span><br><span class="line">pred_test = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample probability: <span class="subst">&#123;pred_test[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">pred_train = pred_train.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">pred_test = pred_test.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample class: <span class="subst">&#123;pred_test[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># sample probability: tensor([0.9934, 0.0066], grad_fn=&lt;SelectBackward0&gt;)</span></span><br><span class="line"><span class="comment"># sample class: 0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy (could've also used our own accuracy function)</span></span><br><span class="line">train_acc = accuracy_score(y_train, pred_train)</span><br><span class="line">test_acc = accuracy_score(y_test, pred_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"train acc: <span class="subst">&#123;train_acc:<span class="number">.2</span>f&#125;</span>, test acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train acc: 0.98, test acc: 0.97</span></span><br></pre></td></tr></table></figure><p>我们还可以根据其他有意义的指标来评估我们的模型，如精确度和召回率。<br>$$<br>accuracy = \frac{TP + FN}{TP + TN + FP + FN}<br>$$<br>$$<br>recall = \frac{TP}{TP + FN}<br>$$<br>$$<br>precision = \frac{TP}{TP + FP}<br>$$<br>$$<br>F1 = 2 * \frac{precision * recall}{precision + recall}<br>$$</p><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">解释</th></tr></thead><tbody><tr><td style="text-align:center">TP</td><td style="text-align:center">truly predicted to be positive and were positive</td></tr><tr><td style="text-align:center">TN</td><td style="text-align:center">truly predicted to negative and where negative</td></tr><tr><td style="text-align:center">FP</td><td style="text-align:center">falsely predicted to be positive but where negative</td></tr><tr><td style="text-align:center">FN</td><td style="text-align:center">falsely predicted to be negative but where positive</td></tr></tbody></table><p>格式化指标，以供前端展示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(y_true, y_pred, classes)</span>:</span></span><br><span class="line">    <span class="string">"""Per-class performance metrics."""</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">"overall"</span>: &#123;&#125;, <span class="string">"class"</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">"weighted"</span>)</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"precision"</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"recall"</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"f1"</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"num_samples"</span>] = np.float64(len(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">        performance[<span class="string">"class"</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">"precision"</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">"recall"</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">"f1"</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">"num_samples"</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br><span class="line"></span><br><span class="line"><span class="comment"># # Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=pred_test, classes=label_encoder.classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 0.9744444444444446,</span></span><br><span class="line"><span class="comment">#     "recall": 0.9733333333333334,</span></span><br><span class="line"><span class="comment">#     "f1": 0.9731408308004051,</span></span><br><span class="line"><span class="comment">#     "num_samples": 150.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "benign": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 0.9310344827586207,</span></span><br><span class="line"><span class="comment">#       "f1": 0.9642857142857143,</span></span><br><span class="line"><span class="comment">#       "num_samples": 58.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "malignant": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.9583333333333334,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 0.9787234042553191,</span></span><br><span class="line"><span class="comment">#       "num_samples": 92.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><p>同样的，利用PyTorch实现的逻辑回归模型建模了一个线性决策边界，我们可视化一下结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiclass_decision_boundary</span><span class="params">(model, X, y)</span>:</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].max() + <span class="number">0.1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].max() + <span class="number">0.1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.linspace(x_min, x_max, <span class="number">101</span>), np.linspace(y_min, y_max, <span class="number">101</span>))</span><br><span class="line">    cmap = plt.cm.Spectral</span><br><span class="line"></span><br><span class="line">    X_test = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()</span><br><span class="line">    y_pred = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line">    _, y_pred = y_pred.max(dim=<span class="number">1</span>)</span><br><span class="line">    y_pred = y_pred.reshape(xx.shape)</span><br><span class="line">    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.RdYlBu)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/8d309a3958c1769ef2403cc39a31dd17.png" alt=""></p><h2 id="Inference">Inference</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inputs for inference</span></span><br><span class="line">X_infer = pd.DataFrame([&#123;<span class="string">"leukocyte_count"</span>: <span class="number">13</span>, <span class="string">"blood_pressure"</span>: <span class="number">12</span>&#125;])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize</span></span><br><span class="line">X_infer = X_scaler.transform(X_infer)</span><br><span class="line"><span class="keyword">print</span> (X_infer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[-0.66859939 -3.09473005]]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">y_infer = F.softmax(model(torch.Tensor(X_infer)), dim=<span class="number">1</span>)</span><br><span class="line">prob, _class = y_infer.max(dim=<span class="number">1</span>)</span><br><span class="line">label = label_encoder.decode(_class.detach().numpy())[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"The probability that you have a <span class="subst">&#123;label&#125;</span> tumor is <span class="subst">&#123;prob.detach().numpy()[<span class="number">0</span>]*<span class="number">100.0</span>:<span class="number">.0</span>f&#125;</span>%"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># The probability that you have a benign tumor is 90%</span></span><br></pre></td></tr></table></figure><h2 id="Unscaled-weights">Unscaled weights</h2><p>同样的，我们亦可以逆标准化我们的权重和偏差。</p><p>注意到只有$X$被标准化过<br>$$<br>\hat{y}_{unscaled} = \sum_{j=1}^k W_{scaled(j)} x_{scaled(j)} + b_{scaled}<br>$$</p><p>已知<br>$$<br>\hat{x}_{scaled} = \frac{x_{j} - \overline{x}_{j}}{\sigma_{j}}<br>$$</p><p>于是<br>$$<br>\hat{y}_{unscaled} = (b_{scaled} - \sum_{j=1}^k W_{scaled(j)} \frac{\overline{x}_j}{\sigma_{j}}) + \sum_j{\frac{W_{scaled(j)}}{\sigma_j}}x_j<br>$$</p><p>对比公式</p><p>$$<br>\hat{y}_{unscaled} = W_{unscaled} x + b_{unscaled}<br>$$</p><p>便可得知<br>$$<br>W_{unscaled} = \frac{W_{scaled(j)}}{\sigma_j}<br>$$</p><p>$$<br>b_{unscaled} = b_{scaled} - \sum_{j=1}^k W_{unscaled(j)} \overline{x}_j<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unstandardize weights</span></span><br><span class="line">W = model.fc1.weight.data.numpy()</span><br><span class="line">b = model.fc1.bias.data.numpy()</span><br><span class="line">W_unscaled = W / X_scaler.scale_</span><br><span class="line">b_unscaled = b - np.sum((W_unscaled * X_scaler.mean_))</span><br><span class="line"><span class="keyword">print</span> (W_unscaled)</span><br><span class="line"><span class="keyword">print</span> (b_unscaled)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[ 0.80800055 -1.47212977]</span></span><br><span class="line"><span class="comment">#  [-0.88854214  0.77129243]]</span></span><br><span class="line"><span class="comment"># [11.4279   13.336911]</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>到这里，我们便完成了PyTorch的逻辑回归任务的介绍。</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Logistic regression - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      使用PyTorch实现逻辑回归。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 机器学习之Logistic Regression (一)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-LogisticRegression-1/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-LogisticRegression-1/</id>
    <published>2023-05-28T11:24:37.000Z</published>
    <updated>2023-06-08T09:42:41.627Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文的目标是使用NumPy实现一个多项式逻辑回归模型 $\hat{y}$ ，以实现对给定的特征$X$预测其目标类别的概率分布。</p><p>$$<br>\hat{y} = \frac{e^{XW_y}}{\sum_j{e^{XW}}}<br>$$</p><p>逻辑回归与前篇所述的线性回归，同属于广义线性回归。皆是建模一条直线(或一个平面)。但不同的是，逻辑回归是一种分类模型。</p><h2 id="Set-up">Set up</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">np.random.seed(SEED)</span><br><span class="line">random.seed(SEED)</span><br></pre></td></tr></table></figure><h2 id="Load-data">Load data</h2><p>我们的任务是基于白细胞计数和血压来确定肿瘤是否为良性（无害）或恶性（有害）。</p><p>请注意，这是一个合成数据集，没有临床意义。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pandas.plotting <span class="keyword">import</span> scatter_matrix</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read from CSV to Pandas DataFrame</span></span><br><span class="line">url = <span class="string">"http://s3.mindex.xyz/datasets/tumors.csv"</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>) <span class="comment"># load</span></span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>) <span class="comment"># shuffle</span></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/7fdb16f4e4e0b9cc36e4c1b765fbc718.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define X and y</span></span><br><span class="line">X = df[[<span class="string">"leukocyte_count"</span>, <span class="string">"blood_pressure"</span>]].values</span><br><span class="line">y = df[<span class="string">"tumor_class"</span>].values</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot data</span></span><br><span class="line">colors = &#123;<span class="string">"benign"</span>: <span class="string">"red"</span>, <span class="string">"malignant"</span>: <span class="string">"blue"</span>&#125;</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=[colors[_y] <span class="keyword">for</span> _y <span class="keyword">in</span> y], s=<span class="number">25</span>, edgecolors=<span class="string">"k"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"leukocyte count"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"blood pressure"</span>)</span><br><span class="line">plt.legend([<span class="string">"malignant"</span>, <span class="string">"benign"</span>], loc=<span class="string">"upper right"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/57d000f09d3f8c9d9f9971618d58ffd9.png" alt=""></p><h2 id="Split-data">Split data</h2><p>我们希望将数据集分成三份，使得每个子集中的类别分布相同，以便进行适当的训练和评估。</p><p><a href="https://scikit-learn.org/stable/index.html" target="_blank" rel="noopener" title="scikit-learn">scikit-learn</a> 提供的方法 <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener" title="train_test_split">train_test_split</a> 可以很容易的做到。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_test_split</span><span class="params">(X, y, train_size)</span>:</span></span><br><span class="line">    <span class="string">"""Split dataset into data splits."""</span></span><br><span class="line">    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)</span><br><span class="line">    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (700, 2), y_train: (700,)</span></span><br><span class="line"><span class="comment"># X_val: (150, 2), y_val: (150,)</span></span><br><span class="line"><span class="comment"># X_test: (150, 2), y_test: (150,)</span></span><br><span class="line"><span class="comment"># Sample point: [18.60187909 18.37050035] → malignant</span></span><br></pre></td></tr></table></figure><p>现在让我们看一下分割后的数据集中样本分布：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Overall class distribution</span></span><br><span class="line">class_counts = dict(collections.Counter(y))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Classes: <span class="subst">&#123;class_counts&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f'm:b = <span class="subst">&#123;class_counts[<span class="string">"malignant"</span>]/class_counts[<span class="string">"benign"</span>]:<span class="number">.2</span>f&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Classes: &#123;'malignant': 611, 'benign': 389&#125;</span></span><br><span class="line"><span class="comment"># m:b = 1.57</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Per data split class distribution</span></span><br><span class="line">train_class_counts = dict(collections.Counter(y_train))</span><br><span class="line">val_class_counts = dict(collections.Counter(y_val))</span><br><span class="line">test_class_counts = dict(collections.Counter(y_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f'train m:b = <span class="subst">&#123;train_class_counts[<span class="string">"malignant"</span>]/train_class_counts[<span class="string">"benign"</span>]:<span class="number">.2</span>f&#125;</span>'</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f'val m:b = <span class="subst">&#123;val_class_counts[<span class="string">"malignant"</span>]/val_class_counts[<span class="string">"benign"</span>]:<span class="number">.2</span>f&#125;</span>'</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f'test m:b = <span class="subst">&#123;test_class_counts[<span class="string">"malignant"</span>]/test_class_counts[<span class="string">"benign"</span>]:<span class="number">.2</span>f&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train m:b = 1.57</span></span><br><span class="line"><span class="comment"># val m:b = 1.59</span></span><br><span class="line"><span class="comment"># test m:b = 1.54</span></span><br></pre></td></tr></table></figure><p>可以看出，分割后的数据集里样本分布大体是接近的</p><h2 id="Label-encoding">Label encoding</h2><p>注意到我们的分类标签是文本。我们需要将其进行编码，以便在模型中使用。</p><p>通常我们使用scikit-learn的<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder" target="_blank" rel="noopener" title="LabelEncoder">LabelEncoder</a>快速编码。</p><p>不过这里我们将编写自己的编码器，以便了解其具体的实现机制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelEncoder</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Label encoder for tag labels."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, class_to_index=&#123;&#125;)</span>:</span></span><br><span class="line">        self.class_to_index = class_to_index <span class="keyword">or</span> &#123;&#125;  <span class="comment"># mutable defaults ;)</span></span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = list(self.class_to_index.keys())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.class_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"&lt;LabelEncoder(num_classes=<span class="subst">&#123;len(self)&#125;</span>)&gt;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        classes = np.unique(y)</span><br><span class="line">        <span class="keyword">for</span> i, class_ <span class="keyword">in</span> enumerate(classes):</span><br><span class="line">            self.class_to_index[class_] = i</span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = list(self.class_to_index.keys())</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        encoded = np.zeros((len(y)), dtype=int)</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            encoded[i] = self.class_to_index[item]</span><br><span class="line">        <span class="keyword">return</span> encoded</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        classes = []</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            classes.append(self.index_to_class[item])</span><br><span class="line">        <span class="keyword">return</span> classes</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;<span class="string">'class_to_index'</span>: self.class_to_index&#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(cls, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"r"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">label_encoder.fit(y_train)</span><br><span class="line"><span class="keyword">print</span> (label_encoder.class_to_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;'benign': 0, 'malignant': 1&#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Encoder</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">y_train = label_encoder.encode(y_train)</span><br><span class="line">y_val = label_encoder.encode(y_val)</span><br><span class="line">y_test = label_encoder.encode(y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"decoded: <span class="subst">&#123;label_encoder.decode([y_train[<span class="number">0</span>]])&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: malignant</span></span><br><span class="line"><span class="comment"># y_train[0]: 1</span></span><br><span class="line"><span class="comment"># decoded: ['malignant']</span></span><br></pre></td></tr></table></figure><p>我们还想计算出类别的权重，这对于在训练期间加权损失函数非常有用。它会告诉模型需要关注样本量不足的类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> enumerate(counts)&#125;</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># counts: [272 428]</span></span><br><span class="line"><span class="comment"># weight: &#123;0: 0.003676470588235294, 1: 0.002336448598130841&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Standardize-data">Standardize data</h2><p>我们需要对数据进行标准化处理（零均值和单位方差），以便某个特定特征的大小不会影响模型学习其权重。</p><p>这里我们只需要标准化输入$X$，因为我们的输出$y$是离散标签，无须标准化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the data (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler().fit(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply scaler on training and test data (don't standardize outputs for classification)</span></span><br><span class="line">X_train = X_scaler.transform(X_train)</span><br><span class="line">X_val = X_scaler.transform(X_val)</span><br><span class="line">X_test = X_scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[0]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[1]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_test[0]: mean: -0.0, std: 1.0</span></span><br><span class="line"><span class="comment"># X_test[1]: mean: 0.1, std: 1.0</span></span><br></pre></td></tr></table></figure><h2 id="Weights">Weights</h2><p>我们的目标是学习一个逻辑回归模型 $\hat{y}$，用来建模自变量 $X$ 与 因变量 $y$ 的关系。</p><p>$$<br>\hat{y} = \frac{e^{W_yX}}{\sum_j{e^{WX}}}<br>$$</p><p>尽管我们的示例任务只涉及两个类别，但我们仍将使用多项式逻辑回归，因为softmax分类器可以推广到任意数量的类别。</p><p><strong>第一步</strong>: 随机初始化模型的权重 $W$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>] <span class="comment"># X is 2-dimensional</span></span><br><span class="line">NUM_CLASSES = len(label_encoder.classes) <span class="comment"># y has two possibilities (benign or malignant)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random weights</span></span><br><span class="line">W = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, NUM_CLASSES)</span><br><span class="line">b = np.zeros((<span class="number">1</span>, NUM_CLASSES))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"W: <span class="subst">&#123;W.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"b: <span class="subst">&#123;b.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># W: (2, 2)</span></span><br><span class="line"><span class="comment"># b: (1, 2)</span></span><br></pre></td></tr></table></figure><h2 id="Model">Model</h2><p><strong>第二步</strong> 计算输入 $X$ 的对数值（$ z = WX $）。然后执行softmax操作得到预测类别的独热编码形式。</p><p>举个例子，如果有三个类别，那么一种可能的预测概率是[0.3, 0.3, 0.4]。<br>$$<br>\hat{y} = softmax(z) = softmax(WX) = \frac{e^{W_yX}}{\sum_j{e^{WX}}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Forward pass [NX2] · [2X2] + [1,2] = [NX2]</span></span><br><span class="line">logits = np.dot(X_train, W) + b</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"logits: <span class="subst">&#123;logits.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample: <span class="subst">&#123;logits[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># logits: (700, 2)</span></span><br><span class="line"><span class="comment"># sample: [0.0151033  0.03500428]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalization via softmax to obtain class probabilities</span></span><br><span class="line">exp_logits = np.exp(logits)</span><br><span class="line">y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_hat: <span class="subst">&#123;y_hat.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample: <span class="subst">&#123;y_hat[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_hat: (700, 2)</span></span><br><span class="line"><span class="comment"># sample: [0.49502492 0.50497508]</span></span><br></pre></td></tr></table></figure><h2 id="Loss">Loss</h2><p><strong>第三步</strong> 使用代价函数比较预测值 $\hat{y}$ （比如：[0.3, 0.3, 0.4]）和目标值 $y$ （比如：[0. 0. 1]）来确定损失 $J$。</p><p>逻辑回归的常见目标函数是交叉熵损失.</p><p>$$<br>J(\theta) = - \sum_i^K{log(\hat{y}_i)} = - \sum_i^K{log(\frac{e^{W_yX_i}}{\sum_j{e^{WX_i}}})}\\<br>$$</p><p>交叉熵损失函数的目标仍然是最小化预测与实际标签之间的差距，从而让模型能够更准确地进行分类.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss</span></span><br><span class="line">correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])</span><br><span class="line">loss = np.sum(correct_class_logprobs) / len(y_train)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 0.69</span></span><br></pre></td></tr></table></figure><h2 id="Gradients">Gradients</h2><p><strong>第四步</strong> 计算损失函数 $J(\theta)$ 相对于权重的梯度。这里假设我们的类别是互斥的。<br>$$<br>\begin{split}<br>\frac{\partial{J}}{\partial{W_j}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{W_j}} = - \frac{1}{\hat{y}} \frac{\partial{\hat{y}}}{\partial{W_j}} \\<br>&amp;= - \frac{1}{\frac{e^{W_yX}}{\sum_j{e^{WX}}}} \frac{\sum_j{e^{WX}e^{W_yX}0 - e^{W_yX}e^{W_jX}X}}{(\sum_j{e^{WX}})^2} = \frac{Xe^{W_jX}}{\sum_j{e^{WX}}} = X\hat{y}<br>\end{split}<br>$$</p><p>$$<br>\begin{split}<br>\frac{\partial{J}}{\partial{W_y}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{W_y}} = - \frac{1}{\hat{y}} \frac{\partial{\hat{y}}}{\partial{W_y}} \\<br>&amp;= - \frac{1}{\frac{e^{W_yX}}{\sum_j{e^{WX}}}} \frac{\sum_j{e^{WX}e^{W_yX}X - e^{W_yX}e^{W_yX}X}}{(\sum_j{e^{WX}})^2} = \frac{1}{\hat{y}} (X\hat{y}^2 - X\hat{y}) = X(\hat{y} - 1)<br>\end{split}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Backpropagation</span></span><br><span class="line">y_hat[range(len(y_hat)), y_train] -= <span class="number">1</span></span><br><span class="line">y_hat /= len(y_train)</span><br><span class="line">dW = np.dot(X_train.T, y_hat)</span><br><span class="line">db = np.sum(y_hat, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="Update-weights">Update weights</h2><p><strong>第五步</strong> 指定一个学习率来更新权重 $W$，惩罚错误的分类奖励正确的分类。<br>$$<br>W_j = W_j - \alpha \frac{\partial{J}}{\partial{W_j}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Update weights</span></span><br><span class="line">W == -LEARNING_RATE * dW</span><br><span class="line">b += -LEARNING_RATE * db</span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><p><strong>第六步</strong>: 重复步骤 2 ~ 5，以最小化损失为目的来训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Initialize random weights</span></span><br><span class="line">W = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, NUM_CLASSES)</span><br><span class="line">b = np.zeros((<span class="number">1</span>, NUM_CLASSES))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training loop</span></span><br><span class="line">NUM_EPOCHS = <span class="number">50</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line"><span class="keyword">for</span> epoch_num <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass [NX2] · [2X2] = [NX2]</span></span><br><span class="line">    logits = np.dot(X_train, W) + b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Normalization via softmax to obtain class probabilities</span></span><br><span class="line">    exp_logits = np.exp(logits)</span><br><span class="line">    y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])</span><br><span class="line">    loss = np.sum(correct_class_logprobs) / len(y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># show progress</span></span><br><span class="line">    <span class="keyword">if</span> epoch_num%<span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Accuracy</span></span><br><span class="line">        y_pred = np.argmax(logits, axis=<span class="number">1</span>)</span><br><span class="line">        accuracy =  np.mean(np.equal(y_train, y_pred))</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch_num&#125;</span>, loss: <span class="subst">&#123;loss:<span class="number">.3</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.3</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backpropagation</span></span><br><span class="line">    y_hat[range(len(y_hat)), y_train] -= <span class="number">1</span></span><br><span class="line">    y_hat /= len(y_train)</span><br><span class="line">    dW = np.dot(X_train.T, y_hat)</span><br><span class="line">    db = np.sum(y_hat, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    W += -LEARNING_RATE * dW</span><br><span class="line">    b += -LEARNING_RATE * db</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0, loss: 0.694, accuracy: 0.093</span></span><br><span class="line"><span class="comment"># Epoch: 10, loss: 0.451, accuracy: 0.973</span></span><br><span class="line"><span class="comment"># Epoch: 20, loss: 0.353, accuracy: 0.973</span></span><br><span class="line"><span class="comment"># Epoch: 30, loss: 0.299, accuracy: 0.973</span></span><br><span class="line"><span class="comment"># Epoch: 40, loss: 0.264, accuracy: 0.976</span></span><br></pre></td></tr></table></figure><h2 id="Evaluation">Evaluation</h2><p>在测试集上评估我们训练好的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegressionFromScratch</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        logits = np.dot(x, W) + b</span><br><span class="line">        exp_logits = np.exp(logits)</span><br><span class="line">        y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> y_hat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluation</span></span><br><span class="line">model = LogisticRegressionFromScratch()</span><br><span class="line">logits_train = model.predict(X_train)</span><br><span class="line">pred_train = np.argmax(logits_train, axis=<span class="number">1</span>)</span><br><span class="line">logits_test = model.predict(X_test)</span><br><span class="line">pred_test = np.argmax(logits_test, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Training and test accuracy</span></span><br><span class="line">train_acc =  np.mean(np.equal(y_train, pred_train))</span><br><span class="line">test_acc = np.mean(np.equal(y_test, pred_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"train acc: <span class="subst">&#123;train_acc:<span class="number">.2</span>f&#125;</span>, test acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train acc: 0.98, test acc: 0.97</span></span><br></pre></td></tr></table></figure><p>可视化我们的结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiclass_decision_boundary</span><span class="params">(model, X, y, savefig_fp=None)</span>:</span></span><br><span class="line">    <span class="string">"""Plot the multiclass decision boundary for a model that accepts 2D inputs.</span></span><br><span class="line"><span class="string">    Credit: https://cs231n.github.io/neural-networks-case-study/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        model &#123;function&#125; -- trained model with function model.predict(x_in).</span></span><br><span class="line"><span class="string">        X &#123;numpy.ndarray&#125; -- 2D inputs with shape (N, 2).</span></span><br><span class="line"><span class="string">        y &#123;numpy.ndarray&#125; -- 1D outputs with shape (N,).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Axis boundaries</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].max() + <span class="number">0.1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].max() + <span class="number">0.1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.linspace(x_min, x_max, <span class="number">101</span>),</span><br><span class="line">                         np.linspace(y_min, y_max, <span class="number">101</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create predictions</span></span><br><span class="line">    x_in = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    y_pred = model.predict(x_in)</span><br><span class="line">    y_pred = np.argmax(y_pred, axis=<span class="number">1</span>).reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot decision boundary</span></span><br><span class="line">    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.RdYlBu)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot</span></span><br><span class="line">    <span class="keyword">if</span> savefig_fp:</span><br><span class="line">        plt.savefig(savefig_fp, format=<span class="string">"png"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/d6ba4a09a521c4639bf58e05962a6ebe.png" alt=""></p><h2 id="Ending">Ending</h2><p>可以看出，使用NumPy实现的代码相对复杂，下一篇我们即将看到PyTorch是如何便捷的实现逻辑回归。</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Logistic regression - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      使用NumPy实现逻辑回归。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · PyTorch的CrossEntropyLoss实现的不对？</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-CrossEntropyLoss/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-CrossEntropyLoss/</id>
    <published>2023-05-26T03:25:41.000Z</published>
    <updated>2023-05-27T13:56:31.413Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>作者在学习机器学习之逻辑回归任务时，遇到的交叉熵计算不符合预期，才发现了PyTorch的别有洞天。</p><p>于是，本文便是结合实验交代了PyTorch中交叉熵损失的真实计算过程。</p><h2 id="数据准备">数据准备</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.random.seed(seed=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">X = np.random.dirichlet(np.ones(<span class="number">3</span>), size=<span class="number">3</span>)</span><br><span class="line">Y = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (X)</span><br><span class="line"><span class="keyword">print</span> (Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[0.13807842 0.76510708 0.09681451]</span></span><br><span class="line"><span class="comment">#  [0.31698662 0.26993158 0.4130818 ]</span></span><br><span class="line"><span class="comment">#  [0.46864621 0.01320047 0.51815332]]</span></span><br><span class="line"><span class="comment"># [0 1 2]</span></span><br></pre></td></tr></table></figure><h2 id="正文">正文</h2><p>大部分博客给出的公式如下：</p><p>$$<br>H = - \sum_i{y_i log(\hat{y}_i)}<br>$$</p><p>其中 $\hat{y}_i$ 为预测值，$y_i$ 为真实值。</p><p>我们在低维空间复现此公式，注意到PyTorch可以采用<a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html" target="_blank" rel="noopener" title="CROSS_ENTROPY">class indices</a>直接取下标进行计算，这里采用同样的方式模拟。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    h = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Y)):</span><br><span class="line">        _h = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> X[i]:</span><br><span class="line">            _h += np.log(X[i][Y[i]])</span><br><span class="line">        h += - _h</span><br><span class="line">    <span class="keyword">return</span> np.around(h / len(Y), <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cross_entropy_(X, Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 3.947</span></span><br></pre></td></tr></table></figure><p>我们看一下PyTorch的计算结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">entroy = nn.CrossEntropyLoss()</span><br><span class="line">entroy(torch.from_numpy(X), torch.from_numpy(Y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor(1.1484, dtype=torch.float64)</span></span><br></pre></td></tr></table></figure><p>可以看到，结果并不相同。所以PyTorch应该是采用了另外的<a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" target="_blank" rel="noopener" title="CROSSENTROPYLOSS">实现方式</a>，而这也是大部分教程没有交代的。</p><p>$$<br>H(x, class) = - log{\frac{e^{x_{class}}}{\sum_i{e^{x_i}}}} = - x_{class} + log{\sum_i{e^{x_i}}}<br>$$</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    h = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Y)):</span><br><span class="line">        _h = sum([np.exp(j) <span class="keyword">for</span> j <span class="keyword">in</span> X[i]])</span><br><span class="line">        h += (- X[i][Y[i]] + np.log(_h))</span><br><span class="line">    <span class="keyword">return</span> np.around(h / len(Y), <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">cross_entropy(X, Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 1.1484</span></span><br></pre></td></tr></table></figure><p>如此，可以看到PyTorch的CrossEntropyLoss的真正计算过程。</p><h2 id="More">More</h2><p>事实上，我们还可以发现，nn.CrossEntropyLoss() 其实是 nn.logSoftmax() 和 nn.NLLLoss() 的整合版本。<br>$$<br>logSoftmax = log{\frac{e^x}{\sum_i{e^{x_i}}}}<br>$$</p><p>$$<br>NLLLoss(x, class) = -x[class]<br>$$</p><p>验证代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">X = np.random.dirichlet(np.ones(<span class="number">3</span>), size=<span class="number">3</span>)</span><br><span class="line">Y = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">X_ = torch.from_numpy(X)</span><br><span class="line">Y_ = torch.from_numpy(Y)</span><br><span class="line"></span><br><span class="line">entroy = nn.CrossEntropyLoss()</span><br><span class="line">print(entroy(X_, Y_))</span><br><span class="line"></span><br><span class="line">softmax = nn.LogSoftmax()</span><br><span class="line">loss = nn.NLLLoss()</span><br><span class="line">print(loss(softmax(X_), Y_))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor(1.0033, dtype=torch.float64)</span></span><br><span class="line"><span class="comment"># tensor(1.0033, dtype=torch.float64)</span></span><br></pre></td></tr></table></figure><h2 id="结论">结论</h2><ol><li><p>nn.CrossEntropyLoss() 的计算公式如下：<br>$$<br>H(x, class) = - log{\frac{e^{x_{class}}}{\sum_i{e^{x_i}}}} = - x_{class} + log{\sum_i{e^{x_i}}}<br>$$</p></li><li><p>nn.CrossEntropyLoss() 是 nn.logSoftmax() 和 nn.NLLLoss() 的整合。</p></li></ol>]]></content>
    
    <summary type="html">
    
      学习需要脚踏实地。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 机器学习之Linear Regression (二)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-LinearRegression-2/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-LinearRegression-2/</id>
    <published>2023-05-21T10:37:38.000Z</published>
    <updated>2023-05-29T06:08:40.370Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>接上篇，本文的目标是使用PyTorch实现一个线性回归模型。</p><h2 id="Generate-data">Generate data</h2><p>我们复用上篇生成的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line">NUM_SAMPLES = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate synthetic data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_data</span><span class="params">(num_samples)</span>:</span></span><br><span class="line">    <span class="string">"""Generate dummy data for linear regression."""</span></span><br><span class="line">    X = np.array(range(num_samples))</span><br><span class="line">    random_noise = np.random.uniform(<span class="number">-10</span>, <span class="number">20</span>, size=num_samples)</span><br><span class="line">    y = <span class="number">3.5</span>*X + random_noise <span class="comment"># add some noise</span></span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate random (linear) data</span></span><br><span class="line">X, y = generate_data(num_samples=NUM_SAMPLES)</span><br><span class="line">data = np.vstack([X, y]).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load into a Pandas DataFrame</span></span><br><span class="line">df = pd.DataFrame(data, columns=[<span class="string">"X"</span>, <span class="string">"y"</span>])</span><br><span class="line">X = df[[<span class="string">"X"</span>]].values</span><br><span class="line">y = df[[<span class="string">"y"</span>]].values</span><br><span class="line"></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/44b3f9578128ebd663285e3dd1ac4120.png" alt=""></p><p>我们将数据绘制成散点图，可以看到它们有很明显的线性趋势。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scatter plot</span></span><br><span class="line">plt.title(<span class="string">"Generated data"</span>)</span><br><span class="line">plt.scatter(x=df[<span class="string">"X"</span>], y=df[<span class="string">"y"</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/48c4cc75b277978a0b5e3eadf9e252e7.png" alt=""></p><h2 id="Split-data">Split data</h2><p>区别于上一篇中我们使用自定义摇骰子的方式分割数据，这里选择使用<a href="https://scikit-learn.org/stable/" target="_blank" rel="noopener" title="scikit-learn">scikit-learn</a>包里提供的<code>train_test_split</code>方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Split (train)</span></span><br><span class="line">X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"train: <span class="subst">&#123;len(X_train)&#125;</span> (<span class="subst">&#123;(len(X_train) / len(X)):<span class="number">.2</span>f&#125;</span>)\n"</span></span><br><span class="line">       <span class="string">f"remaining: <span class="subst">&#123;len(X_)&#125;</span> (<span class="subst">&#123;(len(X_) / len(X)):<span class="number">.2</span>f&#125;</span>)"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train: 35 (0.70)</span></span><br><span class="line"><span class="comment"># remaining: 15 (0.30)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Split (test)</span></span><br><span class="line">X_val, X_test, y_val, y_test = train_test_split(</span><br><span class="line">    X_, y_, train_size=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"train: <span class="subst">&#123;len(X_train)&#125;</span> (<span class="subst">&#123;len(X_train)/len(X):<span class="number">.2</span>f&#125;</span>)\n"</span></span><br><span class="line">  <span class="string">f"val: <span class="subst">&#123;len(X_val)&#125;</span> (<span class="subst">&#123;len(X_val)/len(X):<span class="number">.2</span>f&#125;</span>)\n"</span></span><br><span class="line">  <span class="string">f"test: <span class="subst">&#123;len(X_test)&#125;</span> (<span class="subst">&#123;len(X_test)/len(X):<span class="number">.2</span>f&#125;</span>)"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train: 35 (0.70)</span></span><br><span class="line"><span class="comment"># val: 7 (0.14)</span></span><br><span class="line"><span class="comment"># test: 8 (0.16)</span></span><br></pre></td></tr></table></figure><h2 id="Standardize-data">Standardize data</h2><p>同样的，我们需要对数据进行标准化处理。这里使用<code>scikit-learn</code>里提供的<code>StandardScaler</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the data (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler().fit(X_train)</span><br><span class="line">y_scaler = StandardScaler().fit(y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply scaler on training and test data</span></span><br><span class="line">X_train = X_scaler.transform(X_train)</span><br><span class="line">y_train = y_scaler.transform(y_train).ravel().reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">X_val = X_scaler.transform(X_val)</span><br><span class="line">y_val = y_scaler.transform(y_val).ravel().reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">X_test = X_scaler.transform(X_test)</span><br><span class="line">y_test = y_scaler.transform(y_test).ravel().reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;np.mean(X_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;np.mean(y_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(y_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># mean: 0.4, std: 1.0</span></span><br><span class="line"><span class="comment"># mean: 0.3, std: 0.9</span></span><br></pre></td></tr></table></figure><h2 id="Weights">Weights</h2><p>我们将使用PyTorch的<code>Linear layers</code>来实现一个没有隐含层的神经网络。<br>关于神经网络我们后面会具体学习。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">torch.manual_seed(SEED)</span><br><span class="line"></span><br><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>] <span class="comment"># X is 1-dimensional</span></span><br><span class="line">OUTPUT_DIM = y_train.shape[<span class="number">1</span>] <span class="comment"># y is 1-dimensional</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Inputs</span></span><br><span class="line">N = <span class="number">3</span> <span class="comment"># num samples</span></span><br><span class="line">x = torch.randn(N, INPUT_DIM)</span><br><span class="line"><span class="keyword">print</span> (x.shape)</span><br><span class="line"><span class="keyword">print</span> (x.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"><span class="comment"># [[-1.4836688 ]</span></span><br><span class="line"><span class="comment">#  [ 0.26714355]</span></span><br><span class="line"><span class="comment">#  [-1.8336787 ]]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Weights</span></span><br><span class="line">m = nn.Linear(INPUT_DIM, OUTPUT_DIM)</span><br><span class="line"><span class="keyword">print</span> (m)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"weights (<span class="subst">&#123;m.weight.shape&#125;</span>): <span class="subst">&#123;m.weight[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"bias (<span class="subst">&#123;m.bias.shape&#125;</span>): <span class="subst">&#123;m.bias[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Linear(in_features=1, out_features=1, bias=True)</span></span><br><span class="line"><span class="comment"># weights (torch.Size([1, 1])): -0.2795013189315796</span></span><br><span class="line"><span class="comment"># bias (torch.Size([1])): -0.7643394470214844</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Forward pass</span></span><br><span class="line">z = m(x)</span><br><span class="line"><span class="keyword">print</span> (z.shape)</span><br><span class="line"><span class="keyword">print</span> (z.detach().numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"><span class="comment"># [[-0.34965205]</span></span><br><span class="line"><span class="comment">#  [-0.8390064 ]</span></span><br><span class="line"><span class="comment">#  [-0.25182384]]</span></span><br></pre></td></tr></table></figure><h2 id="Model">Model</h2><p>$$<br>\hat{y} = WX + b<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, output_dim)</span>:</span></span><br><span class="line">        super(LinearRegression, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        y_pred = self.fc1(x_in)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = LinearRegression(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of LinearRegression(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=1, out_features=1, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Loss">Loss</h2><p>同样的，我们使用PyTorch自带的<a href="">Loss Functions</a>, 这里指定<code>MSELoss</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line">y_pred = torch.Tensor([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line">y_true =  torch.Tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>])</span><br><span class="line">loss = loss_fn(y_pred, y_true)</span><br><span class="line">print(<span class="string">"Loss: "</span>, loss.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Loss:  0.75</span></span><br></pre></td></tr></table></figure><h2 id="Optimizer">Optimizer</h2><p>上一篇中我们介绍了使用梯度下降的方法来更新我们的权重。PyTorch中有很多不同的权重更新方法，需要根据不同的场景来选择合适的。详见<a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener" title="TORCH.OPTIM">TORCH.OPTIM</a>。<br>这里我们采用适合大多数场景的<code>ADAM optimizer</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.Tensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.Tensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.Tensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line">NUM_EPOCHS = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    <span class="comment"># https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 1.11</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.10</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.04</span></span><br><span class="line"><span class="comment"># Epoch: 60 | loss: 0.03</span></span><br><span class="line"><span class="comment"># Epoch: 80 | loss: 0.03</span></span><br></pre></td></tr></table></figure><h2 id="Evaluation">Evaluation</h2><p>现在我们准备评估我们训练好的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">pred_train = model(X_train)</span><br><span class="line">pred_test = model(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">train_error = loss_fn(pred_train, y_train)</span><br><span class="line">test_error = loss_fn(pred_test, y_test)</span><br><span class="line">print(<span class="string">f"train_error: <span class="subst">&#123;train_error:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"test_error: <span class="subst">&#123;test_error:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train_error: 0.03</span></span><br><span class="line"><span class="comment"># test_error: 0.04</span></span><br></pre></td></tr></table></figure><p>由于我们只有一个特征，因此可以轻松地对模型进行可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Figure size</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot train data</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plt.scatter(X_train, y_train, label=<span class="string">"y_train"</span>)</span><br><span class="line">plt.plot(X_train, pred_train.detach().numpy(), color=<span class="string">"red"</span>, linewidth=<span class="number">1</span>, linestyle=<span class="string">"-"</span>, label=<span class="string">"model"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot test data</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plt.scatter(X_test, y_test, label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.plot(X_test, pred_test.detach().numpy(), color=<span class="string">"red"</span>, linewidth=<span class="number">1</span>, linestyle=<span class="string">"-"</span>, label=<span class="string">"model"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show plots</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/332563148a19dae3df2c54de08dafccd.png" alt=""></p><h2 id="Inference">Inference</h2><p>训练完模型后，我们可以使用它来对新数据进行预测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Feed in your own inputs</span></span><br><span class="line">sample_indices = [<span class="number">10</span>, <span class="number">15</span>, <span class="number">25</span>]</span><br><span class="line">X_infer = np.array(sample_indices, dtype=np.float32)</span><br><span class="line">X_infer = torch.Tensor(X_scaler.transform(X_infer.reshape(<span class="number">-1</span>, <span class="number">1</span>)))</span><br></pre></td></tr></table></figure><p>由于我们对数据都进行了标准化，所以对预测值需要进行逆操作。<br>$$<br>\hat{y}_{scaled} = \frac{\hat{y} - \mu_{\hat{y}}}{\sigma_{\hat{y}}}<br>$$</p><p>$$<br>\hat{y} = \hat{y}_{scaled} * \sigma_{\hat{y}} + \mu_{\hat{y}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Unstandardize predictions</span></span><br><span class="line">pred_infer = model(X_infer).detach().numpy() * np.sqrt(y_scaler.var_) + y_scaler.mean_</span><br><span class="line"><span class="keyword">for</span> i, index <span class="keyword">in</span> enumerate(sample_indices):</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">f"<span class="subst">&#123;df.iloc[index][<span class="string">'y'</span>]:<span class="number">.2</span>f&#125;</span> (actual) → <span class="subst">&#123;pred_infer[i][<span class="number">0</span>]:<span class="number">.2</span>f&#125;</span> (predicted)"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 28.10 (actual) → 40.99 (predicted)</span></span><br><span class="line"><span class="comment"># 56.45 (actual) → 58.62 (predicted)</span></span><br><span class="line"><span class="comment"># 100.83 (actual) → 93.88 (predicted)</span></span><br></pre></td></tr></table></figure><h2 id="Interpretability">Interpretability</h2><p>线性回归具有高度可解释性的巨大优势。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unstandardize coefficients</span></span><br><span class="line">W = model.fc1.weight.data.numpy()[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">b = model.fc1.bias.data.numpy()[<span class="number">0</span>]</span><br><span class="line">W_unscaled = W * (y_scaler.scale_/X_scaler.scale_)</span><br><span class="line">b_unscaled = b * y_scaler.scale_ + y_scaler.mean_ - np.sum(W_unscaled*X_scaler.mean_)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"[actual] y = 3.5X + noise"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"[model] y_hat = <span class="subst">&#123;W_unscaled[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>X + <span class="subst">&#123;b_unscaled[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [actual] y = 3.5X + noise</span></span><br><span class="line"><span class="comment"># [model] y_hat = 3.5X + 5.7</span></span><br></pre></td></tr></table></figure><h2 id="Regularization">Regularization</h2><p>正则化有助于减少过拟合。本例使用<code>L2正则化</code> (岭回归)。</p><p>通过L2正则化，我们对大的权重值进行惩罚，鼓励权重是较小值。 还有其他类型的正则化，比如L1（套索回归），它可以用于创建稀疏模型，其中一些特征系数被清零，或者结合了L1和L2惩罚的弹性正则化。</p><p>正则化不仅适用于线性回归，您可以使用它来处理任何模型的权重，包括我们将在未来学习到的模型。</p><p>$$<br>J(\theta) = \frac{1}{2} \sum_{i}(WX_i - y_i)^2 + \frac{\lambda}{2} \sum_i{W_i}^2<br>$$</p><p>$$<br>\frac{\partial(J)}{\partial(W)} = (\hat{y} - y)X + \lambda{W}<br>$$</p><p>$$<br>W = W - \alpha{\frac{\partial{J}}{\partial{W}}}<br>$$</p><p>$\lambda$: 正则化系数; $\alpha$: 学习率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">L2_LAMBDA = <span class="number">1e-2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = LinearRegression(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer (w/ L2 regularization)</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 0.67</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.06</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.03</span></span><br><span class="line"><span class="comment"># Epoch: 60 | loss: 0.03</span></span><br><span class="line"><span class="comment"># Epoch: 80 | loss: 0.03</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">pred_train = model(X_train)</span><br><span class="line">pred_test = model(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">train_error = loss_fn(pred_train, y_train)</span><br><span class="line">test_error = loss_fn(pred_test, y_test)</span><br><span class="line">print(<span class="string">f"train_error: <span class="subst">&#123;train_error:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"test_error: <span class="subst">&#123;test_error:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line">train_error: <span class="number">0.03</span></span><br><span class="line">test_error: <span class="number">0.03</span></span><br></pre></td></tr></table></figure><p>对于这个特定的例子，正则化并没有在性能上产生差异，因为我们的数据是从一个完美的线性方程生成的。但是对于大规模真实数据，正则化可以帮助我们的模型很好地泛化。</p><h2 id="Ending">Ending</h2><p>本篇基于上一篇的基础，简单介绍了如何用PyTorch实现线性回归。</p><p>Peace out.</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Linear regression - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      使用PyTorch实现线性回归。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 机器学习之Linear Regression (一)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-LinearRegression-1/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-LinearRegression-1/</id>
    <published>2023-05-18T10:31:52.000Z</published>
    <updated>2023-05-28T07:34:03.591Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文的目标是使用NumPy实现一个线性回归模型 $\hat{y}$ ，该模型通过最小化预测值和真实值之间的距离来拟合出一条最佳的直线(或一个平面)。<br>我们使用标记数据 $(X, y)$ 来训练模型，利用梯度下降的方法来学习权重 $W$ 和 偏差 $b$。</p><p>$$<br>\hat{y} = WX + b<br>$$</p><h2 id="Generate-data">Generate data</h2><p>我们会生成一些简单的数据，数据大致符合线性分布，但加上一点随机噪声以模拟现实情况( $y = 3.5 * X + 噪声$)，意味着这些数据并不完全在一条直线上。</p><p>我们的目标是使模型收敛到类似的线性方程上（由于随机噪声的加入，最终的模型结果可能会有差异）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line">NUM_SAMPLES = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate synthetic data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_data</span><span class="params">(num_samples)</span>:</span></span><br><span class="line">    <span class="string">"""Generate dummy data for linear regression."""</span></span><br><span class="line">    X = np.array(range(num_samples))</span><br><span class="line">    random_noise = np.random.uniform(<span class="number">-10</span>, <span class="number">20</span>, size=num_samples)</span><br><span class="line">    y = <span class="number">3.5</span>*X + random_noise <span class="comment"># add some noise</span></span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate random (linear) data</span></span><br><span class="line">X, y = generate_data(num_samples=NUM_SAMPLES)</span><br><span class="line">data = np.vstack([X, y]).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load into a Pandas DataFrame</span></span><br><span class="line">df = pd.DataFrame(data, columns=[<span class="string">"X"</span>, <span class="string">"y"</span>])</span><br><span class="line">X = df[[<span class="string">"X"</span>]].values</span><br><span class="line">y = df[[<span class="string">"y"</span>]].values</span><br><span class="line"></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/44b3f9578128ebd663285e3dd1ac4120.png" alt=""></p><p>我们将数据绘制成散点图，可以看到它们有很明显的线性趋势。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scatter plot</span></span><br><span class="line">plt.title(<span class="string">"Generated data"</span>)</span><br><span class="line">plt.scatter(x=df[<span class="string">"X"</span>], y=df[<span class="string">"y"</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/48c4cc75b277978a0b5e3eadf9e252e7.png" alt=""></p><h2 id="Split-data">Split data</h2><p>现在我们已经准备好了数据，接下来需要随机将数据集分成三个部分：训练集、验证集和测试集。</p><ul><li>训练集：用来训练我们的模型</li><li>验证集：在训练过程中用来检验我们模型的性能</li><li>测试集：用来检测我们最终训练好的模型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Shuffle data</span></span><br><span class="line"><span class="comment"># 不要错误的将 X 和 y 分开shuffle，务必保持自变量和因变量始终对齐</span></span><br><span class="line">indices = list(range(NUM_SAMPLES))</span><br><span class="line">np.random.shuffle(indices)</span><br><span class="line">X = X[indices]</span><br><span class="line">y = y[indices]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split indices</span></span><br><span class="line">train_start = <span class="number">0</span></span><br><span class="line">train_end = int(<span class="number">0.7</span>*NUM_SAMPLES)</span><br><span class="line">val_start = train_end</span><br><span class="line">val_end = int((TRAIN_SIZE+VAL_SIZE)*NUM_SAMPLES)</span><br><span class="line">test_start = val_end</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data</span></span><br><span class="line">X_train = X[train_start:train_end]</span><br><span class="line">y_train = y[train_start:train_end]</span><br><span class="line">X_val = X[val_start:val_end]</span><br><span class="line">y_val = y[val_start:val_end]</span><br><span class="line">X_test = X[test_start:]</span><br><span class="line">y_test = y[test_start:]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_test: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (35, 1), y_train: (35, 1)</span></span><br><span class="line"><span class="comment"># X_val: (7, 1), y_test: (7, 1)</span></span><br><span class="line"><span class="comment"># X_test: (8, 1), y_test: (8, 1)</span></span><br></pre></td></tr></table></figure><h2 id="Standardize-data">Standardize data</h2><p>我们需要对数据进行标准化处理（零均值和单位方差），这样做的目的是使不同特征之间的值具有可比性，并且能够减少不同特征之间的差异性。</p><p>$$<br>z = \frac{x_i - \mu}{\sigma}<br>$$</p><p>$z$: 标准化后的值;  $x_i$: 第i个输入;  $\mu$: 平均值;  $\sigma$: 标准差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standardize_data</span><span class="params">(data, mean, std)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (data - mean) / std</span><br></pre></td></tr></table></figure><p>需要注意的是，我们将验证集和测试集视为隐藏数据集。因此，我们只使用训练集来确定均值和标准差，以避免偏向我们的训练过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Determine means and stds</span></span><br><span class="line">X_mean = np.mean(X_train)</span><br><span class="line">X_std = np.std(X_train)</span><br><span class="line">y_mean = np.mean(y_train)</span><br><span class="line">y_std = np.std(y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize</span></span><br><span class="line">X_train = standardize_data(X_train, X_mean, X_std)</span><br><span class="line">y_train = standardize_data(y_train, y_mean, y_std)</span><br><span class="line">X_val = standardize_data(X_val, X_mean, X_std)</span><br><span class="line">y_val = standardize_data(y_val, y_mean, y_std)</span><br><span class="line">X_test = standardize_data(X_test, X_mean, X_std)</span><br><span class="line">y_test = standardize_data(y_test, y_mean, y_std)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;np.mean(X_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;np.mean(y_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(y_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># mean: 0.3, std: 1.0</span></span><br><span class="line"><span class="comment"># mean: 0.3, std: 1.0</span></span><br></pre></td></tr></table></figure><h2 id="Weights">Weights</h2><p>我们的目标是学习一个模型 $\hat{y}$ ，用权重向量 $W\in\mathbb{R}^{d}$ 和截距 $b\in\mathbb{R}$ 来表达自变量 $X\in\mathbb{R}^{d}$ 与因变量 $y\in\mathbb{R}$ 之间的线性关系:<br>$$<br>\hat{y} = XW + b\<br>$$</p><p><strong>第一步</strong>: 随机初始化模型的权重 $W$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>] <span class="comment"># X is 1-dimensional</span></span><br><span class="line">OUTPUT_DIM = y_train.shape[<span class="number">1</span>] <span class="comment"># y is 1-dimensional</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random weights</span></span><br><span class="line">W = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, OUTPUT_DIM)</span><br><span class="line">b = np.zeros((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"W: <span class="subst">&#123;W.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"b: <span class="subst">&#123;b.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># W: (1, 1)</span></span><br><span class="line"><span class="comment"># b: (1, 1)</span></span><br></pre></td></tr></table></figure><h2 id="Model">Model</h2><p><strong>第二步</strong>: 给模型喂 $X$ 以获得预测结果 $\hat{y}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Forward pass [NX1] · [1X1] = [NX1]</span></span><br><span class="line">y_pred = np.dot(X_train, W) + b</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_pred: <span class="subst">&#123;y_pred.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_pred: (35, 1)</span></span><br></pre></td></tr></table></figure><h2 id="Loss">Loss</h2><p><strong>第三步</strong>: 通过比较预测值和实际目标值来确定损失 $J$ 。线性回归常见的损失函数是<code>均方误差(MSE)</code>。<br>$$<br>J(\theta) =  \frac{1}{N} \sum_i(y_i - \hat{y}_i)^2<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss</span></span><br><span class="line">N = len(y_train)</span><br><span class="line">loss = (<span class="number">1</span>/N) * np.sum((y_train - y_pred)**<span class="number">2</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># loss: 0.99</span></span><br></pre></td></tr></table></figure><h2 id="Gradients">Gradients</h2><p><strong>第四步</strong>: 计算损失函数 $J(\theta)$ 的梯度，并更新模型的权重<br>$$<br>\rightarrow \frac{\partial(J)}{\partial(W)} = -\frac{2}{N}\sum_i(y_i - \hat{y}_i)^2 * X_i<br>$$</p><p>$$<br>\rightarrow \frac{\partial(J)}{\partial(b)} = -\frac{2}{N}\sum_i(y_i - \hat{y}_i)^2 * 1<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Backpropagation</span></span><br><span class="line">dW = -(<span class="number">2</span>/N) * np.sum((y_train - y_pred) * X_train)</span><br><span class="line">db = -(<span class="number">2</span>/N) * np.sum((y_train - y_pred) * <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>梯度是函数的导数或变化率。它是一个向量，指向函数增长最快的方向。<br>在我们这个例子里面，损失函数 $J$ 在 $W$ 的梯度告诉我们如何改变 $W$ 以最大化 $J$ 。然而我们希望最小化损失，因此我们需要从 $W$ 中减去梯度。</p><h2 id="Update-weights">Update weights</h2><p><strong>第五步</strong>: 利用一个很小的学习率 $\alpha$ 更新权重<br>$$<br>W = W - \alpha \frac{\partial(J)}{\partial(W)}<br>$$</p><p>$$<br>b = b - \alpha \frac{\partial(J)}{\partial(b)}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Update weights</span></span><br><span class="line">W += -LEARNING_RATE * dW</span><br><span class="line">b += -LEARNING_RATE * db</span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><p><strong>第六步</strong>: 重复步骤 2 ~ 5，以最小化损失为目的来训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">NUM_EPOCHS = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random weights</span></span><br><span class="line">W = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, OUTPUT_DIM)</span><br><span class="line">b = np.zeros((<span class="number">1</span>, ))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training loop</span></span><br><span class="line"><span class="keyword">for</span> epoch_num <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass [NX1] · [1X1] = [NX1]</span></span><br><span class="line">    y_pred = np.dot(X_train, W) + b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = (<span class="number">1</span>/len(y_train)) * np.sum((y_train - y_pred)**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Show progress</span></span><br><span class="line">    <span class="keyword">if</span> epoch_num%<span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch_num&#125;</span>, loss: <span class="subst">&#123;loss:<span class="number">.3</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backpropagation</span></span><br><span class="line">    dW = -(<span class="number">2</span>/N) * np.sum((y_train - y_pred) * X_train)</span><br><span class="line">    db = -(<span class="number">2</span>/N) * np.sum((y_train - y_pred) * <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    W += -LEARNING_RATE * dW</span><br><span class="line">    b += -LEARNING_RATE * db</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0, loss: 0.992</span></span><br><span class="line"><span class="comment"># Epoch: 10, loss: 0.043</span></span><br><span class="line"><span class="comment"># Epoch: 20, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 30, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 40, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 50, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 60, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 70, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 80, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 90, loss: 0.032</span></span><br></pre></td></tr></table></figure><p>为了保持简洁，我们在这里没有计算和显示每个Epoch后的验证集损失。在后续的学习会体现出，在验证集上的表现对训练进程有着至关重要的影响（学习率是否合理、什么时候停止训练等等）</p><h2 id="Evaluation">Evaluation</h2><p>接下来看一下我们训练好的模型在测试数据集上的表现如何。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">pred_train = W*X_train + b</span><br><span class="line">pred_test = W*X_test + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train and test MSE</span></span><br><span class="line">train_mse = np.mean((y_train - pred_train) ** <span class="number">2</span>)</span><br><span class="line">test_mse = np.mean((y_test - pred_test) ** <span class="number">2</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"train_MSE: <span class="subst">&#123;train_mse:<span class="number">.2</span>f&#125;</span>, test_MSE: <span class="subst">&#123;test_mse:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train_MSE: 0.03, test_MSE: 0.04</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Figure size</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot train data</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plt.scatter(X_train, y_train, label=<span class="string">"y_train"</span>)</span><br><span class="line">plt.plot(X_train, pred_train, color=<span class="string">"red"</span>, linewidth=<span class="number">1</span>, linestyle=<span class="string">"-"</span>, label=<span class="string">"model"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot test data</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plt.scatter(X_test, y_test, label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.plot(X_test, pred_test, color=<span class="string">"red"</span>, linewidth=<span class="number">1</span>, linestyle=<span class="string">"-"</span>, label=<span class="string">"model"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show plots</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/be75da05ba1c2d284e29c1e95319c6f9.png" alt=""></p><h2 id="Interpretability">Interpretability</h2><p>由于我们对输入和输出进行了标准化，因此我们的权重适配了这些标准化值。因此，我们需要将权重还原为非标准化状态，以便与真实权重（3.5）进行比较。<br>注意到 $X$ 和 $\hat{y}$ 都已经标准化过。</p><p>$$<br>\hat{y}_{scaled} = \sum_{j=1}^k W_{scaled(j)} x_{scaled(j)} + b_{scaled}<br>$$</p><p>已知<br>$$<br>\hat{y}_{scaled} = \frac{\hat{y}_{unscaled} - \overline{y}}{\sigma}<br>$$</p><p>$$<br>\hat{x}_{scaled} = \frac{x_{j} - \overline{x}_{j}}{\sigma_{j}}<br>$$</p><p>于是<br>$$<br>\frac{\hat{y}_{unscaled} - \overline{y}}{\sigma} = \sum_{j=1}^k W_{scaled(j)} (\frac{x_{j} - \overline{x}_{j}}{\sigma_{j}}) + b_{scaled}<br>$$</p><p>进一步可以得到</p><p>$$<br>\hat{y}_{unscaled} = \sum_{j=1}^k W_{scaled(j)} (\frac{\sigma_{y}}{\sigma_{j}}) x_{j} - \sum_{j=1}^k W_{scaled(j)} (\frac{\sigma_{y}}{\sigma_{j}}) \overline{x}_{j} + b_{scaled} \sigma_y + \overline{y}<br>$$</p><p>对比公式</p><p>$$<br>\hat{y}_{unscaled} = W_{unscaled} x + b_{unscaled}<br>$$</p><p>便可得知<br>$$<br>W_{unscaled} = W_{scaled} (\frac{\sigma_{y}}{\sigma_{j}})<br>$$</p><p>$$<br>b_{unscaled} = - \sum_{j=1}^k W_{unscaled(j)} \overline{x}_{j} + b_{scaled} \sigma_y + \overline{y}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unscaled weights</span></span><br><span class="line">W_unscaled = W * (y_std / X_std)</span><br><span class="line">b_unscaled = - np.sum(W_unscaled * X_mean) + b * y_std + y_mean</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"[actual] y = 3.5X + noise"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"[model] y_hat = <span class="subst">&#123;W_unscaled[<span class="number">0</span>][<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>X + <span class="subst">&#123;b_unscaled[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [actual] y = 3.5X + noise</span></span><br><span class="line"><span class="comment"># [model] y_hat = 3.4X + 7.9</span></span><br></pre></td></tr></table></figure><p>从上面的结果可以看出，我们成功的拟合出了这个线性方程。</p><h2 id="Ending">Ending</h2><p>该模型的优势：计算简单，高度可解释, 可以处理连续的和可分类的特征.  而缺点也很明显：只有当数据是线性可分的时候，该模型才能表现良好。</p><p>除了回归任务，你也可以将线性回归用于二元分类任务，其中如果预测的连续值高于阈值，则属于某个类别。 不过未来，我们会介绍更好的分类技术。</p><p>下一篇，我们将介绍如何用PyTorch实现本篇的内容。</p><p>Peace out.</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Linear regression - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      使用NumPy实现线性回归。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · PyTorch</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-PyTorch/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-PyTorch/</id>
    <published>2023-05-17T10:11:12.000Z</published>
    <updated>2023-05-27T13:56:17.775Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文简单介绍了PyTorch这个机器学习框架的必备知识。</p><h2 id="Set-up">Set up</h2><p>首先，我们将导入NumPy和PyTorch库，并设置随机种子以实现可重复性。<br>请注意，PyTorch 也需要一个种子，因为我们将生成随机张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">np.random.seed(seed=SEED)</span><br><span class="line">torch.manual_seed(SEED)</span><br></pre></td></tr></table></figure><h2 id="Basic">Basic</h2><p>下面一些 PyTorch 的基础知识，例如如何创建张量以及将常见的数据结构（列表、数组等）转换为张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating a random tensor</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># normal distribution (rand(2,3) -&gt; uniform distribution)</span></span><br><span class="line">print(<span class="string">f"Type: <span class="subst">&#123;x.type()&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Type: torch.FloatTensor</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-1.4837,  0.2671, -1.8337],</span></span><br><span class="line"><span class="comment">#         [-0.1047,  0.6002, -0.5496]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Zero and Ones tensor</span></span><br><span class="line">x = torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.]])</span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># List → Tensor</span></span><br><span class="line">x = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[1., 2., 3.],</span></span><br><span class="line"><span class="comment">#         [4., 5., 6.]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># NumPy array → Tensor</span></span><br><span class="line">x = torch.Tensor(np.random.rand(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[0.4445, 0.3168, 0.9231],</span></span><br><span class="line"><span class="comment">#         [0.4659, 0.7984, 0.1992]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Changing tensor type</span></span><br><span class="line">x = torch.Tensor(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(<span class="string">f"Type: <span class="subst">&#123;x.type()&#125;</span>"</span>)</span><br><span class="line">x = x.long()</span><br><span class="line">print(<span class="string">f"Type: <span class="subst">&#123;x.type()&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Type: torch.FloatTensor</span></span><br><span class="line"><span class="comment"># Type: torch.LongTensor</span></span><br></pre></td></tr></table></figure><h2 id="Operations">Operations</h2><p>下面探索一些张量的基本操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Addition</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">z = x + y</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;z.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;z&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-0.4446,  0.4933, -1.4847],</span></span><br><span class="line"><span class="comment">#         [ 0.8493,  0.6911, -0.3357]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dot product</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">z = torch.mm(x, y)</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;z.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;z&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 2])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[ 0.2733, -4.0392],</span></span><br><span class="line"><span class="comment">#         [ 1.6385, -4.7220]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Transpose</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line">y = torch.t(x)</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;y.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;y&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[ 0.5920, -0.6301, -0.8856],</span></span><br><span class="line"><span class="comment">#         [ 1.2261, -0.4671, -1.0279]])</span></span><br><span class="line"><span class="comment"># Size: torch.Size([3, 2])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[ 0.5920,  1.2261],</span></span><br><span class="line"><span class="comment">#         [-0.6301, -0.4671],</span></span><br><span class="line"><span class="comment">#         [-0.8856, -1.0279]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reshape</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">z = x.view(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;z.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;z&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([3, 2])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-1.0387,  0.1039],</span></span><br><span class="line"><span class="comment">#         [ 0.5989, -1.4801],</span></span><br><span class="line"><span class="comment">#         [-0.8618, -0.9181]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dangers of reshaping (unintended consequences)</span></span><br><span class="line">x = torch.tensor([</span><br><span class="line">    [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]],</span><br><span class="line">    [[<span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>], [<span class="number">20</span>, <span class="number">20</span>, <span class="number">20</span>, <span class="number">20</span>], [<span class="number">30</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">30</span>]]</span><br><span class="line">])</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"x: \n<span class="subst">&#123;x&#125;</span>\n"</span>)</span><br><span class="line"></span><br><span class="line">a = x.view(x.size(<span class="number">1</span>), <span class="number">-1</span>)</span><br><span class="line">print(<span class="string">f"\nSize: <span class="subst">&#123;a.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"a: \n<span class="subst">&#123;a&#125;</span>\n"</span>)</span><br><span class="line"></span><br><span class="line">b = x.transpose(<span class="number">0</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">print(<span class="string">f"\nSize: <span class="subst">&#123;b.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"b: \n<span class="subst">&#123;b&#125;</span>\n"</span>)</span><br><span class="line"></span><br><span class="line">c = b.view(b.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">print(<span class="string">f"\nSize: <span class="subst">&#123;c.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"c: \n<span class="subst">&#123;c&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3, 4])</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment"># tensor([[[ 1,  1,  1,  1],</span></span><br><span class="line"><span class="comment">#          [ 2,  2,  2,  2],</span></span><br><span class="line"><span class="comment">#          [ 3,  3,  3,  3]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[10, 10, 10, 10],</span></span><br><span class="line"><span class="comment">#          [20, 20, 20, 20],</span></span><br><span class="line"><span class="comment">#          [30, 30, 30, 30]]])</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Size: torch.Size([3, 8])</span></span><br><span class="line"><span class="comment"># a:</span></span><br><span class="line"><span class="comment"># tensor([[ 1,  1,  1,  1,  2,  2,  2,  2],</span></span><br><span class="line"><span class="comment">#         [ 3,  3,  3,  3, 10, 10, 10, 10],</span></span><br><span class="line"><span class="comment">#         [20, 20, 20, 20, 30, 30, 30, 30]])</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Size: torch.Size([3, 2, 4])</span></span><br><span class="line"><span class="comment"># b:</span></span><br><span class="line"><span class="comment"># tensor([[[ 1,  1,  1,  1],</span></span><br><span class="line"><span class="comment">#          [10, 10, 10, 10]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[ 2,  2,  2,  2],</span></span><br><span class="line"><span class="comment">#          [20, 20, 20, 20]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[ 3,  3,  3,  3],</span></span><br><span class="line"><span class="comment">#          [30, 30, 30, 30]]])</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Size: torch.Size([3, 8])</span></span><br><span class="line"><span class="comment"># c:</span></span><br><span class="line"><span class="comment"># tensor([[ 1,  1,  1,  1, 10, 10, 10, 10],</span></span><br><span class="line"><span class="comment">#         [ 2,  2,  2,  2, 20, 20, 20, 20],</span></span><br><span class="line"><span class="comment">#         [ 3,  3,  3,  3, 30, 30, 30, 30]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dimensional operations</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line">y = torch.sum(x, dim=<span class="number">0</span>) <span class="comment"># add each row's value for every column</span></span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;y&#125;</span>"</span>)</span><br><span class="line">z = torch.sum(x, dim=<span class="number">1</span>) <span class="comment"># add each columns's value for every row</span></span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;z&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-0.0355,  0.4145,  0.6798],</span></span><br><span class="line"><span class="comment">#         [-0.2936,  0.1872, -0.2724]])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([-0.3292,  0.6017,  0.4074])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([ 1.0588, -0.3788])</span></span><br></pre></td></tr></table></figure><h2 id="Indexing">Indexing</h2><p>可以使用索引从张量中提取指定的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"x: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"x[:1]: \n<span class="subst">&#123;x[:<span class="number">1</span>]&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"x[:1, 1:3]: \n<span class="subst">&#123;x[:<span class="number">1</span>, <span class="number">1</span>:<span class="number">3</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line">x:</span><br><span class="line"><span class="comment"># tensor([[-0.5524, -0.8358, -2.8240,  0.2564],</span></span><br><span class="line"><span class="comment">#         [ 0.5045, -1.1290,  0.7631,  1.0155],</span></span><br><span class="line"><span class="comment">#         [-1.2475, -0.0335,  0.5442,  0.4280]])</span></span><br><span class="line"><span class="comment"># x[:1]:</span></span><br><span class="line"><span class="comment"># tensor([[-0.5524, -0.8358, -2.8240,  0.2564]])</span></span><br><span class="line"><span class="comment"># x[:1, 1:3]:</span></span><br><span class="line"><span class="comment"># tensor([[-0.8358, -2.8240]])</span></span><br></pre></td></tr></table></figure><h2 id="Slicing">Slicing</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Select with dimensional indices</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">col_indices = torch.LongTensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">chosen = torch.index_select(x, dim=<span class="number">1</span>, index=col_indices) <span class="comment"># values from column 0 &amp; 2</span></span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;chosen&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">row_indices = torch.LongTensor([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">col_indices = torch.LongTensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">chosen = x[row_indices, col_indices] <span class="comment"># values from (0, 0) &amp; (1, 2)</span></span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;chosen&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-1.6357,  0.7964,  0.9450],</span></span><br><span class="line"><span class="comment">#         [-1.6535,  1.8129,  0.9162]])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-1.6357,  0.9450],</span></span><br><span class="line"><span class="comment">#         [-1.6535,  0.9162]])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([-1.6357,  0.9162])</span></span><br></pre></td></tr></table></figure><h2 id="Joining">Joining</h2><p>我们还可以使用concatenate和stack来合并张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor([[-0.7004,  0.8429,  0.8971],</span></span><br><span class="line"><span class="comment">#         [-0.0272,  0.4722,  1.1621]])</span></span><br><span class="line"><span class="comment"># torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Concatenation</span></span><br><span class="line">y = torch.cat([x, x], dim=<span class="number">0</span>) <span class="comment"># concat on a specified dimension</span></span><br><span class="line"><span class="keyword">print</span> (y)</span><br><span class="line"><span class="keyword">print</span> (y.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor([[-0.7004,  0.8429,  0.8971],</span></span><br><span class="line"><span class="comment">#         [-0.0272,  0.4722,  1.1621],</span></span><br><span class="line"><span class="comment">#         [-0.7004,  0.8429,  0.8971],</span></span><br><span class="line"><span class="comment">#         [-0.0272,  0.4722,  1.1621]])</span></span><br><span class="line"><span class="comment"># torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Stacking</span></span><br><span class="line">z = torch.stack([x, x], dim=<span class="number">0</span>) <span class="comment"># stack on new dimension</span></span><br><span class="line"><span class="keyword">print</span> (z)</span><br><span class="line"><span class="keyword">print</span> (z.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor([[[-0.7004,  0.8429,  0.8971],</span></span><br><span class="line"><span class="comment">#          [-0.0272,  0.4722,  1.1621]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[-0.7004,  0.8429,  0.8971],</span></span><br><span class="line"><span class="comment">#          [-0.0272,  0.4722,  1.1621]]])</span></span><br><span class="line"><span class="comment"># torch.Size([2, 2, 3])</span></span><br></pre></td></tr></table></figure><h2 id="Gradients-梯度">Gradients 梯度</h2><p>我们可以使用梯度追踪(gradient bookkeeping) 来计算张量相对于其组成部分的梯度（变化率）。</p><p>梯度是机器学习和深度学习中最重要的概念，没有之一。后续会进一步介绍，这里先简单示例PyTorch如何计算某个函数在某点处的梯度:</p><p>$$<br>y = 3x + 2<br>$$</p><p>$$<br>z = \sum(y/N)<br>$$</p><p>$$<br>\frac{\partial(z)}{\partial(x)} = \frac{\partial(z)}{\partial(y)} \cdot \frac{\partial(y)}{\partial(x)} = \frac{1}{N} \cdot 3 = \frac{1}{3 \cdot 4} \cdot 3 = 0.25<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tensors with gradient bookkeeping</span></span><br><span class="line">x = torch.rand(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="literal">True</span>)  <span class="comment"># requires_grad=True 表示此处需要计算梯度</span></span><br><span class="line">y = <span class="number">3</span>*x + <span class="number">2</span></span><br><span class="line">z = y.mean()</span><br><span class="line">z.backward()  <span class="comment"># 此时开始计算梯度</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"x: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"x.grad: \n<span class="subst">&#123;x.grad&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment"># tensor([[0.1154, 0.1101, 0.4831, 0.1580],</span></span><br><span class="line"><span class="comment">#         [0.4459, 0.2242, 0.9525, 0.8113],</span></span><br><span class="line"><span class="comment">#         [0.0387, 0.1512, 0.9678, 0.7512]], requires_grad=True)</span></span><br><span class="line"><span class="comment"># x.grad:</span></span><br><span class="line"><span class="comment"># tensor([[0.2500, 0.2500, 0.2500, 0.2500],</span></span><br><span class="line"><span class="comment">#         [0.2500, 0.2500, 0.2500, 0.2500],</span></span><br><span class="line"><span class="comment">#         [0.2500, 0.2500, 0.2500, 0.2500]])</span></span><br></pre></td></tr></table></figure><h2 id="CUDA">CUDA</h2><p>我们可以使用CUDA（Nvidia的并行计算平台和API）将张量加载到GPU上进行并行计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set device</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">x = torch.rand(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> (x.is_cuda)</span><br><span class="line">x = torch.rand(<span class="number">2</span>,<span class="number">3</span>).to(device)</span><br><span class="line"><span class="keyword">print</span> (x.is_cuda)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># False</span></span><br><span class="line"><span class="comment"># True</span></span><br></pre></td></tr></table></figure><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; PyTorch - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>到这里，便拥有了Way2AI路上需要的PyTorch的必备知识。我们可以发现，PyTorch的基础操作，与NumPy其实没什么太大的差别。</p><p>事实上NumPy和PyTorch可以相互转换，PyTorch提供了与NumPy兼容的接口，可以方便地将数据从NumPy数组转换为PyTorch张量，并在它们之间进行转换。这使得在使用PyTorch进行深度学习时，可以利用NumPy的强大功能进行数据预处理和后处理。</p><p>一般地，当我们需要进行常规的数值计算、数组操作和数学函数应用时，可以使用NumPy。当我们需要构建、训练和部署神经网络模型时，可以使用PyTorch。</p><p><a href="https://pytorch.org/" target="_blank" rel="noopener" title="PyTorch">PyTorch官网</a> 上有关于PyTorch的全部知识。</p>]]></content>
    
    <summary type="html">
    
      学习如何使用PyTorch这个机器学习框架。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · Pandas</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-Pandas/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-Pandas/</id>
    <published>2023-05-17T08:15:17.000Z</published>
    <updated>2023-05-27T13:56:59.688Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文简单介绍了Pandas这个数据分析处理库的必备知识。</p><h2 id="Set-up">Set up</h2><p>首先，我们将导入NumPy和Pandas库，并设置随机种子以实现可重复性。<br>我们还要下载一个数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">np.random.seed(seed=<span class="number">1024</span>)</span><br></pre></td></tr></table></figure><h2 id="Load-Data">Load Data</h2><p>我们将在 <a href="https://www.kaggle.com/c/titanic/data" target="_blank" rel="noopener" title="Titanic dataset">Titanic</a> 这个数据集上完成学习，这是一个非常常见且丰富的数据集，包含了1912年登上泰坦尼克号的人员相关信息以及他们在远航中幸存与否，非常适合使用Pandas进行探索性数据分析。</p><p>让我们将CSV文件中的数据加载到Pandas dataframe中。header=0表示第一行（索引为0）是一个标题行，其中包含了我们数据集中每个列的名称。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Read from CSV to Pandas DataFrame</span></span><br><span class="line">url = <span class="string">"http://s3.mindex.xyz/datasets/titanic.csv"</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># First few items</span></span><br><span class="line">df.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>输出如下图所示：<br><img src="//s3.mindex.xyz/blog/Courses/11a15080ea7aff309c574598cf6d1f03.png" alt=""></p><p>解释一下数据的特征列：</p><ul><li>PassengerId: ID</li><li>Survived: 存活指标（0 - died, 1 - survived）</li><li>Pclass: 票的等级</li><li>Name: 旅客的全名</li><li>Sex: 性别</li><li>Age: 年龄</li><li>SibSp: 兄弟姐妹 / 配偶</li><li>Parch: 父母 / 子女</li><li>Ticket: 票号</li><li>Fare: 票价</li><li>Cabin: 房间号</li><li>Embarked: 出发的港口</li></ul><h2 id="探索性数据分析-EDA">探索性数据分析 (EDA)</h2><p>现在我们已经加载了数据，准备开始探索以找到有用的信息。<br>我们可以使用 .describe() 方法提取数值特征的一些标准细节。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Describe features</span></span><br><span class="line">df.describe()</span><br></pre></td></tr></table></figure><p>输出如下图所示：<br><img src="//s3.mindex.xyz/blog/Courses/094c0b85aee702ed965db3005be4b6e9.png" alt=""></p><p>导入matplotlib以提供更直观的数据可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Correlation matrix</span></span><br><span class="line">plt.matshow(df.corr())</span><br><span class="line">continuous_features = df.describe().columns</span><br><span class="line">plt.xticks(range(len(continuous_features)), continuous_features, rotation=<span class="string">"45"</span>)</span><br><span class="line">plt.yticks(range(len(continuous_features)), continuous_features, rotation=<span class="string">"45"</span>)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/6429df82847c28e9a8325d674bef1b41.png" alt=""></p><p>我们还可以使用.hist()函数来查看每个特征的值的直方图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Histograms</span></span><br><span class="line">df[<span class="string">"Age"</span>].hist()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/5482837c04020c25d4148ac1c83b5530.png" alt=""></p><p>使用.unique()函数查看特征值的所有类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unique values</span></span><br><span class="line">df[<span class="string">"Embarked"</span>].unique()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># array(['S', 'C', 'Q', nan], dtype=object)</span></span><br></pre></td></tr></table></figure><h2 id="Filtering">Filtering</h2><p>我们可以按照特征甚至是特定特征中的具体值（或值范围）来过滤数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Selecting data by feature</span></span><br><span class="line">df[<span class="string">"Name"</span>].head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 0                              Braund, Mr. Owen Harris</span></span><br><span class="line"><span class="comment"># 1    Cumings, Mrs. John Bradley (Florence Briggs Th...</span></span><br><span class="line"><span class="comment"># 2                               Heikkinen, Miss. Laina</span></span><br><span class="line"><span class="comment"># 3         Futrelle, Mrs. Jacques Heath (Lily May Peel)</span></span><br><span class="line"><span class="comment"># 4                             Allen, Mr. William Henry</span></span><br><span class="line"><span class="comment"># Name: Name, dtype: object</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Filtering</span></span><br><span class="line">df[df[<span class="string">"Sex"</span>]==<span class="string">"female"</span>].head() <span class="comment"># only the female data appear</span></span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/8c221ac048730e95fec59bd917ea6248.png" alt="filtering by sex"></p><h2 id="Sorting">Sorting</h2><p>我们还可以按升序或降序对功能进行排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sorting</span></span><br><span class="line">df.sort_values(<span class="string">"Age"</span>, ascending=<span class="literal">False</span>).head()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/4d34974359deaf3f3b1a27264a4b7ee6.png" alt=""></p><h2 id="Grouping">Grouping</h2><p>我们还可以针对特定分组获取特征的统计数据。在这里，我们想根据乘客是否幸存来查看连续特征的平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Grouping</span></span><br><span class="line">survived_group = df.groupby(<span class="string">"Survived"</span>)</span><br><span class="line">survived_group.mean()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/68441798b47d6714e9ff255212a68128.png" alt=""></p><h2 id="Indexing">Indexing</h2><p>我们可以使用iloc在数据框中获取特定位置的行或列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Selecting row 0</span></span><br><span class="line">df.iloc[<span class="number">0</span>, :]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># PassengerId                          1</span></span><br><span class="line"><span class="comment"># Survived                             0</span></span><br><span class="line"><span class="comment"># Pclass                               3</span></span><br><span class="line"><span class="comment"># Name           Braund, Mr. Owen Harris</span></span><br><span class="line"><span class="comment"># Sex                               male</span></span><br><span class="line"><span class="comment"># Age                               22.0</span></span><br><span class="line"><span class="comment"># SibSp                                1</span></span><br><span class="line"><span class="comment"># Parch                                0</span></span><br><span class="line"><span class="comment"># Ticket                       A/5 21171</span></span><br><span class="line"><span class="comment"># Fare                              7.25</span></span><br><span class="line"><span class="comment"># Cabin                              NaN</span></span><br><span class="line"><span class="comment"># Embarked                             S</span></span><br><span class="line"><span class="comment"># Name: 0, dtype: object</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Selecting a specific value</span></span><br><span class="line">df.iloc[<span class="number">0</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 'Braund, Mr. Owen Harris'</span></span><br></pre></td></tr></table></figure><h2 id="Preprocessing">Preprocessing</h2><p>在探索完数据后，我们可以对数据集进行清洗和预处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Rows with at least one NaN value</span></span><br><span class="line">df[pd.isnull(df).any(axis=<span class="number">1</span>)]</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/9e922e0e75b89fa3d3104e990a5bd4b6.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Drop rows with Nan values</span></span><br><span class="line">df = df.dropna() <span class="comment"># removes rows with any NaN values</span></span><br><span class="line">df = df.reset_index() <span class="comment"># reset's row indexes in case any rows were dropped</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dropping multiple columns</span></span><br><span class="line">df = df.drop([<span class="string">"Name"</span>, <span class="string">"Cabin"</span>, <span class="string">"Ticket"</span>], axis=<span class="number">1</span>) <span class="comment"># we won't use text features for our initial basic models</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Map feature values</span></span><br><span class="line">df[<span class="string">"Sex"</span>] = df[<span class="string">"Sex"</span>].map( &#123;<span class="string">"female"</span>: <span class="number">0</span>, <span class="string">"male"</span>: <span class="number">1</span>&#125; ).astype(int)</span><br><span class="line">df[<span class="string">"Embarked"</span>] = df[<span class="string">"Embarked"</span>].dropna().map( &#123;<span class="string">"S"</span>:<span class="number">0</span>, <span class="string">"C"</span>:<span class="number">1</span>, <span class="string">"Q"</span>:<span class="number">2</span>&#125; ).astype(int)</span><br><span class="line"></span><br><span class="line">df</span><br></pre></td></tr></table></figure><p>结果如下：<br><img src="//s3.mindex.xyz/blog/Courses/ab30159a0aa58a9ca9fa1a2a3826a41a.png" alt=""></p><h2 id="Feature-Engineering-特征工程">Feature Engineering 特征工程</h2><p>我们现在要使用特征工程来创建一个名为FamilySize的列。我们将首先定义一个名为get_family_size的函数，该函数将使用父母和兄弟姐妹数量来确定家庭大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Lambda expressions to create new features</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_family_size</span><span class="params">(sibsp, parch)</span>:</span></span><br><span class="line">    family_size = sibsp + parch</span><br><span class="line">    <span class="keyword">return</span> family_size</span><br></pre></td></tr></table></figure><p>我们就可以使用lambda将该函数应用于每一行（使用每行中兄弟姐妹和父母的数量来确定每行的家庭规模）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">"FamilySize"</span>] = df[[<span class="string">"SibSp"</span>, <span class="string">"Parch"</span>]].apply(<span class="keyword">lambda</span> x: get_family_size(x[<span class="string">"SibSp"</span>], x[<span class="string">"Parch"</span>]), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reorganize headers</span></span><br><span class="line">df = df[[<span class="string">"Pclass"</span>, <span class="string">"Sex"</span>, <span class="string">"Age"</span>, <span class="string">"SibSp"</span>, <span class="string">"Parch"</span>, <span class="string">"FamilySize"</span>, <span class="string">"Fare"</span>, <span class="string">"Embarked"</span>, <span class="string">"Survived"</span>]]</span><br><span class="line"></span><br><span class="line">df</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/917c83ce4d6135c15dd0214fc82aab46.png" alt=""></p><p>特征工程可以与领域专家合作进行，他们可以指导我们在工程和使用哪些特征。</p><h2 id="Save-Data">Save Data</h2><p>最后，让我们将预处理后的数据保存到一个新的CSV文件中以备后用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Saving dataframe to CSV</span></span><br><span class="line">df.to_csv(<span class="string">"processed_titanic.csv"</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="Scaling">Scaling</h2><p>当处理非常大的数据集时，我们的Pandas DataFrames可能会变得非常庞大，对它们进行操作可能会变得非常缓慢或不可行。这就是分布式工作负载或在更高效硬件上运行的软件包派上用场的地方。</p><ul><li><a href="https://dask.org/" target="_blank" rel="noopener" title="Dask">Dask</a>: 使用并行计算来扩展Numpy、Pandas和scikit-learn等软件包在单个/多台机器上的应用。</li><li><a href="https://github.com/rapidsai/cudf" target="_blank" rel="noopener" title="cuDF">cuDF</a>: 在GPU上高效加载和计算dataframe。</li></ul><p>当然，我们可以将它们（<a href="https://github.com/rapidsai/cudf/tree/main/python/dask_cudf" target="_blank" rel="noopener" title="Dask-cuDF">Dask-cuDF</a>）结合在一起，在GPU上对dataframe分块进行操作。</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Pandas - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>到这里，便拥有了Way2AI路上需要的Pandas的必备知识。</p><p>但我们不应该止步于此。<a href="https://pandas.pydata.org/" target="_blank" rel="noopener" title="Pandas">Pandas官网</a> 上有关于Pandas的全部知识。</p>]]></content>
    
    <summary type="html">
    
      使用Pandas库进行数据操作。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · NumPy</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-NumPy/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-NumPy/</id>
    <published>2023-05-17T01:47:59.000Z</published>
    <updated>2023-05-27T13:56:52.237Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文简单介绍了NumPy这个科学计算扩展包的必备知识。</p><h2 id="Set-up">Set up</h2><p>首先我们需要导入NumPy包，做实验的时候可以设置随机种子以实现可重复性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">np.random.seed(seed=<span class="number">1024</span>)</span><br></pre></td></tr></table></figure><p>接下来分别示例 0D（标量）、1D（向量）、2D（矩阵）、3D（3维张量）。</p><p><img src="//s3.mindex.xyz/blog/Courses/49497eb3ffb5cdd0dc88c6e8e2d08270.png" alt="tensors"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scalar</span></span><br><span class="line">x = np.array(<span class="number">6</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x: "</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x ndim: "</span>, x.ndim) <span class="comment"># number of dimensions</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x shape:"</span>, x.shape) <span class="comment"># dimensions</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x size: "</span>, x.size) <span class="comment"># size of elements</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x dtype: "</span>, x.dtype) <span class="comment"># data type</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:  6</span></span><br><span class="line"><span class="comment"># x ndim:  0</span></span><br><span class="line"><span class="comment"># x shape: ()</span></span><br><span class="line"><span class="comment"># x size:  1</span></span><br><span class="line"><span class="comment"># x dtype:  int64 </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Vector</span></span><br><span class="line">x = np.array([<span class="number">1.3</span> , <span class="number">2.2</span> , <span class="number">1.7</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x: "</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x ndim: "</span>, x.ndim)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x shape:"</span>, x.shape)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x size: "</span>, x.size)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x dtype: "</span>, x.dtype) <span class="comment"># notice the float datatype</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:  [1.3 2.2 1.7]</span></span><br><span class="line"><span class="comment"># x ndim:  1</span></span><br><span class="line"><span class="comment"># x shape: (3,)</span></span><br><span class="line"><span class="comment"># x size:  3</span></span><br><span class="line"><span class="comment"># x dtype:  float64</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Matrix</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x ndim: "</span>, x.ndim)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x shape:"</span>, x.shape)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x size: "</span>, x.size)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x dtype: "</span>, x.dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[1 2]</span></span><br><span class="line"><span class="comment">#  [3 4]]</span></span><br><span class="line"><span class="comment"># x ndim:  2</span></span><br><span class="line"><span class="comment"># x shape: (2, 2)</span></span><br><span class="line"><span class="comment"># x size:  4</span></span><br><span class="line"><span class="comment"># x dtype:  int64</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3D Tensor</span></span><br><span class="line">x = np.array([[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], [[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x ndim: "</span>, x.ndim)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x shape:"</span>, x.shape)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x size: "</span>, x.size)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x dtype: "</span>, x.dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[[1 2]</span></span><br><span class="line"><span class="comment">#   [3 4]]</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#  [[5 6]</span></span><br><span class="line"><span class="comment">#   [7 8]]]</span></span><br><span class="line"><span class="comment"># x ndim:  3</span></span><br><span class="line"><span class="comment"># x shape: (2, 2, 2)</span></span><br><span class="line"><span class="comment"># x size:  8</span></span><br><span class="line"><span class="comment"># x dtype:  int64</span></span><br></pre></td></tr></table></figure><p>NumPy当然也提供了几个函数，可以快速创建张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Functions</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"np.zeros((2, 2)):\n"</span>, np.zeros((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"np.ones((2, 2)):\n"</span>, np.ones((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"np.eye((2)):\n"</span>, np.eye((<span class="number">2</span>))) <span class="comment"># identity matrix</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"np.random.random((2, 2)):\n"</span>, np.random.random((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># np.zeros((2, 2)):</span></span><br><span class="line"><span class="comment">#  [[0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0.]]</span></span><br><span class="line"><span class="comment"># np.ones((2, 2)):</span></span><br><span class="line"><span class="comment">#  [[1. 1.]</span></span><br><span class="line"><span class="comment">#  [1. 1.]]</span></span><br><span class="line"><span class="comment"># np.eye((2)):</span></span><br><span class="line"><span class="comment">#  [[1. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1.]]</span></span><br><span class="line"><span class="comment"># np.random.random((2, 2)):</span></span><br><span class="line"><span class="comment">#  [[0.64769123 0.99691358]</span></span><br><span class="line"><span class="comment">#  [0.51880326 0.65811273]]</span></span><br></pre></td></tr></table></figure><h2 id="Indexing">Indexing</h2><p>我们可以使用索引从张量中提取指定的值。<br>请记住，索引从0开始。与使用列表进行索引一样，我们也可以使用负数索引（其中-1是最后一个项目）。</p><p><img src="//s3.mindex.xyz/blog/Courses/f6ba5bef483ad3756ddb81f1d9a05576.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Indexing</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x: "</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[0]: "</span>, x[<span class="number">0</span>])</span><br><span class="line">x[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x: "</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:  [1 2 3]</span></span><br><span class="line"><span class="comment"># x[0]:  1</span></span><br><span class="line"><span class="comment"># x:  [0 2 3]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Slicing</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x column 1: "</span>, x[:, <span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x row 0: "</span>, x[<span class="number">0</span>, :])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x rows 0,1 &amp; cols 1,2: \n"</span>, x[<span class="number">0</span>:<span class="number">2</span>, <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[ 1  2  3  4]</span></span><br><span class="line"><span class="comment">#  [ 5  6  7  8]</span></span><br><span class="line"><span class="comment">#  [ 9 10 11 12]]</span></span><br><span class="line"><span class="comment"># x column 1:  [ 2  6 10]</span></span><br><span class="line"><span class="comment"># x row 0:  [1 2 3 4]</span></span><br><span class="line"><span class="comment"># x rows 0,1 &amp; cols 1,2: </span></span><br><span class="line"><span class="comment">#  [[2 3]</span></span><br><span class="line"><span class="comment">#  [6 7]]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Integer array indexing</span></span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line">rows_to_get = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"rows_to_get: "</span>, rows_to_get)</span><br><span class="line">cols_to_get = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"cols_to_get: "</span>, cols_to_get)</span><br><span class="line"><span class="comment"># Combine sequences above to get values to get</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"indexed values: "</span>, x[rows_to_get, cols_to_get]) <span class="comment"># (0, 0), (1, 2), (2, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[ 1  2  3  4]</span></span><br><span class="line"><span class="comment">#  [ 5  6  7  8]</span></span><br><span class="line"><span class="comment">#  [ 9 10 11 12]]</span></span><br><span class="line"><span class="comment"># rows_to_get:  [0 1 2]</span></span><br><span class="line"><span class="comment"># cols_to_get:  [0 2 1]</span></span><br><span class="line"><span class="comment"># indexed values:  [ 1  7 10]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Boolean array indexing</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x &gt; 2:\n"</span>, x &gt; <span class="number">2</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[x &gt; 2]:\n"</span>, x[x &gt; <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[1 2]</span></span><br><span class="line"><span class="comment">#  [3 4]</span></span><br><span class="line"><span class="comment">#  [5 6]]</span></span><br><span class="line"><span class="comment"># x &gt; 2:</span></span><br><span class="line"><span class="comment">#  [[False False]</span></span><br><span class="line"><span class="comment">#  [ True  True]</span></span><br><span class="line"><span class="comment">#  [ True  True]]</span></span><br><span class="line"><span class="comment"># x[x &gt; 2]:</span></span><br><span class="line"><span class="comment">#  [3 4 5 6]</span></span><br></pre></td></tr></table></figure><h2 id="Arithmetic-运算">Arithmetic 运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Basic math</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x + y:\n"</span>, np.add(x, y)) <span class="comment"># or x + y</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x - y:\n"</span>, np.subtract(x, y)) <span class="comment"># or x - y</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x * y:\n"</span>, np.multiply(x, y)) <span class="comment"># or x * y</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x + y:</span></span><br><span class="line"><span class="comment">#  [[2. 4.]</span></span><br><span class="line"><span class="comment">#  [6. 8.]]</span></span><br><span class="line"><span class="comment"># x - y:</span></span><br><span class="line"><span class="comment">#  [[0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0.]]</span></span><br><span class="line"><span class="comment"># x * y:</span></span><br><span class="line"><span class="comment">#  [[ 1.  4.]</span></span><br><span class="line"><span class="comment">#  [ 9. 16.]]</span></span><br></pre></td></tr></table></figure><h2 id="Dot-product-点积">Dot product 点积</h2><p>在机器学习中，我们最常使用的NumPy操作之一是使用点积进行矩阵乘法。<br>假设我们需要取一个2x3的矩阵a和一个3x2的矩阵b的点积，我们将得到矩阵a的行及矩阵b的列作为点积的输出，也就是得到一个2x2的矩阵。点积能够正确运行需要满足的条件便是内部维度匹配，即示例中，矩阵a有3列，矩阵b有3行。</p><p><img src="//s3.mindex.xyz/blog/Courses/e11a898f34e386460b7dbb42d1cff842.gif" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Dot product</span></span><br><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=np.float64) <span class="comment"># we can specify dtype</span></span><br><span class="line">b = np.array([[<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>], [<span class="number">11</span>, <span class="number">12</span>]], dtype=np.float64)</span><br><span class="line">c = a.dot(b)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"<span class="subst">&#123;a.shape&#125;</span> · <span class="subst">&#123;b.shape&#125;</span> = <span class="subst">&#123;c.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># (2, 3) · (3, 2) = (2, 2)</span></span><br><span class="line"><span class="comment"># [[ 58.  64.]</span></span><br><span class="line"><span class="comment">#  [139. 154.]]</span></span><br></pre></td></tr></table></figure><h2 id="Axis-operations">Axis operations</h2><p>我们还可以沿着特定的轴进行操作。</p><p><img src="//s3.mindex.xyz/blog/Courses/a80be7714f670936b98bb5769037e313.gif" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sum across a dimension</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sum all: "</span>, np.sum(x)) <span class="comment"># adds all elements</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sum axis=0: "</span>, np.sum(x, axis=<span class="number">0</span>)) <span class="comment"># sum across rows</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sum axis=1: "</span>, np.sum(x, axis=<span class="number">1</span>)) <span class="comment"># sum across columns</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[1 2]</span></span><br><span class="line"><span class="comment">#  [3 4]]</span></span><br><span class="line"><span class="comment"># sum all:  10</span></span><br><span class="line"><span class="comment"># sum axis=0:  [4 6]</span></span><br><span class="line"><span class="comment"># sum axis=1:  [3 7]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Min/Max</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"min: "</span>, x.min())</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"max: "</span>, x.max())</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"min axis=0: "</span>, x.min(axis=<span class="number">0</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"min axis=1: "</span>, x.min(axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># min:  1</span></span><br><span class="line"><span class="comment"># max:  6</span></span><br><span class="line"><span class="comment"># min axis=0:  [1 2 3]</span></span><br><span class="line"><span class="comment"># min axis=1:  [1 4]</span></span><br></pre></td></tr></table></figure><h2 id="Broadcast">Broadcast</h2><p>当我们尝试使用看似不兼容的张量形状进行操作时会发生什么？<br>它们的维度不兼容，但是NumPy为何仍然给出了结果？这就是广播的作用。</p><p><img src="//s3.mindex.xyz/blog/Courses/db0288a489a3c03ebdea1be427f1d963.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Broadcasting</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>]) <span class="comment"># vector</span></span><br><span class="line">y = np.array(<span class="number">3</span>) <span class="comment"># scalar</span></span><br><span class="line">z = x + y</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"z:\n"</span>, z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># z:</span></span><br><span class="line"><span class="comment">#  [4 5]</span></span><br></pre></td></tr></table></figure><h2 id="Gotchas">Gotchas</h2><p>在下面的情况中，c的值是多少，它的形状是什么？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = np.array((<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">b = np.expand_dims(a, axis=<span class="number">1</span>)</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line">a.shape <span class="comment"># (3,)</span></span><br><span class="line">b.shape <span class="comment"># (3, 1)</span></span><br><span class="line">c.shape <span class="comment"># (3, 3)</span></span><br><span class="line"><span class="keyword">print</span> (c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># array([[ 6,  7,  8],</span></span><br><span class="line"><span class="comment">#         [ 7,  8,  9],</span></span><br><span class="line"><span class="comment">#         [ 8,  9, 10]])</span></span><br></pre></td></tr></table></figure><p>如果我们不想出现意外的广播行为，就需要小心确保 矩阵a 和 矩阵b 的形状相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = a.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">a.shape <span class="comment"># (3, 1)</span></span><br><span class="line">c = a + b</span><br><span class="line">c.shape <span class="comment"># (3, 1)</span></span><br><span class="line"><span class="keyword">print</span> (c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[ 6]</span></span><br><span class="line"><span class="comment">#  [ 8]</span></span><br><span class="line"><span class="comment">#  [10]]</span></span><br></pre></td></tr></table></figure><h2 id="Transpose-转置">Transpose 转置</h2><p>我们经常需要改变张量的维度，以进行诸如点积之类的操作。如果我们需要交换两个维度，可以对张量进行转置。</p><p><img src="//s3.mindex.xyz/blog/Courses/9fe94457fbf3d53aca519eb20ebf487f.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Transposing</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x.shape: "</span>, x.shape)</span><br><span class="line">y = np.transpose(x, (<span class="number">1</span>, <span class="number">0</span>)) <span class="comment"># flip dimensions at index 0 and 1</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y:\n"</span>, y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y.shape: "</span>, y.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]]</span></span><br><span class="line"><span class="comment"># x.shape:  (2, 3)</span></span><br><span class="line"><span class="comment"># y:</span></span><br><span class="line"><span class="comment">#  [[1 4]</span></span><br><span class="line"><span class="comment">#  [2 5]</span></span><br><span class="line"><span class="comment">#  [3 6]]</span></span><br><span class="line"><span class="comment"># y.shape:  (3, 2)</span></span><br></pre></td></tr></table></figure><h2 id="Reshape">Reshape</h2><p>reshape是另一种改变张量形状的办法。<br>如下面所示，我们reshape后的张量与原始张量具有相同数量的值。我们还可以在一个维度上使用<code>-1</code>，NumPy会根据输入张量自动推断该维度的大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reshaping</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x.shape: "</span>, x.shape)</span><br><span class="line">y = np.reshape(x, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: \n"</span>, y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y.shape: "</span>, y.shape)</span><br><span class="line">z = np.reshape(x, (<span class="number">2</span>, <span class="number">-1</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"z: \n"</span>, z)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"z.shape: "</span>, z.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[1 2 3 4 5 6]]</span></span><br><span class="line"><span class="comment"># x.shape:  (1, 6)</span></span><br><span class="line"><span class="comment"># y:</span></span><br><span class="line"><span class="comment">#  [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]]</span></span><br><span class="line"><span class="comment"># y.shape:  (2, 3)</span></span><br><span class="line"><span class="comment"># z:</span></span><br><span class="line"><span class="comment">#  [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]]</span></span><br><span class="line"><span class="comment"># z.shape:  (2, 3)</span></span><br></pre></td></tr></table></figure><p>reshape函数的工作原理是查看新张量的每个维度，并将原始张量分成相应数量的单元。因此，在这里，新张量<code>index 0</code>处的维度为2，因此我们将原始张量分成2个单元，每个单元都有3个值。</p><p><img src="//s3.mindex.xyz/blog/Courses/1aaa08d5c71d13009b4e969815e1e8a8.png" alt=""></p><h2 id="Joining">Joining</h2><p>我们还可以使用concatenate和stack来合并张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.random((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[0.14950888 0.698439   0.59335256]</span></span><br><span class="line"><span class="comment">#  [0.89991535 0.44445739 0.316785  ]]</span></span><br><span class="line"><span class="comment"># (2, 3)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Concatenation</span></span><br><span class="line">y = np.concatenate([x, x], axis=<span class="number">0</span>) <span class="comment"># concat on a specified axis</span></span><br><span class="line"><span class="keyword">print</span> (y)</span><br><span class="line"><span class="keyword">print</span> (y.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[0.14950888 0.698439   0.59335256]</span></span><br><span class="line"><span class="comment">#  [0.89991535 0.44445739 0.316785  ]</span></span><br><span class="line"><span class="comment">#  [0.14950888 0.698439   0.59335256]</span></span><br><span class="line"><span class="comment">#  [0.89991535 0.44445739 0.316785  ]]</span></span><br><span class="line"><span class="comment"># (4, 3)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Stacking</span></span><br><span class="line">z = np.stack([x, x], axis=<span class="number">0</span>) <span class="comment"># stack on new axis</span></span><br><span class="line"><span class="keyword">print</span> (z)</span><br><span class="line"><span class="keyword">print</span> (z.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[[0.14950888 0.698439   0.59335256]</span></span><br><span class="line"><span class="comment">#   [0.89991535 0.44445739 0.316785  ]]</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#  [[0.14950888 0.698439   0.59335256]</span></span><br><span class="line"><span class="comment">#   [0.89991535 0.44445739 0.316785  ]]]</span></span><br><span class="line"><span class="comment"># (2, 2, 3)</span></span><br></pre></td></tr></table></figure><h2 id="Expanding-Reducing">Expanding / Reducing</h2><p>我们还可以轻松地向张量中添加和删除维度，这样做是为了使张量能够兼容某些操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Adding dimensions</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x.shape: "</span>, x.shape)</span><br><span class="line">y = np.expand_dims(x, axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: \n"</span>, y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y.shape: "</span>, y.shape)   <span class="comment"># notice extra set of brackets are added</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]]</span></span><br><span class="line"><span class="comment"># x.shape:  (2, 3)</span></span><br><span class="line"><span class="comment"># y:</span></span><br><span class="line"><span class="comment">#  [[[1 2 3]]</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#  [[4 5 6]]]</span></span><br><span class="line"><span class="comment"># y.shape:  (2, 1, 3)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Removing dimensions</span></span><br><span class="line">x = np.array([[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]], [[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x.shape: "</span>, x.shape)</span><br><span class="line">y = np.squeeze(x, axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: \n"</span>, y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y.shape: "</span>, y.shape)  <span class="comment"># notice extra set of brackets are gone</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[[1 2 3]]</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#  [[4 5 6]]]</span></span><br><span class="line"><span class="comment"># x.shape:  (2, 1, 3)</span></span><br><span class="line"><span class="comment"># y:</span></span><br><span class="line"><span class="comment">#  [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]]</span></span><br><span class="line"><span class="comment"># y.shape:  (2, 3)</span></span><br></pre></td></tr></table></figure><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; NumPy - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>到这里，便拥有了Way2AI路上需要的NumPy的必备知识。</p><p>但我们不应该止步于此。<a href="https://numpy.org/" target="_blank" rel="noopener" title="NumPy">NumPy官网</a> 上有关于NumPy的全部知识。</p>]]></content>
    
    <summary type="html">
    
      Numerical analysis with the NumPy computing package.
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
</feed>
