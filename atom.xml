<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>愚苏记</title>
  
  <subtitle>To no avail but try.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://neo1989.net/"/>
  <updated>2024-03-18T13:14:52.006Z</updated>
  <id>https://neo1989.net/</id>
  
  <author>
    <name>Neo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Server Sent Event (SSE) with Go</title>
    <link href="https://neo1989.net/HowTo/HowTo-sse-with-go/"/>
    <id>https://neo1989.net/HowTo/HowTo-sse-with-go/</id>
    <published>2024-03-18T09:36:00.000Z</published>
    <updated>2024-03-18T13:14:52.006Z</updated>
    
    <content type="html"><![CDATA[<p>SSE 提供了一种简单而高效的方式，它可以在服务器和网页客户端之间建立一个单向连接，这样服务器就能够实时地发送更新信息，而无需不断地进行数据请求。</p><span id="more"></span><h2 id="理解-Server-Sent-Events-SSE">理解 Server-Sent Events (SSE)</h2><p>Server-Sent Events（服务器发送事件）是 HTML5 规范的一部分，它允许服务器通过一条持久的单一连接向 Web 客户端推送（发送）数据。与 WebSocket（全双工通信协议）不同，后者支持全双工（双向）通信，而服务器发送事件更适合于需要从服务器到客户端的单向通信的场景。</p><h3 id="SSE-如何工作">SSE 如何工作</h3><p>SSE 依赖于客户端的 EventSource API，它让浏览器能够与服务器端点建立一个持久的连接。一旦建立了连接，服务器就可以将事件以简单的文本数据的形式（通常是 “text/event-stream” 格式）发送到客户端。然后，客户端的 JavaScript 可以处理这些事件，并实时刷新网页。</p><h3 id="SSE-的优点">SSE 的优点</h3><ul><li>SSE 使用 HTTP 协议，现有的服务器软件都支持。WebSocket 是一个独立协议。</li><li>SSE 属于轻量级，使用简单；WebSocket 协议相对复杂。</li><li>SSE 默认支持断线重连，WebSocket 需要自己实现。</li><li>SSE 一般只用来传送文本，二进制数据需要编码后传送，WebSocket 默认支持传送二进制数据。</li><li>SSE 支持自定义发送的消息类型。</li></ul><h3 id="Implementing-Server-Sent-Events-with-Go">Implementing Server-Sent Events with Go</h3><p>如果我们想在 Go 应用程序中实施 SSE，就需要建立一个 HTTP 端点供客户端连接。下面是一份详细的步骤指南，还配有一些代码示例：</p><h4 id="Step-1-Create-a-Basic-HTTP-Server">Step 1: Create a Basic HTTP Server</h4><p>首先，我们来使用 net/http 包搭建一个基础的 Go HTTP 服务器。这是一个示例：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line">    <span class="string">&quot;net/http&quot;</span></span><br><span class="line">    <span class="string">&quot;time&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    http.HandleFunc(<span class="string">&quot;/events&quot;</span>, eventsHandler)</span><br><span class="line">    http.ListenAndServe(<span class="string">&quot;:8080&quot;</span>, <span class="literal">nil</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这个示例中，我们设定了一个 HTTP 服务器，它会在 8080 端口进行监听，且设立了一个唯一的端点，“/events”，这个端点将负责处理 SSE 连接。</p><h4 id="Step-2-Implement-the-SSE-Handler">Step 2: Implement the SSE Handler</h4><p>下一步，我们需要实现 SSE 的处理器。这个处理器的作用是向已连接的客户端发送事件。这是一个示例：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">eventsHandler</span><span class="params">(w http.ResponseWriter, r *http.Request)</span></span> &#123;</span><br><span class="line">    <span class="comment">// Set CORS headers to allow all origins. You may want to restrict this to specific origins in a production environment.</span></span><br><span class="line">    w.Header().Set(<span class="string">&quot;Access-Control-Allow-Origin&quot;</span>, <span class="string">&quot;*&quot;</span>)</span><br><span class="line">    w.Header().Set(<span class="string">&quot;Access-Control-Expose-Headers&quot;</span>, <span class="string">&quot;Content-Type&quot;</span>)</span><br><span class="line"></span><br><span class="line">    w.Header().Set(<span class="string">&quot;Content-Type&quot;</span>, <span class="string">&quot;text/event-stream&quot;</span>)</span><br><span class="line">    w.Header().Set(<span class="string">&quot;Cache-Control&quot;</span>, <span class="string">&quot;no-cache&quot;</span>)</span><br><span class="line">    w.Header().Set(<span class="string">&quot;Connection&quot;</span>, <span class="string">&quot;keep-alive&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Simulate sending events (you can replace this with real data)</span></span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">10</span>; i++ &#123;</span><br><span class="line">        fmt.Fprintf(w, <span class="string">&quot;data: %s\n\n&quot;</span>, fmt.Sprintf(<span class="string">&quot;Event %d&quot;</span>, i))</span><br><span class="line">        time.Sleep(<span class="number">2</span> * time.Second)</span><br><span class="line">        w.(http.Flusher).Flush()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Simulate closing the connection</span></span><br><span class="line">    closeNotify := w.(http.CloseNotifier).CloseNotify()</span><br><span class="line">    &lt;-closeNotify</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这段代码中，我们设定了响应头以指明 SSE 的内容类型，禁止了缓存，并且保证了连接的持久性。接着我们使用了一个循环来模拟发送事件。你可以将这部分替换成你想要发送给客户端的实际数据。</p><h4 id="Step-3-Handle-SSE-on-the-Client-Side">Step 3: Handle SSE on the Client Side</h4><p>在客户端，你可以使用 JavaScript 来打开一个 SSE 连接并处理接收到的事件。以下是一个简单的 HTML 和 JavaScript 代码片段：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">title</span>&gt;</span>SSE Example<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;sse-data&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">            <span class="keyword">const</span> eventSource = <span class="keyword">new</span> <span class="title class_">EventSource</span>(<span class="string">&#x27;http://localhost:8080/events&#x27;</span>);</span></span><br><span class="line"><span class="language-javascript">            eventSource.<span class="property">onmessage</span> = <span class="keyword">function</span>(<span class="params">event</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                <span class="keyword">const</span> dataElement = <span class="variable language_">document</span>.<span class="title function_">getElementById</span>(<span class="string">&#x27;sse-data&#x27;</span>);</span></span><br><span class="line"><span class="language-javascript">                dataElement.<span class="property">innerHTML</span> += event.<span class="property">data</span> + <span class="string">&#x27;&lt;br&gt;&#x27;</span>;</span></span><br><span class="line"><span class="language-javascript">            &#125;;</span></span><br><span class="line"><span class="language-javascript">        </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这个 HTML 页面使用 EventSource API 连接到 “/events” 端点，并将接收到的事件添加到一个 <div> 元素中。</p><h4 id="Step-4-Run-the-Go-Application">Step 4: Run the Go Application</h4><p>编译并运行你的 Go 应用程序，然后打开一个网页浏览器，导航到你创建的 HTML 页面。你应该能看到实时事件在网页上显示。</p><p><img src="//s3.mindex.xyz/blog/Courses/efbd29559aab7581c35ed0bce30b5fed.png" alt=""></p><h2 id="Conclusion">Conclusion</h2><p>服务器发送事件（Server-Sent Events，简称 SSE）是一种简洁且高效的技术，可以用来在网页应用中实现实时通信。通过使用 Go 语言，我们可以方便地创建一个 SSE 服务器，这个服务器可以向客户端推送实时更新，从而为用户提供流畅且高效的实时体验。无论是用于展示实时通知，更新数据仪表盘，还是其他任何用途，SSE 都是你网页开发工具集中的重要组成部分。</p><h2 id="Reference">Reference</h2><p><a href="https://www.ruanyifeng.com/blog/2017/05/server-sent_events.html">Server-Sent Events 教程</a><br><a href="https://softwaremill.com/sse-vs-websockets-comparing-real-time-communication-protocols/">SSE vs WebSockets</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SSE 提供了一种简单而高效的方式，它可以在服务器和网页客户端之间建立一个单向连接，这样服务器就能够实时地发送更新信息，而无需不断地进行数据请求。&lt;/p&gt;
    
    </summary>
    
    
      <category term="HowTo" scheme="https://neo1989.net/categories/HowTo/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · Prompt Engineering 概述</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-A-Systematic-Survey-of-Prompt-Engineering-in-LLM/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-A-Systematic-Survey-of-Prompt-Engineering-in-LLM/</id>
    <published>2024-03-14T12:52:32.000Z</published>
    <updated>2024-03-15T12:02:06.384Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Prompt-Engineering">Prompt Engineering</h2><p>按应用领域进行分类，简要概述提示技术的发展历程，从最初的零样本提示技术，一直到现在的最新进展。</p><h3 id="New-Tasks-Without-Extensive-Training">New Tasks Without Extensive Training</h3><h4 id="Zero-Shot-Prompting">Zero-Shot Prompting</h4><p>零样本提示为我们如何利用大语言模型（LLM）提供了全新的视角。这种技术无需依赖大量的训练数据，而是通过精心设计的提示，引导模型去完成前所未有的任务。具体的说，模型在提示中接收到了任务的描述，但并没有标注数据来训练特定的输入-输出映射。然后，模型就会利用自身已有的知识，根据给定的提示为新任务生成预测。</p><p><img src="//s3.mindex.xyz/blog/Courses/bd6a40576eb18f2c0337d0ec2145ee19.png" alt=""></p><h4 id="Few-Shot-Prompting">Few-Shot Prompting</h4><p>少样本引导是通过提供一些输入-输出示例，帮助模型理解特定任务的一种方法，这与零样本引导（不提供任何示例）有所不同。即便只提供少数几个高质量的示例，也已经能够在一定程度上提升模型在复杂任务上的表现。</p><p>然而，少样本引导需要额外的 Token 来包含这些示例，对于更长的文本输入来说，可能会带来一些处理上的困难。<br>此外，提示示例的选取和组织方式对模型的行为有着显著影响，例如模型可能会倾向于选择使用频率较高的词汇，这种偏见可能会影响少样本引导的结果。</p><p>尽管少样本引导能够增强处理复杂任务的能力，特别是在像 GPT-3 这样的大型预训练模型中，但是，精心设计的引导策略对于实现最佳性能和减少模型的无意识偏见至关重要。</p><p><img src="//s3.mindex.xyz/blog/Courses/144b4f22e7e1afc18d1e55de3e7bd4f1.png" alt=""></p><h3 id="Reasoning-and-Logic">Reasoning and Logic</h3><h4 id="Chain-of-Thought-CoT-Prompting">Chain-of-Thought (CoT) Prompting</h4><p><a href="https://arxiv.org/abs/2201.11903" title="CoT">CoT</a> 提示能够更有效地引导大语言模型产生结构化且深思熟虑的回应。我们通过一系列的实验，展示了CoT提示的独特优势，强调了它能够引导大语言模型按照逻辑链条进行推理的能力。这种方式使得模型的回应展现出对给定提示更深入的理解。例如，对于一个需要多步推理的数学文字题目，CoT提示能够呈现出整个推理过程和最终答案，这仿佛就像人类如何将问题分解为逻辑中间步骤一样。</p><p><img src="//s3.mindex.xyz/blog/Courses/cc23793ec5adae2dc2c1a38b23fff338.png" alt=""></p><h4 id="Automatic-Chain-of-Thought-Auto-CoT-Prompting">Automatic Chain-of-Thought (Auto-CoT) Prompting</h4><p>“Let’s think step by step”， 自动引导大语言模型（LLMs）形成推理链。<a href="https://arxiv.org/abs/2210.03493" title="Auto-CoT">Auto-CoT</a> 注意到在单独生成的推理链中可能会出现错误，因此采取了多样化采样的策略以提高模型的鲁棒性。它会提出各种各样的问题，并为每个问题生成多个不同的推理链，从而形成一个最终的示例集。这种自动化的多样化采样策略可以最大限度地减少错误，同时提高了少样本（few-shot）学习的效果，免去了手动制作推理链的繁重工作。</p><p><img src="//s3.mindex.xyz/blog/Courses/697f3251c2e47a8c25d89b959fa931eb.png" alt=""></p><h4 id="Self-Consistency">Self-Consistency</h4><p>与常用的“贪心解码”（每一步都选择最可能的选项）相比，“自我一致性”的解码策略能在使用CoT（Chain-of-Thought，逐步推理）技术指导大语言模型时，更好地提升推理性能。对于那些存在多个可能解决路径的复杂推理任务，&quot;自我一致性&quot;策略能从语言模型的解码器中采样出多样化的推理链条。接着，它通过对这些采样链条进行统计处理（即“边缘化”），来确定最具一致性的最终答案。这种方法的优势在于，对于需要深入分析的问题，通常存在更多的推理路径，这种多样性正是我们找到解决方案的关键。</p><p><img src="//s3.mindex.xyz/blog/Courses/6e31e14eab13341dd9f693aedb1ef816.png" alt=""></p><h4 id="Logical-Chain-of-Thought-LogiCoT-Prompting">Logical Chain-of-Thought (LogiCoT) Prompting</h4><p>大语言模型 (LLMs) 要解决各种领域的复杂多步问题，具备逻辑推理能力是至关重要的。现有的方法，比如 CoT 提示，虽然倡导逐步推理，但在验证机制上却不够有效。</p><p><a href="https://arxiv.org/abs/2305.12147" title="LogiCoT">LogiCoT</a> 采用了&quot;反证法&quot;的思想，对模型生成的每一步推理进行验证，并在发现错误时提供有针对性的反馈进行修正。通过这样一种&quot;思考-验证-修正&quot;的循环过程，LogiCoT 能有效地减少模型在推理过程中的逻辑错误和误导性信息。</p><p><img src="//s3.mindex.xyz/blog/Courses/9ba28acd4ee5b6197afe5c5d428e5bca.png" alt=""></p><h4 id="Chain-of-Symbol-CoS-Prompting">Chain-of-Symbol (CoS) Prompting</h4><p>大语言模型常常在处理涉及复杂空间关系的任务时遇到困难，因为它们主要依赖于自然语言，这可能导致歧义和偏见。</p><p><a href="https://arxiv.org/abs/2305.10276" title="CoS">CoS</a> 使用简洁的符号来替代自然语言。CoS 的优势在于：它可以提供清晰、简洁的提示，增强大语言模型的空间推理能力，并提高人类对模型的理解。然而，CoS 也面临着一些挑战，比如如何扩展和泛化，如何与其他技术集成，以及如何解释基于符号的大语言模型的推理过程等。</p><p><img src="//s3.mindex.xyz/blog/Courses/fa8d491e924cb02ff8449d58d3a1a483.png" alt=""></p><h4 id="Tree-of-Thoughts-ToT-Prompting">Tree-of-Thoughts (ToT) Prompting</h4><p><a href="https://arxiv.org/abs/2305.10601" title="ToT">ToT</a> 通过构建一个包含中间推理步骤的树形结构，来拓展链式思维（CoT）提示的方法，这些步骤被称为 “思维”。每一个&quot;思维&quot;都代表一段有条理的语言序列，指向最终的解决方案。这样的结构让大语言模型能够通过评估每个&quot;思维&quot;在解决问题上的进展，进行更深入的推理。思维树结合了模型生成和评估&quot;思维&quot;的能力，以及广度优先或深度优先等搜索算法。这使得模型能在推理链中进行系统性的探索，预先扩展可能有希望的解决方向，同时在找到错误的解决方案时能够回溯。</p><p><img src="//s3.mindex.xyz/blog/Courses/408297ef014c50ff725acd01558eb70c.png" alt=""></p><h4 id="Graph-of-Thoughts-GoT-Prompting">Graph-of-Thoughts (GoT) Prompting</h4><p><a href="https://arxiv.org/abs/2308.09687" title="GoT">GoT</a>  对传统的顺序方法进行了改进，使其更好地对应人类思维的非线性特性。这个框架支持动态的交互，回溯和评估各种想法，允许从不同的分支中整合和组合思维，打破了思维树的线性结构。它的主要贡献在于，将推理过程模拟成一个有向图，并提供了一个带有多种转换操作的模块化架构。这个框架被视为一种灵活且动态的语言模型提示方式，能够捕捉人类思维过程的复杂性，并提升模型的能力。</p><p><img src="//s3.mindex.xyz/blog/Courses/1dd475a4bcf9855715799415add7466a.png" alt=""></p><h4 id="System-2-Attention-S2A-Prompting">System 2 Attention (S2A) Prompting</h4><p>在基于 Transformer 的大语言模型 (LLM) 中，柔性的关注机制可能会过度考虑无关的信息，这对于生成有效的 token 不利。</p><p><a href="https://arxiv.org/abs/2311.11829" title="S2A">S2A</a> 利用大语言模型的推理能力，通过重新整理输入的上下文，只关注与任务相关的信息。S2A 采用了两步过程，通过重塑上下文和生成更精确的回应，来提高其关注能力和回应质量。S2A 的效果在各种任务中得到了验证，包括回答基于事实的问题，生成长篇文章，以及解决数学文字题目。</p><p><img src="//s3.mindex.xyz/blog/Courses/bbed833d5b6ad0cb192d38cfc5176047.png" alt=""></p><h4 id="Thread-of-Thought-ThoT-Prompting">Thread of Thought (ThoT) Prompting</h4><p><a href="https://arxiv.org/abs/2311.08734" title="ThoT">ThoT</a> 的设计灵感来自人类的认知过程，它能有条不紊地分析大量的信息，将这些信息划分为易于处理的小部分。这个过程分为两个阶段，首先，LLM 会概括和审查每一段信息，然后再精炼这些信息，以便给出最后的回应。ThoT 的灵活性体现在它可以作为一个多功能的插件，用于增强不同模型和提示方法的推理能力。</p><p><img src="//s3.mindex.xyz/blog/Courses/685c878ed68c513cb9025a8428563eb7.png" alt=""></p><h4 id="Chain-of-Table-Prompting">Chain-of-Table Prompting</h4><p>像 CoT、PoT 和 ToT 这样的方法主要是通过自由形式的文本或代码来进行推理，但在处理包含大量数据和复杂结构的表格时，却遭遇了一些困难。</p><p><a href="https://arxiv.org/abs/2401.04398" title="Chain-of-Table">Chain-of-Table</a> 的核心思想是通过动态地在表格上生成并执行常见的 SQL 或 DataFrame 操作，实现步骤之间的逐步推理。这一过程的反复迭代可以改进中间的推理结果，从而提升大语言模型的预测能力，使其能够通过形象的逻辑推理链条进行预测。</p><p><img src="//s3.mindex.xyz/blog/Courses/a10c977d23baa955352761769ce584f2.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/708189f5f2c23f49cf287e871674f3da.png" alt=""></p><h3 id="Reduce-Hallucination">Reduce Hallucination</h3><h4 id="Retrieval-Augmented-Generation-RAG">Retrieval Augmented Generation (RAG)</h4><p>大语言模型 (LLMs) 已经带来了文本生成的革命性变化，但其依赖有限、静态的训练数据，使得在需要外部知识的任务中，准确的响应成为了难题。传统的提示方法并不能解决这个问题，因为它需要耗费大量的重新训练。</p><p><a href="/tags/RAG/" title="RAG">RAG</a> 为我们提供了新的解决方案，它将信息检索巧妙地融入到提示过程中。RAG 能够分析用户的输入，制定出精准的查询，并在预先构建的知识库中查找相关的资源。检索到的信息片段被纳入到原始的提示中，为其提供了丰富的上下文背景。这样增强后的提示，使大语言模型能够生成具有创新性和准确性的响应。RAG 的敏捷性突破了静态限制，对于需要实时知识的任务，它无疑是一种改变游戏规则的技术。</p><p><img src="//s3.mindex.xyz/blog/Courses/2b7824dc44fda311ea348ff4af2ecca2.png" alt=""></p><h4 id="ReAct-Prompting">ReAct Prompting</h4><p>与以往将推理和行动分别处理的研究不同，<a href="https://arxiv.org/abs/2210.03629" title="ReAct">ReAct</a> 让大语言模型（LLMs）有能力同时进行推理过程和特定任务的行动生成。这种交织的过程增强了推理和行动之间的协同效应，使模型在处理异常情况时能够更好地引导、追踪和更新行动计划。ReAct 被广泛应用于各种语言处理和决策制定任务中，并在与当前SOTA相比较的基线测试中展现出了显著的优势。尤其值得一提的是，在问题回答（HotpotQA）和事实验证（Fever）任务中，ReAct 通过与简单的维基百科API的交互，有效地解决了错误传播和产生不真实信息的问题，从而产生了更易于理解的任务解决路径。</p><p><img src="//s3.mindex.xyz/blog/Courses/d398a9d0ea408794f3bb35f78d76a2c3.png" alt=""></p><h4 id="Chain-of-Verification-CoVe-Prompting">Chain-of-Verification (CoVe) Prompting</h4><p><a href="https://arxiv.org/abs/2309.11495" title="CoVe">CoVe</a> 包含四个步骤：模型首先生成初步的回答，然后提出一些验证问题以检查自己的回答，接着独立地解答这些问题，最后根据验证结果修正并产生最终的回答。这种经过深思熟虑的多步骤验证方式，提升了大语言模型的逻辑推理能力，使其即使在面对矛盾信息时也能减少错误。CoVe 的设计理念是模拟人类的验证过程，以此提高大语言模型输出的连贯性和精确性。在列表问题、问答和长篇生成等实验中，CoVe 成功地在保证事实准确性的同时，减少了虚构现象。通过提出具有针对性的验证问题，模型能更好地发现并纠正自身的不准确之处。</p><p><img src="//s3.mindex.xyz/blog/Courses/f77ebfaff72cd60616c655ed8356dd12.png" alt=""></p><h4 id="Chain-of-Note-CoN-Prompting">Chain-of-Note (CoN) Prompting</h4><p>检索增强型语言模型（Retrieval-augmented language models，RALMs）的设计初衷是为了提升大型语言模型（Large Language Models）的能力，通过融合外部知识，以减少模型在生成过程中产生的不真实信息。然而，这些检索到的信息并非总是可靠的，有可能会引导模型产生错误的回应。常规的 RALMs 在评估自身知识是否充足时，往往会遇到困难，尤其是在缺乏足够信息时，这些模型通常无法给出“我不知道”的答案。</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For standard RALM</span></span><br><span class="line">Task Description:</span><br><span class="line">The primary objective is <span class="built_in">to</span> briefly answer <span class="keyword">a</span> specific question.</span><br><span class="line"></span><br><span class="line"><span class="comment"># For RALM with CON</span></span><br><span class="line">Task Description:</span><br><span class="line"><span class="number">1.</span> Read <span class="keyword">the</span> given question <span class="keyword">and</span> Wikipedia passages <span class="built_in">to</span> gather relevant information.</span><br><span class="line"><span class="number">2.</span> Write reading notes summarizing <span class="keyword">the</span> key points <span class="built_in">from</span> these passages.</span><br><span class="line"><span class="number">3.</span> Discuss <span class="keyword">the</span> relevance <span class="keyword">of</span> <span class="keyword">the</span> given question <span class="keyword">and</span> Wikipedia passages.</span><br><span class="line"><span class="number">4.</span> If some passages are relevant <span class="built_in">to</span> <span class="keyword">the</span> given question, provide <span class="keyword">a</span> brief  answer based <span class="keyword">on</span> <span class="title">the</span> <span class="title">passages</span>.</span><br><span class="line"><span class="number">5.</span> If no passage is relevant, directly provide answer <span class="keyword">without</span> considering <span class="keyword">the</span> passages.</span><br></pre></td></tr></table></figure><p><a href="https://arxiv.org/abs/2311.09210" title="CoN">CoN</a> 能够系统地评估文档的相关性，强调关键且可靠的信息，过滤掉无关的内容，从而使得模型的回应更加精确，更具上下文相关性。<br>CoN 不仅是一个提示模板，而且还包含了一个经过微调的可以记笔记模型。因此CoN可以看作是RAG和Fine-Tuning的结合。</p><p><img src="//s3.mindex.xyz/blog/Courses/4512f010ca40405423106942c10baa58.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/8b5f22339915adbdfc67dde8e70ca6a6.png" alt=""></p><h4 id="Chain-of-Knowledge-CoK-Prompting">Chain-of-Knowledge (CoK) Prompting</h4><p><a href="https://arxiv.org/abs/2305.13269" title="CoK">CoK</a>  从人类解决问题的方式中获得灵感，将复杂的任务系统地分解成一系列有序的步骤。这个过程首先是全面的推理准备阶段，建立问题的上下文，并对问题进行框架化。然后，它进入动态知识适应阶段，从各种来源如内部知识库、外部数据库以及给定的提示中，精心收集相关的证据。</p><p><img src="//s3.mindex.xyz/blog/Courses/c7f7c5ea582e777b188323ff20acba7e.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/df831c882f4df1aabade839c782c0716.png" alt=""></p><h3 id="User-Interface">User Interface</h3><h4 id="Active-Prompting">Active Prompting</h4><p><a href="https://arxiv.org/abs/2302.12246">Active-Prompting</a> 引入了一种新的机制，可以确定哪些问题对于注释最具影响力。这种方法借鉴了基于不确定性的主动学习的思想，通过使用各种度量标准来描述不确定性，并选择最具不确定性的问题进行注释。</p><p><img src="//s3.mindex.xyz/blog/Courses/c689e6ee19708f997f565ad61fa60f73.png" alt=""></p><h3 id="Fine-Tuning-and-Optimization">Fine-Tuning and Optimization</h3><h4 id="Automatic-Prompt-Engineer-APE">Automatic Prompt Engineer (APE)</h4><p><a href="https://arxiv.org/abs/2211.01910" title="APE">APE</a> 通过动态地生成和选择对特定任务最有影响力的提示，从而克服了传统手工设计的、固定不变的提示的缺点。这种巧妙的方法会分析用户的输入，制定一系列可能的指令，然后利用强化学习来挑选出最佳的提示。这种提示能够根据不同的上下文环境进行实时调整，提高了模型的适应性。</p><p><img src="//s3.mindex.xyz/blog/Courses/fec997cd64d3cad3f8161207bf98bb7a.png" alt=""></p><h3 id="Knowledge-Based-Reasoning-and-Generation">Knowledge-Based Reasoning and Generation</h3><h4 id="Automatic-Reasoning-and-Tool-use-ART">Automatic Reasoning and Tool-use (ART)</h4><p><a href="https://arxiv.org/abs/2303.09014" title="ART">ART</a> 赋予了大语言模型通过多步骤过程进行推理，以及无缝地利用外部专业知识的能力，从而使其能够应对复杂问题，超越了简单的文本生成。</p><p>ART通过整合专业知识和计算工具，打开了大语言模型的多功能性，使其的输出能与现实世界紧密联系。这使得大语言模型能够在科学研究、数据分析，甚至决策支持等多元领域发挥作用。</p><p>ART不仅超越了传统的提示技术，还通过结构化程序自动化了推理步骤，从而消除了手工制作的需要。其动态的工具整合能力确保了与外部工具的顺畅协作，可以暂停生成过程以融入外部工具的输出结果，然后无缝地恢复生成流程。在一些具有挑战性的基准测试（例如Big-Bench和MMLU）上，实验证明了ART的有效性，其表现甚至超过了传统的提示技术，有时甚至能够达到手工制作示例的效果。</p><p><img src="//s3.mindex.xyz/blog/Courses/736ea666e264e7bb729505c7454b60d6.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/e286a415d47aba340dd72ee42f93bc7a.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/65deeb80598eff36a0d403e7c745a0bc.png" alt=""></p><h3 id="Improving-Consistency-and-Coherence">Improving Consistency and Coherence</h3><h4 id="Contrastive-Chain-of-Thought-CCoT-Prompting">Contrastive Chain-of-Thought (CCoT) Prompting</h4><p>传统用于大语言模型的CoT提示方法往往忽视了一个关键环节：从错误中吸取教训。</p><p>而 <a href="https://arxiv.org/abs/2311.09277" title="CCoT">CCoT</a> 不仅提供了正确的推理示例，还展示了错误的推理过程。试想你在探索一张地图，既有明确的正确路径，也标出了需要避开的误区，这就是CCoT带来的优势！</p><p><img src="//s3.mindex.xyz/blog/Courses/2342eec6fbabec308efa898da263f8b2.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/614cb77a1ae49769c1ce1a48fc895bfd.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/9d0aa48a1d217f14449a7cc30fb93a21.png" alt=""></p><h3 id="Managing-Emotions-and-Tone">Managing Emotions and Tone</h3><h4 id="Emotion-Prompting">Emotion Prompting</h4><p><a href="https://arxiv.org/abs/2307.11760">EmotionPrompt</a> 解决了大语言模型理解情绪线索能力的不确定性。他们从心理学研究中汲取灵感，探索语言对人类表现的影响，将11个情绪刺激语句融入到提示中，旨在提升大语言模型的情绪智能。实验结果显示，这些情绪刺激语句的加入，能够有效地融入到模型的运作中，从而在各种任务中显著提升大语言模型的表现。</p><p><img src="//s3.mindex.xyz/blog/Courses/ac0b8274fbc10014e730d3bb9f460c08.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/ba723c819c6506283f3ba9720a3d91aa.png" alt=""></p><h3 id="Code-Generation-and-Execution">Code Generation and Execution</h3><h4 id="Scratchpad-Prompting">Scratchpad Prompting</h4><p>尽管基于 Transformer 的语言模型在生成基础编程任务的代码上表现卓越，但在涉及到需要精确逻辑推理的复杂、多步骤算法计算中，它们却面临挑战。</p><p><a href="https://arxiv.org/abs/2112.00114" title="Scratchpad Prompting">Scratchpad</a> 更注重任务设计，而非模型的修改，使得模型能在给出最终答案之前，生成一系列的中间计算步骤。</p><p><img src="//s3.mindex.xyz/blog/Courses/badc6033f1cde07372bd7a5d51b50149.png" alt=""></p><h4 id="Program-of-Thoughts-PoT-Prompting">Program of Thoughts (PoT) Prompting</h4><p>语言模型在解决数学表达式时的表现并不理想，主要原因在于它们容易出现算术错误，无法处理复杂的方程，且在表达大量迭代过程时效率低下。</p><p><a href="https://arxiv.org/abs/2211.12588" title="PoT">PoT</a> 倡导在计算步骤中使用外部的编程语言解释器，以提高语言模型的数值推理能力。</p><p><img src="//s3.mindex.xyz/blog/Courses/c7a6067d672639aad0ece1593402d05e.png" alt=""></p><h4 id="Structured-Chain-of-Thought-SCoT-Prompting">Structured Chain-of-Thought (SCoT) Prompting</h4><p><a href="https://arxiv.org/abs/2305.06599" title="SCoT">SCoT</a> 通过将程序结构（如序列、分支和循环结构）融入推理步骤，从而提升了大语言模型在生成结构化源代码方面的表现。这种方法明确地引导大语言模型从源代码的角度思考需求，相比于 CoT 提示，其在代码生成方面的效果得到了显著提升。</p><p><img src="//s3.mindex.xyz/blog/Courses/336065b71cdb63da883607248c7fe400.png" alt=""></p><h4 id="Chain-of-Code-CoC-Prompting">Chain-of-Code (CoC) Prompting</h4><p><a href="https://arxiv.org/abs/2312.04474" title="CoC">CoC</a> 通过利用编写代码的方式来改善LM的推理能力，适用于逻辑和语义任务。CoC 鼓励LMs把语义子任务格式化为灵活的伪代码，这样就可以让解释器捕捉到未定义的行为，并通过一个被称为&quot;LM模拟器&quot;（LMulator）的工具来模拟这些行为。</p><p><img src="//s3.mindex.xyz/blog/Courses/920be294f8d31bcb2dfc99c9b48f718a.png" alt=""></p><h3 id="Optimization-and-Efficiency">Optimization and Efficiency</h3><h4 id="Optimization-by-Prompting-OPRO">Optimization by Prompting (OPRO)</h4><p><a href="https://arxiv.org/abs/2309.03409" title="OPRO">OPRO</a> 使用自然语言的提示，根据问题的描述，逐步生成解决方案，从而使得快速适应不同任务和个性化优化过程成为可能。通过在诸如线性回归和旅行商问题这样的经典问题上的案例研究，展示了LLMs在优化问题上的巨大潜力。此外，OPRO还探索了如何优化提示，以在自然语言处理任务中最大化准确性，这也突显出LLMs的敏感性。</p><p><img src="//s3.mindex.xyz/blog/Courses/fc2a4795b407fb610709971983ea9db8.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/fece14e6124c739fb7768d5ef7379ca6.png" alt=""></p><h3 id="Understanding-User-Intent">Understanding User Intent</h3><h4 id="Rephrase-and-Respond-RaR-Prompting">Rephrase and Respond (RaR) Prompting</h4><p><a href="https://arxiv.org/abs/2311.04205" title="RaR">RaR</a> 让大语言模型有能力在一次提示中对问题进行重新表述和扩展，从而提高了模型理解问题和回答问题的准确性。他们还开发了一种两步骤的 RaR 变种，这种变种结合了重新表述和回应的大语言模型，显著提高了各种任务的性能。研究强调，相比于人类随意提出的问题，重新表述的问题能够增强语义的清晰度，解决问题本身的模糊性。</p><p><img src="//s3.mindex.xyz/blog/Courses/11b29a315244c8ee5370bfe49edf1078.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/e586e329ea1017532ef7dc13886d48e6.png" alt=""></p><h3 id="Metacognition-and-Self-Reflection">Metacognition and Self-Reflection</h3><h4 id="Take-a-Step-Back-Prompting">Take a Step Back Prompting</h4><p><a href="https://arxiv.org/abs/2310.06117" title="Step Back Prompting">Step Back</a> 方法包含两个步骤，即“抽象”和“推理”的整合。通过广泛的实验，将“Step Back”应用于PaLM-2L在STEM（科学、技术、工程和数学）、知识问答和多跳推理等各种推理密集型任务中，实验结果表明，这种方法能显著提升模型的推理能力。</p><p><img src="//s3.mindex.xyz/blog/Courses/0117ebc52d00cbb97b43e4cbfc1d4b1b.png" alt=""></p><h2 id="Conclusion">Conclusion</h2><p>提示工程未来的发展潜力巨大，元学习和混合提示架构等新兴趋势预示着它的能力将得到进一步提升。然而，我们在发展的同时，必须高度重视道德问题，强调负责任的开发和部署，确保其能够积极地融入我们的生活中。</p><h2 id="Source">Source</h2><p><a href="https://arxiv.org/abs/2402.07927" title="A Systematic Survey of Prompt Engineering">A Systematic Survey of Prompt Engineering</a></p>]]></content>
    
    <summary type="html">
    
      在人工智能领域，&quot;提示工程&quot;正成为一种变革性的力量，它释放了大语言模型（LLMs）的巨大潜力。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="LLM" scheme="https://neo1989.net/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 如何构建企业级 RAG 系统</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-How-to-Build-an-Enterprise-RAG-System/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-How-to-Build-an-Enterprise-RAG-System/</id>
    <published>2024-03-06T05:34:01.000Z</published>
    <updated>2024-03-07T12:31:58.896Z</updated>
    
    <content type="html"><![CDATA[<h1>Background</h1><p>在深入探讨 RAG 架构之前，可以先回顾一下前篇，在构建 RAG 系统的最新研究里提出的<a href="/Theses/THESIS-Seven-Failure-Points-When-Engineering-a-RAG-System/">七个常见的失败的地方</a>。</p><p>这里简单罗列一下，分别是：</p><ul><li>Missing Content</li><li>Missed the Top Ranked Documents</li><li>Not in Context - Consolidation strategy Limitations</li><li>Not Extracted</li><li>Wrong Format</li><li>Incorrect Specificity</li><li>Incomplete</li></ul><h1>How to</h1><p>下面的 RAG 系统架构图可以帮助我们了解每个组件在系统中的位置以及其具体使用方式。</p><p><img src="//s3.mindex.xyz/blog/Courses/a873343d21d57c34bef56b4fa638119e.gif" alt=""></p><p>接下来我们将详细探讨每个组件的设计需求和作用，以及建立这些组件的最佳实践。</p><h2 id="User-authentication">User authentication</h2><p>一切从这里开始 —— 我们系统的第一环节！</p><p>在用户开始与聊天机器人交互之前，我们需要出于多种原因进行用户身份验证。身份验证不仅可以保证系统的安全，还能提供个性化的服务，这对于企业级系统来说是至关重要的。</p><h3 id="Access-Control">Access Control</h3><p>认证机制确保只有经过授权的用户才能访问系统。它有助于控制谁可以与系统进行交互，以及他们可以执行哪些操作。</p><h3 id="Data-Security">Data Security</h3><p>保护敏感数据的重要性不言而喻。通过用户认证，我们可以防止未经授权的人员访问机密信息，从而避免数据泄露和非法数据操作。</p><h3 id="User-Privacy">User Privacy</h3><p>认证机制能确保只有用户本人可以访问其个人信息和账户详情，从而保护了用户的隐私。这一点对于建立用户的信任非常关键。</p><h3 id="Legal-Compliance">Legal Compliance</h3><p>很多地区和行业都有相关的法规，要求各类组织实行严格的用户认证制度，以保护用户的数据和隐私。遵循这些规定，可以帮助避免法律纠纷和可能的罚款。</p><h3 id="Accountability">Accountability</h3><p>认证机制能保证行为的责任归属，因为它将系统内的各种操作与特定的用户账户关联起来。这种机制对于审计和追踪用户活动非常重要，可以帮助我们识别和处理任何安全事件或者可疑行为。</p><h3 id="Personalization-and-Customization">Personalization and Customization</h3><p>身份验证的功能在于让系统能够识别每一个独立的用户，从而实现对用户体验的个性化和定制化。这其中可能包括为用户量身定制的内容、偏好设置和个人设置。</p><p>利用 <a href="https://www.youtube.com/watch?v=vqAirwfYgrY">AWS Cognito</a>，<a href="https://www.youtube.com/watch?v=vBUk293QSKY">Firebase Authentication</a> 这类服务，你可以轻松地在移动设备和网页应用中实现用户的注册和身份验证。</p><h2 id="Input-guardrail-输入护栏">Input guardrail 输入护栏</h2><p>我们必须防止用户输入可能含有有害信息或私人信息的内容。</p><p>最近的研究显示，<a href="https://llm-attacks.org/">Jailbreak LLMs</a> 其实并不困难。在这种情况下，输入护栏就显得尤为重要了。接下来，我们一起来看看在哪些场景下我们需要使用护栏。</p><h3 id="Anonymization">Anonymization</h3><p>输入防护机制可以将个人身份信息（Personal Identifiable Information, PII）如姓名、地址或联系方式进行匿名化处理或者删除。这样做有助于保护用户隐私，防止恶意行为导致敏感信息的泄露。</p><h3 id="Restrict-substrings">Restrict substrings</h3><p>禁止使用可能被利用进行 SQL 注入、跨站脚本（XSS）或其他类型注入攻击的特定字符序列或模式，能够防止安全漏洞的产生或者阻止不良行为的发生。</p><h3 id="Restrict-topics">Restrict topics</h3><p>为了限制与可能不适当、冒犯或违反社区规定的特定主题相关的讨论或输入，对涉及仇恨言论、歧视或者不适当的内容进行筛选是非常重要的。</p><h3 id="Restrict-code">Restrict code</h3><p>我们必须防止注入可执行代码，因为这可能会破坏系统的安全，或者引发所谓的“代码注入攻击”。</p><h3 id="Restrict-language">Restrict language</h3><p>我们需要确保文本输入是用正确的语言或脚本编写的，以避免在处理过程中产生潜在的误解或错误。</p><h3 id="Detect-prompt-injection">Detect prompt injection</h3><p>我们需要采取措施，防止有人试图注入误导性或有害的提示，这些提示可能会操纵系统，或者以我们无法预料的方式影响大语言模型的行为。</p><h3 id="Limit-tokens">Limit tokens</h3><p>我们需要对用户输入的Token或字符数量设定一个上限，这样可以防止系统资源被耗尽，同时也能防止所谓的“拒绝服务攻击”。</p><h3 id="Detect-toxicity">Detect toxicity</h3><p>我们需要采取措施，比如实施&quot;毒性过滤器&quot;，这样就能识别并阻止那些包含有害或者辱骂性语言的输入。</p><p>为了保护你的 RAG 系统不受这些问题的影响，你可以使用 Meta 公司的 <a href="https://towardsdatascience.com/safeguarding-your-rag-pipelines-a-step-by-step-guide-to-implementing-llama-guard-with-llamaindex-6f80a2e07756">Llama Guard</a> 工具。你既可以自己来托管这个工具，也可以选择使用像 <a href="https://aws.amazon.com/blogs/machine-learning/llama-guard-is-now-available-in-amazon-sagemaker-jumpstart/">Sagemaker</a> 这样的托管服务。但是，请不要期待它能完美地检测出所有的有毒内容。</p><h2 id="Query-rewriter">Query rewriter</h2><p>当用户的提问成功通过了我们设置的防护机制后，我们就会把这些问题交给一个叫做“问题重塑器”的工具。有时候，用户提出的问题可能模糊不清，或者我们需要更多的上下文信息才能更好地理解他们的真正意图。这时，“问题重塑”就能派上用场。这项技术的主要作用是转换和优化用户的问题，使其更清晰、更精准，更能反映出用户的真实需求。下面，我们就来介绍一些最常用的“问题重塑”技巧。</p><h3 id="Rewrite-based-on-history">Rewrite based on history</h3><p>在这种方法中，系统会借助用户的提问历史来理解对话的背景，并优化后续的提问。我们来看一个关于信用卡咨询的例子。</p><p>提问历史：</p><p>“你有多少信用卡？”</p><p>“铂金卡和金卡每年需要支付费用吗？”</p><p>“比较一下两者的特性。”</p><p>我们需要根据用户的提问历史来理解对话的背景，判断出用户的意图以及各个问题之间的关联，然后生成一个符合这个对话背景的新问题。</p><p>重塑后的问题：“比较一下铂金卡和金卡的特性。”</p><h3 id="Create-subqueries">Create subqueries</h3><p>处理复杂的问题时，有时候会遇到信息检索的困难。为了解决这个问题，我们可以将一个大的问题分解成若干个更具体的小问题，这样就可以更准确地找到回答这些问题所需的相关信息。这种方法被 LlamaIndex 称为<a href="https://docs.llamaindex.ai/en/stable/examples/query_engine/sub_question_query_engine.html">&quot;子问题查询引擎&quot;</a>。</p><p>举个例子，如果我们要比较&quot;铂金信用卡和黄金信用卡的特性&quot;，系统会为每种卡片生成一个子问题，每个子问题都专注于原始问题中提到的一种卡片。</p><p>重写的子查询：</p><ul><li>“铂金信用卡有哪些特性？”</li><li>“黄金信用卡有哪些特性？”</li></ul><h3 id="Create-similar-queries">Create similar queries</h3><p>为了更大概率地找到正确的文档，我们会根据用户的输入生成类似的查询。这样做的目的是为了解决在语义匹配或词汇匹配中存在的检索限制。</p><p>比如，当用户询问信用卡的特性时，系统会产生相关的查询。我们会运用同义词、相关词语或者特定领域的知识，以生成更符合用户查询意图的问题。</p><p>生成的相似查询：</p><p>“我想了解白金信用卡” -&gt; “告诉我白金信用卡有哪些优点。”</p><h2 id="Encoder">Encoder</h2><p>当我们拥有原始查询和重构后的查询，我们会将它们转化为向量以便于检索。在构建你的 RAG 系统时，选择何种编码器可能是最重要的决策。下面我们来探讨为什么会这样，以及在选择文本编码器时需要考虑哪些因素。</p><h3 id="Leveraging-MTEB-benchmarks">Leveraging MTEB benchmarks</h3><p>如果你想全面评估编码器的能力，那么 <a href="https://huggingface.co/spaces/mteb/leaderboard">‘Massive Text Embedding Benchmark’</a>（MTEB）是你的首选参考资源。这个基准测试能够帮助你根据向量维度、平均检索性能和模型大小，进行深入的编码器选择。虽然 MTEB 提供了许多有价值的信息，但我们需要对其结果保持一定的怀疑，因为并没有一种万能的评估基准，而且模型的训练数据具体信息可能并未全部公开。</p><p>MTEB 不仅提供了对 OpenAI、Cohere 和 Voyager 等热门嵌入的性能洞察，还揭示出某些开源模型的性能与前述模型相当。然而，这些结果只能作为一个大概的参考，可能无法准确预测这些嵌入在你特定领域的具体表现。因此，<strong>在最终确定编码器选择之前，对你的数据集进行全面的评估是至关重要的</strong>，这也凸显了定制化评估方法的重要性。</p><h3 id="Custom-evaluation">Custom evaluation</h3><p>编码器不一定能够始终保持最佳表现，特别是在处理敏感信息的时候。因此，<a href="https://www.rungalileo.io/blog/mastering-rag-improve-performance-with-4-powerful-metrics">自定义的评估</a>方式在这种情况下就显得尤为关键。以下是三种进行自定义评估的方法。</p><h4 id="Evaluation-by-annotation">Evaluation by annotation</h4><p>创建专门的数据集，并通过注释的方式获取到&quot;金标签&quot;。完成注释后，可以利用诸如平均倒数排名（Mean Reciprocal Rank，MRR）和归一化折扣累积增益（Normalized Discounted Cumulative Gain，NDCG）等检索指标，来定量地评估不同编码器的性能。</p><h4 id="Evaluation-by-model">Evaluation by model</h4><p>采用和注释方式相似的数据生成流程，但是将大语言模型（LLM）或者跨编码器用作评估工具。这样可以在所有编码器之间建立相对的排名。然后，对排名前三的编码器进行人工评估，就能得到精确的性能指标。</p><h4 id="Evaluation-by-clustering">Evaluation by clustering</h4><p>尝试使用多种聚类技术，并在不同的 Silhouette 分数（这是一种评估聚类效果的指标，反映簇内向量的相似性）下分析聚类的数据量（也就是我们所说的“覆盖范围”）。你可以试验一些算法，比如 HDBSCAN，调整它们的参数以达到最佳的聚类效果。这种基于聚类的评估方法能够深入揭示数据点的分布和分组情况，从而帮助我们选择能够满足特定测评指标的编码器。</p><h3 id="Consideration-Of-Selecting-A-Text-Encoder">Consideration Of Selecting A Text Encoder</h3><p>当你在挑选编码器时，你需要在自行开发的编码器和公共可用的编码器之间做出选择。你可能会被自行开发的编码器的易用性所吸引，但是在这两种选择之间，有一些具体的取舍需要考虑。这个决定至关重要，因为它将决定你的系统的性能和响应速度。</p><h4 id="Querying-cost">Querying cost</h4><p>在语义搜索中，要确保用户体验的流畅，关键在于嵌入式 API 服务必须始终可用。OpenAI 和其他类似的服务提供商提供了可靠的 API，这就避免了你需要自行管理服务器的问题。然而，如果选择开源模型，就需要根据模型的大小和响应速度的需求，投入一定的开发工作。较小的模型（参数量最多为 110M）可以使用 CPU 实例进行部署，而更大的模型可能需要使用 GPU 来满足响应速度的要求。</p><h4 id="Indexing-cost">Indexing cost</h4><p>建立语义搜索需要对文档进行索引，这一过程需要投入一定的成本。索引和查询过程是由同一编码器完成的，因此，索引的成本大小取决于我们选择的编码器服务。为了便于服务的重置或将索引转移到其他的向量数据库，我们建议将嵌入向量单独存储起来。如果忽略了这个步骤，就可能需要重新计算这些相同的嵌入向量，这无疑会增加不必要的工作量。</p><h4 id="Storage-Cost">Storage Cost</h4><p>对于需要索引数以百万计的向量的应用来说，向量数据库的存储成本成为了一个重要的考虑因素。存储成本与向量的维度成正比，也就是说，维度越高，存储成本就越高。例如，OpenAI 的这种 1526 维的嵌入向量就会产生最大的存储成本。要估算存储成本，我们可以计算每个文档中的平均单元数（即短语或句子的数量），然后据此进行推算。</p><h4 id="Language-Support">Language Support</h4><p>如果你想让系统支持非英语的语言，你有两种选择：一是使用能够处理多种语言的编码器，二是结合使用翻译系统和专门处理英语的编码器</p><h4 id="Search-latency">Search latency</h4><p>语义搜索的延迟与嵌入的维度线性增长。为了最小化延迟，选择低维度的嵌入是更好的。</p><h4 id="Privacy">Privacy</h4><p>在诸如金融和医疗等对数据隐私要求极高的敏感领域，使用像 OpenAI 这样的服务可能会面临一些挑战。</p><h2 id="Document-ingestion">Document ingestion</h2><p>文档摄取系统负责管理数据的处理和保存。在建立索引的过程中，每一个文档都会被切分成小块，然后通过嵌入模型将这些小块转化为嵌入向量。接下来，这些原始的小块和对应的嵌入向量都会被存储在数据库中进行索引。下面，我们来详细了解一下文档摄取系统的各个组成部分。</p><h3 id="Document-parser">Document parser</h3><p>文档解析器在处理各种文档格式中显得尤为重要，它能有效地从这些文件中提取出结构化的信息。这不仅包括处理可能含有图像和表格的 PDF 文件，还有其他各种格式的文件。</p><h4 id="Document-formats">Document formats</h4><p>文档解析器需要能够熟练处理各种文档格式，比如 PDF、Word、Excel 等，以保证在处理不同类型文档时的灵活性。这包括识别和处理文档中嵌入的内容，如超链接、多媒体元素或注释，从而能够完整地展现文档的内容。</p><h4 id="Table-recognition">Table recognition</h4><p>识别并从文档中的表格提取数据对于维护信息的结构化非常关键，尤其是在报告或研究论文中。从表格中提取元数据，如表头、行列信息，有助于我们更好地理解文档的组织结构。对于这样的任务，像 <a href="https://huggingface.co/spaces/nielsr/tatr-demo">Table Transformer</a> 这样的模型可能会很有用。</p><h4 id="Image-recognition">Image recognition</h4><p>我们在文档中的图像上应用 OCR 技术，以便识别和提取文本信息，这样就可以将这些信息纳入索引，方便后续的查找和使用。</p><h4 id="Metadata-extraction">Metadata extraction</h4><p>元数据是关于文档的附加信息，不包括在主要内容中。这些信息包括作者、创建日期、文档类型、关键词等。元数据不仅提供了有价值的上下文，帮助我们更好地组织文档，还可以通过考虑元数据属性来提升搜索结果的相关性。我们可以使用自然语言处理和光学字符识别技术（NLP/OCR）来提取这些元数据，并将它们作为特殊字段与文档一起进行索引。</p><h2 id="Chunker">Chunker</h2><p>你如何对长篇文本进行分词可以决定你的词嵌入（embeddings）的质量和搜索系统的性能。如果划分得过小，可能无法回答某些问题；如果划分得过大，那么结果可能会包含多余的、无关的信息（也就是“噪声”）。你可以利用 <a href="https://www.youtube.com/watch?v=qaPMdcCqtWk">summarisation</a> 技术来减少这种噪声，同时也能降低文本的大小、编码的成本和存储的成本。</p><p>划分文本（也称为“分块”）是一个重要但常被忽视的话题。它可能需要类似于特征工程（一种从原始数据中提取有用特征的过程）的专业知识。举个例子，对于 Python 代码库的分块，可能会根据像 def/class 这样的前缀进行划分。</p><p><img src="//s3.mindex.xyz/blog/Courses/94ac6c938c06bc88b4f3511a33418754.png" alt=""></p><h2 id="Indexer">Indexer</h2><p>索引器，正如你可能已经猜到的，负责创建文档的索引，这是一种有结构的数据结构（试着快速重复说这句话三次吧…）。<br>索引器使得搜索和检索操作变得更为高效。<strong>高效的索引对于快速和准确地检索文档至关重要。</strong> 它的工作包括将文档的分块或 Token 映射到它们在文档集合中的对应位置。索引器在文档检索中执行了一些重要的任务，如创建索引以及添加、更新或删除文档。</p><p>作为 RAG 系统的核心组成部分，索引器面临着各种可以影响系统整体效率和性能的挑战和问题。</p><h3 id="Scalability-issues">Scalability issues</h3><p>随着文档数量的增长，保持索引的高效和快速变得越来越具有挑战性。当系统在处理大量文档时遇到困难，可能会引发可扩展性的问题，从而导致索引和检索的速度变慢。</p><h3 id="Real-time-index-updates">Real-time index updates</h3><p>让索引保持实时更新是一项颇具挑战性的任务，特别是在那些文件频繁增加、修改或删除的系统中。我们需要让实时 API 和索引更新机制能够无缝衔接，同时又不牺牲系统的性能，这无疑是一项持续的挑战。</p><h3 id="Consistency-and-atomicity">Consistency and atomicity</h3><p>在面对同时发生的多个文档更新或修改时，要保证操作的连贯性和不可分割性是一项复杂的任务。我们需要确保，即使在同时发生的各种改动中，索引的更新也能够保持数据的正确性。这就需要我们进行精心的设计和实施。</p><h3 id="Optimizing-storage-space">Optimizing storage space</h3><p>对大量文档进行索引可能需要大量的存储空间。如何在保证索引依然易于访问和高效响应的同时，优化存储空间，是一项持续的挑战，尤其是在需要考虑存储成本的情况下。</p><h3 id="Security-and-access-control">Security and access control</h3><p>我们必须实施适当的安全措施和访问控制，以防止未经授权的索引修改。确保只有经过授权的用户或程序才能进行CURD等操作，这样可以帮助我们保护文档库的完整性。</p><h3 id="Monitoring-and-maintenance">Monitoring and maintenance</h3><p>定期对索引器的健康状况和性能进行监控是必不可少的。要发现诸如索引失败、资源瓶颈或过时的索引等问题，我们需要完善的监控和维护流程，以保证系统能够持续稳定运行。</p><p>这些都是软件工程中一些具有挑战性但广为人知的问题，我们可以通过遵循优秀的软件设计实践来应对这些挑战。</p><h2 id="Data-storage">Data storage</h2><p>鉴于我们需要处理各种类型的数据，每种类型的数据都需要有专门的存储空间。深入理解每种存储类型的特点和使用场景是非常关键的。</p><h3 id="Embeddings">Embeddings</h3><p>数据库类型: SQL/NoSQL</p><p>将文档的嵌入信息单独存储，可以让我们在不必为整个文档集合重新计算这些嵌入信息的情况下，迅速地更新索引。而且，这种嵌入信息的存储方式同时也起到了备份的作用，即使面临系统故障或更新，也能保住那些关键信息不丢失。</p><h3 id="Documents">Documents</h3><p>数据库类型: NoSQL</p><p>把文档以原始的形式存储下来对于长期保存是至关重要的。这种原始的形式是各种处理步骤的基石，比如建立索引、解析文本和检索信息。同时，保持文档的原始形式为未来对系统的升级提供了便利，因为原始文档始终保持完好，需要时可以重新进行处理。</p><h3 id="Chat-history">Chat history</h3><p>数据库类型: NoSQL</p><p>对于 RAG 系统来说，存储聊天历史是支持其对话功能的关键。通过存储聊天历史，系统能够获取到用户过去的查询、响应和偏好信息，进而根据用户的特定上下文，对未来的交互进行个性化调整。这些历史数据是一份宝贵的资源，能够为我们的机器学习系统提供改进的研究依据。</p><h3 id="User-feedback">User feedback</h3><p>数据库类型: SQL/NoSQL</p><p>用户反馈是通过 RAG 应用中的各种交互方式系统化地收集的。在大多数大语言模型（LLM）系统中，用户可以通过点赞/点踩、星级评价和文本反馈等方式进行反馈。这些用户反馈形成了一份宝贵的信息库，它们反映了用户的体验和感受，为我们不断优化和提升系统提供了重要依据。</p><h2 id="Vector-database">Vector database</h2><p>作为 RAG 中关键的信息检索部件，向量数据库对于语义搜索的实现至关重要。然而，为了避免可能的问题，我们需要谨慎选择这个部件。在这个选择过程中，有几个关于 <a href="https://vdbs.superlinked.com/">向量数据库</a> 的因素需要我们来考量。下面，让我们一起来探讨其中的一些。</p><h3 id="Recall-vs-Latency">Recall vs. Latency</h3><p>在向量数据库中，我们需要在提高召回率（即获取相关结果的能力）与减少延迟（即返回结果的速度）之间找到平衡。不同的索引技术，如 <a href="https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#flat-indexes">Flat</a>、<a href="https://weaviate.io/blog/ann-algorithms-vamana-vs-hnsw">HNSW</a>（Hierarchical Navigable Small World）、<a href="https://www.pinecone.io/learn/series/faiss/product-quantization/">PQ</a>（Product quantization）、<a href="https://erikbern.com/2015/09/24/nearest-neighbor-methods-vector-models-part-1">ANNOY</a> 和 <a href="https://zilliz.com/learn/DiskANN-and-the-Vamana-Algorithm">DiskANN</a>，在追求速度和召回率之间做出了不同的<a href="https://ann-benchmarks.com/">trade-offs</a>。你可以通过对自己的数据和查询进行基准测试，从而做出更明智的决策。</p><h3 id="Cost">Cost</h3><p>采用云原生数据库和托管解决方案，通常是根据你的数据存储量和查询频率来收费。这种模式适合数据量大的组织，可以帮助他们规避基础设施的投入成本。在选择这种模式时，需要考虑的关键因素包括：预测数据集的增长速度，团队的技术能力，数据的敏感程度，以及理解选择云托管解决方案会带来的成本影响。</p><p>另一方面，自行托管数据库可以让组织对自己的基础设施有更多的控制权，可能还能降低一些成本。但是，这也意味着组织需要负责管理和维护这些基础设施，包括考虑其可扩展性，安全性和定期更新的问题。</p><h3 id="Insertion-speed-vs-Query-speed">Insertion speed vs. Query speed</h3><p>在数据处理中，插入速度和查询速度的平衡至关重要。你需要寻找那些能应对高速数据流插入需求的供应商。然而，对于大多数组织而言，更应优先考虑的是查询速度。在系统高负载时，你需要评估向量的插入速度和查询的延迟时间，以便做出明智的选择。</p><h3 id="In-memory-vs-On-disk-index-storage">In-memory vs. On-disk index storage</h3><p>在内存存储和硬盘存储之间做选择，需要权衡速度和成本。内存存储虽然速度快，但有些场景下可能需要存储的向量数据量超过了内存的容量。像内存映射文件这样的技术，可以在不影响搜索速度的前提下，扩大向量的存储规模。像 DiskANN 中的 Vamana 这样的新型索引技术，也承诺提供高效的超内存索引服务。</p><h3 id="Full-Text-search-vs-Vector-Hybrid-search">Full-Text search vs. Vector Hybrid search</h3><p>仅仅依赖向量搜索可能无法满足企业级应用的需求。另一方面，混合式搜索，它融合了密集型和稀疏型的方法，可能需要更多的实现工作。典型的实现方式包括：建立一个密集型向量索引，一个稀疏型的倒排索引，并加入一个重新排序的步骤。在 <a href="https://www.pinecone.io/learn/hybrid-search-intro/">Pinecone</a>, <a href="https://weaviate.io/blog/hybrid-search-fusion-algorithms">Weaviate</a> 和 <a href="https://www.elastic.co/blog/improving-information-retrieval-elastic-stack-hybrid">Elasticsearch</a> 这些工具中，我们可以通过一个名为 alpha 的参数来调整密集型元素和稀疏型元素之间的平衡。</p><p><img src="//s3.mindex.xyz/blog/Courses/c5d2cdcbacf199bedbab9dc15aacea48.png" alt=""></p><h3 id="Filtering">Filtering</h3><p>在现实的搜索场景中，我们经常需要根据元数据的特性进行筛选。虽然在搜索之前进行筛选看似直接有效，但这样可能会遗漏一些相关的搜索结果。如果筛选的属性在整个数据集中所占比例较小，那么在搜索后进行筛选可能会遇到问题。像 <a href="https://weaviate.io/developers/weaviate/concepts/prefiltering">Weaviate</a> 这样的搜索工具采用了自定义的过滤方式，它先进行预筛选，然后结合倒排索引片段和 HNSW 索引片段进行高效的语义搜索。</p><h2 id="Techniques-for-improving-retrieval">Techniques for improving retrieval</h2><p>近期的研究揭示，<a href="https://arxiv.org/abs/2302.00093">LLMs可能会被无关的信息轻易分散注意力</a>，且当上下文信息过多（如检索到的前K个文档）时，由于其注意力模式的特性，可能会<a href="https://arxiv.org/abs/2307.03172">遗失部分上下文信息</a>。因此，寻找相关且多样化的文档以优化信息检索过程变得尤为关键。接下来，我们将探讨一些已被证实能有效提升信息检索效率的技术。</p><h3 id="Hypothetical-document-embeddings-HyDE">Hypothetical document embeddings (HyDE)</h3><p>我们可以采用 HyDE 技术来应对检索性能不佳的问题，特别是在处理那些可能让信息查找变得困难的短语或不匹配的查询时。HyDE 的独特之处在于，它利用像 GPT 这样的模型生成了一些&quot;假设性文档&quot;。这些文档虽然可能包含虚构或不准确的细节，但却能捕捉到重要的模式。然后，一个智能文本编码器将这些假设性文档转化为向量嵌入。相较于直接嵌入查询，这种方式更能有效地在文档集合中找到与其相似的实际文档。</p><p>通过实验，我们发现 HyDE 的效果优于其他先进的方法，因此，它是提升 RAG 系统性能的有效工具。</p><h3 id="Query-routing">Query routing</h3><p>在处理多个索引时，查询路由显示出其优势，它能将查询精准地指向最相关的索引，从而实现有效的信息检索。这种方法通过确保每次查询都能找到最适合的索引，优化了信息检索的准确性和速度。</p><p>在企业搜索的场景中，数据从各种来源进行索引，如技术文档、产品文档、任务和代码仓库等，此时查询路由就显得尤为重要。比如，如果用户正在搜索与特定产品功能相关的信息，查询可以被精准地指向包含产品文档的索引，从而提高搜索结果的准确性。</p><h3 id="Reranker">Reranker</h3><p>当编码器的信息检索效果不尽如人意，无法提供最佳质量时，我们会使用一种名为 <a href="https://medium.com/@abul.aala.fareh/different-reranking-techniques-in-llamaindex-6a56ed1f30a3">reranker</a> 的工具来优化文档的排名。现在，一种常见的做法是在交叉编码器的环境中使用如 <a href="https://huggingface.co/BAAI/bge-large-en-v1.5">BGE-large</a> 这样的开源单一编码器 Transformer。近期，一些只使用解码器的新方法，如 <a href="https://arxiv.org/abs/2309.15088">RankVicuna</a>、<a href="https://arxiv.org/abs/2304.09542">RankGPT</a> 和 <a href="https://arxiv.org/abs/2312.02724">RankZephyr</a>，进一步提升了重新排序器的性能。</p><p>引入重新排序器确实有其优点，它能减少大语言模型在生成响应时的错误预测 ( <a href="https://www.rungalileo.io/blog/deep-dive-into-llm-hallucinations-across-generative-tasks">LLM 幻觉</a> )，同时也能提升系统在处理非专业领域问题时的泛化能力。然而，这也伴随着一些挑战。复杂的重新排序器可能会因为计算负载过大而导致响应延迟，这对于需要实时反馈的应用来说可能是个问题。此外，部署高级的重新排序器可能会消耗大量的资源，因此需要仔细权衡性能提升和资源利用之间的关系。</p><h3 id="Maximal-Marginal-Relevance-MMR">Maximal Marginal Relevance (MMR)</h3><p>MMR（最大边缘相关性）是一种设计用来提升搜索结果多样性的方法，避免结果的重复性。MMR 的关注点并不仅仅在于找到最相关的搜索结果，而是在相关性和多样性之间寻找平衡。这就像在聚会上为朋友介绍新朋友。首先，根据朋友的喜好，找到最匹配的人。然后，再找一个和前者有些不同的人。这个过程会一直持续，直到达到了预定的介绍人数。通过这种方式，MMR 确保呈现的搜索结果既丰富多样，又高度相关，尽可能地减少了重复性。</p><h3 id="Autocut">Autocut</h3><p>Weaviate 的 autocut 功能是设计用来通过检测得分接近的搜索结果群组来限制返回的搜索结果数量。它的工作方式是通过分析搜索结果的得分，并找出其中的显著变化，这种变化可能意味着搜索结果从高度相关转变为相对不那么相关。</p><p>比如，我们有一个搜索任务，返回的对象距离值如下：</p><p>[0.1899, 0.1901, 0.191, 0.21, 0.215, 0.23]。</p><p>使用 Autocut 后，我们得到的结果是：</p><ul><li>autocut-1: [0.1899, 0.1901, 0.191]</li><li>autocut-2: [0.1899, 0.1901, 0.191, 0.21, 0.215]</li><li>autocut-3: [0.1899, 0.1901, 0.191, 0.21, 0.215, 0.23]</li></ul><h3 id="Recursive-retrieval">Recursive retrieval</h3><p><img src="//s3.mindex.xyz/blog/Courses/98f56b7139fa65b965494ac3f17644fc.png" alt=""></p><p>递归检索，也被称为“小块到大块”的检索技术，它在检索过程中处理小的文本块，同时返回大的父级上下文供语言模型进行信息整合。小的文本块可以提高检索的精确度，而大的文本块则为语言模型提供了更丰富的上下文信息。这种顺序化的过程首先关注那些信息密集的小单位，以提高检索的准确性，然后将这些小单位有效地与它们的大的父级上下文关联起来，以便进行信息整合。</p><h3 id="Sentence-window-retrieval">Sentence window retrieval</h3><p>检索过程选取一个句子，并返回该句子所在的一段上下文。这种基于句子的上下文检索方式（<a href="https://docs.llamaindex.ai/en/latest/examples/node_postprocessor/MetadataReplacementDemo.html">Sentence window retrieval</a>）确保我们得到的信息不仅准确，而且与其上下文紧密相关，从而提供了关于主要句子周围的全面信息。</p><h2 id="Generator">Generator</h2><p>既然我们已经讨论了所有的检索组件，那么接下来我们来谈谈生成器。它需要我们仔细考虑和权衡，主要是在自我托管的推理部署和私有 API 服务之间做出选择。这本身是一个非常大的话题，我会尽量简明扼要地介绍，避免让你感到信息过载。</p><h3 id="API-considerations">API considerations</h3><p>在评估用于大语言模型（LLMs）的 API 服务器时，我们必须优先考虑那些能确保顺畅集成和强大性能的功能。一个设计精良的 API 不仅应能轻松启动各种流行的大语言模型，还需要考虑到诸如生产环境的准备就绪、安全防护，以及对生成内容的<a href="https://www.rungalileo.io/blog/5-techniques-for-detecting-llm-hallucinations">幻觉检测</a>等关键因素。值得一提的是，<a href="https://github.com/huggingface/text-generation-inference">HuggingFace 的 TGI 服务</a>就很好地实现了这些原则，它提供了一套全面的功能集。接下来，让我们一起来理解一下大语言模型服务器中最常用且重要的一些功能。</p><h4 id="Performance">Performance</h4><p>一个高效的 API 必须优先考虑性能，以满足各种用户的需求。Tensor 并行性（Tensor parallelism）是一个显著的特性，它能够在多个 GPU 上加速推理过程，从而提高整体的处理速度。此外，连续批处理（continuous batching）的设计可以处理更多的请求，从而提高了系统的总体吞吐量，使系统响应更快，扩展性更强。引入了量化技术（quantization），特别是 bitsandbytes 和 GPT-Q，这进一步优化了 API，使其在各种使用场景中都能提供更高的效率。利用优化过的 Transformer（Transformer）代码，可以在最常用的架构上顺利进行推理，确保了系统的高效运行。</p><h4 id="Generation-quality-enhancers">Generation quality enhancers</h4><p>要提升生成内容的质量，API 需要融入能够改变输出结果的特性。其中，logits 处理器（包括温度缩放（temperature scaling）、top-p、top-k 以及重复惩罚（repetition penalty）等功能）让用户可以按照自己的需求定制输出结果。另外，停止序列（stop sequences）赋予用户对生成过程的控制权，使他们能够更好地管理和优化内容生成的过程。对于检测生成内容是否偏离实际（幻觉检测，hallucination detection）非常重要的 log 概率，作为另一层优化手段，能确保生成的内容与预期的上下文保持一致，避免产生误导性的信息。</p><h4 id="Security">Security</h4><p>API 的安全性至关重要，尤其是在处理大语言模型（LLMs）和企业级应用场景时。Safetensors 的权重加载功能（Safetensors weight loading）就是一个关键的特性，它可以防止未经授权的篡改模型参数，从而确保模型安全地部署。此外，引入水印技术（watermarking）也增强了安全性，使得在使用大语言模型的过程中可以进行追踪和负责任管理。</p><h4 id="User-experience">User experience</h4><p>在提升用户体验方面，Token 流式传输显得至关重要，它能实现无缝的交互效果。通过运用服务器发送事件（Server-Sent Events, SSE）技术，我们可以优化 Token 的流式传输，从而提升 API 的实时反馈效能，使得用户获得更流畅、更富交互性的使用体验。这种方式保证了用户能够分步接收到 AI 生成的内容，从而提升了大语言模型（LLM）的整体用户参与度和易用性。</p><h3 id="Self-hosted-inference">Self-hosted inference</h3><p>如果我们选择自行托管推理服务，就需要在像 AWS、GCP 或 Azure 这样的云服务平台上部署大语言模型（LLM）。在此过程中，服务器的选择（例如 TGI、Ray 或 FastAPI）变得至关重要，因为这将直接影响到系统的性能和成本。在选择过程中，我们需要考虑到计算效率、部署的便利性，以及所选服务器与大语言模型之间的兼容性。</p><p>衡量大语言模型（LLM）的推理性能是非常关键的，像 ‘<a href="https://github.com/ray-project/llmperf-leaderboard">Anyscale 的 LLMPerf 排行榜</a>’ 这样的评比工具就显得无比重要。它根据关键的性能指标，如首个 Token 的响应时间（TTFT），Token 之间的延迟（ITL）和成功率，来对提供推理服务的公司进行排名。对于评估托管模型的各种特性，负载测试和<a href="https://www.rungalileo.io/blog/mastering-rag-8-scenarios-to-test-before-going-to-production">正确性测试</a>都是必不可少的。</p><p>在新的研究方法中，<a href="https://predibase.com/blog/lorax-the-open-source-framework-for-serving-100s-of-fine-tuned-llms-in">Predibase 的 LoRAX</a> 提出了一种创新的方式，能高效地运行经过微调的大语言模型（LLM）。它成功地解决了如何利用共享的 GPU 资源来同时运行多个经过微调的模型的挑战。</p><h3 id="Private-API-services">Private API services</h3><p>像 OpenAI、Fireworks、Anyscale、Replicate、Mistral、Perplexity 和 Together 这些公司所提供的大语言模型（LLM）API 服务，为我们呈现了多种不同的部署策略。我们必须要深入理解这些服务的特性、价格模型以及<a href="https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation">大语言模型的性能指标</a>。比如说，OpenAI 采用的是基于 Token 的定价方式，它对输入和输出的 Token 进行了区分，这可能会对使用 API 的总体成本产生重大影响。当我们在进行私有 API 服务和自托管大语言模型的<a href="https://www.rungalileo.io/blog/mastering-rag-improve-performance-with-4-powerful-metrics">成本比较</a>时，必须要考虑到 GPU 的成本、使用情况以及可扩展性等问题。对于一些人来说，速度限制可能会成为一个制约因素。</p><h2 id="Prompting-techniques-for-improving-RAG">Prompting techniques for improving RAG</h2><p>有许多方法可以用来提升 RAG 的输出质量。在 ‘<a href="https://www.rungalileo.io/blog/mastering-rag-llm-prompting-techniques-for-reducing-hallucinations">Reducing Hallucinations</a>’ 这篇文章中，我们对五种最有效的方法进行了深入的探讨。许多新的技巧的性能甚至超过了 CoT （Chain-of-Thought）。而且，你还可以将这些技巧组合起来，以尽可能减少生成的幻想内容。</p><p><img src="//s3.mindex.xyz/blog/Courses/186b9f8e41cd2168627090767f4a4d57.png" alt=""></p><h2 id="Output-guardrail">Output guardrail</h2><p>输出防护机制的功能与其对应的输入防护机制相似，但专门用于检测生成内容中可能出现的问题。其主要聚焦于识别 <a href="https://www.rungalileo.io/blog/mastering-rag-8-scenarios-to-test-before-going-to-production">RAG 评估</a>过程中可能出现的虚构信息、竞品的提及，以及可能对品牌造成的潜在伤害。其目标是防止生成与品牌价值观不符的不准确或道德上有争议的信息。通过积极监控和分析生成的内容，这个防护机制确保生成的内容在事实上的准确，道德上的合规，并与品牌的指南保持一致。</p><h2 id="User-feedback-2">User feedback</h2><p>一旦生成并提供了输出，获取用户的正反馈是非常有助于我们的。用户反馈对于不断优化 RAG 系统的运行机制至关重要，这是一个持续不断的过程，而非一次性的任务。这不仅涵盖了如重新索引和实验重复运行等常规的自动化任务，也包括系统化地整合用户的反馈，以实现系统的大幅度提升。</p><p>对系统改进影响最大的手段在于积极解决底层数据中存在的问题。RAG 系统应该包含一个处理用户反馈并推动持续改进的迭代工作流程。</p><h3 id="User-interaction-and-feedback-collection">User interaction and feedback collection</h3><p>用户通过与 RAG 系统的交互，使用如 👍/ 👎或星级评分等功能来反馈他们的使用体验。这些多元化的反馈方式为我们提供了一份关于系统性能的宝贵资料，它记录了用户的实际体验和感受。</p><h3 id="Issue-identification-and-diagnostic-inspection">Issue identification and diagnostic inspection</h3><p>收集完反馈后，团队可以进行深入的分析，找出可能性能不佳的问题。这个过程包括检查检索的资源，仔细分析，以确定问题出在哪一环节——是检索过程、生成过程，还是底层数据源。</p><h3 id="Data-improvement-strategies">Data improvement strategies</h3><p>一旦发现问题，尤其是那些源于数据本身的问题，团队可以策略性地制定提升数据质量的计划。这可能包括修复不完整的信息，或者重新整理结构混乱的内容。</p><h3 id="Evaluation-and-testing-protocols">Evaluation and testing protocols</h3><p>在进行了数据改进后，系统需要对之前性能不佳的查询进行<a href="https://www.rungalileo.io/blog/mastering-rag-8-scenarios-to-test-before-going-to-production">严格的重新评估</a>。这些评估的结果可以被系统化地融入到测试套件中，以确保我们能够持续地根据实际交互进行审查和优化。</p><p>通过让用户积极参与这个全面的反馈循环，RAG 系统不仅能够解决通过自动化流程发现的问题，还能够充分利用用户的丰富体验。</p><h2 id="Observability">Observability</h2><p>构建一个 RAG 系统的工作并不仅仅在于将其投入生产。即使我们已经设置了强大的防护措施并且有高质量的<a href="https://www.rungalileo.io/blog/optimizing-llm-performance-rag-vs-finetune-vs-both">微调</a>数据，但是一旦模型投入生产，就需要进行持续的监控。对于生成式 AI 应用来说，除了常规的度量指标，如延时和成本，还需要对大语言模型进行<a href="https://docs.rungalileo.io/galileo/llm-studio/llm-monitor">特定的观察</a>，以便检测和纠正诸如产生不真实的输出、处理超出其训练领域的查询，以及链式处理出现的问题等。接下来，让我们来了解一下大语言模型观察的关键因素。</p><h3 id="Prompt-analysis-and-optimization">Prompt analysis and optimization</h3><p>识别与输入提示相关的问题，并利用实时的生产数据进行迭代改进，使用强大的评估机制来检测并处理像 AI 产生不真实的“幻觉”这样的问题。</p><h3 id="Traceability-in-LLM-applications">Traceability in LLM applications</h3><p>从 Langchain 和 LlamaIndex 等常见框架中获取大语言模型（LLM）的运行轨迹，以便对输入提示和处理步骤进行调试。</p><h3 id="Information-retrieval-enhancement">Information retrieval enhancement</h3><p>对 RAG 参数进行故障排查和评估，以优化对大语言模型（LLM）性能至关重要的信息提取过程。</p><h3 id="Alerting">Alerting</h3><p>如果系统运行出现异常，比如错误增多、响应延迟增高或者 AI 产生不真实的“幻觉”，你将会收到警报。</p><p>实时监控是观察生产环境中应用程序性能、运行状态和整体健康状况的关键。要密切关注服务等级协议（SLA）的执行情况，并设置警报系统，以便及时处理任何偏离正常的情况。通过分析使用模式和资源消耗，有效地跟踪运行大语言模型（LLM）应用的相关成本，以助你进行成本优化。</p><h2 id="Caching">Caching</h2><p>对于规模化运营的公司来说，成本可能会成为一个阻碍。在这种情况下，缓存是一种极好的节省资金的策略。缓存的过程包括将输入提示和对应的回应存储在数据库中，以便在后续使用时进行检索。这种策略性的缓存机制使得大语言模型（LLM）应用具备三个独特的优势，可以更快速、更经济地产生响应。</p><h3 id="Enhanced-production-inference">Enhanced production inference</h3><p>缓存技术能够使生产过程中的模型预测更快速、更经济。通过使用已缓存的响应，某些查询请求可以实现近乎零延迟，从而优化了用户的使用感受。</p><h3 id="Accelerated-development-cycles">Accelerated development cycles</h3><p>在开发阶段，缓存的应用极大地方便了我们，因为它避免了我们对于同样的提示反复调用 API 的必要。这使得开发周期变得更快、更省成本。</p><h3 id="Data-storage-2">Data storage</h3><p>拥有一个全面存储所有提示的数据库，可以极大地简化大语言模型的微调过程。借助存储的提示-回应对，我们能更高效地基于已积累的数据进行模型优化。</p><p>如果你真的想要提升效率，可以使用 <a href="https://github.com/zilliztech/GPTCache">GPTCache</a> 来为精确匹配和相似匹配实现缓存。它提供了一些重要的指标，如缓存命中率、延迟和召回率，这些指标能够帮助我们了解缓存的性能，从而进行持续的优化，确保达到最佳的效率。</p><h2 id="Multi-tenancy-多租户">Multi-tenancy 多租户</h2><p>在 SaaS 模型中，经常需要处理多租户的情况，这就需要我们在便捷性和保护用户隐私之间找到平衡。对于 RAG 系统的多用户环境，我们的目标是打造一个既能高效检索信息，又能尊重每个用户数据隐私的系统。简单来说，就是要保证每个用户与系统的交互都是独立的，确保系统只处理与当前用户相关的信息。</p><p>实现这样的多用户环境的一种简单方法就是利用元数据。当我们向系统添加文档时，我们会在元数据中加入特定的用户信息。这样，每个文档就与特定的用户建立了关联。当用户进行检索时，系统会利用这些元数据进行过滤，只展示与当前用户相关的文档。然后，系统会进行智能检索，找到对当前用户最重要的信息。这种方式避免了不同用户的私人信息混淆，保证了每个人的数据安全和隐私。</p><p>学习<a href="https://blog.llamaindex.ai/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b">如何使用 Llamaindex 实现多租户</a>。</p><h1>Conclusion</h1><p>我们应当认识到，构建一个健壮且可扩展的企业级 RAG 系统，需要精细地协调各个相互关联的组件。无论是用户认证，输入限制，查询重写，编码，文档摄取，还是像向量数据库和生成器这样的检索组件，每一步都在决定着系统的性能。</p><p>在这个 RAG 系统不断变革的领域里，我们希望这篇实用的指南能为开发者和领导者提供实际可行的见解！</p><h1>Source</h1><p><a href="https://www.rungalileo.io/blog/mastering-rag-how-to-architect-an-enterprise-rag-system">How To Architect An Enterprise RAG System</a></p>]]></content>
    
    <summary type="html">
    
      Mastering RAG.
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="RAG" scheme="https://neo1989.net/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>必读论文 · Seven Failure Points When Engineering a RAG System (2024)</title>
    <link href="https://neo1989.net/Theses/THESIS-Seven-Failure-Points-When-Engineering-a-RAG-System/"/>
    <id>https://neo1989.net/Theses/THESIS-Seven-Failure-Points-When-Engineering-a-RAG-System/</id>
    <published>2024-02-29T05:56:11.000Z</published>
    <updated>2024-03-01T02:43:07.150Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract">Abstract</h2><p>随着软件工程师在应用程序中不断引入一种名为检索增强生成（Retrieval Augmented Generation, RAG）的策略，以实现语义搜索功能，RAG 系统的应用也日趋广泛。RAG 系统的核心在于寻找与搜索问题在语义上相匹配的文档，并将这些文档交给大语言模型（LLM），如 ChatGPT，依靠 LLM 来抽取准确的答案。RAG 系统旨在：a) 减少大语言模型产生错误回应的风险，b) 为生成的回答关联上来源和参考文献，以及 c) 避免对文档进行元数据标注的必要。但是，RAG 系统也存在局限性，这些局限性源于信息检索系统本身的缺陷以及对大语言模型的依赖性。在本文中，我们通过三个不同领域：研究、教育和生物医学的案例研究，分析了 RAG 系统的失败经验。我们总结了这些案例中的教训，并提出了设计 RAG 系统时应考虑的七个潜在失败点。从我们的工作中得出两个主要的结论是：1) RAG 系统的验证只能在实际运行中才能进行，以及 2) RAG 系统的稳健性是通过不断的迭代优化而形成的，而非在最初设计时就能完全预设的。</p><h2 id="Introduction">Introduction</h2><p>大语言模型（LLMs）的最新进展，包括 ChatGPT，为软件工程师提供了构建新型人机交互（HCI）解决方案的新工具，能够帮助他们完成复杂的任务，概括文档，解答特定文件中的问题，以及创造新的内容。不过，LLMs 在处理最新知识或企业数据库中的特定领域知识方面还存在不足。目前有两种解决策略：a) 对 LLMs 进行微调（继续用特定领域的资料训练 LLM），这需要管理或提供一个经过微调的 LLM；或 b) 使用基于检索的增强生成（RAG）系统，这种系统依赖 LLMs 利用现有的（可扩展的）知识资料来生成答案。这两种方法在数据隐私与安全、可扩展性、成本、所需技能等方面各有利弊。本文将重点讨论 RAG 系统。</p><p>检索增强生成（RAG）系统提出了一个具有吸引力的解决方案来应对这一挑战。它将检索机制与大语言模型（LLMs）的生成功能相结合，能够生成与上下文相关、准确且最新的信息。RAG 系统融合了信息检索和大语言模型的生成才能。其检索部分致力于从数据仓库中寻找与用户问题相关的信息，而生成部分则利用这些检索到的信息作为背景，以产生针对用户问题的答案。RAG 系统的一个重要应用是，它使得所有非结构化的信息都能被索引和查询，这大大缩短了开发周期，无需构建知识图谱，同时也减少了数据整理和清洗的工作量。</p><p>软件工程师在构建 RAG 系统时，首先需要对获取的不同格式的领域知识进行预处理，并将其作为工件存放。接着，他们要将这些经过处理的信息储存在适当的数据仓库（例如向量数据库）中，选择或者整合一种合适的查询与工件匹配策略，对找到的工件进行排序，并通过调用大语言模型（LLMs）的 API，结合用户的查询和相关的上下文文档。目前，关于构建 RAG 系统的新技术不断涌现，但要了解这些技术如何适用于特定的应用场景以及它们的实际效果如何，还需要进一步的探索。</p><p>在本研究中，我们通过三个案例研究分享了所学到的经验和七个导致失败的关键因素。本文旨在为实践者提供参考，并为 RAG 系统的研究发展指明方向。据我们所知，这是首次就构建鲁棒 RAG 系统面临的挑战提供实证分析。随着大语言模型（LLMs）技术的不断进步，软件工程领域的专家们需要负起责任，分享如何构建基于 LLMs 的鲁棒系统的知识。这项工作对于提升 RAG 系统的鲁棒性来说，是一个重要的进展。</p><p>本项研究的主要问题包括：</p><ul><li><p>在构建 RAG 系统时，我们会遇到哪些失败的环节？(第5节) 为了探究这些潜在的失败环节，我们以 BioASQ 数据集为基础开展了一项实证研究。该实验包含了15,000篇文档和1,000组问题与答案。我们首先对所有文档建立索引，随后利用 GPT-4 运行查询并记录生成的答案。接下来，我们用 OpenAI 的评估工具对所有问题与答案进行了验证。我们对所有出现差异的案例、被标记为错误的案例以及随机选取的标记为正确的案例进行了手动审查，以此来分析并识别出现问题的模式。</p></li><li><p>当我们要构建一个 RAG 系统时，有哪些关键因素需要我们仔细考虑呢？在这里，我们将分享从三个 RAG 系统实施案例中吸取的宝贵经验。这些案例不仅展示了我们在实施过程中遇到的挑战，也揭示了我们从中获取的深刻洞察。</p></li></ul><p>这项研究带来的主要贡献包括：</p><ul><li>我们罗列了 RAG 系统中可能出现的各种失败点 (FP)。</li><li>我们分享了三个 RAG 系统实施案例的实践经验，其中两个案例的系统现正于 Deakin 大学运行。</li><li>我们根据从三个案例研究中吸取的经验，为 RAG 系统的研究指明了一条新的道路。</li></ul><h2 id="Related-Work">Related Work</h2><p>RAG是指在预训练和推理阶段，利用文档来提升大语言模型的性能。但是，要知道，训练这样的模型需要大量的计算资源、数据准备时间，因此，如果能直接使用未经训练或微调的 RAG，无疑是一种极具吸引力的方案。然而，当我们试图使用大语言模型来提取信息时，便会遭遇一些挑战，比如处理长篇文本的性能问题。</p><p>最近的一项调查显示，大语言模型在 RAG 流程中被广泛应用，包括检索器、数据生成、重写器和阅读器。我们的研究从软件工程的角度出发，旨在深入探讨工程师在实践中可能遇到的问题，以及为了实现当前最先进的 RAG 系统，需要进行哪些软件工程的研究。</p><p>近期有研究对 RAG 系统进行了基准测试，但并未关注在实施过程中可能出现的问题。软件工程研究已经探讨了 RAG 系统在代码相关任务中的应用。然而，RAG 系统的应用范围远超软件工程任务。本文从实践者的角度出发，补充了现有的研究，揭示了在实施 RAG 系统过程中可能遇到的挑战。</p><p>RAG 系统中产生的错误和失败与其他信息检索系统有许多相同之处，如 1) 缺乏查询重写的评价标准，2) 文档的重新排名，以及 3) 高效的内容概括。这些问题，我们的研究结果已经证实。而与大语言模型的语义和生成特性相关的部分，如评估事实准确性，便是 RAG 系统所独有的挑战。</p><h2 id="Retrieval-Augmented-Generation">Retrieval Augmented Generation</h2><p>随着 ChatGPT、Claude 和 Bard 等大语言模型服务的广泛应用，人们开始探索将它们作为问答系统的可能性。虽然这些系统的表现令人瞩目，但也存在两个根本性的问题：1) &quot;幻觉&quot;问题 —— 即大语言模型生成的答案看似正确，实则错误；2) &quot;无法控制&quot;问题 —— 除了通过精心设计的提示，我们无法直接指导或修改模型的输出内容。为了解决这些问题，人们设计了 RAG 系统，这是一种信息检索方法，旨在克服直接使用大语言模型所带来的限制。</p><p>RAG 的工作方式是，首先将用自然语言表达的查询问题转化为嵌入，以在大量文档中进行语义搜索。在找到相关的文档后，这些文档会被送到一个大语言模型中，由模型生成答案。如 图1 所示，RAG 系统主要包含两个步骤：一是建立索引，二是进行查询。更多的细节可以参考相关的研究调查报告。</p><p><img src="//s3.mindex.xyz/blog/Theses/7822f2a103869fb54aa6f83968687c27.png" alt="图1：创建一个检索增强生成（RAG）系统，需要进行索引和查询两个步骤。通常情况下，索引步骤在系统开发阶段完成，而查询步骤则在系统实际运行时进行。在我们的研究中，我们找到的可能导致系统失败的环节，都在图表中用红色框进行了标注。同时，所有必须完成的步骤，也都在图表中用下划线进行了标识。"></p><h3 id="Index-Process">Index Process</h3><p>在 RAG 系统中，我们采用一种称为 “嵌入” 的技术来帮助检索系统工作。“嵌入” 是一种压缩了的文档语义表示方式，可以想象成是一个由数字组成的向量。在建立索引的过程中，我们会把每个文档切分成更小的片段，然后用一个特殊的模型将这些片段转化成 “嵌入”。这些原始的片段和它们对应的 “嵌入” 会被一起存储在数据库中。在这个过程中，软件工程师需要做出一些设计决策，比如如何合理地切分文档，以及每个片段应该有多大。如果切分的片段太小，某些问题可能就无法得到完整的答案；而如果片段太大，那么生成的答案可能会包含一些无关的信息。</p><p>不同类型的文档在处理和分块策略上有不同的需求。例如，对于视频内容，我们需要一个转录系统来提取音频并在编码前将其转换为文本（详见 <a href="#Cognitive-Reviewer">Cognitive Reviewer</a> ）。此外，选择何种嵌入方式也至关重要，因为更改嵌入策略需要重新对所有分块进行索引。在选择嵌入方式时，我们应根据其在语义上检索正确答案的能力来决定。这个过程会受到分块大小、预期的问题类型、内容的结构以及应用领域的影响。</p><h3 id="Query-Process">Query Process</h3><p>查询过程是在实时运行中进行的。首先，我们将自然语言形式的问题转化为一般性的查询。为了让这个查询更具普遍性，我们使用了大语言模型，这使得我们可以在新的查询中加入更多的上下文信息，比如之前的聊天历史。然后，我们根据新的查询计算出一个嵌入，用于在数据库中寻找相关的文档。我们会使用如余弦相似度这样的相似性方法来检索最相似的 top k 个文档（向量数据库有一些技术，如倒排索引，可以加速检索过程）。直观来说，与查询在语义上更接近的块更有可能包含我们需要的答案。</p><p>检索到的文档将被重新排序，以便让含有答案的文档块尽可能地排在前面。接下来的阶段是“整合器”（Consolidator），它负责处理这些文档块。这个步骤的存在是为了解决大语言模型面临的两个主要限制：1）Token 的数量限制；2）处理速度的限制。像 OpenAI 这样的服务会对输入的文本量设定一个上限。这就限制了我们可以用于提取答案的文档块的数量，因此我们需要一个策略来精简并链接这些文档块，以便从中获取答案。这些在线服务还会限制在一定时间内可以使用的 Token 数量，这就限制了系统的响应速度。因此，软件工程师在设计 RAG 系统时需要考虑这些权衡。</p><p>在 RAG 流程的最后阶段，答案会从生成的文本中被提取出来。在这个阶段，我们需要从输入的问题中筛选出有用的信息，同时遵循一些特定的格式要求，比如把答案列成一个选项列表，然后生成最后的输出结果。要实现 RAG 系统，我们需要设计多种不同的问题和答案处理方式。这样做可以确保我们能得到和特定领域相关的问题。通过使用 LLM 从文本中实时提取答案，我们可以开发出一些新的应用领域，比如实时的问题回答系统。不过，RAG 系统的测试是非常困难的，因为我们没有现成的数据可以用来测试。我们只能通过生成一些模拟的数据，或者先行试运行系统来进行实验性的测试。</p><h2 id="Case-Studies">Case Studies</h2><p>本研究通过三个案例研究，探讨了在 RAG 系统的实施过程中可能遇到的挑战。表 1 展示了每个案例研究的概述。BioASQ 案例研究的所有脚本、数据，以及每个失败环节的示例，都已在网上公开。由于涉及到保密问题，另外两个案例研究并未包含在内。</p><p><img src="//s3.mindex.xyz/blog/Theses/920285f4374640c3cea4bf404a8cdf66.png" alt="表1：这是本文所提到的 RAG 案例研究的概要。"></p><h3 id="Cognitive-Reviewer">Cognitive Reviewer</h3><p>Cognitive Reviewer 是一款 RAG 系统，旨在帮助研究人员分析科学文档。研究人员可以设定研究问题或目标，并上传一系列相关的研究论文。接着，系统会根据设定的目标对所有文档进行排序，供研究人员进行人工审阅。此外，研究人员还可以直接向系统提出关于所有文档的问题。目前，Deakin University 的博士生们正在使用 Cognitive Reviewer 来支持他们的文献综述工作。Cognitive Reviewer 在运行时进行索引处理，并依赖于一个强大的数据处理流程来处理上传的文档，也就是说，在开发阶段无法进行质量控制。此系统还采用了一种排名算法来对上传的文档进行排序。</p><h3 id="AI-Tutor">AI Tutor</h3><p>AI 导师是一个 RAG 系统，学生可以向系统提出关于课程的问题，答案则基于学习资料。学生可以通过查看答案来源的列表来核实答案。AI 导师通过整合到 Deakin 大学的学习管理系统，对包括 PDF 文档、视频和文本文件在内的所有内容进行编码和索引。在这个编码和索引的过程中，我们使用深度学习模型 Whisper 对视频进行转录，然后将其分解成可管理的部分。AI 导师是在 2023 年 8 月到 11 月期间开发的，用于一个在 2023 年 10 月 30 日开始的拥有 200 名学生的课程试点项目。我们的目标是分享实施过程中的经验教训，并在试点项目结束时分享后续的发现。这个 RAG 流程包括一种可以简化用户查询的重写功能。我们设计了一个聊天界面，其中用户和 AI 导师之间之前的对话被用作每个问题的上下文。重写功能会考虑这种上下文，重新构造用户的查询，以解决像 “请进一步解释这个概念” 这样的模糊请求。</p><h3 id="Biomedical-Question-and-Answer">Biomedical Question and Answer</h3><p>在之前的案例中，我们主要研究的是内容规模较小的文件。为了进一步探索大规模数据的问题，我们使用了 BioASQ 数据集，构建了一个 RAG 系统。BioASQ 数据集由生物医学专家编制，包含了问题、相关文档链接和答案，答案的形式可能是yes/no、文本摘要、事实或者列表。我们从这个数据集中下载了 4017 篇开放获取的文档，并提出了 1000 个问题。所有的文档都经过了索引处理，并在 RAG 系统中进行了问题提问。接着，我们运用 OpenAI 实现的 OpenEvals 技术对生成的问题进行了评估。在所有生成的问题中，我们手动审查了 40 个问题，以及所有被 OpenEvals 标记为不准确的问题。我们发现，在这个领域，自动评估的结果通常比人类评估者更为保守。但是，需要注意的是，BioASQ 是一个特定领域的数据集，而进行评审的并非专家。也就是说，大语言模型可能在某些方面比非专家更有见解。</p><h2 id="Failure-Points-of-RAG-Systems">Failure Points of RAG Systems</h2><p>通过我们的案例研究，我们发现了一系列即将在下文中详述的问题。接下来的部分，我们将探讨这样一个研究问题：在构建 RAG 系统过程中，都有哪些可能导致失败的环节？</p><ul><li><p><strong>Missing Content</strong> 首个问题出现在，当我们提出一个无法从现有文档中找到答案的问题时。在最好的情况下，RAG 系统会回应：“对不起，我无法回答这个问题。”然而，对于那些与内容相关，但是并没有明确答案的问题，系统可能会被误导，产生错误的回答。</p></li><li><p><strong>Missed the Top Ranked Documents</strong>  问题的答案其实在文档中，但由于排名不够高，没有被返回给用户。理论上，系统会对所有文档进行排名，然后在后续步骤中使用。但实际上，系统只会返回排名 top k 的文档，其中 k 是基于性能选择的一个值。</p></li><li><p><strong>Not in Context - Consolidation strategy Limitations</strong> 虽然包含答案的文档已经从数据库中检出，但却未能被纳入用于生成答案的上下文中。这种情况通常发生在数据库返回大量文档，并进行了整合处理以提取答案的情况下</p></li><li><p><strong>Not Extracted</strong> 虽然答案确实存在于上下文中，但大语言模型却未能抽取出正确的答案。这种情况通常发生在上下文中存在过多的干扰信息或者信息之间存在矛盾的时候。</p></li><li><p><strong>Wrong Format</strong> 问题需要以特定的格式（如表格或列表）提取信息，但大语言模型并未按照这个要求执行。</p></li><li><p><strong>Incorrect Specificity</strong> 虽然用户可以从响应中获取答案，但答案可能过于笼统或者过于详细，无法满足用户的实际需求。这种情况通常出现在 RAG 系统的设计者对某个问题有特定的预期结果，比如教师对学生的期望。在这种情况下，我们需要提供的不仅仅是答案，还应包含具体的教育内容。另外，当用户不清楚如何准确提问，或者提问过于宽泛时，也可能导致返回的答案过于模糊或过于详细。</p></li><li><p><strong>Incomplete</strong> 虽然不完整的答案并不算是错误，但却可能遗漏了一些原本可以从上下文中获取的信息。比如，如果一个问题是“文档 A、B 和 C 中都涵盖了哪些关键点？”，这种情况下，将问题分开来逐一提问可能会是一个更好的策略。</p></li></ul><h2 id="Lessons-and-Future-Research-Directions">Lessons and Future Research Directions</h2><p><img src="//s3.mindex.xyz/blog/Theses/18513d54285f06b15a9a5d8fb6b3e3cb.png" alt="表2：从三个案例研究中我们汲取了宝贵的经验，这对未来RAG的实施提供了重要的参考"></p><p>我们从三个案例研究中学到的教训已在 表2 中列出。对于研究问题&quot;在构建 RAG 系统时，有哪些关键考虑因素？&quot;，我们的发现如下：根据我们的总结，我们找到了几个与 RAG 有关的潜在研究领域，具体如下：</p><h3 id="Chunking-and-Embeddings">Chunking and Embeddings</h3><p>文档分块看似简单，然而分块的质量以多种方式影响检索过程，尤其是通过影响块的嵌入，进而影响块与用户查询的相似性和匹配。有两种分块的方式：一种是基于启发式的分块（通过使用标点符号，段落结束等方式），另一种是语义分块（利用文本中的语义信息来确定块的开始和结束）。进一步的研究应探讨这两种方法之间的权衡，以及它们对关键下游过程，如嵌入和相似性匹配的影响。一个系统性的评估框架，对分块技术在诸如查询相关性和检索准确性等指标上的效果进行比较，将对这个领域有所贡献。</p><p>嵌入是另一个热门的研究领域，包括为多媒体和多模态块（如表格、图形、公式等）生成嵌入。通常在系统开发过程中或新文档被索引时，会创建一次块嵌入。查询预处理在很大程度上影响了 RAG 系统的性能，尤其是在处理负面或模糊查询时。我们需要进一步研究架构模式和方法，以应对嵌入的固有限制（匹配质量是领域特定的）。</p><h3 id="RAG-vs-Finetuning">RAG vs Finetuning</h3><p>大语言模型（LLMs）因其大量的训练数据和在发布前对模型进行的微调，被视为优秀的全球模型。但是，这些模型是通用的（可能并不了解你特定领域的具体知识），并且知识库并不是最新的（存在知识截止日期）。微调和 RAG 提供了两种可能的定制方式，每种方式都有其独特的权衡。微调需要收集内部数据集来调整和训练大语言模型。然而，你所有的数据都会被集成到模型中，你需要解决安全/隐私问题（谁能访问什么）。此外，随着基础模型本身的改进或者你获得新的数据添加到模型中，你需要再次进行微调。另一方面，RAG 系统似乎提供了一个实用的解决方案，允许你根据需要划分你的数据，并只把相关的数据片段纳入到上下文中，让大语言模型从这些上下文中生成答案。这方便了用新的文档持续更新知识，并且也给了用户对哪些数据片段可访问的控制权。然而，对于数据片段的嵌入、检索和上下文融合的最优策略，仍然是研究的热点。未来的研究应系统地比较微调和 RAG 的各种因素，包括准确性、延迟、运行成本和鲁棒性。</p><h3 id="Testing-and-Monitoring-RAG-systems">Testing and Monitoring RAG systems</h3><p>对于 RAG 系统，其软件工程的最佳实践仍在探索阶段。软件测试和测试用例的生成是其中需要进一步优化的领域。RAG 系统需要的问题和答案通常是特定于应用的，在对非结构化文档进行索引时，这些问题和答案往往无法直接获取。目前有一些新的研究尝试使用大语言模型从多个文档中生成问题。然而，如何生成与特定领域相关的、符合实际情况的问题和答案，这仍然是一个未解决的问题。</p><p>一旦获得了合适的测试数据，我们还需要质量度量标准来帮助工程师进行质量取舍。使用大语言模型的成本高昂，同时也带来了延迟问题，每次新版本的发布都会改变其性能特性。这种特性在机器学习系统中已经被研究过，但是针对基于大语言模型的系统（如RAGs）所需的适应性策略（如果有的话）尚未实施。另一种思路是将自适应系统的理念融入到 RAG 系统的监控和调整中，其他机器学习应用的初步工作已经开始尝试这种方法。</p><h2 id="CONCLUSION">CONCLUSION</h2><p>RAG 系统是一种革新性的信息检索技术，它巧妙地运用了大语言模型（LLM）。现在，软件工程师越来越多地通过实施语义搜索或参与新的代码相关任务，来与 RAG 系统进行互动。本文分享了我们在进行三个案例研究的过程中，包括对15000份文档和1000个问题的实证研究，所得到的宝贵经验与发现。这些发现为实践者提供了实施 RAG 系统时可能面临的挑战，为他们指明了方向。我们还对 RAG 系统的未来研究方向进行了探讨，包括：1) 如何进行分块和嵌入，2) RAG 与微调的关系，以及 3) 如何进行测试和监控。随着研究的深入，大语言模型将会拥有更多对工程师和研究人员有价值的新功能。本文是首次从软件工程的角度对 RAG 系统进行深入的探讨。</p><h2 id="Source">Source</h2><p><a href="https://arxiv.org/abs/2401.05856">Seven Failure Points When Engineering a Retrieval Augmented Generation System</a></p>]]></content>
    
    <summary type="html">
    
      Retrieval Augmented Generation System.
    
    </summary>
    
    
      <category term="Theses" scheme="https://neo1989.net/categories/Theses/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="RAG" scheme="https://neo1989.net/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>必读论文 · Attention Is All You Need (2017)</title>
    <link href="https://neo1989.net/Theses/THESIS-Attention-is-All-You-Need/"/>
    <id>https://neo1989.net/Theses/THESIS-Attention-is-All-You-Need/</id>
    <published>2024-02-26T09:11:53.000Z</published>
    <updated>2024-02-28T04:25:03.166Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract">Abstract</h2><p>主流的序列转换模型都建立在复杂的循环神经网络或卷积神经网络之上，它们都包含编码器和解码器。而且，最优秀的模型还会通过注意力机制（attention mechanism）将编码器和解码器连接起来。我们提出了一种新的网络架构——Transformer，它完全依赖于注意力机制，而不再需要复杂的循环和卷积过程。在两项机器翻译任务的实验中，这种模型在质量上表现出色，同时具有更高的并行性，训练时间也大大缩短。在WMT 2014年的英德翻译任务中，我们的模型达到了28.4的<a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>分数，比现有的最好成绩（包括集成模型）提高了2个以上的BLEU分数。在WMT 2014年的英法翻译任务中，我们的模型在八个GPU上训练3.5天后，创造了41.8的SOTA BLEU新纪录，这只是最好模型训练成本的一小部分。我们还证明，无论是在大规模训练数据还是有限训练数据的情况下，Transformer都能成功应用于英语句法解析任务，从而证明了其良好的泛化性能。</p><h2 id="Introduction">Introduction</h2><p>循环神经网络，尤其是长短期记忆（Long Short-Term Memory）和门控循环神经网络（Gated Recurrent Neural Networks），在序列建模和转换问题上已经成为SOTA方法，这些问题包括语言建模和机器翻译。自那时起，大量的研究工作不断尝试突破循环语言模型和编码-解码架构的技术限制。</p><p>循环模型通常会根据输入和输出序列中的符号位置进行计算。它们会生成一系列的隐藏状态 $h_t$ ，这个状态是由前一状态 $h_{t-1}$ 和当前位置 $t$ 的输入共同决定的。这种固有的顺序性使得在单个训练样例中进行并行化变得不可能，这在处理长序列时尤其关键，因为内存限制使得无法在多个样例间进行批处理。最近的研究通过使用分解技巧和条件计算，不仅大大提升了计算效率，而且在后者的情况下，模型的性能也得到了提升。然而，顺序计算的基本限制仍然无法被克服。</p><p>注意力机制已经成为各种任务中序列建模和转导模型的关键部分，它可以在不考虑输入或输出序列中的距离的情况下，模拟出数据间的依赖关系。然而，除少数情况外，这种注意力机制通常都是与循环网络一起使用的。</p><p>在这项工作中，我们提出了 Transformer，这是一种新的模型架构，它不再依赖循环网络，而是完全依赖于注意力机制来处理输入和输出之间的全局依赖关系。Transformer 允许更高效的并行处理，只需在八个 P100 GPU 上训练短短十二小时，就能达到前所未有的翻译质量。</p><h2 id="Background">Background</h2><p>‘Extended Neural GPU’，‘ByteNet’ 和 ‘ConvS2S’ 都秉承了减少顺序计算的设计理念，它们都采用卷积神经网络作为基础模块，能够并行处理所有输入和输出的隐藏表示。然而，在这些模型中，如果想要建立两个任意输入或输出位置之间的关系，所需的计算步骤会随着位置间距离的增加而增加，‘ConvS2S’ 是线性增长，而 ‘ByteNet’ 则是对数增长。这使得模型在学习远距离位置间的关系时面临更大的困难。相比之下，Transformer 将这个问题简化为固定数量的操作，尽管这样做会因为平均注意力加权位置而降低有效的分辨率，但我们在第3.2节中介绍的多头注意力机制可以有效地解决这个问题。</p><p>自注意力，有时也被称为内部注意力，是一种将单一序列中不同位置的信息进行关联，以此来计算出序列的表征的注意力机制。自注意力已经在许多任务中取得了成功，这些任务包括理解阅读内容、进行抽象的总结、理解文本的内在含义，以及学习与具体任务无关的句子表征。</p><p>端到端记忆网络是基于循环注意力机制构建的，而不是依赖于序列对齐的循环机制。这种网络已经在简洁语言的问题回答和语言模型构建任务上展现出了优秀的性能。</p><p>据我们所知，Transformer 是首个完全依赖自注意力来计算其输入和输出表示的转换模型，它并未使用任何序列对齐的 RNN 或卷积。在接下来的部分，我们将详细介绍 Transformer，阐述自注意力的重要性，并探讨它相较于 ‘ICLR (2016)’，‘Neural machine translation in linear time (2017)’ 和 ‘Convolutional seq2seq learning (2017)’ 等模型的优势。</p><h2 id="Model-Architecture">Model Architecture</h2><p>大部分具有竞争力的神经序列转换模型都采用了编码器-解码器的结构。在此结构中，编码器将一个符号表示的输入序列 $(x_1, …, x_n)$ 映射到一个连续表示的序列 $z = (z_1, …, z_n)$。给定 $z$，解码器便逐个生成输出序列 $y = (y_1, …, y_n)$ 的符号。在每一步中，模型都是自回归的，即在生成下一个符号时，会利用之前生成的符号作为额外的输入。</p><p>Transformer 模型采用了如 图1 所示的结构，其中编码器和解码器部分都采用了堆叠的自注意力机制和逐点的全连接层。</p><p><img src="//s3.mindex.xyz/blog/Theses/d64b985edbb3a6d5a521cda18f3ab35f.png" alt="图1：Transformer - 模型架构。"></p><h3 id="Encoder-and-Decoder-Stacks">Encoder and Decoder Stacks</h3><p><strong>Encoder</strong>: 编码器由 $N = 6$ 个完全相同的层组成。每一层都由两个子层构成：第一个子层是多头自注意力机制，第二个子层是简单的位置相关的全连接前馈网络。我们采用了一种设计，即每个子层的输出都会与其输入进行残差连接，然后进行层归一化处理，即 $LayerNorm(x + Sublayer(x))$，其中 $Sublayer(x)$ 是子层自身的功能。为了实现这种残差连接，模型中所有的子层以及嵌入层都会输出维度为 $d_{model} = 512$ 的结果。</p><p><strong>Decoder</strong>: 解码器的构造与编码器类似，也是由 6 个完全相同的层组成。但在每一层中，解码器除了拥有编码器的两个子层外，还增加了第三个子层，这个子层的作用是对编码器所有层的输出进行多头自注意力处理。就像编码器一样，我们在每个子层的输入和输出之间添加了残差连接，并进行层归一化处理。为了防止解码器的自注意力子层处理时，后面的位置能看到前面的信息，我们对其进行了修改。通过这种方式，加上输出嵌入向后偏移一个位置，我们可以确保在预测第 $i$ 个位置的输出时，只能使用位置 $i$ 之前的已知输出。</p><h3 id="Attention">Attention</h3><p>注意力函数可以理解为，它接收一个查询和一组键值对，然后生成一个输出结果。这个查询、键、值以及输出结果都可以被视为向量。输出结果实际上是值向量的加权和，而每个值的权重则是由查询和相应键的匹配程度决定的。</p><p><img src="//s3.mindex.xyz/blog/Theses/77f1cfcd5ed58276e8d605972e6ce520.png" alt="图2：（左图）缩放的点积注意力；（右图）多头注意力，有多个并行运行的注意力层构成"></p><h4 id="Scaled-Dot-Product-Attention">Scaled Dot-Product Attention</h4><p>我们将这种特定的注意力机制称为“缩放点积注意力”（见 图2）。输入由维度为 $d_k$ 的查询项（queries）和键（keys），以及维度为 $d_v$ 的数值（values）组成。我们首先计算各query与所有keys的点积，然后各自除以 $\sqrt{d_k}$，最后通过 softmax 函数得出各个数值的权重。</p><p>在实际应用中，我们会同时对一组查询进行注意力函数的计算，这些查询被整合成一个矩阵 Q。同样，键和值也被整合成矩阵 K 和 V。我们按照以下方式计算得出输出矩阵：<br>$$<br>Atteention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}}) V<br>$$</p><p>最常用的两种注意力机制是加性注意力和点积（乘性）注意力。点积注意力与我们所使用的算法几乎一样，只不过有一个 $\frac{1}{\sqrt{d_k}}$ 的缩放因子。加性注意力则是通过一个单隐藏层的前馈网络来计算相似度函数。虽然这两种方法在理论上的复杂度差不多，但在实际应用中，<strong>点积注意力因为能够利用高度优化的矩阵乘法代码，所以运行速度更快，也更节省存储空间</strong>。</p><p>对于较小的 $d_k$ 值，这两种注意力机制的表现类似，但是当 $d_k$ 值增大时，如果不进行缩放处理，加性注意力的表现会优于点积注意力。我们推测，<strong>对于较大的 $d_k$ 值，点积的结果可能会变得非常大，这会使 $softmax$ 函数进入到梯度极小的区域。为了抵消这种影响，我们将点积的结果乘以 $\frac{1}{\sqrt{d_k}}$ 进行缩放</strong>。</p><h4 id="Multi-Head-Attention">Multi-Head Attention</h4><p>我们发现，直接对 $d_m$ 维的键（keys）、值（values）和查询（queries）进行单一的自注意力（attention）处理不如将它们线性地投影到 $d_k$、$d_k$ 和 $d_v$ 维度更有利。这种线性投影我们会进行 $h$ 次，每次都使用不同的学习到的线性投影。然后，我们在每个投影后的键、值和查询上并行地执行自注意力函数，得到 $d_v$  维的输出值。这些输出值会被连接起来，然后再次投影，得到最终的结果，这个过程在 图2 中有详细的展示。</p><p>多头注意力（Multi-head attention）的设计使模型能够在不同的位置，同时关注不同的信息表示子空间。而如果只使用单一的注意力机制，这种能力就会被平均化操作所抑制。</p><p>$$<br>MultiHead(Q, K, V) = Concat(head_1, …, head_h) W^O \\<br>where \enspace head_i = Attention(QW_{i}^Q, KW_{i}^K, VW_{i}^V)<br>$$</p><p>在这里，所说的投影其实就是参数矩阵。其中 $W_{i}^Q \in \mathbb{R}^{d_{model} \times d_k}$，$W_{i}^K \in \mathbb{R}^{d_{model} \times d_k}$，$W_{i}^V \in \mathbb{R}^{d_{model} \times d_v}$，$W_{i}^O \in \mathbb{R}^{d_{model} \times hd_v}$。</p><p>在我们的研究中，我们采用了 $h = 8$ 个并行的注意力层，也就是我们所说的8个&quot;头&quot;。对于这8个&quot;头&quot;，我们都设定了 $d_k = d_v = \frac{d_{model}}{h} = 64$。由于每个&quot;头&quot;的数据规模缩小了，所以总的计算成本与使用全数据规模的单一注意力层差不多。</p><h4 id="Applications-of-Attention-in-our-Model">Applications of Attention in our Model</h4><p>Transformer 在三种不同的方式中使用了多头注意力：</p><ul><li><p>在“编码器-解码器注意力”层中，查询来自前一个解码器层，而记忆键和值来自编码器的输出。这使得解码器中的每个位置都能关注输入序列中的所有位置。这模仿了序列到序列模型中典型的编码器-解码器注意力机制，如 ‘Google’s neural machine translation system, (2016)’, ‘Neural machine translation by jointly learning to align and translate (2014)’, ‘Convolutional seq2seq learning (2017)’</p></li><li><p>编码器由多个自注意力层组成。在每一个自注意力层里，用于生成注意力分数的键（keys）、值（values）和查询（queries）都源自同一处，即编码器前一层的输出。这样的设计使得编码器当前层的每一个元素都能够接收并处理前一层所有元素的信息。</p></li><li><p>解码器内部的自注意力层同样使得每个位置能够接收到该位置之前所有位置的信息。为了维持解码器的自回归特性，我们必须防止信息从右向左流动。这一目标是通过在缩放点积注意力机制中进行操作实现的：我们会对softmax函数的输入进行遮蔽处理，即将那些不应该被当前位置所接收的信息的权重设置成极小值（负无穷），从而有效地切断了这些不合规则的信息连接。</p></li></ul><h3 id="Position-wise-Feed-Forward-Networks">Position-wise Feed-Forward Networks</h3><p>除了自注意力（Self-attention）子层之外，我们的编码器和解码器的每一层还包括一个独立且对每个位置都相同处理的全连接前馈神经网络。这个网络由两个全连接层（linear transformations）构成，在两层之间使用了ReLU激活函数。</p><p>$$<br>FFN(x) = max(0, xW_1 + b_1)W_2 + b_2<br>$$</p><p>尽管不同位置上的线性变换是一致的，但从一层到另一层，它们会使用不同的参数集。换一种方式来说，就像是使用了卷积核尺寸为 $1$ 的两个卷积操作。模型的输入和输出维度 $d_{model} = 512$，而内部层的维度 $d_{ff} = 2048$。</p><h3 id="Embeddings-and-Softmax">Embeddings and Softmax</h3><p>就像其他的序列转换模型一样，我们利用学习得到的嵌入技术（embeddings）将输入的Token和输出的Token转化为维度为 $d_{model}$ 的向量。我们也采用常规的线性变换学习方法和softmax函数，来把解码器的输出转换成预测下一个Token可能性的概率值。在我们的模型中，我们采用了一个与’Using the output embedding to improve language models (2016)'相似的方法，即在两个嵌入层和softmax之前的线性变换中共享相同的权重矩阵。在嵌入层，我们还会将这些权重与 $\sqrt{d_{model}}$ 相乘。</p><h3 id="Positional-Encoding">Positional Encoding</h3><p>因为我们的模型既不使用递归结构也不使用卷积操作，为了使模型能够理解序列中Token的顺序信息，我们必须提供关于Token在序列中的相对或绝对位置的信息。为了做到这一点，在编码器和解码器的最底层，我们向输入的嵌入数据中添加了“位置编码”。这些位置编码的维度（$d_{model}$）与嵌入数据的维度相同，使得它们可以直接相加。至于位置编码的选择，既有通过学习得到的，也有固定不变的多种方案。</p><p>在这项研究中，我们采用了频率各异的正弦与余弦函数：</p><p>$$<br>PE_{(pos, 2i)} = sin(\frac{pos}{1000^{2i/d_{model}}}) \\<br>PE_{(pos, 2i+1)} = cos(\frac{pos}{1000^{2i/d_{model}}})<br>$$</p><p>在这个公式中，$pos$ 表示序列中的位置，$i$ 代表维度。 换句话说，位置编码的每个维度都对应域一个正弦波形。这些波形的波长按几何数级递增，范围 从 $2\pi$ 到 $10000 \cdot 2\pi$。我们选择这种函数是基于一个假设：它能够让模型轻松地根据 Token 之间的相对位置进行自注意力学习。因为无论偏移量 k 如何变化，位置编码 $PE_{pos+k}$ 都能够用位置编码 $PE_{pos}$ 的线性组合来表示。这样一来，模型就能够通过简单的数学变换来识别和处理序列中 Token 的位置关系。</p><p>我们还试验了采用学习型的位置嵌入（learned positional embeddings）方法，结果显示两种方法得出的成果相差无几（参见 表3 第（E）行）。我们选择了基于正弦波的方法，因为这可能使模型能够处理超出训练时序列长度的数据。</p><h2 id="Why-Self-Attention">Why Self-Attention</h2><p>在这一部分，我们将自注意力层与循环层和卷积层进行了比较。这些层通常用于处理符号表示的变长序列（$x_1$, …, $x_n$）到另一个等长序列（$z_1$, …, $z_n$）的映射问题，其中 $x_i, z_i \in \mathbb{R}^d$ 空间中的向量，比如这样的映射在典型的序列转换编码器或解码器的隐藏层中非常常见。我们选择使用自注意力的动机是基于三个我们所期望的特性。</p><p>在考量每一层的运算效率时，我们首先要评估的是该层的总体计算复杂度。 其次，我们会考察计算任务中可以并行处理的部分，这部分的大小可以通过必须顺序执行的操作次数来衡量。</p><p>第三个考量因素是网络内部处理长程数据依赖时信号传递的路径长度。在众多序列转换任务中，学习这种长程依赖关系是一个核心挑战。能否有效学习这种依赖关系，关键在于信号在网络中传递的路径有多长，无论是前向传递还是后向传递。简而言之，输入与输出序列之间任意两点的路径越短，网络学习长程依赖就越容易。因此，我们也对比了由不同类型的层构成的网络中，任意两个输入和输出位置之间的最大路径长度。</p><p><img src="//s3.mindex.xyz/blog/Theses/84163f518d756c1a1ca877da25396d5b.png" alt="表1：对于不同类型的层，其最大路径长度、每个层的复杂度以及最小序列操作数量各不相同。这里，n 代表序列的长度，d 代表特征维度，k 是卷积核的大小，而 r 则是自注意力限制区域的大小。"></p><p>如 表1 所展示的，自注意力层能够通过一系列固定数量的操作，将所有位置相互连接起来，相比之下，递归层（recurrent layer）则需要 $O(n)$ 次操作。在计算复杂度上，自注意力层在序列长度 $n$ 小于其表示空间的维度 $d$ 的情况下，比递归层更为高效，而这种情况在当前机器翻译中的 SOTA 模型所使用的句子表示，如 <strong>word-piece</strong> 和 <strong>byte-pair</strong> 表示法中非常普遍。为了在处理极长序列的任务时提高计算效率，自注意力可以被限制在仅考虑输入序列中，围绕各自输出位置的大小为 $r$ 的局部邻域内。这种做法会导致最大路径长度变为 $O(n/r)$ 。我们计划在未来的研究中进一步探索这一方法。</p><p>在我们的模型中，单层卷积网络由于核宽度 $k$ 小于序列长度 $n$，不能直接将每个输入位置与输出位置完全连接起来。如果使用连续的核，要实现完全连接需要叠加大约 $n/k$ 层卷积网络；而采用膨胀卷积的话，大约需要 $log_k(n)$ 层。这样会使得网络内任意两点间的最长路径长度增加。一般来说，卷积层的计算成本比循环层要高，大概高出 $k$ 倍。不过，采用可分离卷积可以大幅降低计算复杂度，降至 $O(k · n · d + n · d^2)$。即便核宽度 $k$ 等于 n，可分离卷积的计算复杂度也仅相当于自注意力层和点对点前馈网络层相结合的复杂度，这正是我们模型所采用的策略。</p><p>另外，自注意力还有一个额外好处，那就是能够促成模型的可解释性更强。我们对模型中的注意力分布进行了详细检查，并在附录中展示并讨论了若干例子。我们发现，不仅单独的注意力头确实学会了执行不同的任务，而且许多注意力头的行为似乎还与句子的句法和语义结构密切相关。</p><h2 id="Training">Training</h2><p>本节将详细介绍我们的模型训练方案。</p><h3 id="Training-Data-and-Batching">Training Data and Batching</h3><p>我们使用了包含约450万个句子对的标准 ‘WMT 2014 English-German’ 数据集进行模型训练。这些句子通过字节对编码（byte-pair encoding）方法进行处理，形成了一个大约含有37000个 Token 的共用词汇表，用于源语言和目标语言。在英语-法语的训练中，我们采用了规模更大的 ‘WMT 2014 English-French’ 数据集，它包含了3600万句子，并且使用了包含32000个词片（word-piece）的词汇表来分割 Token。为了提高训练效率，我们按照句子的大致长度将它们分组打包，每个训练批次都包含了一组句子对，这些句子对总共大约包含25000个源语言 Token 和25000个目标语言 Token。</p><h3 id="Hardware-and-Schedule">Hardware and Schedule</h3><p>我们在一台搭载了8块 NVIDIA P100 GPUs 的计算机上对模型进行了训练。基础模型采用论文中详述的超参数，每个训练步骤耗时约0.4秒。基础模型总共训练了100,000步，持续了12小时。至于我们的大型模型（详见 表3 最后一行），每步训练时间为1.0秒。这些大型模型训练了300,000步，用时3.5天。</p><h3 id="Optimizer">Optimizer</h3><p>我们选用了 Adam 优化器，并设置参数 $β_1 = 0.9, β_2 = 0.98$ 以及 $ε = 10^{-9}$。在整个训练过程中，我们按照特定的公式调整了学习率</p><p>$$<br>l_{rate} =  d_{model}^{-0.5} \cdot min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})<br>$$</p><p>这意味着在训练的最初阶段（前 4000 步），我们会逐步增加学习速率，之后则根据训练步数增长的平方根逐渐减慢增速。这里的“逐步增加”是指学习速率会按照一个固定的规律线性上升，而“逐渐减慢增速”则是随着步数的增加，增加的幅度会越来越小。</p><h3 id="Regularization">Regularization</h3><p>在训练过程中，我们使用了三种不同的正则化手段来防止模型过拟合：</p><p><strong>Residual Dropout</strong>  在将每个子层的输出与其输入相加并进行规范化处理之前，我们会使用 Dropout 技术。此外，在编解码器的嵌入向量和位置编码相加的部分，我们也采用了 Dropout。对于基础模型，我们设置的 $P_{dropout} = 0.1$。</p><p><strong>Label Smoothing</strong> 在训练过程中，我们使用了$ε_{ls} = 0.1$的标签平滑技术。尽管这样做会增加模型的预测困难度（即提高了困惑度），使得模型的预测更加保守，但它实际上提升了模型的准确率和BLEU评分。</p><h2 id="Result">Result</h2><h3 id="Machine-Translation">Machine Translation</h3><p><img src="//s3.mindex.xyz/blog/Theses/3d4e0e9e97745a2a482c9d4068d2cc63.png" alt="表2：在将英语翻译成德语和法语的 newstest2014 测试中，Transformer 模型的表现超越了以往最先进的模型，取得了更高的 BLEU 评分，而且所需的训练成本仅为过去模型的一小部分。"></p><p>在WMT 2014的英德翻译任务上，我们的大型Transformer模型（在表2中标记为Transformer (big)）的性能超越了此前所有公布过的最好模型（包括那些集成模型），其BLEU评分高达28.4，刷新了此前的最佳成绩。该模型的具体配置详见 表3 的最后一栏。该模型的训练仅用了8个P100 GPU花费了3.5天。甚至我们的基础版模型也超越了所有先前公布的模型，而且训练成本远低于其他任何有竞争力的模型。</p><p><img src="//s3.mindex.xyz/blog/Theses/8cdbbb740de70e749ed7562ec46484d6.png" alt="表3：Transformer 模型的不同版本在英德翻译的开发测试集 newstest2013 上的表现各有差异。未列出的值与基础模型相同。所有的性能指标都是针对这个特定的测试集。我们根据字节对编码（Byte-Pair Encoding, BPE）计算的困惑度是基于pre_wordpiece，因此与通常基于单词计算的困惑度不同，不应直接比较二者。"></p><p>在 WMT 2014 的英法翻译任务中，我们的大型 Transformer 模型取得了 41.0 的 BLEU 分数，不仅超过了此前所有公开的单模型记录，而且其训练成本还不足先进模型的四分之一。这个专为英法翻译训练的 Transformer（大型）模型，采用了 $P_{dropout} = 0.1$，相较于常规的 0.3 有所降低。</p><p>在基础模型的训练中，我们采用了一个特殊的方法：将最后 5 次保存的模型参数（每 10 分钟保存一次）进行平均，以此得到最终的单一模型。而对于大型模型，我们则平均了最后 20 次的保存点。在模型推理时，我们使用了大小为 4 的 beam search 技术，并设置了序列长度惩罚因子（$α = 0.6$），这些超参数都是在开发数据集上通过多次试验确定的。此外，我们把模型生成文本的最大长度限制在 $输入文本长度 + 50 个词$ 以内，不过如果能够提前得出结论，我们也会尽早结束生成过程。</p><p>表2 汇总了我们的研究成果，并把我们的翻译质量和训练成本与文献中报道的其他模型架构做了对比。我们估计了训练一个模型所需要的浮点运算量，这是通过计算训练时间、使用的 GPU 数目以及每个 GPU 的单精度浮点运算能力的平均值来得出的。</p><h3 id="Model-Variations">Model Variations</h3><p>为了评判 Transformer 各部分的重要性，我们对基础模型进行了多种修改，并在开发集 newstest2013 上测试了这些改动对英译德翻译性能的影响。我们采用了之前章节描述的 Beam Search 策略，但没有进行 checkpoint 的平均处理。相关的结果展示在 表3 中。</p><p>在 表3 的行 (A) 中，我们调整了 Transformer 中的注意力机制的头数 (attention heads) 和对应键 (key) 与值 (value) 的维数，正如<a href="#Multi-Head-Attention">Multi-Head Attention</a> 节所详述，同时确保整体的计算量保持不变。虽然仅使用一个头的注意力机制的翻译质量比最优参数配置低了 0.9 BLEU 分，但过多的头数也会导致翻译质量的降低。</p><p>在表 3 的行（B）中，我们发现当减少了用于计算注意力的关键参数大小 $d_k$  后，模型的性能有所下降。这暗示了评估模型各部分之间的相互关系并非易事，可能需要一个比简单的点乘运算更为复杂的计算方式来提升效果。进一步地，从行（C）和（D）的数据可以看出，一如所料，模型规模越大，其表现通常越好；而且，使用dropout技术能有效防止模型过度学习特定数据的问题。在行（E）中，我们尝试将模型中用于捕捉不同位置信息的正弦波形位置编码替换成了通过学习得到的位置嵌入，结果显示，新模型的表现与原始模型相差无几。</p><h3 id="English-Constituency-Parsing">English Constituency Parsing</h3><p>为了探究 Transformer 是否能够适应其他种类的任务，我们对其在英文成分句法分析（一种分析句子结构的任务）上的表现进行了实验。这项任务具有其独特的挑战性：它要求输出结果具有严格的结构性，并且输出的长度通常会显著超过输入的长度。而且，传统的循环神经网络（RNN）基于序列的模型在数据量不大的情况下，还未能达到最先进（SOTA）的水平。</p><p>我们在Penn Treebank的大约有4万条WSJ数据集上训练了一个有4层且模型维度 $d_{model} = 1024$ 的 Transformer。同时，我们还尝试了半监督学习的方式，利用了大约 1700 万句的高质量语料库，包括 BerkleyParser 语料库。在 ‘WSJ only’ 训练中，我们使用了 16000 个Token的词汇表；而在半监督学习中我们使用了32000个Token的词汇表。</p><p>在开发数据集上，我们只进行了有限的实验来确定 dropout 设置（这包括自注意力和残差连接的设置，详见第 5.4 节）、学习速率和 beam 宽度，所有其他的参数设置都遵循了原先用于英语到德语翻译的基本模型。在模型进行推断时，我们把最大输出长度设定为输入长度加上 300。无论是在只使用WSJ数据集的实验还是在半监督学习的设定中，我们都采用了 21 的 beam 宽度和 0.3 的长度惩罚系数（α）。</p><p>如 表4 所示，尽管我们的模型没有进行特定任务的细致调优，但它的表现出乎意料地出色，其结果超过了除了递归神经网络语法（Recurrent Neural Network Grammar，RNN Grammar）以外的所有以往公布过的模型。</p><p><img src="//s3.mindex.xyz/blog/Theses/84f649a70e83efb38b0934d80efc1adb.png" alt="表4：Transformer 模型在英文成分句法分析方面表现出了良好的适应性和准确度。"></p><p>与基于循环神经网络（RNN）的序列对序列模型不同，Transformer 在只使用包含 4 万句子的华尔街日报（WSJ）训练集进行训练时，其性能甚至超过了 Berkeley-Parser。</p><h2 id="Conclusion">Conclusion</h2><p>在这篇文章中，我们首次介绍了 Transformer 模型，这是一个创新的序列转录模型，它完全依赖于注意力机制（attention mechanisms），用多头自注意力（multi-headed self-attention）技术替代了传统编解码器结构中常用的循环网络层。</p><p>在翻译任务中，Transformer 的训练速度显著快于那些基于循环或卷积层的结构。在 WMT 2014 的英德和英法翻译挑战中，我们的模型都达到了新的行业最高水平（SOTA）。特别是在英德翻译任务中，我们的最佳模型的表现甚至超越了此前所有公开的模型集合。</p><p>我们对注意力机制模型的未来发展前景感到非常期待，并打算将其应用于更多领域。我们的计划是将 Transformer 应用到文本以外的其他类型的数据输入和输出，例如图像、音频和视频，并且研究能够有效处理这些大型数据的局部和有限制的注意力机制。此外，我们还致力于研究如何让内容生成变得更加非线性，这也是我们的研究目标之一。</p><p>我们用来训练和评估模型的代码可以在 <a href="https://github.com/tensorflow/tensor2tensor">github</a> 找到。</p><h2 id="Source">Source</h2><p><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></p>]]></content>
    
    <summary type="html">
    
      Transformer.
    
    </summary>
    
    
      <category term="Theses" scheme="https://neo1989.net/categories/Theses/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="NLP" scheme="https://neo1989.net/tags/NLP/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>必读论文 · BERT (2019)</title>
    <link href="https://neo1989.net/Theses/THESIS-bert/"/>
    <id>https://neo1989.net/Theses/THESIS-bert/</id>
    <published>2024-02-22T13:44:02.000Z</published>
    <updated>2024-02-26T09:39:49.614Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract">Abstract</h2><p>我们要介绍的是一种新的语言表示模型，名为 BERT ，一种基于Transformer双向编码表征器。与近期的语言表现模型不同，BERT 的设计目标是从未经标注的文本中预训练深度双向表示（deep bidirectional representations），这意味着它在所有层级上同时考虑左右两侧的上下文信息。因此，只需对预训练的 BERT 模型增加一层输出层进行微调，就能构建出适用于各种任务的SOTA模型，如问题回答和语言推断，而无需对原有结构进行大量的任务相关的改动。</p><p>BERT 的设计理念十分简洁，而且在实践中展现出强大的性能。它在十一个自然语言处理任务中刷新了最佳成绩，这包括将 GLUE 的得分提升到了 80.5%（相较于之前提升了 7.7 个百分点），将 MultiNLI 的准确率提升到 86.7%（相较于之前提升了 4.6%），将 SQuAD v1.1 的问题回答测试 F1 分数提升到 93.2（相较于之前提升了 1.5 点），以及将 SQuAD v2.0 的测试 F1 分数提升到 83.1（相较于之前提升了 5.1 点）</p><h2 id="Introduction">Introduction</h2><p>预训练的语言模型已经被证明在提升许多自然语言处理任务的性能方面具有显著效果。这包括一些句子级别的任务，如自然语言推理和改写，这些任务试图通过深入分析句子来预测它们之间的关系。还有一些是Token级别的任务，如命名实体识别和问题回答，这些任务要求模型能够在Token级别给出精确的输出结果。</p><p>目前，我们有两种策略可以将预训练的语言表示应用到下游任务中：一种是基于特征的方法，另一种是微调方法。基于特征的方法，比如ELMo，会使用一种特别设计的架构，这种架构将预训练的表示作为额外的特征。而微调方法，比如生成预训练Transformer（OpenAI GPT），只引入极少的针对特定任务的参数，并通过对所有预训练参数进行微调来训练下游任务。这两种策略在预训练阶段有着共同的目标，都是通过单向语言模型来学习和掌握通用的语言表达方式。</p><p>我们认为，现有的技术方法限制了预训练模型的潜力，特别是在进行模型微调的过程中。最主要的限制在于，传统的语言模型都是单向的，这就限制了我们在预训练阶段可以选择的架构类型。举个例子，在 OpenAI GPT 中，作者们采用了一种从左到右的架构，其中每个单词（Token）只能在 Transformer 的自注意力（self-attention）层中关注到其前面的单词。对于句子级别的任务，这样的限制并不是最理想的，而且在将微调的方法应用于例如问答这类需要处理单词级别的任务时，这样的限制可能会带来很大的问题，因为在这种情况下，能够融合来自前后两个方向的上下文信息是非常关键的。</p><p>在本论文中，我们提出了一种名为BERT的方法，这是一种基于Transformer的双向编码表示，以此改进了微调方法。BERT采用了一种受Cloze(完形填空)任务启发的预训练策略，称为&quot;掩码语言模型&quot;（MLM），以解决之前提到的单向性问题。在掩码语言模型中，我们会随机遮挡输入中的部分tokens，然后尝试仅根据其上下文来预测这些被遮挡的词的原始词汇id。与传统的自左向右的语言模型预训练不同，MLM能够同时考虑左右两个方向的上下文，这使得我们可以预训练一个深度的双向Transformer。除了掩码语言模型，我们还引入了一种&quot;下一句预测&quot;的任务，以便同时预训练文本对的表示。</p><p>我们的论文主要贡献如下：</p><ul><li><p>我们强调了双向预训练在语言表示中的重要性。与 ‘Radford et al. (2018)’ 采用的单向语言模型预训练不同，BERT 通过使用掩码语言模型来实现深度双向表示的预训练。这与 ‘Peters et al.(2018a)’ 他们通过简单组合独立训练的左向右和右向左的语言模型的方法形成了鲜明对比。</p></li><li><p>我们发现，预训练的表示能够减少对大量精细设计的任务特定架构的依赖。BERT 是首个基于微调的表示模型，它在众多的句子级别和 token 级别的任务上都达到了SOTA性能，超过了许多专为特定任务设计的架构。</p></li><li><p>BERT 提升了十一个 NLP 任务的 SOTA 水平。此<a href="https://github.com/google-research/bert">链接</a>可以找到相关的代码和预训练模型。</p></li></ul><h2 id="Related-Work">Related Work</h2><p>预训练通用语言表达的研究历史悠久，本节我们将简单回顾一下这个领域中最常用的一些方法。</p><h3 id="基于特征无监督的方法">基于特征无监督的方法</h3><p>学习广泛适用的词表征是几十年来的研究热点，涵盖了非神经网络模型和神经网络模型。预训练的词嵌入是现代自然语言处理系统的重要组成部分，它们相比从零开始训练的词嵌入，可以带来显著的性能提升。在预训练词嵌入向量的过程中，我们通常采用了从左到右的语言模型预测目标，以及在上下文中区分正确和错误词汇的目标。</p><p>这些技术已经被扩展应用到更大的语言单位上，如句子或者段落的嵌入表示。在训练句子表示的过程中，以往的研究主要采用了几种方法：一种是通过目标函数对候选的下一句进行排序；另一种是在给定前一句的表示后，从左到右生成下一句的单词；还有一种是利用去噪自动编码器派生的目标函数。</p><p>ELMo及其前身在另一维度上拓展了传统的词嵌入研究。它们从左向右和右向左的语言模型中提取出对上下文敏感的特征。每个Token的上下文表示是由其左向右和右向左的表示拼接而成。ELMo通过将上下文词嵌入与特定任务的架构相结合，成功地提升了一些主要NLP任务（包括问题回答，情感分析，和命名实体识别）的SOTA水平。‘Melamud et al. (2016)’ 提出了一种新的学习上下文表示的方法，他们设计了一个任务，通过长短期记忆网络（LSTMs）来预测左右上下文中的一个词。这种方法在某种程度上与ELMo类似，它们的模型都是基于特征的，而不是深度双向的。‘Fedus et al. (2018)’ 证明使用Cloze任务可以有效地提升文本生成模型的稳定性和鲁棒性。</p><h3 id="基于微调的无监督方法">基于微调的无监督方法</h3><p>就像基于特征的方法一样，这个方向的初步工作只是从未标记的文本中预训练了词嵌入参数。</p><p>近期，一种新型的句子或文档编码器开始受到关注，它能生成具有上下文信息的 Token 表示，并且这种编码器可以通过无标签的文本进行预训练，然后在有监督的下游任务中进行微调。这种方法的优点在于，我们只需要学习少量的参数。正是因为这个优势，OpenAI 的 GPT 在 GLUE 基准的许多句子级任务中创下了前所未有的 SOTA 成果。而且，从左到右的语言建模和自动编码器等目标已经被用于这种模型的预训练。</p><h3 id="从有监督数据中进行迁移学习">从有监督数据中进行迁移学习</h3><p>一些研究已经证明，从大规模数据集的监督任务，如自然语言推理和机器翻译，进行迁移学习是有效的。在计算机视觉领域，研究也展示了大型预训练模型在迁移学习中的重要性，一种有效的策略就是微调那些使用 ImageNet 数据集预训练过的模型。</p><h2 id="BERT">BERT</h2><p>本节将详细介绍BERT及其实现过程。我们的框架分为两个步骤：预训练和微调。在预训练阶段，模型会在各种预训练任务上对无标签数据进行训练。微调阶段，首先使用预训练的参数对BERT模型进行初始化，然后利用下游任务的标签数据对所有参数进行微调。尽管所有的下游任务都使用同样的预训练参数进行初始化，但每个任务都会有自己单独的经过微调的模型。图 1 中的问答示例将在本节中作为持续进行的示例进行讲解。</p><p><img src="//s3.mindex.xyz/blog/Theses/65ee5b6bf0176abff3884c1c91aeca99.png" alt="图1: BERT 的预训练和微调过程的总览。除了输出层外，预训练和微调阶段使用的网络架构是一样的。同样的预训练模型参数被用于初始化不同下游任务的模型。在微调阶段，所有的参数都会被进一步调整。[CLS] 是一个特殊的符号，我们在每个输入样例的开始位置添加它，而 [SEP] 则是一个用于分隔不同部分的特殊 Token。"></p><p>BERT的一个独特特点是，无论是在哪种任务中，其架构都保持一致。预训练阶段的架构和最终应用于下游任务的架构之间的差别极其微小。</p><ul><li><p><strong>Model Architecture</strong></p><p>BERT 的模型架构是一种多层双向 Transformer 编码器，这种架构基于 ‘Vaswani 等人 (2017)’ 的原始实现，并已经在 ‘tensor2tensor’ 库中公开。由于 Transformer 的使用已经非常普遍，且我们的实现几乎与原始的完全一致，因此我们不会详细描述这个模型架构的背景。如需进一步了解，读者可以参考 ‘Vaswani 等人 (2017)’ 的文章，或者阅读像是 “The Annotated Transformer” 这样的优秀教程。</p><p>在我们的研究中，我们以 L 来表示层数，以 H 来表示隐藏层的大小，以 A 来表示自注意力头的数量。主要的性能结果我们将以两种模型规模来展示：‘BERT_BASE’ (L=12, H=768, A=12, 总参数量=110M) 和 ‘BERT_LARGE’ (L=24, H=1024, A=16, 总参数量=340M)。</p><p>我们选择 BERT_BASE 的原因是为了与 OpenAI GPT 的模型大小做出比较。然而，最关键的区别在于，BERT Transformer 采用了双向自注意力机制（即每个 Token 可以关注到其左右两侧的上下文），而 GPT Transformer 则采用了受限的自注意力机制（即每个 Token 只能关注到其左侧的上下文）。</p></li><li><p><strong>Input/Output Representations</strong></p><p>为了让 BERT 能够处理各种下游任务，我们设计了一种输入表示方式，这种方式可以在一个 Token 序列中清晰地表示出单个句子，也可以表示出两个有关联的句子（例如，⟨ Question, Answer ⟩）。在我们的研究中，“句子”可以是一段连续的文本片段，而不仅限于实际的语言学意义上的句子。“序列”则是指输入给BERT的 Token序列，这可能是一个独立的句子，或者是两个句子组合在一起。</p><p>我们采用了WordPiece嵌入方法，其词汇表规模为30000个Token。每个序列的第一个Token总是一个特殊的分类Token（[CLS]）。对应这个Token的最后一个隐藏状态被用作分类任务中的序列总体表示。我们会将一对句子组合成一个单独的序列。我们通过两种方式来区分这些句子。首先，我们使用一个特殊的Token（[SEP]）来分隔它们。其次，我们为每个Token添加一个训练得到的嵌入，用来标识它是属于句子A还是句子B。如图 1 所示，我们将输入的嵌入记做E，[CLS]标记的最后一个隐向量记做C，以及第 i 个输入token的最后一个隐向量记做 $ T_i $ 。</p><p>对于一个特定的token，我们通过将对应的token、分段、和位置的嵌入相加，来构建它的输入表示。图2即为这种构建方式的可视化展示。</p></li></ul><p><img src="//s3.mindex.xyz/blog/Theses/51c91502fae0b24682733616c21a384c.png" alt="图2：BERT 的输入表示方法。输入嵌入是 token 嵌入、分段嵌入和位置嵌入的总和。"></p><h3 id="Pre-training-BERT">Pre-training BERT</h3><p>我们的方法与 ‘Peters et al. (2018a)’ 和 ‘Radford et al. (2018)’ 的方法有所不同，他们采用的是传统的顺序（从左至右或从右至左）语言模型进行 BERT 的预训练。而我们则选择使用两种特定的无监督学习任务来预训练 BERT，这些任务将在本节中详细介绍。你可以在图 1 的左侧部分看到这个预训练步骤的示意图。</p><ul><li><p><strong>Task #1: Masked LM</strong></p><p>直观上看，深度双向模型必然比单向模型（从左到右或从右到左）或者是简单地将一个从左到右的模型和一个从右到左的模型拼接在一起的方法更强大。然而，遗憾的是，我们通常只能以从左到右或从右到左的方式来训练标准的条件语言模型，因为如果允许模型同时考虑两个方向的信息，每个单词就能间接地“看到”自己，这就使得模型可以轻易地在多层次的上下文中预测出目标单词。</p><p>为了训练出深度的双向表示，我们采用了一种简单的方法：随机遮蔽输入中的一部分 tokens，然后预测这些被遮蔽的 tokens。我们把这个过程称为 “遮蔽语言模型”（Masked LM，简称 MLM），在学术界，这种任务有时也被称为 “填空任务”（Cloze task）。在这个过程中，对应被遮蔽 tokens 的最后一层隐藏向量会被输入到一个 softmax 函数中，这个函数会输出一个覆盖整个词汇表的概率分布，这与传统的语言模型是一样的。在我们的所有实验中，我们会随机遮蔽每个序列中 15% 的所有 WordPiece tokens（词片 tokens）。与 “降噪自编码器”（Denoising Auto-encoders）不同的是，我们只预测被遮蔽的词，而不是重构整个输入。</p><p>尽管这种方法让我们可以得到一个预训练的双向模型，但它也带来了一个问题，那就是在预训练和微调阶段出现了不匹配，因为在微调阶段并没有使用 [MASK] token。为了解决这个问题，我们在“遮蔽”单词时，并不总是用 [MASK] token 来替换。在生成训练数据时，我们会随机选取 15% 的 token 位置进行预测。如果选中了第 i 个 token，那么我们会这样替换它：80% 的情况下用 [MASK] token 替换，10% 的情况下用随机的 token 替换，剩下 10% 的情况下保持原样。然后，我们会用交叉熵损失函数来预测 $T_i$ 的原始 token。</p></li><li><p><strong>Task #2: Next Sentence Prediction (NSP)</strong></p><p>如问题回答（QA）和自然语言推理（NLI）等许多重要的下游任务，都是基于理解两个句子之间的关系，而这种关系并非能直接通过语言模型获取。为了训练出能理解句子关系的模型，我们采用了预训练方法，针对一个“下一句预测任务”，该任务可以简单地从任何单语语料库中生成。具体来说，当我们为每个预训练样本选择句子 A 和句子 B 时，有 50% 的概率 B 是真正紧跟在 A 后面的句子（我们标记为“IsNext”），另外 50% 的概率 B 是从语料库中随机抽取的句子（我们标记为“NotNext”）。如图 1 所示，C 被用于下一句预测（NSP）。尽管它很简单，但我们在第 5.1 节证明，这种预训练方式对于 QA 和 NLI 这两种任务都大有裨益。</p><p>NSP 任务与 ‘Jernite et al. (2017)’ 和 ‘Logeswaran and Lee (2018)’ 的研究中使用的目标表示学习方法密切相关。但是，在以前的研究中，只有句子嵌入被应用到下游任务中，而 BERT 则将所有参数应用于初始化最终任务的模型参数。</p></li><li><p><strong>Pre-training data</strong></p><p>我们的预训练过程大致遵循了现有的语言模型预训练方法。在预训练语料库的选择上，我们使用了 BooksCorpus（8亿词）和英文维基百科（25亿词）。对于维基百科，我们只提取了文本段落，忽略了列表、表格和标题。与使用如’Billion Word Benchmark’这样的打乱的句子级别语料库不同，选择文档级别的语料库以提取长的连贯序列是非常关键的。</p></li></ul><h3 id="Fine-tuning-BERT">Fine-tuning BERT</h3><p>BERT的微调过程相当直观，这得益于Transformer中的自注意力机制，它使得BERT能够处理许多下游任务，无论这些任务是涉及单个文本还是文本对，只需要更改相应的输入和输出即可。对于涉及文本对的应用，常见的做法是先独立地对每段文本进行编码，然后再应用双向交叉注意力，这种做法可以参见 ‘Parikh et al. (2016)’ 和 ‘Seo et al. (2017)’ 的研究。然而，BERT采用了自注意力机制，将这两个阶段融为一体。也就是说，通过自注意力机制对连接后的文本对进行编码，实际上已经包含了两个句子之间的双向交叉注意力。</p><p>对于每项任务，我们只需将特定任务的输入和输出接入到 BERT 中，并进行全面的参数微调。在输入端，预训练时用到的句子 A 和句子 B 在不同任务中有不同的对应关系，例如：（1）在改述任务中，它们对应于一对待改述的句子；（2）在蕴含(entailment)任务中，它们对应于假设和前提两部分；（3）在问答任务中，它们对应于问题和答案段落；（4）在文本分类或序列标记任务中，它们对应于一段文本和一个空标记。在输出端，token 的表示形式会被用于 token 级别的任务，如序列标记或问答，而 [CLS] 的表示形式会被用于分类任务，如蕴含或情感分析。</p><p>相较于预训练，微调的计算资源和时间消耗更少。无论是在单个 Cloud TPU 上，还是使用 GPU，只需最多一小时或几小时，就可以重现本文中所有的结果，前提是使用完全相同的预训练模型。关于任务特定的详细内容，我们已在第4节的对应小节中进行了描述。</p><h3 id="Experiments">Experiments</h3><p>在本节中，我们将为大家展示 BERT 在 11 个自然语言处理任务上的微调成果。</p><h4 id="GLUE">GLUE</h4><p>General Language Understanding Evaluation（GLUE）基准测试是汇集了多种自然语言理解任务的一项测试。</p><p>为了对 GLUE 进行微调，我们将输入序列（无论是单个句子还是句子对）处理为第3节所描述的形式，并使用与第一个输入 token ([CLS]) 相对应的最终隐藏向量 $ C \in \mathbb{R}^{H} $ 作为整体的表达形式。微调过程中新引入的唯一参数是分类层的权重 $ W \in \mathbb{R}^{K * H}$，其中 K 是标签的数量。我们用 C 和 W 来计算常规的分类损失，如 $ log(softmax(CW^{T}))$。</p><p><img src="//s3.mindex.xyz/blog/Theses/7f0168120059ef69a4c7b0a8ea572fe6.png" alt="表1：GLUE 测试结果。"></p><p>如 表1 所示，BERT_BASE 和 BERT_LARGE 在所有任务中均表现优秀，相较于先前的最先进技术，平均准确度分别提高了4.5% 和 7.0%。值得注意的是，除了注意力掩蔽(masking)的差异，BERT_BASE 和 OpenAI GPT 在模型架构上几乎完全相同。在最大且被广泛报告的 GLUE 任务 MNLI 中，BERT 实现了4.6%的绝对准确度提升。在官方 GLUE 排行榜10中，BERT_LARGE 的得分为80.5，而在本文写作时，OpenAI GPT 的得分为72.8。</p><p>BERT_LARGE 在所有任务中都显著优于 BERT_BASE，尤其在训练数据非常少的任务中表现更为出色。我们将在第5.2节更深入地探讨模型大小对性能的影响。</p><h4 id="SQuAD-v1-1">SQuAD v1.1</h4><p>斯坦福问答数据集（SQuAD v1.1）包含了10万对由众包产生的问题和答案。给定一个问题和一个包含答案的维基百科段落，任务就是要在段落中预测出答案文本的位置。</p><p>如图 1 所示，在问答任务中，我们将输入的问题和段落打包成一个序列，其中问题使用 A 嵌入，段落使用 B 嵌入。在模型的微调阶段，我们引入了两个特殊的向量，一个叫做&quot;起始向量&quot; S，另一个叫做&quot;结束向量&quot; E。这两个向量的作用是帮助我们计算出单词i作为答案开始部分的概率。通过计算 $T_i$ 和 $S$ 的点积，然后在段落中所有单词上进行 softmax 运算转化成概率。最后我们得到的概率就表示这个单词作为答案开始部分的可能性。</p><p>公式表示为 $ P_i = \frac{e^{S · T_i}}{\sum_{j}{e^{S · T_j}}}$ 。</p><p>对于答案的结束部分，我们也使用一种类似的计算方法。这个方法会给从位置 i 到位置 j 的一段文字打分， 定义为 $S · T_i + E · T_j$，其中 $j ≥ i$ 的最高得分范围被用作预测的答案。训练目标是正确的开始和结束位置的对数似然之和。我们进行了 3 轮的微调，学习率为 5e-5，批量大小为 32。</p><p>表2 展示了领先的排行榜成绩以及顶级已发布系统的表现。SQuAD 排行榜上的最佳成绩并未提供最新的公开系统描述，并且在训练他们的系统时，可以使用任何公开的数据。因此，我们在自己的系统中通过先在 TriviaQA 上进行模型微调（fine-tuning），再在 SQuAD 上进行微调，以实现适当的数据增强策略。</p><p><img src="//s3.mindex.xyz/blog/Theses/2196b4faa7855fb90e7be4193ca78081.png" alt="表2：SQuAD 1.1 的结果。BERT ensemble是有7套系统组成，使用了不同的预训练阶段的checkpoints和微调的seeds"></p><p>我们表现最好的系统在集成模式下，F1 分数比排行榜顶部的系统高出1.5个点，作为单一系统，也高出1.3个F1分数。事实上，我们的单一 BERT 模型在 F1 分数上甚至超过了顶级的集成系统。即使没有 TriviaQA 的微调数据，我们的 F1 分数也只会下降0.1-0.4个点，但仍然以很大的优势领先于所有现有的系统。</p><h4 id="SQuAD-v2-0">SQuAD v2.0</h4><p>SQuAD 2.0 任务在 SQuAD 1.1 问题定义的基础上进行了扩展，允许在给定的段落中可能不存在短答案，这使得问题更具现实性。</p><p>为了完成这项任务，我们采用了一种简单的策略，将SQuAD v1.1的BERT模型进行了扩展。对于那些没有答案的问题，我们将其答案的开始和结束都定位在 [CLS] Token 上。也就是说，答案的开始和结束的可能位置被扩展，包括了 [CLS] Token 的位置。在进行预测时，我们会比较无答案范围的得分 ($s_{null} = S·C + E·C$) 和最佳非空答案范围的得分 ($s_{\hat{i},j} = max_{j \geq i} S·T_i + E·T_j$)。只有当 $s_{null} &gt; s_{\hat{i},j} + \tau$时，我们才会预测出一个非空的答案。这个阈值 $τ$ 是我们在开发数据集上通过最大化 F1 分数来选择的。对于这个模型，我们没有使用 TriviaQA 的数据进行训练。我们进行了两个周期的微调，使用的学习率为5e-5，批量大小为48。</p><p><img src="//s3.mindex.xyz/blog/Theses/9046b856a7ab91a6243235b6c92dede8.png" alt="表3：SQuAD 2.0 的结果。我们排除了所有包含 BERT 作为一部分的项目。"></p><p>我们在 表3 中展示了与先前的排行榜成绩和已发表的顶级研究相比的结果，但并未包括那些使用 BERT 作为组成部分的系统。我们发现，与之前最好的系统相比，F1 分数提高了 5.1。</p><h3 id="SWAG">SWAG</h3><p>Situations With Adversarial Generations (SWAG) 数据集包含了 113k 个句子配对填充示例，这些示例被用于评估基于实际常识的推理能力。给定一个句子，任务是在四个选项中选择最有可能的接续。</p><p>在对 SWAG 数据集进行微调时，我们构造了四个输入序列，每个序列包含给定的句子（A）和一个可能的延续（B）的连接。唯一引入的任务特定参数是一个向量，其与 [CLS] token 表示 C 的点积代表每个选项的得分，该得分通过 softmax 层进行归一化。我们对模型进行了3轮微调，学习率为2e-5，批量大小为16。结果在 表4 中展示。BERT_LARGE 的表现超过了 ESIM+ELMo (作者的基线) 27.1%，超过了 OpenAI GPT 8.3%。</p><p><img src="//s3.mindex.xyz/blog/Theses/8432ea7f4389fd068a343d25edca5776.png" alt="表4：SWAG 的开发集和测试集的准确率。人类的表现是通过在 SWAG 论文中报告的 100 个样本来衡量的。"></p><h3 id="Ablation-Studies">Ablation Studies</h3><p>在这一部分，我们通过消融实验，探讨了 BERT 的多个关键要素，以便更好地理解它们各自的重要性。</p><h4 id="Effect-of-Pre-training-Tasks">Effect of Pre-training Tasks</h4><p>我们通过评估两个使用与 BERT_BASE 完全相同的预训练数据、微调方案和超参数的预训练目标，展示了 BERT 的深度双向性的重要性:</p><p><strong>No NSP</strong>: 一种双向模型，该模型使用 “masked LM”（MLM）进行训练，但不包括 “next sentence prediction”（NSP）任务。</p><p><strong>LTR &amp; No NSP</strong>: 一个只考虑左侧上下文的模型，它采用的是标准的从左到右（Left-to-Right，LTR）语言模型训练方式，而不是被遮蔽的语言模型（Masked Language Model，MLM）。在微调阶段，我们也坚持了这种“只看左边”的规则，因为如果不这样做，预训练和微调阶段的处理方式就会不一致，这可能导致模型在实际任务中的表现下降。另外，这个模型在预训练阶段并没有执行“下一句预测”（Next Sentence Prediction，NSP）任务。这个模型可以直接和 OpenAI 的 GPT 进行比较，不过我们使用了更大的训练数据集，我们自己的输入数据表示方法，以及我们自己的微调策略。</p><p>我们首先研究了 NSP 任务带来的影响。在 表5 中，我们展示了去掉 NSP 在 QNLI、MNLI 和 SQuAD 1.1 上显著降低了性能。接下来，我们通过比较“No NSP”和“LTR &amp; No NSP”来评估训练双向表示的影响。LTR 模型在所有任务上的表现都比 MLM 模型差，其中在 MRPC 和 SQuAD 上的下降尤为显著。</p><p>对于 SQuAD 任务，我们可以直观地理解，一个从左到右（LTR）的模型在预测单个字符（token）时表现会很差，因为这些字符级别的隐含状态缺乏右侧的上下文信息。为了尽可能地提升这个从左到右的系统，我们在其上增加了一个随机初始化的双向长短期记忆网络（BiLSTM）。这确实在 SQuAD 任务上取得了显著的改善，但是与预训练的双向模型相比，其结果仍然相差甚远。而且，BiLSTM 在 GLUE 任务上的表现反而下降了。</p><p>我们意识到，也可以像 ELMo 那样，分别训练从左到右（LTR）和从右到左（RTL）的模型，然后将每个字符（token）的表示形式作为这两个模型的结合。但是，这种方式存在以下问题：(a) 它的成本是单个双向模型的两倍；(b) 对于像问答（QA）这样的任务，这种方式并不直观，因为从右到左的模型无法根据问题来决定答案；© 它的能力不如深度双向模型，因为深度双向模型可以在每一层都同时使用左右两侧的上下文信息。</p><h4 id="Effect-of-Model-Size">Effect of Model Size</h4><p>在这一部分，我们研究了模型规模对微调任务精度的影响。我们训练了多个BERT模型，这些模型在层数、隐藏单元和注意力头的数量上有所不同，但在超参数和训练流程上，它们与前文描述的保持一致。</p><p>如 表6 所示，我们给出了在选定的GLUE任务上的结果。在这个表格中，我们给出了在进行5次随机微调后，开发集（Dev Set）准确度的平均值。我们可以看到，模型规模更大的BERT模型能在所有四个数据集上都带来准确度的显著提升，即使是在只有3600个标注训练样本，与预训练任务大相径庭的MRPC数据集上也是如此。同时，令人惊讶的是，我们在已经相对于现有研究来说规模较大的模型基础上，还能取得如此显著的提升。例如，'Vaswani et al. (2017)'探索的最大的Transformer模型是(L=6, H=1024, A=16)，编码器的参数有100M，而我们在文献库中找到的最大的Transformer模型是(L=64, H=512, A=2)，参数有235M。相比之下，BERT_BASE模型包含了110M参数，BERT_LARGE模型包含了340M参数。</p><p><img src="//s3.mindex.xyz/blog/Theses/c46e2f0f3b153c2c018d6a32ca759608.png" alt="表6：对BERT模型大小进行消融实验。#L = 层数的数量；#H = 隐藏层大小；#A = 注意力头的数量。“LM (ppl)”是保留训练数据的被遮蔽的语言模型困惑度。"></p><p>大家都知道，如果我们增加模型的规模，将会在大规模任务，如机器翻译和语言建模，上取得持续的改进。这一点可以通过查看 表6 中展示的预留训练数据的语言模型复杂度得到证实。然而，我们认为，这是第一项有力地证明，只要模型得到了充分的预训练，即使在非常小规模的任务上，极大地增加模型规模也能带来显著改进的研究。在 ‘Peters et al. (2018b)’ 的研究中，他们针对预训练的双向语言模型大小从两层增加到四层的影响，展示了混合的结果。而 ‘Melamud et al. (2016)’ 则提到，将隐藏维度从 200 增加到 600 有所帮助，但是进一步增加到 1000 并未带来更多的改进。这两项研究都采用了基于特征的方法。我们的假设是，当模型直接在下游任务上进行微调，并且只使用极少量的随机初始化的附加参数时，即使下游任务数据非常少，任务特定的模型也能从更大，更具表达力的预训练表示中受益。</p><h4 id="Feature-based-Approach-with-BERT">Feature-based Approach with BERT</h4><p>我们迄今为止展示的所有BERT结果都是采用了微调策略，也就是在预训练模型的基础上增加了一个简单的分类层，并在具体任务上对所有参数进行了统一的微调。然而，基于特征的方法，也就是从预训练模型中提取出固定的特征，也有其独特的优点。首先，不是所有的任务都能被Transformer编码器架构轻松地表达出来，因此有时需要添加特定于任务的模型架构。其次，一种有效的计算策略是先预计算一次训练数据的复杂表示，然后在这个表示的基础上使用更经济的模型进行多次实验。</p><p>在这一部分，我们将 BERT 模型应用于 CoNLL-2003 命名实体识别 (NER) 任务，以此来对比两种不同的方法。在为 BERT 模型提供输入的过程中，我们采用了一种能够保留字母大小写的 WordPiece 模型，并且还包括了数据所能提供的最大的文档环境信息。遵循常规的做法，我们将这个任务设定为一个标记任务，但是在模型的输出部分，我们并未使用条件随机场（CRF）层。在对 NER 标签集进行 token 级别的分类时，我们选择了第一个子 token 的表示作为输入。</p><p>为了探究微调方法的影响，我们采用了一种基于特征的方法：从 BERT 的一个或多个层中提取激活值，而不对 BERT 的任何参数进行微调。这些上下文嵌入被作为输入送入一个随机初始化的两层 768 维的 BiLSTM，然后再进入分类层。</p><p><img src="//s3.mindex.xyz/blog/Theses/f31d3b1d7cf7187e0aba53f8323dbf12.png" alt="表7："></p><p>结果见 表7。BERT_LARGE 的表现与最先进的方法相媲美。表现最好的方法是将预训练的 Transformer 模型中最顶层的四个隐藏层的 token 表示进行拼接，这仅比微调整个模型的 F1 分数低 0.3。这证明了 BERT 对于微调和基于特征的方法都是有效的。</p><h2 id="Conclusion">Conclusion</h2><p>近期的研究发现，通过使用语言模型进行迁移学习，且在预训练阶段引入大量无监督数据，可以显著提升语言理解系统的性能。特别地，这种方法甚至使得资源匮乏的任务也能从深度单向模型（Deep Unidirectional Models）中获益。我们的主要贡献在于，我们将这些研究成果推广到了深度双向模型（Deep Bidirectional Models），使得同一个预训练模型能够成功应对各种各样的自然语言处理（NLP）任务。</p><h2 id="Source">Source</h2><p><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>]]></content>
    
    <summary type="html">
    
      Bidirectional Encoder Representations from Transformers.
    
    </summary>
    
    
      <category term="Theses" scheme="https://neo1989.net/categories/Theses/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="NLP" scheme="https://neo1989.net/tags/NLP/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>好好说话</title>
    <link href="https://neo1989.net/Notes/NOTE-communication/"/>
    <id>https://neo1989.net/Notes/NOTE-communication/</id>
    <published>2024-02-18T08:24:01.000Z</published>
    <updated>2024-02-18T13:52:30.585Z</updated>
    
    <content type="html"><![CDATA[<h3 id="乔哈里视窗">乔哈里视窗</h3><p>将沟通的信息比作一个窗子，它被分为4个区域：公开区、隐蔽区、盲目区、未知区，人的有效沟通就是这四个区域的有机融合。</p><p>公开区：自己知道，他人知道；往往关系越近、共同信息知晓越多的两个人公开区域会越大。<br>盲点区：自己不知道，他人知道；在很多情况下，我们都会有盲点区，自身的学识、所处的环境等都会造成我们与对方所了解的信息不对等。<br>隐秘区：自己知道，他人不知道；造成隐蔽区的原因在于我们能不好意思跟其他人说、忘了对其他人说、没沟通清楚造成误解、误以为别人知道等。<br>未知区：自己不知道，他人不知道；</p><h3 id="沟通漏斗">沟通漏斗</h3><ul><li>你心里想的，100%</li><li>你说出来的，80%</li><li>别人听到的，60%</li><li>别人听懂的，40%</li><li>别人执行的，20%</li></ul><h3 id="FIRE-接收模型">FIRE 接收模型</h3><p>首先注意到<strong>事实</strong>，并对这些事实<strong>进行解读</strong>，根据解读的结果，经历情绪的<strong>反应</strong>，期望得到想要的<strong>结果</strong></p><p>FIRE模型能帮助我们认清哪些事实，哪些是主观想法。它最大的好处就是，当你听到一个难以入耳的反馈时，它能帮助你平静理性的展开分析，从而得到一个有效的解决方案。</p><p>F-事实：确实存在的事情，具体、公正、客观、不带感情色彩。事实是真实谈话的基础<br>I-解读：事件发生，我们依据经验对事实进行解读，得出这一事实的目的或意义<br>R-反应：根据解读结果，我们会产生相应的情绪反应<br>E-结果：经历情绪反应后，我们就会期望某种结果</p><h3 id="PREP-沟通法则">PREP 沟通法则</h3><p>先说<strong>结论</strong>，再说<strong>原因</strong>，接着用<strong>事例</strong>辅助证明观点。最后重复<strong>结论</strong>，构成一次完整的表述。</p><p>P-结论：结论先行，沟通中的黄金法则。抛出明确的观点，让对方清楚接下来的谈话是围绕哪个结论展开<br>R-依据：通过有效的数据，有力的事实验证结论的可靠性。<br>E-事例：用事例引起倾听者的共情和想象<br>P-重述结论：强化双方对结论的共识</p><h3 id="STAR-叙事模型">STAR 叙事模型</h3><p>S-情境：所发生的事情的背景，在所阐述的事实中所发生的背景情况<br>T-任务：在背景环境下所承担的角色及所执行的任务，以及要达成的目标<br>A-行动：在任务当中如何去操作执行任务的，重点是行动过程<br>R-结果：付诸行动，完成任务所达到的效果，一般是可量化的指标</p><h3 id="SCQRTV-方案模型">SCQRTV 方案模型</h3><p>该模型符合人对事物的体验路径，与大脑思考策略执行的逻辑契合度极高，能够让倾听者更容易接受你的想法。</p><p>体验路径：感官 - 情感 - 思考 - 行为 - 识别<br>思考路径：是什么 - 怎么了 - 为什么 - 怎么办 - 结果如何<br>执行路径：分析 - 判断 - 推理 - 决策 - 评估</p><p>S-情境：客观地描述时间情境，明确问题<br>C-冲突：提出与现实相违背的内容的疑问<br>R-原因：分析事件的原因与动机<br>T-策略：解决问题的方法，进行决策<br>V-价值：产生价值，创造价值</p><h3 id="FFC-赞美法则">FFC 赞美法则</h3><p>赞美对方时，先说出<strong>内心的感受</strong>，然后陈述你的奇特感受和<strong>客观事实</strong>，最后将被赞美人和同类让<strong>进行对比</strong>，让对方认为就是这样</p><p>F-感受：从细节出发，说出内心的感受。<br>F-事实：陈述带给你这种感受的客观事实。<br>C-对比：与相似的人/事进行对比，突出对方的优点。</p><h3 id="RIDE-说服模型">RIDE 说服模型</h3><p>利用人们心理<strong>趋利避害</strong>的天然潜意识，通过暴露风险，阐述利益，继而引入差异性及影响，来说服他人接受你的观点</p><p>R-风险： 不采纳方案会带来的风险<br>I-利益：采纳方案带来的利益，从之前的方案抛出风险，降低对方的预期，继而引入利益点，提高预期，会使对方更好地接受<br>D-差异：事实举例说明自身建议与其他方案的差异之处<br>E-影响：方案本身所能带来的负面影响。太完美的东西反而不真实，小缺点瑕不掩瑜</p><h3 id="GROW-模型">GROW 模型</h3><p><strong>用提问代替说教</strong>，这样可以引导对方<strong>自己找到</strong>问题的解决方法。有效避免对方产生<strong>抵触情绪</strong>。</p><p>G-目标：明确要实现的目标，你想要什么？<br>R-现状：分析当前状况、聚焦目标。现在是什么情况？<br>Q-方案：找出可供选择的方案。有哪些方案？<br>W-意愿：强化意愿。你将要从什么动作开始？</p><h3 id="电梯演讲">电梯演讲</h3><p>麦肯锡认为，一般人们只能记住一二三，记不住四五六，所以凡是要归纳在3条以内。</p><p>Hook 吸引：通过现状、存在的问题、现状产生的原因、解决方案等吸引对方注意力。<br>Mutual Benefit 给利：提出具体解决方案，让对方意识到你的价值存在。<br>Call to Action 收网：指出上述方法的依据及理由，并留下联系方式。</p><h3 id="Tips">Tips</h3><p>明确双方所表达的语义是否一致，掌握好措辞；若不一致需及时了解，打破双方误解。<br>结论先行，也就是PREP核心思想。<br>阐述事情先讲背景，再讲内容。<br>在沟通之前，明确此次沟通的目的，了解所沟通对象的诉求。<br>不论沟通对象是谁，结构化表达，能够让对方更好地理解你说的话。若当面沟通无法阐述清楚，则预先打个腹稿或者以文字形式输出。<br>切莫自己一顿疯狂输出，也记得倾听对方的想法、方案。<br>区分哪些是真正的事实，哪些是对方/自己解读后的观点，基于客观事实和行为模式是寻求真相谈话的基础。<br>沟通方式、沟通模型仅仅只是招式，最重要的还是<strong>沟通的内容</strong></p>]]></content>
    
    <summary type="html">
    
      沟通和执行相差甚远的时候，多半是沟通模式出了问题
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Wisdom" scheme="https://neo1989.net/tags/Wisdom/"/>
    
      <category term="SoftSkills" scheme="https://neo1989.net/tags/SoftSkills/"/>
    
  </entry>
  
  <entry>
    <title>18 Emotional Equations</title>
    <link href="https://neo1989.net/Notes/NOTE-18-Emotional-Equations/"/>
    <id>https://neo1989.net/Notes/NOTE-18-Emotional-Equations/</id>
    <published>2024-01-30T13:50:25.000Z</published>
    <updated>2024-02-18T13:53:59.519Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Dealing-with-Difficult-Times">Dealing with Difficult Times</h2><h3 id="绝望">绝望</h3><p>绝望 = 苦难 - 意义 （Despair = Suffing - Meaning）</p><p>绝望就是当承受的苦难变得毫无意义时的结果。</p><p>在一个人的低估时期，比如弗兰克（《活出生命的意义》的作者）被投入集中营时，苦难实际上是一个常量，所以为了降低绝望感，最好是把注意力转移到寻求更多的意义上。</p><h3 id="失望">失望</h3><p>失望 = 期望 - 现实（Disappointment = Expectations - Reality）</p><p>失望是你的期望和现实之间的差距。</p><p>应对失望的方法之一是，在最初试图改变某些事情的结果时保持乐观，然后随着时间的推移，一旦力有不逮就调低期望来振作精神，以对抗潜在的负面结果。另一种应对机制是稀释现实的重要性。问问自己，这个结果对你来说究竟有多重要，真的重要吗？</p><h3 id="遗憾">遗憾</h3><p>遗憾 = 失望 + 责任感 （Regret = Disappointment +  Responsibility）</p><p>遗憾是对不幸的个人选择而导致的结果的不满。即没能力改变现实的失望，再被个人选择的责任感放大。遗憾中有期望和向往，这是一种更加成熟的情绪，失望与遗憾的主要区别就在于你的责任感。</p><h3 id="猜忌">猜忌</h3><p>猜忌 = 不信任 / 自尊 （Jealousy = Mistrust / Self-esteem）</p><p>有两个关键因素影响着人们的猜忌心理：你不信任的程度和你的自尊心。</p><p>猜忌是害怕失去原本属于自己的东西，常在爱恨情仇中出现。妒忌则是见到别人拥有你想有的东西时感到的挫折。爱情中的猜忌通常包含三个主体：你、你的爱人和你的情敌。而妒忌一般只涉及你自己和拥有你想要的东西的那个人。猜忌中隐藏着对失去的害怕而妒忌中隐藏着对得到的期待。</p><h3 id="妒忌">妒忌</h3><p>妒忌 = (傲慢 + 虚荣) / 慈悲（Envy = (Pride + Vanity) / Kindness）</p><p>妒忌和自我觉知有很大的关联。妒忌和自恋师承一脉。</p><p>一个自我觉知膨胀的人会很傲慢和虚荣。可以直接反制妒忌的美德是慈悲或者慷慨的精神，当极度妒忌某人时，注入一些慈悲心将会帮助我们淡化不满并让它转化为动力，比如让自己更努力工作去获得类似的成就，或者转变为认同，比如走出仔细，国懿仲沉浸在为他人感到开心的更加广阔的人生。</p><h3 id="焦虑">焦虑</h3><p>焦虑= 不确定性 * 无力感 （Anxiety = Uncertainty * Powerlessness）</p><p>这个方程中存在两个变量：你不知道的（不确定性）和你不能控制的（无力感）。通常二者是相互影响的：你感觉越不确定，你就会感觉越无力。</p><p>因为感觉无力会使人精神衰弱，所以这个方程运用乘法产生一个指数的结果。然而如果你能改变其中一个变量，减小到接近零，你就可以明显降低你的焦虑。</p><p>一般来说，少一点焦虑不只是会让你感觉好一点，还会让你更好地对待生活。对某件事情十分确定却感到无力去改变它时，我们是会感觉很不自在但你更多是会逆来顺受，而不会感到焦虑。同样，感觉不确定却充满力量，意味着你对任何迎面而来的失去都有应对的勇气和信心，那也意味着你的焦虑消除了。</p><h2 id="Getting-the-Most-out-of-Your-Work-life">Getting the Most out of Your Work life</h2><h3 id="使命">使命</h3><p>使命 = 快乐 / 痛苦 （Calling = Pleasure / Pain）</p><p>你越是生活在使命中，就能感受到快乐而忽略掉痛苦。<br>你越是生活在使命中，快乐就越能主宰痛苦。</p><h3 id="工作狂">工作狂</h3><p>工作狂 = 你在逃避什么？/ 你为何而活？（Workaholism = What are you running from? / What are you living for?）</p><p>工作狂是上瘾的一种形式，简单来说，各种成瘾都和我们在逃避什么相关。<br>通常我们会沉醉于哪些能够改善我们心情的事物（包括工作），一定程度上是因为我们迫切需要逃脱那些正在吞噬我们的情绪或恐惧。在你剥下任何成瘾症的表层情绪后，你会发现一些隐藏的共同情绪，比如自卑、不可爱、羞耻以及诸如害怕亲密、失败甚至成功等各种恐惧。</p><p>从“工作——休息——工作”循环中抽离出来，反思你究竟为什么而活，反思在我们的生命中，还有哪些细微的愉快感受值得我们更多的关注与投入，能够帮助我们从失衡的状态拉回来，从一种习惯性行为的舒服中得到释放。</p><h3 id="心流">心流</h3><p>心流 = 技能 / 挑战 （Flow = Skill / Challenge）</p><p>当参加有组织」有方向和有明确目标的活动时，我们所具备的技能能应对所感知到的挑战，达到平衡状态时，就会处于一种心流状态。<br>能让我们体验到心流的地方之一就是在工作中。在工作中达到心流状态意味着你将所有情绪投入到某项能够是你达到巅峰表现的活动中了。<br>进入心流状态与失去自我意识有关。这不是说你得迷失自我，更不是值你昏迷不醒。是意味着，有那么亦可，你意识不到你自己的存在。平时我们是在对自己的意识、脑子里想什么，以及绳梯感觉如何都极度清醒的状态中生活的。但是当我们处于心流状态时，我们暂时切断了自己与这些感受的联系，让我们去拓宽“我们是谁”和“我们能做什么”的观念。</p><h3 id="好奇心">好奇心</h3><p>好奇心 = 惊奇 + 敬畏 （Curiosity = Wonder + Awe）</p><p>好奇先通常是没有预设目标的行为，它是大脑的养分。有大量证据显示，好奇心好似血管内的血液，是一种让我们永保青春、不可或却并给予生命肯定的情绪。<br>好奇心的两个组成部分是：惊奇和敬畏。纯粹的好奇心蕴含着高度的敬意和公平，我们只有满怀惊奇与敬畏，才会乐于去学习、去爱、去犯错和去生活。心理学家托德·卡什丹将好奇心比作“成长的引擎”。</p><h2 id="Defining-Who-You-Are">Defining Who You Are</h2><h3 id="真实性">真实性</h3><p>真实性 = 自我觉知 * 勇气 （Autheniticity = (Self-Awwareness) * Courage ）</p><p>要认识到你自己需要两种基本工具，把你的真实自我从大理石中雕琢出来，它们就是自我觉知的勇气。两者都极为重要。没有勇气的自我觉知意味着你认识了自己，但是其他人并没有认识你。有勇气却缺乏自我觉知的人会表现得装腔作势。因此这是一个乘法运算，而不是简单的加法。</p><h3 id="自恋">自恋</h3><p>$自恋 = 特权 * {(自尊心)}^2 $ （Narcissism = Entitlement * (self-esteem)**2）</p><p>自恋的人自尊心强到极致，一个有正常自尊心的人会关心别人，而自恋者则太过关注自己，以至于与周围的人断了关联，只会关注他人是如何看待或者服务自己的。<br>自尊心极强的人还会产生特权思想，当想着自己很独特时，你会相信自己有权享受特殊待遇。</p><h3 id="正直">正直</h3><p>正直 = 真实 * 无形 * 可靠 （Integrity = Autheniticity * Invisibility * Reliability ）</p><p>有三个情绪变量可以组合成正直，这就是真实、无形和可靠。<br>正直是“完整”的内在感觉，就是那种我们身上的特点合而为一的感觉。正直不需要观众，一个正直的人就算没有人在注意，也会做正当的事。 可靠包括兼容性、忠诚，还有言行一致。无论你面临什么样的情况，正直都能够跟你的价值观进行可靠的结合。如果你看到了一个人的正直，那么他很有可能在践行你所钦佩的生活和做人的方式。</p><h2 id="Finding-Contentment">Finding Contentment</h2><h3 id="幸福">幸福</h3><p>幸福 = 拥有的 / 想要的 （Happiness = Wanting what you have / Having what you want）</p><p>成功和幸福经常被混为一谈，但是成功是策略上的最大化和最优化，而幸福则倾向于对现状的知足和感恩。心理学家在临床研究中表明，幸福感当中最重要的一部分是表达和感受恩典。该调查同时表明，哪些“策略上的最大化者”（受成功激励的人）的幸福感远远不及那些“知足者”。</p><p>幸福的人更关注“舒适的生活”而不是“更好的生活”。</p><h3 id="喜悦">喜悦</h3><p>喜悦 = 爱 - 恐惧 （Joy = Love - Fear）</p><p>寻求爱可能毫无所得，但寻求喜悦可能却是获得爱的一种方式。爱就像一种电流，我们可以选择关闭或者打开，这一切只是基于我们是否愿意生活在黑暗的恐惧之中，只要我们生活在爱中，喜悦的情绪就会缓缓流入。</p><h3 id="活力">活力</h3><p>活力 = 积极的频率 / 消极的频率 （Thriving = Frequency of Positive / Frequency of Negative）</p><p>活力是积极频率与消极频率的比，且要大于等于3.</p><p>巴西的社会学家 Marcial Losada 和心理学的领军人物 芭芭拉·弗雷德里克森 进行了一场关于积极情绪的实验，实验表明，一旦积极情绪与消极情绪的比例达到3:1，人们就会感到充满希望，情绪高昂，这个比例被心理学家称为“洛萨达比例”或“洛萨达线”。</p><h3 id="信仰">信仰</h3><p>信仰 = 信念 / 智力 （Faith = Belif / Intellect）</p><p>我们所有人都会信仰或者信赖某些事物，无神论者也一样，信仰存在于他们的思想中、本质中、宇宙中，以及所有事物中。<br>信仰和信念经常被当作同义词，但其实前者是一种可能没有明显证据的信赖，而后者则混杂有经验证据。越理性智力水平越高，对于某些事物的信仰就越弱，这就是很多无神论者有些可能会认为信仰宗教的人没有智力的原因。<br>信仰并不嫩解开我们所有的疑惑，有时可能只是让我们愉快地忽视了问题的存在。但是信仰能够给我们一定程度的信心，更加自由地感受生活。想萨姆·哈里斯这样的无神论者也承认：“信仰能让我们当中的很多人平静地承担起生命中的苦楚，而这是我们在纯理性的世界中力有不逮的。”<br>多项事实证明，在很多情况下，药用安慰剂能够发挥很大作用，部分好似因为我们相信的到了某种能够治愈我们的药而产生的信仰。</p><h3 id="智慧">智慧</h3><p>智慧 = 经历的开方 （$Wisdom = \sqrt{Experience}$）</p><p>智慧就是简化生活的复杂性，将分散的东西集中到核心。<br>察觉到规律并将其简化为普遍真理，然后创造成简单的认知模式，在无意识情况下运用到待人处世中。智慧之美在于它的简单。<br>大多数研究人员关于智慧的研究都一致地显示出，智慧的一个品质便是『经历』。这也是为什么我们会认为那些已经有几十年生活经历的人会比年轻人更加智慧的原因。</p><h2 id="Reference">Reference</h2><p><a href="https://medium.com/@hashim.alzain/emotional-equations-book-summary-6348cc98ab81">Emotional Equations</a></p>]]></content>
    
    <summary type="html">
    
      保持平静的简单秘诀。
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Wisdom" scheme="https://neo1989.net/tags/Wisdom/"/>
    
      <category term="SoftSkills" scheme="https://neo1989.net/tags/SoftSkills/"/>
    
      <category term="Health" scheme="https://neo1989.net/tags/Health/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · Building Advanced RAG</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-Building-Advanced-RAG/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-Building-Advanced-RAG/</id>
    <published>2024-01-24T12:01:43.000Z</published>
    <updated>2024-02-29T14:24:02.398Z</updated>
    
    <content type="html"><![CDATA[<p><img src="//s3.mindex.xyz/blog/Courses/fa53038bd6116d6d7897b88d8b3e59b7.png" alt="A comprehensive RAG CheatSheet detailing motivations for RAG as well as techniques and strategies for progressing beyond Basic or Naive RAG builds."></p><h2 id="Basic-RAG">Basic RAG</h2><p>主流的 RAG 主要涉及从外部知识库召回文档，并将这些文档连同用户的查询一起传递给大语言模型，以此生成回应。<br>也就是说，RAG 包含了 <strong>召回部分</strong>、<strong>外部知识库</strong> 以及 <strong>生成部分</strong> 三个组成部分。</p><p>LlamaIndex Basic RAG 示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> SimpleDirectoryReader, VectorStoreIndex</span><br><span class="line"></span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">documents = SimpleDirectoryReader(input_dir=<span class="string">&quot;...&quot;</span>).load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># build VectorStoreIndex that takes care of chunking documents</span></span><br><span class="line"><span class="comment"># and encoding chunks to embeddings for future retrieval</span></span><br><span class="line">index = VectorStoreIndex.from_documents(documents=documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The QueryEngine class is equipped with the generator</span></span><br><span class="line"><span class="comment"># and facilitates the retrieval and generation steps</span></span><br><span class="line">query_engine = index.as_query_engine()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your Default RAG</span></span><br><span class="line">response = query_engine.query(<span class="string">&quot;A user&#x27;s query&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="Success-Requirements-for-RAG">Success Requirements for RAG</h2><p>要想让一个RAG被认定为成功（即能为用户的问题提供有用且相关的答案），实际上有两个核心要求：</p><ul><li><strong>召回</strong> 必须是用户查询最相关的文档。</li><li><strong>生成</strong> 必须能够充分利用召回的文档来有效的回答用户查询。</li></ul><h2 id="Advanced-RAG">Advanced RAG</h2><p>一旦我们明确了成功的标准，就可以说，构建先进的 RAG 主要是要运用更精细的技术和策略（应用于召回或生成组件），以确保最终达到这些标准。<br>进一步说，我们可以把这些精细的技术分为两类：一类是独立解决两大成功要求中的一项（或多或少）的技术，另一类则是同时应对这两大要求的技术。</p><h3 id="召回">召回</h3><p>接下来，我们将简述几种更高级的技术，这些技术能帮助我们实现第一个要求：</p><h4 id="Chunk-Size-Optimization">Chunk-Size Optimization</h4><p>因为大语言模型的上下文长度有限，所以在构建外部知识库时，我们需要将文档切分成多个部分。如果切分的部分过大或过小，都可能给生成组件带来问题，从而导致生成的回答不准确。</p><p>LlamaIndex Chunk Size Optimization <a href="https://github.com/run-llama/llama_index/blob/main/docs/examples/param_optimizer/param_optimizer.ipynb">示例</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> ServiceContext</span><br><span class="line"><span class="keyword">from</span> llama_index.param_tuner.base <span class="keyword">import</span> ParamTuner, RunResult</span><br><span class="line"><span class="keyword">from</span> llama_index.evaluation <span class="keyword">import</span> SemanticSimilarityEvaluator, BatchEvalRunner</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Perform hyperparameter tuning as in traditional ML via grid-search</span></span><br><span class="line"><span class="comment">### 1. Define an objective function that ranks different parameter combos</span></span><br><span class="line"><span class="comment">### 2. Build ParamTuner object</span></span><br><span class="line"><span class="comment">### 3. Execute hyperparameter tuning with ParamTuner.tune()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Define objective function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">objective_function</span>(<span class="params">params_dict</span>):</span><br><span class="line">    chunk_size = params_dict[<span class="string">&quot;chunk_size&quot;</span>]</span><br><span class="line">    docs = params_dict[<span class="string">&quot;docs&quot;</span>]</span><br><span class="line">    top_k = params_dict[<span class="string">&quot;top_k&quot;</span>]</span><br><span class="line">    evals_qs = params_dict[<span class="string">&quot;eval_qs&quot;</span>]</span><br><span class="line">    ref_response_strs = params_dict[<span class="string">&quot;ref_response_strs&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># build RAG pipeline</span></span><br><span class="line">    index = _build_index(chunk_size, docs)  <span class="comment"># hlper function</span></span><br><span class="line">    query_engine = index.as_query_engine(similarity_top_k=top_k)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform inference with RAG pipeline on a privoded questions `eval_qs`</span></span><br><span class="line">    pred_response_objs = get_responses(</span><br><span class="line">        eval_qs, query_engine, show_progress=true</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform evaluations of predictions by comparing them to reference</span></span><br><span class="line">    <span class="comment"># response `ref_response_strs`</span></span><br><span class="line">    evaluator = SemanticSimilarityEvaluator(...)</span><br><span class="line">    eval_batch_runner = BatchEvalRunner(</span><br><span class="line">        &#123;<span class="string">&quot;semantic_similarity&quot;</span>: evaluator&#125;, workers=<span class="number">2</span>, show_progress=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    eval_results = eval_batch_runner.evaluate_responses(</span><br><span class="line">        evals_qs, responses=pred_response_objs, reference=ref_response_strs</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get semantic similarity metric</span></span><br><span class="line">    mean_score = np.array(</span><br><span class="line">        [r.score <span class="keyword">for</span> r <span class="keyword">in</span> eval_results[<span class="string">&quot;semantic_similarity&quot;</span>]]</span><br><span class="line">    ).mean()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> RunResult(score=mean_score, params=params_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Build ParamTuner object</span></span><br><span class="line">param_dict = &#123;<span class="string">&quot;chunk_size&quot;</span>: [<span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>]&#125;</span><br><span class="line">fixed_param_dict = &#123;</span><br><span class="line">    <span class="string">&quot;top_k&quot;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">&quot;docs&quot;</span>: docs,</span><br><span class="line">    <span class="string">&quot;evals_qs&quot;</span>: evals_qs[:<span class="number">10</span>],</span><br><span class="line">    <span class="string">&quot;ref_response_strs&quot;</span>: ref_response_strs[:<span class="number">10</span>],</span><br><span class="line">&#125;</span><br><span class="line">param_tuner = ParamTuner(</span><br><span class="line">    param_fn=objective_function,</span><br><span class="line">    param_dict=param_dict,</span><br><span class="line">    fixed_param_dict=fixed_param_dict,</span><br><span class="line">    show_progress=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Execute hyperparameter search</span></span><br><span class="line">results = param_tuner.tune()</span><br><span class="line">best_result = results.best_run_result</span><br><span class="line">best_chunk_size = results.best_run_result.params[<span class="string">&quot;chunk_size&quot;</span>]</span><br></pre></td></tr></table></figure><h4 id="Structured-External-Knowledge">Structured External Knowledge</h4><p>在复杂的情况下，我们可能需要构建一个比基本向量索引更有结构的外部知识库，这样才能在处理有明显区别的外部知识源时，进行递归召回或者路径召回。</p><p>LlamaIndex Recursive Retrieval <a href="https://docs.llamaindex.ai/en/stable/examples/retrievers/recursive_retriever_nodes.html">示例</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> SimpleDirectoryReader, VectorStoreIndex</span><br><span class="line"><span class="keyword">from</span> llama_index.node_parser <span class="keyword">import</span> SentenceSplitter</span><br><span class="line"><span class="keyword">from</span> llama_index.schema <span class="keyword">import</span> IndexNode</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Build a recursive retriever that retrieves using small chunks</span></span><br><span class="line"><span class="comment">### but passes associated larger chunks to the generation stage</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">documents = SimpleDirectoryReader(</span><br><span class="line">    input_file=<span class="string">&quot;...&quot;</span></span><br><span class="line">).load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># build parent chunks via NodeParser</span></span><br><span class="line">node_parser = SentenceSplitter(chunk_size=<span class="number">1024</span>)</span><br><span class="line">base_nodes = node_parser.get_nodes_from_documents(documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define smaller child chunks</span></span><br><span class="line">sub_chunk_sizes = [<span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line">sub_node_parsers = [</span><br><span class="line">    SentenceSplitter(chunk_size=c, chunk_overlap=<span class="number">20</span>) <span class="keyword">for</span> c <span class="keyword">in</span> sub_chunk_sizes</span><br><span class="line">]</span><br><span class="line">all_nodes = []</span><br><span class="line"><span class="keyword">for</span> base_node <span class="keyword">in</span> base_nodes;</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> sub_node_parsers:</span><br><span class="line">        sub_nodes = n.get_nodes_from_documents([base_node])</span><br><span class="line">        sub_inodes = [</span><br><span class="line">            IndexNode.from_text_node(sn, base_node.node_id) <span class="keyword">for</span> sn <span class="keyword">in</span> sub_nodes</span><br><span class="line">        ]</span><br><span class="line">        all_nodes.extend(sub_inodes)</span><br><span class="line">    <span class="comment"># also add original node to node</span></span><br><span class="line">    original_node = IndexNode.from_text_node(base_node, base_node.node_id)</span><br><span class="line">    all_nodes.append(original_node)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define a VectorStoreIndex with all of the nodes</span></span><br><span class="line">vector_index_chunk = VectorStoreIndex(</span><br><span class="line">    all_nodes, service_context=service_context</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define a VectorStoreIndex with all of the nodes</span></span><br><span class="line">vector_index_chunk = VectorStoreIndex(</span><br><span class="line">    all_nodes, service_context=service_context</span><br><span class="line">)</span><br><span class="line">vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build RecursiveRetriever</span></span><br><span class="line">all_node_dict = &#123;n.node_id: n <span class="keyword">for</span> n <span class="keyword">in</span> all_nodes&#125;</span><br><span class="line">retriever_chunk = RecursiveRetriever(</span><br><span class="line">    <span class="string">&quot;vector&quot;</span>,</span><br><span class="line">    retriever_dict=&#123;<span class="string">&quot;vector&quot;</span>: vector_retriever_chunk&#125;,</span><br><span class="line">    node_dict=all_nodes_dict,</span><br><span class="line">    verbose=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build RetrieverQueryEngine using recursive_retriever</span></span><br><span class="line">query_engine_chunk = RetrieverQueryEngine.from_args(</span><br><span class="line">    retriever_chunk, service_context=service_context</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># perform inference with advanced RAG (i.e. query engine)</span></span><br><span class="line">response = query_engine_chunk.query(</span><br><span class="line">    <span class="string">&quot;Can you tell me about the key concepts for safety finetuning&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="Other-useful-links">Other useful links</h4><p>我们提供了一些指南，展示了在复杂情况下如何应用其他高级技术来确保准确的召回。以下是其中一些选定的链接：</p><ul><li><a href="https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine.html">Building External Knowledge using Knowledge Graphs</a></li><li><a href="https://docs.llamaindex.ai/en/stable/examples/vector_stores/elasticsearch_auto_retriever.html">Performing Mixed Retrieval with Auto Retrievers</a></li><li><a href="https://docs.llamaindex.ai/en/stable/examples/retrievers/simple_fusion.html">Building Fusion Retrievers</a></li><li><a href="https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding.html">Fine-tuning Embedding Models used in Retrieval</a></li><li><a href="https://docs.llamaindex.ai/en/stable/examples/query_transformations/HyDEQueryTransformDemo.html">Transforming Query Embeddings (HyDE)</a></li></ul><h3 id="生成">生成</h3><p>与上一节类似，我们提供了一些高级技术的示例，这些技术的目的是确保召回到的文档能够很好地对齐LLM的生成器。</p><h4 id="Information-Compression">Information Compression</h4><p>大语言模型（LLM）不仅受到上下文长度的限制，而且如果召回到的文档中含有太多的无关信息（也就是噪声），可能会使生成的回答质量下降。</p><p>LlamaIndex Information Compression <a href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/LongLLMLingua.html">示例</a> ：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> SimpleDirectoryReader, VectorStoreIndex</span><br><span class="line"><span class="keyword">from</span> llama_index.query_engine <span class="keyword">import</span> RetrieverQueryEngine</span><br><span class="line"><span class="keyword">from</span> llama_index.postprocessor <span class="keyword">import</span> LongLLMLinguaPostprocessor</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Define a Postprocessor object, here LongLLMLinguaPostprocessor</span></span><br><span class="line"><span class="comment">### Build QueryEngine that uses this Postprocessor on retrieved docs</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Postprocessor</span></span><br><span class="line">node_postprocessor = LongLLMLinguaPostprocessor(</span><br><span class="line">    instruction_str=<span class="string">&quot;Given the context, please answer the final question&quot;</span>,</span><br><span class="line">    target_token=<span class="number">300</span>,</span><br><span class="line">    rank_method=<span class="string">&quot;longllmlingua&quot;</span>,</span><br><span class="line">    additional_compress_kwargs=&#123;</span><br><span class="line">        <span class="string">&quot;condition_compare&quot;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&quot;condition_in_question&quot;</span>: <span class="string">&quot;after&quot;</span>,</span><br><span class="line">        <span class="string">&quot;context_budget&quot;</span>: <span class="string">&quot;+100&quot;</span>,</span><br><span class="line">        <span class="string">&quot;reorder_context&quot;</span>: <span class="string">&quot;sort&quot;</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define VectorStoreIndex</span></span><br><span class="line">documents = SimpleDirectoryReader(input_dir=<span class="string">&quot;...&quot;</span>).load_data()</span><br><span class="line">index = VectorStoreIndex.from_documents(documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define QueryEngine</span></span><br><span class="line">retriever = index.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br><span class="line">retriever_query_engine = RetrieverQueryEngine.from_args(</span><br><span class="line">    retriever, node_postprocessor=[node_postprocessor]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Used your advanced RAG</span></span><br><span class="line">response = retriever_query_engine.query(<span class="string">&quot;A user&#x27;s query&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="Result-Re-Rank">Result Re-Rank</h4><p>大语言模型（LLM）存在被称为“<a href="https://arxiv.org/abs/2307.03172">中间迷失</a>”的现象，即模型主要关注输入提示的两端。因此，在将召回到的文档传递给生成部分之前，对它们进行重新排序是有帮助的。</p><p>LlamaIndex Re-Ranking For Better Generation <a href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/CohereRerank.html">示例</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> SimpleDirectoryReader, VectorStoreIndex</span><br><span class="line"><span class="keyword">from</span> llama_index.postprocessor.cohere_rerank <span class="keyword">import</span> CohereRerank</span><br><span class="line"><span class="keyword">from</span> llama_index.postprocessor <span class="keyword">import</span> LongLLMLinguaPostprocessor</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Define a Postprocessor object, here CohereRerank</span></span><br><span class="line"><span class="comment">### Build QueryEngine that uses this Postprocessor on retrieved docs</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Build CohereRerank post retrieval processor</span></span><br><span class="line">api_key = os.environ[<span class="string">&quot;COHERE_API_KEY&quot;</span>]</span><br><span class="line">cohere_rerank = CohereRerank(api_key=api_key, top_n=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build QueryEngine (RAG) using the post processor</span></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;...&quot;</span>).load_data()</span><br><span class="line">index = VectorStoreIndex.from_documents(documents=documents)</span><br><span class="line">query_engine = index.as_query_engine(</span><br><span class="line">    similarity_top_k=<span class="number">10</span>,</span><br><span class="line">    node_postprocessor=[cohere_rerank],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your advanced RAG</span></span><br><span class="line">response = query_engine.query(</span><br><span class="line">    <span class="string">&quot;What did Sam Altman do in this essay?&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="召回-生成">召回&amp;生成</h3><p>在这一部分，我们研究了一些高级方法，这些方法利用召回和生成的相互配合，旨在提高召回效果，同时生成出更精确的对用户查询的回应。</p><h4 id="Generator-Enhanced-Retrieval">Generator-Enhanced Retrieval</h4><p>这些方法利用大语言模型的内在推理能力，在进行召回之前优化用户的查询，以便更准确地找出生成有用回应所需的信息。</p><p>LlamaIndex Generator-Enhanced Retrieval <a href="https://docs.llamaindex.ai/en/stable/examples/query_engine/flare_query_engine.html">示例</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> llama_index.query_engine <span class="keyword">import</span> FLAREInstructQueryEngine</span><br><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> (</span><br><span class="line">    VectorStoreIndex,</span><br><span class="line">    SimpleDirectoryReader,</span><br><span class="line">    ServiceContext,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Build a FLAREInstructQueryEngine which has the generator LLM play</span></span><br><span class="line"><span class="comment">### a more active role in retrieval by prompting it to elicit retrieval</span></span><br><span class="line"><span class="comment">### instructions on what it needs to answer the user query.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Build FLAREInstructQueryEngine</span></span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;...&quot;</span>).load_data()</span><br><span class="line">index = VectorStoreIndex.from_documents(documents=documents)</span><br><span class="line">index_query_engine = index.as_query_engine(similarity_top_k=<span class="number">2</span>)</span><br><span class="line">service_context = ServiceContext.from_defaults(llm=OpenAI(model=<span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line">flare_query_engine = FLAREInstructQueryEngine(</span><br><span class="line">    query_engine=index_query_engine,</span><br><span class="line">    service_context=service_context,</span><br><span class="line">    max_iterations=<span class="number">7</span>,</span><br><span class="line">    verbose=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your advanced RAG</span></span><br><span class="line">response = flare_query_engine(</span><br><span class="line">    <span class="string">&quot;Can you tell me about the author&#x27;s trajectory in the startup world?&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="Iterative-Retrieval-Generator-RAG">Iterative Retrieval-Generator RAG</h4><p>在一些复杂的场景下，我们可能需要进行多步骤的推理，才能给出对用户查询的有用且相关的回答。</p><p>LlamaIndex Iterative Retrieval-Generator <a href="https://docs.llamaindex.ai/en/stable/examples/evaluation/RetryQuery.html#retry-query-engine">示例</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.query_engine <span class="keyword">import</span> RetryQueryEngine</span><br><span class="line"><span class="keyword">from</span> llama_index.evaluation <span class="keyword">import</span> RelevancyEvaluator</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Build a RetryQueryEngine which performs retrieval-generation cycles</span></span><br><span class="line"><span class="comment">### until it either achieves a passing evaluation or a max number of </span></span><br><span class="line"><span class="comment">### cycles has been reached</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Build RetryQueryEngine</span></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;...&quot;</span>).load_data()</span><br><span class="line">index = VectorStoreIndex.from_documents(documents=documents)</span><br><span class="line">base_query_engine = index.as_query_engine()</span><br><span class="line">query_response_evaluator = RelevancyEvaluator() <span class="comment"># evaluator to critique retrieval-generation cycles</span></span><br><span class="line"></span><br><span class="line">retry_query_engine = RetryQueryEngine(</span><br><span class="line">    base_query_engine, query_response_evaluator</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your advanced rag</span></span><br><span class="line">retry_response = retry_query_engine(<span class="string">&quot;A user&#x27;s query&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="RAG的评估指标">RAG的评估指标</h2><p>对 RAG 系统进行评估，无疑是非常重要的。在<a href="https://arxiv.org/pdf/2312.10997.pdf">《Retrieval-Augmented Generation for Large Language Models: A Survey》</a>中，作者们提出了7个评估指标，这些指标在CheetSheet右上角部分有所体现。</p><p>llama-index 库包含了一些评估抽象，还集成了对 RAGAs 的支持，以帮助开发者从这些评估指标的角度，理解他们的 RAG 系统在多大程度上达到了预期的成功要求。下面，我们列举了一些精选的评估笔记本指南:</p><ul><li><a href="https://docs.llamaindex.ai/en/latest/examples/evaluation/answer_and_context_relevancy.html">Answer Relevancy and Context Relevancy</a></li><li><a href="https://www.notion.so/LlamaIndex-Platform-0754edd9af1c4159bde12649c184c8ef?pvs=21">Faithfulness</a></li><li><a href="https://github.com/run-llama/llama_index/blob/main/docs/examples/evaluation/retrieval/retriever_eval.ipynb">Retrieval Evaluation</a></li><li><a href="https://docs.llamaindex.ai/en/stable/examples/evaluation/batch_eval.html">Batch Evaluations with BatchEvalRunner</a></li></ul><h2 id="Reference">Reference</h2><p>希望在你阅读完这篇博客文章后，能有更多的信心和准备，去运用这些精妙的技术来打造先进的 RAG 系统！</p><p><a href="https://blog.llamaindex.ai/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b">A Cheat Sheet and Some Recipes For Building Advanced RAG</a></p>]]></content>
    
    <summary type="html">
    
      一份全面的 RAG 速查手册，详细阐述了选择 RAG 的理由，以及如何运用技巧和策略，超越基础或初阶的 RAG 构建。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="RAG" scheme="https://neo1989.net/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>正则表达式</title>
    <link href="https://neo1989.net/CheatSheet/CHEATSHEET-Regular-Expressions/"/>
    <id>https://neo1989.net/CheatSheet/CHEATSHEET-Regular-Expressions/</id>
    <published>2024-01-20T08:56:50.000Z</published>
    <updated>2024-01-26T16:29:50.084Z</updated>
    
    <content type="html"><![CDATA[<p>正则表达式用于描述文本模式，因此可以用来检测文本中是否存在特定模式，从更长的字符串中提取出子字符串，或者对文本进行一些调整。正则表达式可以非常简洁，用来描述特定的单词，也可以更复杂一些，用来查找像 URL 中的顶级域名这样的不明确的字符模式。</p><p>本文是基于Python的解析引擎。</p><h2 id="定义">定义</h2><ul><li><p>原字符：原字符是正则表达式中最基础的元素。它直接对应你写下的字符。比如，如果你想要代表一个 “r”，你直接写 r 就可以了。</p></li><li><p>特殊字符：特殊字符用于告诉正则表达式引擎，接下来的字符有特殊的含义。我们通常在特殊字符前面加一个 \ ，它们可以用来表示一行的开头，一行的结尾，或匹配任何单独的字符。</p></li><li><p>字符集：字符集是用来告诉正则表达式引擎，去寻找一组字符中的任意一个。它由 [ 和 ] 表示，你想要寻找的字符就放在这两个括号之间。</p></li><li><p>捕获组：捕获组是由一对圆括号表示的。它们可以让你把多个正则表达式归为一组，然后对这个组应用其他正则表达式的功能，比如量词（后面会提到）。</p></li></ul><h2 id="锚点">锚点</h2><p>锚点用于定位字符的前后位置。</p><table><thead><tr><th style="text-align:center">语法</th><th style="text-align:center">描述</th><th style="text-align:center">示例</th><th style="text-align:center">示例可匹配</th><th style="text-align:center">示例不可匹配</th></tr></thead><tbody><tr><td style="text-align:center">^</td><td style="text-align:center">匹配行首</td><td style="text-align:center">^r</td><td style="text-align:center"><strong>r</strong>abbit <br/> <strong>r</strong>accoon</td><td style="text-align:center">parrot <br/> ferret</td></tr><tr><td style="text-align:center">$</td><td style="text-align:center">匹配行尾</td><td style="text-align:center">t$</td><td style="text-align:center">rabbi<strong>t</strong> <br/> roo<strong>t</strong></td><td style="text-align:center">trap <br/> nice</td></tr><tr><td style="text-align:center">\A</td><td style="text-align:center">匹配行首</td><td style="text-align:center">\Ar</td><td style="text-align:center"><strong>r</strong>abbit <br/> <strong>r</strong>accoon</td><td style="text-align:center">parrot <br/> ferret</td></tr><tr><td style="text-align:center">\Z</td><td style="text-align:center">匹配行尾</td><td style="text-align:center">t\Z</td><td style="text-align:center">rabbi<strong>t</strong> <br/> roo<strong>t</strong></td><td style="text-align:center">trap <br/> nice</td></tr><tr><td style="text-align:center">\b</td><td style="text-align:center">匹配词首或词尾</td><td style="text-align:center">\bfox\b</td><td style="text-align:center">the <strong>fox</strong> ran</td><td style="text-align:center">foxskin</td></tr><tr><td style="text-align:center">\B</td><td style="text-align:center">匹配非空格字符中间的字符</td><td style="text-align:center">\Bee\B</td><td style="text-align:center">b<strong>ee</strong>f</td><td style="text-align:center">tree</td></tr></tbody></table><h2 id="匹配字符类型">匹配字符类型</h2><p>你可以根据字符的类型进行匹配，比如字母、数字等，而不仅仅是特定的字符。</p><table><thead><tr><th style="text-align:center">语法</th><th style="text-align:center">描述</th><th style="text-align:center">示例</th><th style="text-align:center">示例可匹配</th><th style="text-align:center">示例不可匹配</th></tr></thead><tbody><tr><td style="text-align:center">.</td><td style="text-align:center">除了换行之外的所有内容</td><td style="text-align:center">c.e</td><td style="text-align:center"><strong>cle</strong>an <br/> <strong>che</strong>ap</td><td style="text-align:center">acert <br/> cent</td></tr><tr><td style="text-align:center">\d</td><td style="text-align:center">匹配数字</td><td style="text-align:center">\d</td><td style="text-align:center"><strong>6060</strong> ~ <strong>228</strong> <br/> <strong>2</strong>b</td><td style="text-align:center">two <br/> +*+</td></tr><tr><td style="text-align:center">\D</td><td style="text-align:center">匹配非数字</td><td style="text-align:center">\D</td><td style="text-align:center">6060 <strong>~</strong> 228 <br/> 2<strong>b</strong></td><td style="text-align:center">12 <br/> 333</td></tr><tr><td style="text-align:center">\w</td><td style="text-align:center">匹配单词字符</td><td style="text-align:center">\wello\w</td><td style="text-align:center"><strong>hello呀</strong></td><td style="text-align:center">hell呀</td></tr><tr><td style="text-align:center">\W</td><td style="text-align:center">匹配非单词字符</td><td style="text-align:center">hell\W</td><td style="text-align:center"><strong>hell🎸</strong>no</td><td style="text-align:center">hello</td></tr><tr><td style="text-align:center">\s</td><td style="text-align:center">匹配空白</td><td style="text-align:center">hell\s</td><td style="text-align:center">‘<strong>hell\t</strong>no’</td><td style="text-align:center">hello</td></tr><tr><td style="text-align:center">\S</td><td style="text-align:center">匹配非空白</td><td style="text-align:center">hell\S</td><td style="text-align:center"><strong>hello</strong></td><td style="text-align:center">hell no</td></tr><tr><td style="text-align:center">\元字符</td><td style="text-align:center">对元字符进行转义以匹配元字符</td><td style="text-align:center">\.\.\.</td><td style="text-align:center">no <strong>…</strong> no</td><td style="text-align:center">world</td></tr></tbody></table><h2 id="字符集合">字符集合</h2><p>字符集合是一组或一系列的字符。</p><table><thead><tr><th style="text-align:center">语法</th><th style="text-align:center">描述</th><th style="text-align:center">示例</th><th style="text-align:center">示例可匹配</th><th style="text-align:center">示例不可匹配</th></tr></thead><tbody><tr><td style="text-align:center">[xy]</td><td style="text-align:center">匹配指定的任意字符</td><td style="text-align:center">r[ea]</td><td style="text-align:center">g<strong>ra</strong>y <br/> g<strong>re</strong>at</td><td style="text-align:center">grip <br/> groot</td></tr><tr><td style="text-align:center">[x-y]</td><td style="text-align:center">匹配一段连续的字符</td><td style="text-align:center">[a-e]</td><td style="text-align:center"><strong>a</strong>m<strong>be</strong>r <br/> <strong>b</strong>r<strong>a</strong>n<strong>d</strong></td><td style="text-align:center">fox <br/> zoo</td></tr><tr><td style="text-align:center">[^xy]</td><td style="text-align:center">匹配指定之外的字符</td><td style="text-align:center">r[ea]</td><td style="text-align:center">g<strong>ri</strong>p <br/> g<strong>ro</strong>ot</td><td style="text-align:center">gray <br/> great</td></tr><tr><td style="text-align:center">[^-]</td><td style="text-align:center">匹配指定的元字符</td><td style="text-align:center">4[\^\.]\d</td><td style="text-align:center"><strong>4.2</strong> <br/> <strong>4^3</strong></td><td style="text-align:center">44 <br/> 33</td></tr></tbody></table><h2 id="重复">重复</h2><p>你可以找出重复出现的字符，而不仅仅是单个的字符。</p><table><thead><tr><th style="text-align:center">语法</th><th style="text-align:center">描述</th><th style="text-align:center">示例</th><th style="text-align:center">示例可匹配</th><th style="text-align:center">示例不可匹配</th></tr></thead><tbody><tr><td style="text-align:center">x*</td><td style="text-align:center">匹配0个或多个</td><td style="text-align:center">ar*o</td><td style="text-align:center">cac<strong>ao</strong> <br/> c<strong>arro</strong>t</td><td style="text-align:center">arugula <br/> artichoke</td></tr><tr><td style="text-align:center">x+</td><td style="text-align:center">匹配1个或多个</td><td style="text-align:center">re+</td><td style="text-align:center">g<strong>ree</strong>n <br/> t<strong>ree</strong></td><td style="text-align:center">trap <br/> ruined</td></tr><tr><td style="text-align:center">x?</td><td style="text-align:center">匹配0个或1个</td><td style="text-align:center">ro?a</td><td style="text-align:center"><strong>roa</strong>st <br/> <strong>ra</strong>nt</td><td style="text-align:center">root <br/> rear</td></tr><tr><td style="text-align:center">x{m}</td><td style="text-align:center">匹配m次</td><td style="text-align:center">\we{2}\w</td><td style="text-align:center"><strong>deer</strong> <br/> <strong>seer</strong></td><td style="text-align:center">red <br/> enter</td></tr><tr><td style="text-align:center">x{m,}</td><td style="text-align:center">匹配m次或更多</td><td style="text-align:center">2{3,}4</td><td style="text-align:center"><strong>2222224</strong></td><td style="text-align:center">224</td></tr><tr><td style="text-align:center">x{m,n}</td><td style="text-align:center">匹配m到n之间的次数</td><td style="text-align:center">2{2,3}4</td><td style="text-align:center"><strong>224</strong> <br/> <strong>2224</strong></td><td style="text-align:center">24 <br/> 22224</td></tr><tr><td style="text-align:center">x+?</td><td style="text-align:center">尽可能少地匹配，懒惰模式</td><td style="text-align:center">re+?</td><td style="text-align:center">t<strong>re</strong>eeeee</td><td style="text-align:center">trout</td></tr></tbody></table><h2 id="捕获，选择与反向引用">捕获，选择与反向引用</h2><p>如果你想从一段字符串中提取特定的部分，你可以进行捕获操作，甚至可以给你捕获的这些部分命名。</p><table><thead><tr><th style="text-align:center">语法</th><th style="text-align:center">描述</th><th style="text-align:center">示例</th><th style="text-align:center">示例可匹配</th><th style="text-align:center">示例不可匹配</th></tr></thead><tbody><tr><td style="text-align:center">(x)</td><td style="text-align:center">捕获一个模式</td><td style="text-align:center">(iss)+</td><td style="text-align:center">M<strong>ississ</strong>ippi <br/> m<strong>iss</strong>ed</td><td style="text-align:center">mist <br/> persist</td></tr><tr><td style="text-align:center">(?:x)</td><td style="text-align:center">匹配但不捕获</td><td style="text-align:center">(?:ab)(cd)</td><td style="text-align:center">ab<strong>cd</strong></td><td style="text-align:center">accd</td></tr><tr><td style="text-align:center">(?P&lt;name&gt;x)</td><td style="text-align:center">捕获且命名</td><td style="text-align:center">(?P&lt;a&gt;\d)(?P&lt;b&gt;\d)\d*</td><td style="text-align:center"><strong>1325</strong> <br/> a: 1 <br/> b: 3</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">(x</td><td style="text-align:center">y)</td><td style="text-align:center">匹配多种可能的模式</td><td style="text-align:center">(re</td><td style="text-align:center">ba)</td></tr><tr><td style="text-align:center">\n</td><td style="text-align:center">引用之前的捕获，其中 n 是组索引，从 1 开始编号</td><td style="text-align:center">(b)(\w*)\1</td><td style="text-align:center"><strong>blob</strong> <br/> <strong>brib</strong>e</td><td style="text-align:center">bear <br/> bring</td></tr><tr><td style="text-align:center">(?P=name)</td><td style="text-align:center">引用已命名的捕获</td><td style="text-align:center">(?P&lt;a&gt;5)(\d*)(?P=a)</td><td style="text-align:center"><strong>51245</strong> <br/> <strong>55</strong></td><td style="text-align:center">523</td></tr></tbody></table><h2 id="前瞻后顾">前瞻后顾</h2><p>你可以设定一些特定的字符必须在你的匹配项前后出现，但这些字符并不会被包含进匹配结果中。</p><table><thead><tr><th style="text-align:center">语法</th><th style="text-align:center">描述</th><th style="text-align:center">示例</th><th style="text-align:center">示例可匹配</th><th style="text-align:center">示例不可匹配</th></tr></thead><tbody><tr><td style="text-align:center">(?=x)</td><td style="text-align:center">预览接下来的字符，但不把它们纳入匹配结果中</td><td style="text-align:center">an(?=an)</td><td style="text-align:center">b<strong>an</strong>ana</td><td style="text-align:center">band</td></tr><tr><td style="text-align:center">(?!x)</td><td style="text-align:center">预览下一个字符以避免匹配</td><td style="text-align:center">ai(?!n)</td><td style="text-align:center">f<strong>ai</strong>l</td><td style="text-align:center">faint</td></tr><tr><td style="text-align:center">(?&lt;=x)</td><td style="text-align:center">检查前面的字符以进行匹配，但不会把这些字符纳入匹配结果中</td><td style="text-align:center">(?&lt;=tr)a</td><td style="text-align:center">tr<strong>a</strong>il</td><td style="text-align:center">tail</td></tr><tr><td style="text-align:center">(?&lt;!x)</td><td style="text-align:center">查看前面的字符以避免匹配</td><td style="text-align:center">(?!tr)a</td><td style="text-align:center">be<strong>a</strong>r</td><td style="text-align:center">trail</td></tr></tbody></table><h2 id="Reference">Reference</h2><p><a href="https://www.datacamp.com/cheat-sheet/regular-expresso">Regular Expressions</a></p>]]></content>
    
    <summary type="html">
    
      能极大地提升我们对文本数据的处理能力。
    
    </summary>
    
    
      <category term="CheatSheet" scheme="https://neo1989.net/categories/CheatSheet/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="NLP" scheme="https://neo1989.net/tags/NLP/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 语义搜索</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-Semantic-Search/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-Semantic-Search/</id>
    <published>2024-01-10T04:21:28.000Z</published>
    <updated>2024-01-13T06:55:59.489Z</updated>
    
    <content type="html"><![CDATA[<p>语义搜索通过理解搜索查询的内容来提高搜索准确性。与传统搜索引擎不同，传统搜索引擎仅根据词法匹配查找文档，而语义搜索还可以找到同义词。</p><h3 id="背景">背景</h3><p>语义搜索的基本思想是将语料库中的所有条目（无论是句子、段落还是文档）都嵌入到一个向量空间中。</p><p>在搜索时，查询会被嵌入到相同的向量空间中，然后从语料库中找到与查询最接近的嵌入。这些嵌入应该与查询具有高度语义重叠。</p><p><img src="//s3.mindex.xyz/blog/Courses/a88a6ad41612242bacf9371252618da4.png" alt=""></p><h3 id="对称与非对称">对称与非对称</h3><p>对称语义搜索是指你的查询和语料库中的条目长度大致相同，并且具有相同数量的内容。一个例子是: 通过搜索类似的问题 “如何在线学习 Python？” ，你想找到一个像“如何在网络上学习 Python？”这样的条目。对于对称任务，你可能可以在语料库中翻找到查询和对应的条目。</p><p>非对称语义搜索是指你通常有一个简短的查询（例如一个问题或一些关键词），你想找到一个更长的段落来回答查询。像 “什么是 Python” 的查询，你想找到段落“Python 是一种解释型、高级且通用的编程语言。Python 的设计理念是……”。对于非对称任务，在语料库中翻找通常没有意义。</p><p>选择适合任务类型的模型非常重要。</p><p>适合对称语义搜索的模型：<a href="https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models">Pre-Trained Sentence Embedding Models</a></p><p>适合非对称语义搜索的模型：<a href="https://www.sbert.net/docs/pretrained-models/msmarco-v3.html">Pre-Trained MS MARCO Models</a></p><h3 id="Python">Python</h3><p>在数据量不大的语料库中（条目数量最多大约100万），我们有能力计算出搜索词与语料库内每一个条目之间的余弦相似度。</p><p>在接下来的示例中，我们创建了一个包括几个样本句子的小型语料库，并为这个语料库以及我们的搜索词分别计算了它们的嵌入向量。</p><p>接着，我们运用 <code>sentence_transformers.util.cos_sim()</code> 函数来测量搜索词与语料库中所有条目之间的余弦相似性。</p><p>面对庞大的语料库，对所有评分逐一排序实在是效率太低。所以，我们采用了 <code>torch.topk</code> 函数来直接提取得分最高的前 k 个条目。</p><p>下面是一个简单的示例；参见 <a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search.py">semantic_search.py</a>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">This is a simple application for sentence embeddings: semantic search</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">We have a corpus with various sentences. Then, for a given query sentence,</span></span><br><span class="line"><span class="string">we want to find the most similar sentence in this corpus.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This script outputs for various queries the top 5 most similar sentences in the corpus.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer, util</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">embedder = SentenceTransformer(<span class="string">&#x27;all-MiniLM-L6-v2&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Corpus with example sentences</span></span><br><span class="line">corpus = [</span><br><span class="line">    <span class="string">&#x27;A man is eating food.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;A man is eating a piece of bread.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;The girl is carrying a baby.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;A man is riding a horse.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;A woman is playing violin.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Two men pushed carts through the woods.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;A man is riding a white horse on an enclosed ground.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;A monkey is playing drums.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;A cheetah is running behind its prey.&#x27;</span></span><br><span class="line">]</span><br><span class="line">corpus_embeddings = embedder.encode(corpus, convert_to_tensor=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Query sentences:</span></span><br><span class="line">queries = [<span class="string">&#x27;A man is eating pasta.&#x27;</span>, <span class="string">&#x27;Someone in a gorilla costume is playing a set of drums.&#x27;</span>, <span class="string">&#x27;A cheetah chases prey on across a field.&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity</span></span><br><span class="line">top_k = <span class="built_in">min</span>(<span class="number">5</span>, <span class="built_in">len</span>(corpus))</span><br><span class="line"><span class="keyword">for</span> query <span class="keyword">in</span> queries:</span><br><span class="line">    query_embedding = embedder.encode(query, convert_to_tensor=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We use cosine-similarity and torch.topk to find the highest 5 scores</span></span><br><span class="line">    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[<span class="number">0</span>]</span><br><span class="line">    top_results = torch.topk(cos_scores, k=top_k)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n\n======================\n\n&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Query:&quot;</span>, query)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nTop 5 most similar sentences in corpus:&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> score, idx <span class="keyword">in</span> <span class="built_in">zip</span>(top_results[<span class="number">0</span>], top_results[<span class="number">1</span>]):</span><br><span class="line">        <span class="built_in">print</span>(corpus[idx], <span class="string">&quot;(Score: &#123;:.4f&#125;)&quot;</span>.<span class="built_in">format</span>(score))</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk</span></span><br><span class="line"><span class="string">    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)</span></span><br><span class="line"><span class="string">    hits = hits[0]      #Get the hits for the first query</span></span><br><span class="line"><span class="string">    for hit in hits:</span></span><br><span class="line"><span class="string">        print(corpus[hit[&#x27;corpus_id&#x27;]], &quot;(Score: &#123;:.4f&#125;)&quot;.format(hit[&#x27;score&#x27;]))</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="速度优化">速度优化</h3><p>要想让 <code>sentence_transformers.util.cos_sim()</code> 方法运行得更快，最好的做法是将 <code>query_embeddings</code> 和 <code>corpus_embeddings</code> 存在同一块 GPU 设备上。这样做可以明显提升处理性能。</p><p>另外，我们还可以对语料库嵌入进行标准化处理，使每个语料库嵌入的长度都为 1。这样，我们就可以通过点积运算来计算得分了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus_embeddings = corpus_embeddings.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">corpus_embeddings = util.normalize_embeddings(corpus_embeddings)</span><br><span class="line"></span><br><span class="line">query_embeddings = query_embeddings.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">query_embeddings = util.normalize_embeddings(query_embeddings)</span><br><span class="line">hits = util.semantic_search(query_embeddings, corpus_embeddings, score_function=util.dot_score)</span><br></pre></td></tr></table></figure><h3 id="ElasticSearch">ElasticSearch</h3><p>从 7.3 版本开始，<a href="https://www.elastic.co/elasticsearch/">ElasticSearch</a> 推出了一个新功能，即能够索引密集向量 (dense vectors)，并将其用于对文档进行评分。所以，我们可以利用 ElasticSearch 对文档以及嵌入向量（embeddings）进行索引，以此在搜索时使用对应的嵌入向量寻找相关的文档信息。</p><p>ElasticSearch的一个优点是，它便于向索引中添加新的文档，而且能够和我们的向量一起存储其他数据。但缺点是它的性能较慢，这是因为它需要将搜索的嵌入内容和每一个已经存储的嵌入内容进行比较。这种操作的时间成本是线性的，对于大规模（超过 100k）的数据集来说，可能会慢得无法接受。</p><p>更多详细信息，请参见 <a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_elasticsearch.py">semantic_search_quora_elasticsearch.py</a>。</p><h3 id="近似最近邻点">近似最近邻点</h3><p>如果使用精确的最近邻搜索方法（如 <code>sentence_transformers.util.semantic_search</code> 所采用的方式），在一个巨大的语料库中进行查找，特别是这个语料库中包含数百万个嵌入，可能会花费大量的时间。</p><p>在这种情况下，近似最近邻（Approximate Nearest Neighor，ANN）可能会很有帮助。这里，数据被划分为相似的嵌入小部分。利用索引可以有效地进行搜索，甚至在有数百万的向量时也能在毫秒内检索到最高相似性的嵌入（即最近的邻居）。</p><p>不过，结果未必都是精确的。可能有些具有高度相似性的向量被遗漏了。这就是我们称它为“近似最近邻居”的原因。</p><p>所有的人工神经网络（ANN）方法都通常需要调整一到多个参数，以达到召回率与搜索速度的权衡。如果你追求极高的搜索速度，可能会错过一些重要的搜索结果。反之，如果你期望得到高召回率，搜索的速度就可能会变慢。</p><p>近似最近邻搜索库中，<a href="https://github.com/spotify/annoy" title="Annoy">Annoy</a>、<a href="https://github.com/facebookresearch/faiss" title="FAISS">FAISS</a> 和 <a href="https://github.com/nmslib/hnswlib/" title="hnswlib">hnswlib</a> 都很热门。但是，我个人更偏向 <code>hnswlib</code>，因为它不仅使用起来十分简单，性能卓越，而且包含了许多在实际应用中至关重要的特色功能。</p><p>示例：</p><ul><li><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_hnswlib.py" title="semantic_search_quora_hnswlib.py">semantic_search_quora_hnswlib.py</a></li><li><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_annoy.py" title="semantic_search_quora_annoy.py">semantic_search_quora_annoy.py</a></li><li><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_faiss.py" title="semantic_search_quora_faiss.py">semantic_search_quora_faiss.py</a></li></ul><h3 id="召回和重排">召回和重排</h3><p>对于复杂的语义搜索场景，建议使用「召回和重排」流程：</p><p><img src="//s3.mindex.xyz/blog/Courses/3d66117e5374e1f95d858d7d422fc22e.png" alt=""></p><p>当我们有一个搜索请求时，我们会首先使用一个检索系统，这个系统能够找出大约 100 个可能的结果，这些结果可能与我们的搜索请求相关。在进行检索时，我们可以选择使用词汇搜索，比如说使用 ElasticSearch 这样的工具，或者我们也可以选择使用双向编码器进行深度检索。</p><p>但是，这个检索系统可能会找到一些与搜索请求并不太相关的文档。因此，在第二步，我们会使用一个基于交叉编码器的重新排序系统，这个系统会评估所有候选结果与搜索请求的相关性。</p><p>最终，我们将得到一个排名的结果列表，这个列表可以直接呈现给用户。</p><h4 id="召回-Bi-Encoder">召回: Bi-Encoder</h4><p>在寻找候选结果集的过程中，我们可以选择使用词汇搜索（比如 ElasticSearch），或者我们也可以选择使用在这个代码库中实现的双向编码器。</p><p>词汇搜索是在你的文档库中寻找与查询词完全匹配的内容，它无法识别同义词、缩写或拼写的不同形式。而语义搜索（也被称为密集检索）则是将搜索的关键词转化为向量空间的形式，然后找出在这个向量空间中与其最接近的文档。</p><p>语义搜索弥补了词汇搜索的不足，能够识别同义词和缩写词。</p><h4 id="重排：Cross-Encoder">重排：Cross-Encoder</h4><p>检索器需要能够高效处理包含数百万条目的大型文档库。但是，有时候它可能会找出一些与查询无关的结果。</p><p>利用 Cross-Encoder 的重新排序技术，我们可以大幅提升搜索结果的质量。在这个过程中，我们会把搜索请求和可能的文档同时输入到 Transformer 网络中，然后网络会输出一个介于 0 到 1 之间的分数，这个分数代表了文档与搜索请求的匹配程度。</p><p><img src="s3.mindex.xyz/blog/Courses/73b1bebb8fcf7b548f1828585c696898.png" alt=""></p><p>Cross-Encoders的优势在于它们能提供更出色的性能，这是因为它们能在处理查询和文档时运用注意力机制。</p><p>如果我们要对大量的（查询，文档）对进行评分，那将会非常耗时。所以，我们采用的策略是使用检索器先生成一组可能的候选者，比如 100 个，然后再通过 Cross-Encoder 对这些候选者进行重新排序。</p><h3 id="完整示例">完整示例</h3><h4 id="相似问题检索">相似问题检索</h4><p><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_pytorch.py">semantic_search_quora_pytorch.py</a> [<a href="https://colab.research.google.com/drive/11GunvCqJuebfeTlgbJWkIMT0xJH6PWF1?usp=sharing">Colab Version</a>] 是一个基于Quora重复问题数据集的应用案例。通过它，用户可以输入任何问题，然后代码会运用 <code>sentence_transformers.util.semantic_search</code> 方法从数据集中找出与输入问题最相近的问题。模型是 distilbert-multilingual-nli-stsb-quora-ranking，它的主要任务是去识别类似的问题，并且它支持超过50种语言。所以，无论用户用这50多种语言中的何种来提问，都可以得到有效的答案。这是一个对称的搜索任务，因为搜索查询的长度和内容与语料库中的问题相同。</p><h4 id="相似出版物检索">相似出版物检索</h4><p><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_publications.py">semantic_search_publications.py</a> [<a href="https://colab.research.google.com/drive/12hfBveGHRsxhPIUMmJYrll2lFU4fOX06?usp=sharing">Colab Version</a>] 这个示例演示了如何找到与某篇科学论文相似的其他论文。我们的语料库由在 EMNLP 2016 - 2018 会议上发表的所有论文组成。在搜索过程中，我们会输入最近发表的论文的标题和摘要，然后在我们的语料库中寻找相关的论文。我们使用的是 <a href="https://arxiv.org/abs/2004.07180">SPECTER</a> 模型。这个搜索任务是对称的，因为我们的语料库中的论文和我们搜索的内容都是由标题和摘要组成。</p><h4 id="问答检索">问答检索</h4><p><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_wikipedia_qa.py">semantic_search_wikipedia_qa.py</a> [<a href="https://colab.research.google.com/drive/11GunvCqJuebfeTlgbJWkIMT0xJH6PWF1?usp=sharing">Colab Version</a>]：这个例子展示了一个在 <a href="https://ai.google.com/research/NaturalQuestions/">Natural Questions dataset</a> 数据集上进行训练的模型。这个数据集包含了大约十万条真实的 Google 搜索请求，以及从维基百科获取并附带注解的段落，这些段落提供了问题的答案。这是一个非对称搜索任务的典型例子。在这个例子中，我们使用了体积较小的 <a href="https://simple.wikipedia.org/wiki/Main_Page">Simple English Wikipedia</a> 作为语料库，这样它就可以轻松地加载到内存中。</p><p><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.py">retrieve_rerank_simple_wikipedia.py</a> [<a href="https://colab.research.google.com/github/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb">Colab Version </a>]：这个脚本采用了 <strong>召回和重排</strong> 的策略，是一个非对称搜索任务的典型例子。我们把所有维基百科的文章切分成各个段落，并用双向编码器进行编码处理。当有新的查询或问题输入时，我们也用同样的双向编码器进行编码，然后找出与之余弦相似度最高的段落。然后，我们用一个交叉编码器对找到的候选段落进行重新排序，最终将得分最高的5个段落展示给用户。我们使用的模型是在 <a href="https://github.com/microsoft/MSMARCO-Passage-Ranking/">MS Marco Passage Reranking datase</a> 数据集上进行训练的，这个数据集包含了大约 500k 来自 Bing 搜索的真实查询。</p><h3 id="Reference">Reference</h3><p><a href="https://www.sbert.net/examples/applications/semantic-search/README.html">Semantic Search</a></p>]]></content>
    
    <summary type="html">
    
      语义搜索通过理解搜索查询的内容来提高搜索准确性。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>《我就是你啊》</title>
    <link href="https://neo1989.net/Notes/NOTE-Sept-graines-de-lumiere-dans-le-coeur-des-guerriers/"/>
    <id>https://neo1989.net/Notes/NOTE-Sept-graines-de-lumiere-dans-le-coeur-des-guerriers/</id>
    <published>2024-01-06T12:20:47.000Z</published>
    <updated>2024-02-18T13:50:18.460Z</updated>
    
    <content type="html"><![CDATA[<p>必要前提：记住“武力对抗”会适得其反。在整个余人交流的过程中，都要避免陷入<strong>解释</strong>、<strong>威胁</strong>和<strong>人身攻击</strong>的怪圈当中。</p><h3 id="第一步：平复自己的情绪">第一步：平复自己的情绪</h3><p>当我们感觉内心出现了想要攻击对方的冲动时，我们需要尽力控制住它。比如，我们可以通过以下几种方法做到这一点：</p><ul><li>纠正对事实的误判；</li><li>通过深呼吸来分散注意力；</li><li>或只需收住自己想要伸出的拳头；</li><li>或其他任何一种可能奏效的方法。</li></ul><p>在与人交流的过程中，每当你感到内心再次燃起了这种冲动，就需要在脑海里回顾这一步。完成这一步只需要几秒钟的时间，这大概是最难完成，也是最重要的一步。</p><h3 id="第二步：平复他人的情绪">第二步：平复他人的情绪</h3><p>如果与你对话的人能够保持冷静，那就好说了，你就可以直接跳过这一步。但是如果对方不冷静呢，你又该怎么做？答案是 “什么也不要做”，或者说 “几乎什么也不要做”。尤其不要和对方说“你冷静点儿！”或者“你生气是没有用的！”。此时你应该遵循“<strong>不唱反调，不做评判</strong>”的原则。具体如何做到这一点呢？你可以用 “同意” “好的” “是的” “没错” 等来回复对方。这些字眼可以向对方传达一种信息：“你这样说以及你选择以这样的方式说，自然有你的道理。我愿意与你探讨这个问题。”这样做你会收获到惊人的效果：对方的情绪起初会有些波动，但随后便会逐渐稳定，直到最后慢慢平复。实现这一步，只需要你说出 “<strong>我同意</strong>” 的一刹那就够了。 如果你成功控制住了自己的情绪，又成功平复了他人的情绪，那就可以进入下一步了。</p><h3 id="第三步：试着理解他人而非让他人被理解">第三步：试着理解他人而非让他人被理解</h3><p>如何做到这一步呢？最简单的方法就是 “<strong>向对方提问</strong>”。最有效的问题就是：“你为什么步同意我的观点呢？”之后便要倾听他的回答，并试着站在他们的角度看问题，甚至是设身处地地为他人的利益着想。努力去理解和接受别人的观点，而非将自己的观点强加于人。</p><p>你要学会从他们在做解释时所说的话语中寻找双方的“共识”，并欣然接受对方的观点。这是双方达成“共识”的先决条件。这时候你多一些对别人的“私心”：让自己多为对方的利益考虑！。</p><p>一旦你理解了他人的想法，解决方法便会自己现身，分歧也就迎刃而解了。但通常来说，做到这一点还不够，你需要继续完成下一步…</p><h3 id="第四步：通过复述别人的话来让对方明白“你以及理解了对方的观点”">第四步：通过复述别人的话来让对方明白“你以及理解了对方的观点”</h3><p>如果你想让别人倾听你的观点，你需要先让对方发言。然后再用自己的话将你所理解的对方的观点讲一遍。之后问对方 “我说得对吗？”。这样你会收获意想不到的神奇效果——对方会眉头上扬，露出满意的笑容，大赞一声“对啊！”。而后他便会闭上嘴，听你说话。复述有两个好处：第一，你可以检验一下自己是否真的理解了对方的观点；第二，让你的对话者知道自己被人理解了，进而打消继续争论的念头。</p><p>但实现这一步有一个前提条件——不要因为用错了一个词，让你之前的努力都付之东流。这个词就是接下来这一步的关键词。这个词的使用也是一门艺术。</p><h3 id="第五步：使用表并列的词汇提出自己的观点，而非将双方观点对立">第五步：使用表并列的词汇提出自己的观点，而非将双方观点对立</h3><p>如何做到这一点呢？你可以运用以下这些字眼，比如：就我而言、对我来说、与此同时、从我的角度出发…而不要用极其生硬的 “没错，但是…”。然后等到双方都明确了对方的观点之后，问问你自己 “如何才能让对方的诉求得到满足，同时又能达到自己的目的呢？”。如此一来，你就可以发挥两个人的聪明材质，来寻求解决方案。接下来，我们开始进入第六步。</p><h3 id="第六步：提出解决方案">第六步：提出解决方案</h3><p>采用可能的双赢方案。如果我们自己找不到可行的方案，试着问问别人有没有好主意吧。让拿回我们可以一起针对其进行讨论。</p><p>如果找不到任何双赢的方法，我们可以采用妥协折中的方式解决问题。人们往往会接受折中的方案，因为这样至少恩纳锅够建立友好写作的关系。</p><p>那么，如果连折中的方案也不存在呢？这种情况非常罕见，但并非绝不可能发生。此时，可以与对方协商，再花点时间一起探讨可能的出路。尽管最终不一定能找到解决方案，但至少维持了良好的关系。而这种良性关系对于未来协作的达成至关重要。</p><p><img src="https://s3.mindex.xyz/blog/Notes/bf65eb5d1adba66e1f624e828eaf5b7f.png" alt=""></p><h3 id="Reference">Reference</h3><p>[1] <a href="https://book.douban.com/subject/35445960/">我就是你啊 : 走进他人内心的7项修炼</a></p>]]></content>
    
    <summary type="html">
    
      化解一场纷争需要经历哪些重要的步骤？
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Wisdom" scheme="https://neo1989.net/tags/Wisdom/"/>
    
      <category term="SoftSkills" scheme="https://neo1989.net/tags/SoftSkills/"/>
    
  </entry>
  
  <entry>
    <title>日落收集 (二)</title>
    <link href="https://neo1989.net/SeizeTheDay/COLLECTION-sunsets-2/"/>
    <id>https://neo1989.net/SeizeTheDay/COLLECTION-sunsets-2/</id>
    <published>2023-12-31T14:34:15.000Z</published>
    <updated>2024-02-18T05:53:03.288Z</updated>
    
    <content type="html"><![CDATA[<h3 id="May-20-2023">May 20, 2023</h3><p><img src="//s3.mindex.xyz/blog/Notes/a8dcfd31636d678b6bc786345a7342a9.png" alt="西湖·太子湾公园 | 浙江"></p><h3 id="May-13-2023">May 13, 2023</h3><p><img src="//s3.mindex.xyz/blog/Notes/02be63a08b4d3f50fe9f022c13f21291.png" alt="清水路·环湖大道 | 苏州"></p><h3 id="Apr-15-2023">Apr 15, 2023</h3><p><img src="//s3.mindex.xyz/blog/Notes/97765437415756031a4b064691e3fc5e.png" alt="清水路·环湖大道 | 苏州"></p><h3 id="Mar-11-2023">Mar 11, 2023</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/d1c296dc199be5df7087034673e4267a.png" alt="On the Rock @ NamPhrae | Thailand"></p><h3 id="Mar-10-2023">Mar 10, 2023</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/3454d0d46192236de9293e1261e6a65c.png" alt="Route 107 | Thailand"></p><h3 id="Mar-4-2023">Mar 4, 2023</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/7d8d79718363def5e1f35fc64c131589.png" alt="米堆山 | 苏州"></p><h3 id="Jan-30-2023">Jan 30, 2023</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/cb9d279f773c498fb81893d84e3801a1.png" alt="滨江中路 | 上海"></p>]]></content>
    
    <summary type="html">
    
      日落尤其温柔，人间皆是浪漫。
    
    </summary>
    
    
      <category term="SeizeTheDay" scheme="https://neo1989.net/categories/SeizeTheDay/"/>
    
    
  </entry>
  
  <entry>
    <title>高效做事的底层逻辑</title>
    <link href="https://neo1989.net/Notes/NOTE-work-efficiently/"/>
    <id>https://neo1989.net/Notes/NOTE-work-efficiently/</id>
    <published>2023-12-15T09:26:25.000Z</published>
    <updated>2023-12-17T06:03:20.899Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第一步：WOOP模型，让你对目标动力十足">第一步：WOOP模型，让你对目标动力十足</h2><p><strong>W</strong>ish: 明确愿望和目标是什么？<br><strong>O</strong>utcome: 实现愿望后有什么收获？<br><strong>O</strong>bstacle: 追求目标过程中有哪些障碍？<br><strong>P</strong>lan: 制定什么样的计划来克服那些障碍？</p><h2 id="第二步：SMART法则，设定更科学的目标">第二步：SMART法则，设定更科学的目标</h2><p><strong>S</strong>pecific: 目标设定必须明确具体，很多目标之所以不能实现，就是因为设定之初模棱两可。<br><strong>M</strong>easureable: 所谓可衡量，就是要有一组明确的数据。用数据作为衡量目标是否实现的标准。<br><strong>A</strong>ttainable: 设定的目标要在自己的能力范围之内。<br><strong>R</strong>elevant: 所设立ide目标要与其他目标相关联。<br><strong>T</strong>ime-bound: 目标是要有时间限制的。设定一个目标完成的期限，促进目标的达成。</p><h2 id="第三步：用GRAI定期复盘">第三步：用GRAI定期复盘</h2><p><strong>G</strong>oal: 当初立了哪些flag。期望的结果是什么？<br><strong>R</strong>esult: 对照目标，完成的怎么样了？<br><strong>A</strong>nalysis: 成功或失败的关键原因是什么？<br><strong>I</strong>nsight: 得失的体会是什么？是否有规律性的东西值得思考并指导下一次行动计划？</p><h2 id="第四步：用PDCA不断优化">第四步：用PDCA不断优化</h2><p><strong>P</strong>lan:<br>- 分析现状，找出问题<br>- 分析产生问题的影响因素<br>- 找出主要因素<br>- 设定目标，指定计划</p><p><strong>D</strong>o: 按照预订计划，努力实现预期目标</p><p><strong>C</strong>heck: 评估结果，检查效果，分析原因</p><p><strong>A</strong>ction:<br>- 将成功经验进行标准化及流程制定，作为下次行动的参考<br>- 总结经验和问题，为开展新一轮PDCA提供依据</p>]]></content>
    
    <summary type="html">
    
      4个步骤让你效率暴涨。
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="SoftSkills" scheme="https://neo1989.net/tags/SoftSkills/"/>
    
  </entry>
  
  <entry>
    <title>查理·芒格 语录</title>
    <link href="https://neo1989.net/Notes/NOTE-Charles-Thomas-Munger/"/>
    <id>https://neo1989.net/Notes/NOTE-Charles-Thomas-Munger/</id>
    <published>2023-11-29T07:36:36.000Z</published>
    <updated>2023-12-17T09:21:12.799Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于认知">关于认知</h2><ul><li>变化总在发生，你不去迎接进步的变化，就会等到退步的变化。</li><li>我们能取得今时今日的成就，不是因为我们的能力比别人高出多少，而是我们比别人更清楚自己能力的大小。</li><li>决定结果的主要有两个因素：一个是形势，一个是人。形势太强，任凭你有多大能力，都无济于事。</li><li>一个人，手里拿着锤子，看什么都像钉子。</li><li>只要做好准备，在人生中抓住几个机会。迅速地采取适当的行动，去做简单而合乎逻辑的事情，这辈子的财富就会得到极大的增长。</li><li>我们很清楚自己的不足，很清楚有很多试我们做不到，所以我们谨小慎微地留在我们的“能力圈”中。“能力圈”是沃伦提出的概念。沃伦和我都认为，我们的“能力圈”是一个非常小的圆圈。</li><li>在显示世界中，学会看透本质，我们能活得更从容。探究本质并非朝夕之功，必须有板凳要坐十年冷的精神。</li><li>如果你想要说服别人，要诉诸利益，而非诉诸理性。</li><li>卓越的人很少，有机会追随他们，和他们走到一起，或许值得付出溢价，将来可能获得丰厚的回报。</li><li>保持理性是一种道德律令。在不该犯傻的时候千万别犯傻。</li><li>和被人比是比不过来的，无论做什么，都是一山更比一山高，强中自有强中手。</li><li>所有人都看好的机会，最容易发生踩踏，造成的损失最惨烈。</li><li>每一天都追求比醒来的那一刻多增长一分智慧；每一天都追求有能力承担更大的责任；每一天都追求尽善尽美地完成所有工作。日复一日，年复一年，你终将出人头地。</li><li>按照我的经验，解决问题的最佳方法是不让问题出现。</li><li>所有人的潜意识里都有这样的偏见：给别人提建议时，以为是在为别人考虑，其实是从自己的利益出发。</li><li>我们能成功，不是因为我们善于解决难题，而是因为我们善于远离难题。我们只是找简单的事做而已。</li><li>反过来想，总是反过来想。</li><li>如何才能成功？严格自律、遵守道德，找到志同道合的人，抓住难得的大机会，说出来都是些很简单的道理。</li><li>我不会因为人性而感到意外，也不会花太多时间感受背叛，我总是低下头调整自己去适应这类事情，我不喜欢任何成为受害者的感受。我不是受害者，我是幸存者。</li><li>人类都试图变得精明，而我只想证明自己并不是在做傻事，但这比许多人想想的要困难的多。</li></ul><h2 id="关于成长">关于成长</h2><ul><li>想要得到一件东西，最稳妥的方法就是让自己配得上它。</li><li>找出你最擅长的事情，然后持之以恒，乐此不疲地去把它做好。</li><li>承认无知是智慧的开始。</li><li>一个不能从别人的经验汇总学习的人，一辈子注定一只摔跟头。</li><li>把问题彻底想明白，问题就解决了一半。</li><li>我不会质疑过去，而是从过去中学习，为未来做决定。</li><li>如果你真想成功，真想取得别人无法取得的成就，就甘坐冷板凳，日复一日地阅读。如果你想拥有良好的认知能力，如果你想比别人更具智慧，能在艰难时刻有更好的表现，除了拿出大量时间思考，别无他法。</li><li>我这辈子遇到的聪明人没有不每天阅读的——一个都没有。</li><li>独学而无友，则孤陋寡闻。所有人都需要找到志同道合的人，相互切磋、共同进步。</li><li>我们成功源自我的长期专注。</li><li>在过去的任何一年，如果你一次都没有推翻过自己最中意的想法，那么这一年就算浪费了。</li><li>我喜欢能够坦然承认自己很愚蠢的人。我知道，如果正面承认自己的错误，我会表现得更好。这是一个非常棒的学习窍门。</li><li>我见识过很多取得很大成就的人。虽然他们既不是最聪明的人，甚至也不是最勤奋的人，但他们都是很善于学习的人。</li><li>极度专业化才是成功之道。比起理解整个世界来说，大多数人更加擅长专攻一个方面。</li><li>我做过很多傻事，我一直在和自己的成见做斗争。消除错误的想法是一件好事，我把消除错误的想法作为自己的一种追求。</li><li>进步不总是肉眼可见，而是往往出现在不经意间，但进步总是源于长期坚持，源于每一天的努力。</li><li>如果不终身学习，你们将不会取得很高的成就。光靠已有的知识，你们在生活中走不了多远。</li><li>只要能达到正确的终点，路途再颠簸，我都受得了。</li><li>脚踏实地，一步一个脚印，坚持不懈地长期努力，这是我的成功之道。</li></ul><h2 id="关于人生">关于人生</h2><ul><li>生活的铁律就是，只有20%的人能够取得比其他80%的人优秀的成绩。</li><li>不要同一头猪摔跤，因为这样你会把全身弄脏，而对方却乐此不疲。</li><li>你不必非常出色，只要在很长、很长的时间内保持比其他人聪明一点点就够了。</li><li>如果你的生活方式是正确的，那么到了晚年只会比年轻时更加幸福。</li><li>我会尽我所能逆流而上，而不是去预测潮汐何时到来。</li><li>如果你专注的时间周期足够长，你不断地为解决难题而努力，你就会跌跌撞撞地得到一个答案。这是人生的半个秘方。</li><li>任何人都会有错过机会的时候，这是命中注定的。我一直认为，改变不了的事情，不要太纠结。<strong>牢骚和抱怨是人生中的大忌。</strong></li><li>在生活中，很多人抱残守缺，他们满脑子的旧思想，新思想根本进不去。有句德国谚语说得好：“我们总是老的太快，聪明的太迟”。所有人都有这个问题。</li><li>身处逆境的时候，你要有一股咬紧牙关、埋头苦干的拼劲。怨天尤人、牢骚不断，只能越来越苦、越来越难。</li><li>想要什么，就立刻要得到，这样的人不但一事无成，还可能坠入深渊。</li><li>抵抗衰老的最佳措施就是在老之前好好生活。</li><li>生活总是以某种防护死伤害某人，又以某种方式帮助他人。面对生活中的打击，每个人都应该积极应对。</li><li>人生的困难一个接一个，每个困难都是对我们的一次考验，都是我们表现自己的机会。</li><li>很多人总是一味地逃避，不愿承受短期的痛苦。自找苦吃、主动吃眼前的苦，这才是正确的处世态度。</li><li>当逆境不期而至时，我们应该敢于迎难而上，这才是一种积极向上的人生态度。整天哼哼唧唧地怨天尤人，谁都救不了你。</li><li>坚持做有意义的事；坚持做有价值的人；坚持追求理智、正直、诚信，总有一天，一定能获得成功。</li><li>要想幸福，第一条，降低自己的预期。这一点是你自己能掌控的。</li><li>我觉得犯嫉妒这种罪的人最蠢，得不到一丁点快乐，整个人都被痛苦包围着，何必遭这份罪呢？</li><li>托马斯·卡莱尔有一句名言：“与其为朦胧的未来而烦恼忧虑，不如脚踏实地地、做好眼前的事。” 这句话说的很对。大多数时候，我们应该把眼前的事做好，尽人事，听天命。</li></ul><h2 id="关于商业管理">关于商业管理</h2><ul><li>最理想的公司，每年创造的现金高于净利润，能为所有者提供大量可自由支配的现金。</li><li>充分认清客观条件的限制，充分认识自身能力的限制，谨小慎微地在限制范围内活动，这是赚钱的诀窍。这个诀窍，与其说是“谦卑”，不如说是“有克制的贪婪”。</li><li>一家公司建立好了文化之后，就能走上良性循环的轨道。</li><li>为了防范风险，我们制定的规矩，恰恰是不赚最后一个铜板。</li><li>任何一家高杠杆的金融机构，无论管理者多么尽职尽责，都可能遭遇意外的损失。关键是遭遇意外之后，能否第一时间解决问题。在问题暴露出来以后，很多公司首先想到的是如何隐瞒，如何用会计手段蒙混过去。我们认为，应该不遮不掩、立刻解决。</li><li>即使拥有诚实守信的优良传统，时间久了，制度漏洞还是会毁掉优良传统。</li><li>裁员成本是一项巨大的隐形负债。很多公司因为裁员而支付的成本高达几亿、几十亿美金。公司明明要为缩小规模而付出成本，但这项成本并没有在资产负债表上体现出来。</li><li>我们从不签署允许我们懒惰的合同，以免我们走向堕落。</li><li>纵观商业史，很多公司辉煌过，赚过大钱，但是当它们被新的科技浪潮淘汰后，它们的家底很快就会耗光，最终走向消亡。</li><li>在服务业，只有全力以赴，为客户消除所有痛点，才能超越竞争对手。</li><li>经营一家公司，你懂的延迟满足，能把公司经营的越来越好。在人生中懂得延迟满足，你死的时候能很风光。</li><li>在沃伦眼中，优秀的管理者是这样的：你把他从火车上扔下去，扔到一个偏僻的小镇，不给他钱，他在这个小镇上诚实本分地经营，用不了多长时间，又发家致富了。</li><li>在与别人合作的过程中，沃伦和我都是首先以高标准要求自己。因为有优秀的人与我们一道努力，我们才能取得今天的成绩。</li><li>要找到优秀的伴侣，只有一个办法，就是自己得配得上。同样的道理，要找到优秀的人共事，你自己首先要是一个优秀的人。</li><li>我们不懂具体的软件业务，那我们怎么领导每日期刊公司呢？我们主要靠知人善任。</li><li>在做管理工作的过程中，最容易犯的错误是，已经发现该换人了，但迟迟下不了决心，拖了很长时间，才把不合适的人换掉。即使是有着多年管理经验的人，也很容易犯这个错误。</li><li>公司越大，越难建立起正确的文化。大公司特别容易患上官僚主义这个通病。</li><li>人们钻空子，肯定是因为激励制度有漏洞。</li><li>职业生涯的三条规则：不要销售你自己都不愿意买的东西；不要为你不尊重和不欣赏的人工作；只和你喜欢的人一起工作。</li><li>我们很少换人，不是因为我们软弱或愚蠢，而是因为我们一开始就把人选对了。</li><li>我愿意和优秀的人共事，不愿意和平庸的人为伍。</li><li>信任是你自己赢得的。你自己做事总是很靠谱，时间久了，别人自然会信任你。</li><li>我觉得在面对难题的时候，列一张清单非常有用。在单子上一列，所有问题一目了然，能把问题考虑得更周全，不会有什么遗漏。</li><li>凡是往简单处想，往认真处行。</li></ul><h2 id="关于投资理念">关于投资理念</h2><ul><li>真正做收购是好事多磨，要熬过辛苦的等待，经历反复的波折。</li><li>大多数时候，我们什么都不做。我们出手的时候很少。即使是出手的时候，我们也是如履薄冰，对可能承担的风险感到不安。</li><li>有些人收集邮票，而我收集疯狂和荒谬，然后避开它们。</li><li>钱多机会少，总比钱少机会多强。</li><li>我们只在很少的时候，能看透重大的机会。</li><li>我们始终把眼前所有的投资机会进行比较，力求找到当下最合理的投资逻辑，这才是重中之重。找到了最合理的同欧字逻辑之后，无论周期波动如何剧烈，是顺境还是逆境，我们都泰然自若。这就是我们的投资之道。</li><li>投资要选容错率高的好生意。有点管理问题，有点困难，有点错误，好生意照样还是好生意。</li><li>按我们这种方式投资，必须准确判断一家公司的前景。也就是说，你不但要能看出来，一家公司现在的生意是好生意，而且要能看出来，它在将来的很长时间里仍然是好生意。</li><li>买入好生意长期持有才是正道。</li><li>真正的好公司，现在的价格，大家可能觉得很贵，其实不贵。</li><li>格雷厄姆提出了安全边际的原则，这个概念永不过时。“市场是我们的仆人，不是我们的老师”。</li><li>做投资，一个是必须等大眼睛等待机会出现，另一个是机会出现时，必须果断出手。</li><li>钓鱼的第一条原则是，在有鱼的地方钓。钓鱼的第二条规则是，记住第一条规则。投资是同样的道理。</li><li>归根结底，投资只有价值投资一种。为什么这么说？因为我们每做一笔投资，把钱投进去，都是为了将来能获得更多的价值。</li><li>首先，要找自己能看懂的机会，不做自己看不懂的投资。然后，要踏踏实实地去做大量实际的工作。</li><li>成功的投资即需要进取心又需要耐心，而且还需要准备好在机会出现时抓住它，因为在这个世界上，机会不会持续很久。</li><li>我们能够成功，不是因为我们善于解决难题，而是因为我们善于远离难题。我们只是找到了容易做的事情。</li></ul>]]></content>
    
    <summary type="html">
    
      卓越的人很少，有机会追随他们，和他们走到一起...
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Wisdom" scheme="https://neo1989.net/tags/Wisdom/"/>
    
  </entry>
  
  <entry>
    <title>SQ3R阅读法</title>
    <link href="https://neo1989.net/Notes/NOTE-SQ3R/"/>
    <id>https://neo1989.net/Notes/NOTE-SQ3R/</id>
    <published>2023-07-11T11:33:19.000Z</published>
    <updated>2023-07-11T11:39:47.879Z</updated>
    
    <content type="html"><![CDATA[<p><img src="//s3.mindex.xyz/tmp/fc643b514ad76d94a6f205de37e747f8.png" alt=""></p><h4 id="Survey">Survey</h4><p>快速扫描章节小标题，识别出来几个关键点，如果有章节小结的，重点阅读。</p><h4 id="Question">Question</h4><p>把章节的标题换成一个问题，阅读这章节的目的就是为了回答这个问题。</p><h4 id="Read">Read</h4><p>带着问题去阅读，阅读过程中始终记得为这个问题寻找答案</p><h4 id="Recite">Recite</h4><p>阅读完之后，用自己的话尝试解答这个问题，如果回答不出来，就重复以上四个步骤，直到回答出来位置。</p><h4 id="Review">Review</h4><p>最后再次回顾，并用自己的话来复述整本书的主要观点。</p>]]></content>
    
    <summary type="html">
    
      方法
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="SoftSkills" scheme="https://neo1989.net/tags/SoftSkills/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · Embeddings （下）</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-Embeddings-2/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-Embeddings-2/</id>
    <published>2023-07-06T05:02:57.000Z</published>
    <updated>2023-07-10T14:17:47.993Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>上篇文章简单介绍了Embeddings，以及Glove。本篇将简单介绍加入Embedding层的CNN。</p><p>注意，所有的前置工作与<a href="https://neo1989.net/Way2AI/Way2AI-CNN/" title="卷积神经网络">《Way2AI · 卷积神经网络》</a>这篇文章里的介绍没有太大区别，最大的区别在于建模的时候加入了Embeddings层。</p><h2 id="Set-up">Set up</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip install numpy==1.21.2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_seeds</span>(<span class="params">seed=<span class="number">1024</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Set seeds for reproducibility.&quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)  <span class="comment"># multi-GPU</span></span><br><span class="line"></span><br><span class="line">set_seeds(seed=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">cuda = <span class="literal">True</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span>(torch.cuda.is_available() <span class="keyword">and</span> cuda) <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">torch.set_default_tensor_type(&#123;<span class="string">&quot;cuda&quot;</span>: <span class="string">&quot;torch.cuda.FloatTensor&quot;</span>, <span class="string">&quot;cpu&quot;</span>: <span class="string">&quot;torch.FloatTensor&quot;</span>&#125;.get(<span class="built_in">str</span>(device)))</span><br></pre></td></tr></table></figure><h2 id="Load-data">Load data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">&quot;https://s3.mindex.xyz/datasets/news.csv&quot;</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>)</span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df[<span class="string">&quot;title&quot;</span>][:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 0     Israel announces West Bank housing plan; barri...</span></span><br><span class="line"><span class="comment"># 1     Red Sox #39;s Feat: As far back as I can remember</span></span><br><span class="line"><span class="comment"># 2     J.P. Morgan Cancels IBM Outsourcing Deal (Reut...</span></span><br><span class="line"><span class="comment"># 3                          Intel Names Otellini New CEO</span></span><br><span class="line"><span class="comment"># 4     Branson Launches Virgin Atlantic Flights to Au...</span></span><br><span class="line"><span class="comment">#                             ...</span></span><br><span class="line"><span class="comment"># 95    Yahoo Profit Surges on Sales of Ads, Google Stock</span></span><br><span class="line"><span class="comment"># 96                                     DirecT Touchdown</span></span><br><span class="line"><span class="comment"># 97         Struggling Bucs Best Dismal Bears, 19-7 (AP)</span></span><br><span class="line"><span class="comment"># 98    Romania PM, Bucharest Mayor Battle for Preside...</span></span><br><span class="line"><span class="comment"># 99                      Glazer Quest for United Falters</span></span><br><span class="line"><span class="comment"># Name: title, Length: 100, dtype: object</span></span><br></pre></td></tr></table></figure><h2 id="Processing">Processing</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> PorterStemmer</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">nltk.download(<span class="string">&quot;stopwords&quot;</span>)</span><br><span class="line">STOPWORDS = stopwords.words(<span class="string">&quot;english&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (STOPWORDS[:<span class="number">5</span>])</span><br><span class="line">porter = PorterStemmer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">text, stopwords=STOPWORDS</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Conditional preprocessing on our text unique to our task.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Lower</span></span><br><span class="line">    text = text.lower()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove stopwords</span></span><br><span class="line">    pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;\b(&quot;</span> + <span class="string">r&quot;|&quot;</span>.join(stopwords) + <span class="string">r&quot;)\b\s*&quot;</span>)</span><br><span class="line">    text = pattern.sub(<span class="string">&quot;&quot;</span>, text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove words in parenthesis</span></span><br><span class="line">    text = re.sub(<span class="string">r&quot;\([^)]*\)&quot;</span>, <span class="string">&quot;&quot;</span>, text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Spacing and filters</span></span><br><span class="line">    text = re.sub(<span class="string">r&quot;([-;;.,!?&lt;=&gt;])&quot;</span>, <span class="string">r&quot; \1 &quot;</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">&quot;[^A-Za-z0-9]+&quot;</span>, <span class="string">&quot; &quot;</span>, text) <span class="comment"># remove non alphanumeric chars</span></span><br><span class="line">    text = re.sub(<span class="string">&quot; +&quot;</span>, <span class="string">&quot; &quot;</span>, text)  <span class="comment"># remove multiple spaces</span></span><br><span class="line">    text = text.strip()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply to dataframe</span></span><br><span class="line">preprocessed_df = df.copy()</span><br><span class="line">preprocessed_df.title = preprocessed_df.title.apply(preprocess)</span><br></pre></td></tr></table></figure><h2 id="Split-data">Split data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_val_test_split</span>(<span class="params">X, y, train_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Split dataset into data splits.&quot;&quot;&quot;</span></span><br><span class="line">    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)</span><br><span class="line">    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Data</span></span><br><span class="line">X = preprocessed_df[<span class="string">&quot;title&quot;</span>].values</span><br><span class="line">y = preprocessed_df[<span class="string">&quot;category&quot;</span>].values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (84000,), y_train: (84000,)</span></span><br><span class="line"><span class="comment"># X_val: (18000,), y_val: (18000,)</span></span><br><span class="line"><span class="comment"># X_test: (18000,), y_test: (18000,)</span></span><br><span class="line"><span class="comment"># Sample point: ibm wins time talks pension case → Sci/Tech</span></span><br></pre></td></tr></table></figure><h2 id="Label-encoding">Label encoding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LabelEncoder</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Label encoder for tag labels.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, class_to_index=&#123;&#125;</span>):</span><br><span class="line">        self.class_to_index = class_to_index <span class="keyword">or</span> &#123;&#125;  <span class="comment"># mutable defaults ;)</span></span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = <span class="built_in">list</span>(self.class_to_index.keys())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.class_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&lt;LabelEncoder(num_classes=<span class="subst">&#123;<span class="built_in">len</span>(self)&#125;</span>)&gt;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, y</span>):</span><br><span class="line">        classes = np.unique(y)</span><br><span class="line">        <span class="keyword">for</span> i, class_ <span class="keyword">in</span> <span class="built_in">enumerate</span>(classes):</span><br><span class="line">            self.class_to_index[class_] = i</span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = <span class="built_in">list</span>(self.class_to_index.keys())</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, y</span>):</span><br><span class="line">        encoded = np.zeros((<span class="built_in">len</span>(y)), dtype=<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(y):</span><br><span class="line">            encoded[i] = self.class_to_index[item]</span><br><span class="line">        <span class="keyword">return</span> encoded</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, y</span>):</span><br><span class="line">        classes = []</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(y):</span><br><span class="line">            classes.append(self.index_to_class[item])</span><br><span class="line">        <span class="keyword">return</span> classes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, fp</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fp, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;<span class="string">&#x27;class_to_index&#x27;</span>: self.class_to_index&#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">cls, fp</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fp, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Encode</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">label_encoder.fit(y_train)</span><br><span class="line">NUM_CLASSES = <span class="built_in">len</span>(label_encoder)</span><br><span class="line"><span class="built_in">print</span>(label_encoder.class_to_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;&#x27;Business&#x27;: 0, &#x27;Sci/Tech&#x27;: 1, &#x27;Sports&#x27;: 2, &#x27;World&#x27;: 3&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert labels to tokens</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line">y_train = label_encoder.encode(y_train)</span><br><span class="line">y_val = label_encoder.encode(y_val)</span><br><span class="line">y_test = label_encoder.encode(y_test)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: Sci/Tech</span></span><br><span class="line"><span class="comment"># y_train[0]: 1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> <span class="built_in">enumerate</span>(counts)&#125;</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># counts: [21000 21000 21000 21000]</span></span><br><span class="line"><span class="comment"># weights: &#123;0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Tokenizer">Tokenizer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> more_itertools <span class="keyword">import</span> take</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tokenizer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, char_level, num_tokens=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 pad_token=<span class="string">&quot;&lt;PAD&gt;&quot;</span>, oov_token=<span class="string">&quot;&lt;UNK&gt;&quot;</span>,</span></span><br><span class="line"><span class="params">                 token_to_index=<span class="literal">None</span></span>):</span><br><span class="line">        self.char_level = char_level</span><br><span class="line">        self.separator = <span class="string">&quot;&quot;</span> <span class="keyword">if</span> self.char_level <span class="keyword">else</span> <span class="string">&quot; &quot;</span></span><br><span class="line">        <span class="keyword">if</span> num_tokens: num_tokens -= <span class="number">2</span> <span class="comment"># pad + unk tokens</span></span><br><span class="line">        self.num_tokens = num_tokens</span><br><span class="line">        self.pad_token = pad_token</span><br><span class="line">        self.oov_token = oov_token</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> token_to_index:</span><br><span class="line">            token_to_index = &#123;pad_token: <span class="number">0</span>, oov_token: <span class="number">1</span>&#125;</span><br><span class="line">        self.token_to_index = token_to_index</span><br><span class="line">        self.index_to_token = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.token_to_index.items()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.token_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&lt;Tokenizer(num_tokens=<span class="subst">&#123;<span class="built_in">len</span>(self)&#125;</span>)&gt;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit_on_texts</span>(<span class="params">self, texts</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.char_level:</span><br><span class="line">            texts = [text.split(<span class="string">&quot; &quot;</span>) <span class="keyword">for</span> text <span class="keyword">in</span> texts]</span><br><span class="line">        all_tokens = [token <span class="keyword">for</span> text <span class="keyword">in</span> texts <span class="keyword">for</span> token <span class="keyword">in</span> text]</span><br><span class="line">        counts = Counter(all_tokens).most_common(self.num_tokens)</span><br><span class="line">        self.min_token_freq = counts[-<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> token, count <span class="keyword">in</span> counts:</span><br><span class="line">            index = <span class="built_in">len</span>(self)</span><br><span class="line">            self.token_to_index[token] = index</span><br><span class="line">            self.index_to_token[index] = token</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">texts_to_sequences</span>(<span class="params">self, texts</span>):</span><br><span class="line">        sequences = []</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> texts:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.char_level:</span><br><span class="line">                text = text.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">            sequence = []</span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> text:</span><br><span class="line">                sequence.append(self.token_to_index.get(</span><br><span class="line">                    token, self.token_to_index[self.oov_token]))</span><br><span class="line">            sequences.append(np.asarray(sequence))</span><br><span class="line">        <span class="keyword">return</span> sequences</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sequences_to_texts</span>(<span class="params">self, sequences</span>):</span><br><span class="line">        texts = []</span><br><span class="line">        <span class="keyword">for</span> sequence <span class="keyword">in</span> sequences:</span><br><span class="line">            text = []</span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> sequence:</span><br><span class="line">                text.append(self.index_to_token.get(index, self.oov_token))</span><br><span class="line">            texts.append(self.separator.join([token <span class="keyword">for</span> token <span class="keyword">in</span> text]))</span><br><span class="line">        <span class="keyword">return</span> texts</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, fp</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fp, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;</span><br><span class="line">                <span class="string">&quot;char_level&quot;</span>: self.char_level,</span><br><span class="line">                <span class="string">&quot;oov_token&quot;</span>: self.oov_token,</span><br><span class="line">                <span class="string">&quot;token_to_index&quot;</span>: self.token_to_index</span><br><span class="line">            &#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">cls, fp</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fp, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tokenize</span></span><br><span class="line">tokenizer = Tokenizer(char_level=<span class="literal">False</span>, num_tokens=<span class="number">5000</span>)</span><br><span class="line">tokenizer.fit_on_texts(texts=X_train)</span><br><span class="line">VOCAB_SIZE = <span class="built_in">len</span>(tokenizer)</span><br><span class="line"><span class="built_in">print</span> (tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output </span></span><br><span class="line"><span class="comment"># &lt;Tokenizer(num_tokens=5000)&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample of tokens</span></span><br><span class="line"><span class="built_in">print</span> (take(<span class="number">5</span>, tokenizer.token_to_index.items()))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;least freq token&#x27;s freq: <span class="subst">&#123;tokenizer.min_token_freq&#125;</span>&quot;</span>) <span class="comment"># use this to adjust num_tokens</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [(&#x27;&lt;PAD&gt;&#x27;, 0), (&#x27;&lt;UNK&gt;&#x27;, 1), (&#x27;39&#x27;, 2), (&#x27;b&#x27;, 3), (&#x27;gt&#x27;, 4)]</span></span><br><span class="line"><span class="comment"># least freq token&#x27;s freq: 14</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert texts to sequences of indices</span></span><br><span class="line">X_train = tokenizer.texts_to_sequences(X_train)</span><br><span class="line">X_val = tokenizer.texts_to_sequences(X_val)</span><br><span class="line">X_test = tokenizer.texts_to_sequences(X_test)</span><br><span class="line">preprocessed_text = tokenizer.sequences_to_texts([X_train[<span class="number">0</span>]])[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Text to indices:\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  (preprocessed) → <span class="subst">&#123;preprocessed_text&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  (tokenized) → <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Text to indices:</span></span><br><span class="line"><span class="comment">#   (preprocessed) → ibm wins time talks pension case</span></span><br><span class="line"><span class="comment">#   (tokenized) → [ 31  32  69  26 715 100]</span></span><br></pre></td></tr></table></figure><h2 id="Padding">Padding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pad_sequences</span>(<span class="params">sequences, max_seq_len=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Pad sequences to max length in sequence.&quot;&quot;&quot;</span></span><br><span class="line">    max_seq_len = <span class="built_in">max</span>(max_seq_len, <span class="built_in">max</span>(<span class="built_in">len</span>(sequence) <span class="keyword">for</span> sequence <span class="keyword">in</span> sequences))</span><br><span class="line">    padded_sequences = np.zeros((<span class="built_in">len</span>(sequences), max_seq_len))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> <span class="built_in">enumerate</span>(sequences):</span><br><span class="line">        padded_sequences[i][:<span class="built_in">len</span>(sequence)] = sequence</span><br><span class="line">    <span class="keyword">return</span> padded_sequences</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2D sequences</span></span><br><span class="line">padded = pad_sequences(X_train[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span> (padded.shape)</span><br><span class="line"><span class="built_in">print</span> (padded)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># (3, 8)</span></span><br><span class="line"><span class="comment"># [[3.100e+01 3.200e+01 6.900e+01 2.600e+01 7.150e+02 1.000e+02 0.000e+00</span></span><br><span class="line"><span class="comment">#   0.000e+00]</span></span><br><span class="line"><span class="comment">#  [3.568e+03 9.000e+00 4.520e+03 2.000e+00 1.000e+00 2.396e+03 7.760e+02</span></span><br><span class="line"><span class="comment">#   1.500e+01]</span></span><br><span class="line"><span class="comment">#  [1.000e+01 1.094e+03 7.600e+01 5.960e+02 5.740e+02 8.000e+02 0.000e+00</span></span><br><span class="line"><span class="comment">#   0.000e+00]]</span></span><br></pre></td></tr></table></figure><h2 id="Dataset">Dataset</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">FILTER_SIZES = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">2</span>, <span class="number">5</span>)) <span class="comment"># bi, tri and 4 grams</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, X, y, max_filter_size</span>):</span><br><span class="line">        self.X = X</span><br><span class="line">        self.y = y</span><br><span class="line">        self.max_filter_size = max_filter_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&lt;Dataset(N=<span class="subst">&#123;<span class="built_in">len</span>(self)&#125;</span>)&gt;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        X = self.X[index]</span><br><span class="line">        y = self.y[index]</span><br><span class="line">        <span class="keyword">return</span> [X, y]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">self, batch</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Processing on a batch.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Get inputs</span></span><br><span class="line">        batch = np.array(batch)</span><br><span class="line">        X = batch[:, <span class="number">0</span>]</span><br><span class="line">        y = batch[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pad sequences</span></span><br><span class="line">        X = pad_sequences(X)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cast</span></span><br><span class="line">        X = torch.LongTensor(X.astype(np.int32))</span><br><span class="line">        y = torch.LongTensor(y.astype(np.int32))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_dataloader</span>(<span class="params">self, batch_size, shuffle=<span class="literal">False</span>, drop_last=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">return</span> torch.utils.data.DataLoader(</span><br><span class="line">            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,</span><br><span class="line">            shuffle=shuffle, drop_last=drop_last, pin_memory=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create datasets</span></span><br><span class="line">max_filter_size = <span class="built_in">max</span>(FILTER_SIZES)</span><br><span class="line">train_dataset = Dataset(X=X_train, y=y_train, max_filter_size=max_filter_size)</span><br><span class="line">val_dataset = Dataset(X=X_val, y=y_val, max_filter_size=max_filter_size)</span><br><span class="line">test_dataset = Dataset(X=X_test, y=y_test, max_filter_size=max_filter_size)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Datasets:\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  Train dataset:<span class="subst">&#123;train_dataset.__str__()&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  Val dataset: <span class="subst">&#123;val_dataset.__str__()&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  Test dataset: <span class="subst">&#123;test_dataset.__str__()&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Sample point:\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  X: <span class="subst">&#123;train_dataset[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  y: <span class="subst">&#123;train_dataset[<span class="number">0</span>][<span class="number">1</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Datasets:</span></span><br><span class="line"><span class="comment">#   Train dataset:&lt;Dataset(N=84000)&gt;</span></span><br><span class="line"><span class="comment">#   Val dataset: &lt;Dataset(N=18000)&gt;</span></span><br><span class="line"><span class="comment">#   Test dataset: &lt;Dataset(N=18000)&gt;</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: [ 31  32  69  26 715 100]</span></span><br><span class="line"><span class="comment">#   y: 1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create dataloaders</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_dataloader = train_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">val_dataloader = val_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">test_dataloader = test_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">batch_X, batch_y = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Sample batch:\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  X: <span class="subst">&#123;<span class="built_in">list</span>(batch_X.size())&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  y: <span class="subst">&#123;<span class="built_in">list</span>(batch_y.size())&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Sample point:\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  X: <span class="subst">&#123;batch_X[<span class="number">0</span>]&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  y: <span class="subst">&#123;batch_y[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Sample batch:</span></span><br><span class="line"><span class="comment">#   X: [64, 10]</span></span><br><span class="line"><span class="comment">#   y: [64]</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: tensor([ 31,  32,  69,  26, 715, 100,   0,   0,   0,   0])</span></span><br><span class="line"><span class="comment">#   y: 1</span></span><br></pre></td></tr></table></figure><h2 id="Model">Model</h2><p>可视化一下模型的前向传播.</p><ul><li>首先对输入tokenizer化 (batch_size, max_seq_len)</li><li>然后我们对tokenizered输入进行embed (batch_size, max_seq_len, embedding_dim)</li><li>接下来，使用filters（filter_size, vocab_size, num_filter)进行卷积，然后批归一化。我们讲使用三个不同size的filter（2, 3 和 4）分别充当bi-gram, tri-gram 和 4-gram 特征提取器。</li><li>紧跟着，应用一维max polling，从特征图中提取最相关信息以做出决策</li><li>再接一个含dropout的全连接层</li><li>最后再使用一个softmax全连接层以输出最终的类别概率</li></ul><p><img src="//s3.mindex.xyz/tmp/89231298b192a7831de7c18f7c52f6ad.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">HIDDEN_DIM = <span class="number">100</span></span><br><span class="line">DROPOUT_P = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dim, vocab_size, num_filters,</span></span><br><span class="line"><span class="params">                 filter_sizes, hidden_dim, dropout_p, num_classes,</span></span><br><span class="line"><span class="params">                 pretrained_embeddings=<span class="literal">None</span>, freeze_embeddings=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 padding_idx=<span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Filter sizes</span></span><br><span class="line">        self.filter_sizes = filter_sizes</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize embeddings</span></span><br><span class="line">        <span class="keyword">if</span> pretrained_embeddings <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.embeddings = nn.Embedding(</span><br><span class="line">                embedding_dim=embedding_dim, num_embeddings=vocab_size,</span><br><span class="line">                padding_idx=padding_idx)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).<span class="built_in">float</span>()</span><br><span class="line">            self.embeddings = nn.Embedding(</span><br><span class="line">                embedding_dim=embedding_dim, num_embeddings=vocab_size,</span><br><span class="line">                padding_idx=padding_idx, _weight=pretrained_embeddings)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Freeze embeddings or not</span></span><br><span class="line">        <span class="keyword">if</span> freeze_embeddings:</span><br><span class="line">            self.embeddings.weight.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Conv weights</span></span><br><span class="line">        self.conv = nn.ModuleList(</span><br><span class="line">            [nn.Conv1d(in_channels=embedding_dim,</span><br><span class="line">                       out_channels=num_filters,</span><br><span class="line">                       kernel_size=f) <span class="keyword">for</span> f <span class="keyword">in</span> filter_sizes])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FC weights</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout_p)</span><br><span class="line">        self.fc1 = nn.Linear(num_filters*<span class="built_in">len</span>(filter_sizes), hidden_dim)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, channel_first=<span class="literal">False</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Embed</span></span><br><span class="line">        x_in, = inputs</span><br><span class="line">        x_in = self.embeddings(x_in)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Rearrange input so num_channels is in dim 1 (N, C, L)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> channel_first:</span><br><span class="line">            x_in = x_in.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Conv outputs</span></span><br><span class="line">        z = []</span><br><span class="line">        max_seq_len = x_in.shape[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">for</span> i, f <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.filter_sizes):</span><br><span class="line">            <span class="comment"># `SAME` padding</span></span><br><span class="line">            padding_left = <span class="built_in">int</span>((self.conv[i].stride[<span class="number">0</span>]*(max_seq_len-<span class="number">1</span>) - max_seq_len + self.filter_sizes[i])/<span class="number">2</span>)</span><br><span class="line">            padding_right = <span class="built_in">int</span>(math.ceil((self.conv[i].stride[<span class="number">0</span>]*(max_seq_len-<span class="number">1</span>) - max_seq_len + self.filter_sizes[i])/<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Conv + pool</span></span><br><span class="line">            _z = self.conv[i](F.pad(x_in, (padding_left, padding_right)))</span><br><span class="line">            _z = F.max_pool1d(_z, _z.size(<span class="number">2</span>)).squeeze(<span class="number">2</span>)</span><br><span class="line">            z.append(_z)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Concat conv outputs</span></span><br><span class="line">        z = torch.cat(z, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FC layers</span></span><br><span class="line">        z = self.fc1(z)</span><br><span class="line">        z = self.dropout(z)</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Using-GloVe">Using GloVe</h2><p>先实现一些方便能够将预训练的GloVe加载到我们的模型中的公共方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_glove_embeddings</span>(<span class="params">embeddings_file</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Load embeddings from a file.&quot;&quot;&quot;</span></span><br><span class="line">    embeddings = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(embeddings_file, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        <span class="keyword">for</span> index, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(fp):</span><br><span class="line">            values = line.split()</span><br><span class="line">            word = values[<span class="number">0</span>]</span><br><span class="line">            embedding = np.asarray(values[<span class="number">1</span>:], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            embeddings[word] = embedding</span><br><span class="line">    <span class="keyword">return</span> embeddings</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_embeddings_matrix</span>(<span class="params">embeddings, word_index, embedding_dim</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Create embeddings matrix to use in Embedding layer.&quot;&quot;&quot;</span></span><br><span class="line">    embedding_matrix = np.zeros((<span class="built_in">len</span>(word_index), embedding_dim))</span><br><span class="line">    <span class="keyword">for</span> word, i <span class="keyword">in</span> word_index.items():</span><br><span class="line">        embedding_vector = embeddings.get(word)</span><br><span class="line">        <span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            embedding_matrix[i] = embedding_vector</span><br><span class="line">    <span class="keyword">return</span> embedding_matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create embeddings</span></span><br><span class="line">embeddings_file = <span class="string">&#x27;glove.6B.&#123;0&#125;d.txt&#x27;</span>.<span class="built_in">format</span>(EMBEDDING_DIM)</span><br><span class="line">glove_embeddings = load_glove_embeddings(embeddings_file=embeddings_file)</span><br><span class="line">embedding_matrix = make_embeddings_matrix(</span><br><span class="line">    embeddings=glove_embeddings, word_index=tokenizer.token_to_index,</span><br><span class="line">    embedding_dim=EMBEDDING_DIM)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;&lt;Embeddings(words=<span class="subst">&#123;embedding_matrix.shape[<span class="number">0</span>]&#125;</span>, dim=<span class="subst">&#123;embedding_matrix.shape[<span class="number">1</span>]&#125;</span>)&gt;&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;Embeddings(words=5000, dim=100)&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Experiments">Experiments</h2><p>接下来，我们将进行三个实验：</p><ul><li>随机初始化的embeddings (fine-tuned)</li><li>GloVe embeddings (frozen)</li><li>GloVe embeddings (fine-tuned)</li></ul><p>先定义我们的Trainer</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">NUM_FILTERS = <span class="number">50</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-3</span></span><br><span class="line">PATIENCE = <span class="number">5</span></span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Trainer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, device, loss_fn=<span class="literal">None</span>, optimizer=<span class="literal">None</span>, scheduler=<span class="literal">None</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set params</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.device = device</span><br><span class="line">        self.loss_fn = loss_fn</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self.scheduler = scheduler</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">self, dataloader</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Train step.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Set model to train mode</span></span><br><span class="line">        self.model.train()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over train batches</span></span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Step</span></span><br><span class="line">            batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">            inputs, targets = batch[:-<span class="number">1</span>], batch[-<span class="number">1</span>]</span><br><span class="line">            self.optimizer.zero_grad()  <span class="comment"># Reset gradients</span></span><br><span class="line">            z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">            J = self.loss_fn(z, targets)  <span class="comment"># Define loss</span></span><br><span class="line">            J.backward()  <span class="comment"># Backward pass</span></span><br><span class="line">            self.optimizer.step()  <span class="comment"># Update weights</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Cumulative Metrics</span></span><br><span class="line">            loss += (J.detach().item() - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">eval_step</span>(<span class="params">self, dataloader</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Validation or test step.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line">        y_trues, y_probs = [], []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Step</span></span><br><span class="line">                batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">                inputs, y_true = batch[:-<span class="number">1</span>], batch[-<span class="number">1</span>]</span><br><span class="line">                z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">                J = self.loss_fn(z, y_true).item()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Cumulative Metrics</span></span><br><span class="line">                loss += (J - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line">                y_trues.extend(y_true.cpu().numpy())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, np.vstack(y_trues), np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict_step</span>(<span class="params">self, dataloader</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Prediction step.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">        y_probs = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Forward pass w/ inputs</span></span><br><span class="line">                inputs, targets = batch[:-<span class="number">1</span>], batch[-<span class="number">1</span>]</span><br><span class="line">                z = self.model(inputs)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, num_epochs, patience, train_dataloader, val_dataloader</span>):</span><br><span class="line">        best_val_loss = np.inf</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">            <span class="comment"># Steps</span></span><br><span class="line">            train_loss = self.train_step(dataloader=train_dataloader)</span><br><span class="line">            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)</span><br><span class="line">            self.scheduler.step(val_loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Early stopping</span></span><br><span class="line">            <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">                best_val_loss = val_loss</span><br><span class="line">                best_model = self.model</span><br><span class="line">                _patience = patience  <span class="comment"># reset _patience</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                _patience -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> _patience:  <span class="comment"># 0</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Stopping early!&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Logging</span></span><br><span class="line">            <span class="built_in">print</span>(</span><br><span class="line">                <span class="string">f&quot;Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> | &quot;</span></span><br><span class="line">                <span class="string">f&quot;train_loss: <span class="subst">&#123;train_loss:<span class="number">.5</span>f&#125;</span>, &quot;</span></span><br><span class="line">                <span class="string">f&quot;val_loss: <span class="subst">&#123;val_loss:<span class="number">.5</span>f&#125;</span>, &quot;</span></span><br><span class="line">                <span class="string">f&quot;lr: <span class="subst">&#123;self.optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]:<span class="number">.2</span>E&#125;</span>, &quot;</span></span><br><span class="line">                <span class="string">f&quot;_patience: <span class="subst">&#123;_patience&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> best_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_metrics</span>(<span class="params">y_true, y_pred, classes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Per-class performance metrics.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">&quot;overall&quot;</span>: &#123;&#125;, <span class="string">&quot;class&quot;</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">&quot;weighted&quot;</span>)</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;precision&quot;</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;recall&quot;</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;f1&quot;</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;num_samples&quot;</span>] = np.float64(<span class="built_in">len</span>(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(classes)):</span><br><span class="line">        performance[<span class="string">&quot;class&quot;</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">&quot;precision&quot;</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">&quot;recall&quot;</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">&quot;f1&quot;</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">&quot;num_samples&quot;</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br></pre></td></tr></table></figure><h3 id="Random-initialization">Random initialization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">PRETRAINED_EMBEDDINGS = <span class="literal">None</span></span><br><span class="line">FREEZE_EMBEDDINGS = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = CNN(</span><br><span class="line">    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,</span><br><span class="line">    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,</span><br><span class="line">    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,</span><br><span class="line">    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)</span><br><span class="line">model = model.to(device) <span class="comment"># set device</span></span><br><span class="line"><span class="built_in">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of CNN(</span></span><br><span class="line"><span class="comment">#   (embeddings): Embedding(5000, 100, padding_idx=0)</span></span><br><span class="line"><span class="comment">#   (conv): ModuleList(</span></span><br><span class="line"><span class="comment">#     (0): Conv1d(100, 50, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (1): Conv1d(100, 50, kernel_size=(3,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (2): Conv1d(100, 50, kernel_size=(4,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   )</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=150, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(<span class="built_in">list</span>(class_weights.values())).to(device)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer &amp; scheduler</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">    optimizer, mode=<span class="string">&quot;min&quot;</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trainer module</span></span><br><span class="line">trainer = Trainer(model=model, device=device, loss_fn=loss_fn,</span><br><span class="line">                  optimizer=optimizer, scheduler=scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">best_model = trainer.train(NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 1 | train_loss: 0.78800, val_loss: 0.64168, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 2 | train_loss: 0.49324, val_loss: 0.60757, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 3 | train_loss: 0.38917, val_loss: 0.63572, lr: 1.00E-03, _patience: 4</span></span><br><span class="line"><span class="comment"># Epoch: 4 | train_loss: 0.31891, val_loss: 0.70638, lr: 1.00E-03, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 5 | train_loss: 0.26606, val_loss: 0.76403, lr: 1.00E-03, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 6 | train_loss: 0.22631, val_loss: 0.79747, lr: 1.00E-04, _patience: 1</span></span><br><span class="line"><span class="comment"># Stopping early!</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions</span></span><br><span class="line">test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Determine performance</span></span><br><span class="line">performance = get_metrics(</span><br><span class="line">    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)</span><br><span class="line"><span class="built_in">print</span> (json.dumps(performance[<span class="string">&quot;overall&quot;</span>], indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &quot;precision&quot;: 0.8065551302331581,</span></span><br><span class="line"><span class="comment">#   &quot;recall&quot;: 0.8066666666666666,</span></span><br><span class="line"><span class="comment">#   &quot;f1&quot;: 0.8062901077799052,</span></span><br><span class="line"><span class="comment">#   &quot;num_samples&quot;: 18000.0</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><h3 id="Glove-frozen">Glove (frozen)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">PRETRAINED_EMBEDDINGS = embedding_matrix</span><br><span class="line">FREEZE_EMBEDDINGS = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = CNN(</span><br><span class="line">    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,</span><br><span class="line">    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,</span><br><span class="line">    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,</span><br><span class="line">    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)</span><br><span class="line">model = model.to(device) <span class="comment"># set device</span></span><br><span class="line"><span class="built_in">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of CNN(</span></span><br><span class="line"><span class="comment">#   (embeddings): Embedding(5000, 100, padding_idx=0)</span></span><br><span class="line"><span class="comment">#   (conv): ModuleList(</span></span><br><span class="line"><span class="comment">#     (0): Conv1d(100, 50, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (1): Conv1d(100, 50, kernel_size=(3,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (2): Conv1d(100, 50, kernel_size=(4,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   )</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=150, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(<span class="built_in">list</span>(class_weights.values())).to(device)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer &amp; scheduler</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">    optimizer, mode=<span class="string">&quot;min&quot;</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trainer module</span></span><br><span class="line">trainer = Trainer(model=model, device=device, loss_fn=loss_fn,</span><br><span class="line">                  optimizer=optimizer, scheduler=scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">best_model = trainer.train(NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 1 | train_loss: 0.51462, val_loss: 0.49800, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 2 | train_loss: 0.43604, val_loss: 0.49792, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 3 | train_loss: 0.39698, val_loss: 0.50526, lr: 1.00E-03, _patience: 4</span></span><br><span class="line"><span class="comment"># Epoch: 4 | train_loss: 0.36507, val_loss: 0.51659, lr: 1.00E-03, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 5 | train_loss: 0.33745, val_loss: 0.53612, lr: 1.00E-03, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 6 | train_loss: 0.31418, val_loss: 0.56722, lr: 1.00E-04, _patience: 1</span></span><br><span class="line"><span class="comment"># Stopping early!</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions</span></span><br><span class="line">test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Determine performance</span></span><br><span class="line">performance = get_metrics(</span><br><span class="line">    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)</span><br><span class="line"><span class="built_in">print</span> (json.dumps(performance[<span class="string">&quot;overall&quot;</span>], indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &quot;precision&quot;: 0.8264024010717701,</span></span><br><span class="line"><span class="comment">#   &quot;recall&quot;: 0.8269444444444445,</span></span><br><span class="line"><span class="comment">#   &quot;f1&quot;: 0.8263287754212785,</span></span><br><span class="line"><span class="comment">#   &quot;num_samples&quot;: 18000.0</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><h3 id="GloVe-fine-tuned">GloVe (fine-tuned)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">PRETRAINED_EMBEDDINGS = embedding_matrix</span><br><span class="line">FREEZE_EMBEDDINGS = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = CNN(</span><br><span class="line">    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,</span><br><span class="line">    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,</span><br><span class="line">    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,</span><br><span class="line">    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)</span><br><span class="line">model = model.to(device) <span class="comment"># set device</span></span><br><span class="line"><span class="built_in">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of CNN(</span></span><br><span class="line"><span class="comment">#   (embeddings): Embedding(5000, 100, padding_idx=0)</span></span><br><span class="line"><span class="comment">#   (conv): ModuleList(</span></span><br><span class="line"><span class="comment">#     (0): Conv1d(100, 50, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (1): Conv1d(100, 50, kernel_size=(3,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (2): Conv1d(100, 50, kernel_size=(4,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   )</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=150, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Lossclass_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer &amp; scheduler</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">    optimizer, mode=<span class="string">&quot;min&quot;</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trainer module</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, device=device, loss_fn=loss_fn,</span><br><span class="line">    optimizer=optimizer, scheduler=scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">best_model = trainer.train(</span><br><span class="line">    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 1 | train_loss: 0.48751, val_loss: 0.45729, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 2 | train_loss: 0.38391, val_loss: 0.45669, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 3 | train_loss: 0.33045, val_loss: 0.47826, lr: 1.00E-03, _patience: 4</span></span><br><span class="line"><span class="comment"># Epoch: 4 | train_loss: 0.27825, val_loss: 0.52608, lr: 1.00E-03, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 5 | train_loss: 0.22646, val_loss: 0.60470, lr: 1.00E-03, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 6 | train_loss: 0.18130, val_loss: 0.70291, lr: 1.00E-04, _patience: 1</span></span><br><span class="line"><span class="comment"># Stopping early!</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions</span></span><br><span class="line">test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Determine performance</span></span><br><span class="line">performance = get_metrics(</span><br><span class="line">    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)</span><br><span class="line"><span class="built_in">print</span> (json.dumps(performance[<span class="string">&quot;overall&quot;</span>], indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &quot;precision&quot;: 0.8246875013006352,</span></span><br><span class="line"><span class="comment">#   &quot;recall&quot;: 0.8251666666666667,</span></span><br><span class="line"><span class="comment">#   &quot;f1&quot;: 0.8248028697657125,</span></span><br><span class="line"><span class="comment">#   &quot;num_samples&quot;: 18000.0</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><p>Ok, 保存一些必要的模型数据，以供后续能够完整的加载和使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save artifacts</span></span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="built_in">dir</span> = Path(<span class="string">&quot;cnn&quot;</span>)</span><br><span class="line"><span class="built_in">dir</span>.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">label_encoder.save(fp=Path(<span class="built_in">dir</span>, <span class="string">&quot;label_encoder.json&quot;</span>))</span><br><span class="line">tokenizer.save(fp=Path(<span class="built_in">dir</span>, <span class="string">&quot;tokenizer.json&quot;</span>))</span><br><span class="line">torch.save(best_model.state_dict(), Path(<span class="built_in">dir</span>, <span class="string">&quot;model.pt&quot;</span>))</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(Path(<span class="built_in">dir</span>, <span class="string">&quot;performance.json&quot;</span>), <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    json.dump(performance, indent=<span class="number">2</span>, sort_keys=<span class="literal">False</span>, fp=fp)</span><br></pre></td></tr></table></figure><h2 id="Inference">Inference</h2><p>接下来看看如何利用模型进行推理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_probability_distribution</span>(<span class="params">y_prob, classes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Create a dict of class probabilities from an array.&quot;&quot;&quot;</span></span><br><span class="line">    results = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, class_ <span class="keyword">in</span> <span class="built_in">enumerate</span>(classes):</span><br><span class="line">        results[class_] = np.float64(y_prob[i])</span><br><span class="line">    sorted_results = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">sorted</span>(</span><br><span class="line">        results.items(), key=<span class="keyword">lambda</span> item: item[<span class="number">1</span>], reverse=<span class="literal">True</span>)&#125;</span><br><span class="line">    <span class="keyword">return</span> sorted_results</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load artifacts</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">label_encoder = LabelEncoder.load(fp=Path(<span class="built_in">dir</span>, <span class="string">&quot;label_encoder.json&quot;</span>))</span><br><span class="line">tokenizer = Tokenizer.load(fp=Path(<span class="built_in">dir</span>, <span class="string">&quot;tokenizer.json&quot;</span>))</span><br><span class="line">model = CNN(</span><br><span class="line">    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,</span><br><span class="line">    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,</span><br><span class="line">    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,</span><br><span class="line">    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)</span><br><span class="line">model.load_state_dict(torch.load(Path(<span class="built_in">dir</span>, <span class="string">&quot;model.pt&quot;</span>), map_location=device))</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># CNN(</span></span><br><span class="line"><span class="comment">#   (embeddings): Embedding(5000, 100, padding_idx=0)</span></span><br><span class="line"><span class="comment">#   (conv): ModuleList(</span></span><br><span class="line"><span class="comment">#     (0): Conv1d(100, 50, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (1): Conv1d(100, 50, kernel_size=(3,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (2): Conv1d(100, 50, kernel_size=(4,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   )</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=150, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize trainer</span></span><br><span class="line">trainer = Trainer(model=model, device=device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dataloader</span></span><br><span class="line">text = <span class="string">&quot;The final tennis tournament starts next week.&quot;</span></span><br><span class="line">X = tokenizer.texts_to_sequences([preprocess(text)])</span><br><span class="line"><span class="built_in">print</span> (tokenizer.sequences_to_texts(X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [&#x27;final tennis tournament starts next week&#x27;]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y_filler = label_encoder.encode([label_encoder.classes[<span class="number">0</span>]]*<span class="built_in">len</span>(X))</span><br><span class="line">dataset = Dataset(X=X, y=y_filler, max_filter_size=max_filter_size)</span><br><span class="line">dataloader = dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line">y_prob = trainer.predict_step(dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line">label_encoder.decode(y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Class distributions</span></span><br><span class="line">prob_dist = get_probability_distribution(y_prob=y_prob[<span class="number">0</span>], classes=label_encoder.classes)</span><br><span class="line"><span class="built_in">print</span> (json.dumps(prob_dist, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &quot;Sports&quot;: 1.0,</span></span><br><span class="line"><span class="comment">#   &quot;World&quot;: 7.881690092248483e-12,</span></span><br><span class="line"><span class="comment">#   &quot;Sci/Tech&quot;: 1.270132816196673e-13,</span></span><br><span class="line"><span class="comment">#   &quot;Business&quot;: 2.3282168800871726e-18</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><p>推理结果是 “The final tennis tournament starts next week.” 这篇文章属于 “Sports” 这个分类。</p><p>我们可以看看不同的n-gram提取器，在最大池化层里提取都是什么。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">sample_index = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;Original text:\n<span class="subst">&#123;text&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;\nPreprocessed text:\n<span class="subst">&#123;tokenizer.sequences_to_texts(X)[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;\nMost important n-grams:&quot;</span>)</span><br><span class="line"><span class="comment"># Process conv outputs for each unique filter size</span></span><br><span class="line"><span class="keyword">for</span> i, filter_size <span class="keyword">in</span> <span class="built_in">enumerate</span>(FILTER_SIZES):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Identify most important n-gram (excluding last token)</span></span><br><span class="line">    popular_indices = collections.Counter([np.argmax(conv_output) \</span><br><span class="line">            <span class="keyword">for</span> conv_output <span class="keyword">in</span> conv_outputs[i]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get corresponding text</span></span><br><span class="line">    start = popular_indices.most_common(<span class="number">1</span>)[-<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">    n_gram = <span class="string">&quot; &quot;</span>.join([token <span class="keyword">for</span> token <span class="keyword">in</span> tokens[start:start+filter_size]])</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">f&quot;[<span class="subst">&#123;filter_size&#125;</span>-gram]: <span class="subst">&#123;n_gram&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Original text:</span></span><br><span class="line"><span class="comment"># The final tennis tournament starts next week.</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Preprocessed text:</span></span><br><span class="line"><span class="comment"># final tennis tournament starts next week</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Most important n-grams:</span></span><br><span class="line"><span class="comment"># [2-gram]: tennis tournament</span></span><br><span class="line"><span class="comment"># [3-gram]: final tennis tournament</span></span><br><span class="line"><span class="comment"># [4-gram]: final tennis tournament starts</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>如你所见，加入Embedding层的卷积神经网络模型的表现，相较于只有one-hot编码的模型，性能上有了很大的提升。</p>]]></content>
    
    <summary type="html">
    
      Explore and motivate the need for representation via embeddings.
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · Embeddings （上）</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-Embeddings-1/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-Embeddings-1/</id>
    <published>2023-07-04T07:28:36.000Z</published>
    <updated>2023-07-04T15:35:49.798Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>虽然One-Hot编码能够将离散变量表示为二进制向量，且能保留结构化信息，但它有两个主要的缺点：</p><ul><li>线性依赖词表的大小。这在语料库很大的情况下会带来问题如维数巨大且稀疏</li><li>单个token的表示，不保留其相对于其它token的关系</li></ul><p>本文将简单介绍embeddings，及它是如何解决one-hot编码的所有缺点。</p><h2 id="Learning-embeddings">Learning embeddings</h2><p>我们将通过使用PyTorch建模来学习embeddings，不过首先，我们学习一下专门用于嵌入和主题建模的库<a href="https://radimrehurek.com/gensim/" title="Gensim">Gensim</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.download(<span class="string">&quot;punkt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split text into sentence</span></span><br><span class="line">tokenizer = nltk.data.load(<span class="string">&quot;tokenizers/punkt/english.pickle&quot;</span>)</span><br><span class="line">book = requests.get(<span class="string">&quot;https://s3.mindex.xyz/datasets/harrypotter.txt&quot;</span>).content</span><br><span class="line">sentences = tokenizer.tokenize(<span class="built_in">str</span>(book))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;<span class="subst">&#123;<span class="built_in">len</span>(sentences)&#125;</span> sentences&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 12449 sentences</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Conditional preprocessing on our text.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Lower</span></span><br><span class="line">    text = text.lower()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Spacing and filters</span></span><br><span class="line">    text = re.sub(<span class="string">r&quot;([-;;.,!?&lt;=&gt;])&quot;</span>, <span class="string">r&quot; \1 &quot;</span>, text)  <span class="comment"># separate punctuation tied to words</span></span><br><span class="line">    text = re.sub(<span class="string">&quot;[^A-Za-z0-9]+&quot;</span>, <span class="string">&quot; &quot;</span>, text)  <span class="comment"># remove non alphanumeric chars</span></span><br><span class="line">    text = re.sub(<span class="string">&quot; +&quot;</span>, <span class="string">&quot; &quot;</span>, text)  <span class="comment"># remove multiple spaces</span></span><br><span class="line">    text = text.strip()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Separate into word tokens</span></span><br><span class="line">    text = text.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocess sentences</span></span><br><span class="line"><span class="built_in">print</span> (sentences[<span class="number">11</span>])</span><br><span class="line">sentences = [preprocess(s) <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line"><span class="built_in">print</span> (sentences[<span class="number">11</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Snape nodded, but did not elaborate.</span></span><br><span class="line"><span class="comment"># [&#x27;snape&#x27;, &#x27;nodded&#x27;, &#x27;but&#x27;, &#x27;did&#x27;, &#x27;not&#x27;, &#x27;elaborate&#x27;]</span></span><br></pre></td></tr></table></figure><p>embeddings的核心就是单词表示，且这种表示不只是依赖单词本身，而且依赖它的上下文。我们有几种不同的方法可以实现这一目标：</p><ul><li>给定上下文中的单词，预测目标单词（CBOW )</li><li>给定目标词，预测上下文词（skip-gram)</li><li>给定一个文本序列，预测下一个单词（ LM ）</li></ul><p>上面这些方法都涉及到创建数据来训练模型。句子中的每个单词都成为目标单词，上下文由窗口决定。</p><p>如下图（skip-gram），窗口大小为2。我们对语料库中的每个句子重复此操作，以产生用于无监督任务的训练数据。这个任务的核心逻辑是，相似的词会出现在相似的上下文中，我们可以通过反复的使用这种(target, context)文本对来学习这种关系。</p><p><img src="//s3.mindex.xyz/tmp/7bf17eefb4ff89642692d20685c9cb1a.webp" alt=""></p><p>我们可以使用上述任何一种方法来应用Embeddings。在任务中到底选择哪种方案，可能更多的需要依靠在监督任务上的表现来做选择。</p><h3 id="Word2Vec">Word2Vec</h3><p>当我们有大量的词汇表需要应用Embeddings时，事情会变得复杂。回想一下在反向传播中使用softmax更新正确的和不正确的分类权重，这种情况下每一次反向传播都意味着一个巨大的计算。因此解决方案是使用<a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" title="Word2Vec Negative Sampling">负采样</a>，它只更新正确的类和随机一部分不正确的类（NEGATIVE_SAMPLING = 20）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> KeyedVectors</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"></span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">WINDOW = <span class="number">5</span></span><br><span class="line">MIN_COUNT = <span class="number">3</span></span><br><span class="line">SKIP_GRAM = <span class="number">1</span></span><br><span class="line">NEGATIVE_SAMPLING = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">w2v = Word2Vec(</span><br><span class="line">    sentences=sentences, vector_size=EMBEDDING_DIM,</span><br><span class="line">    window=WINDOW, min_count=MIN_COUNT,</span><br><span class="line">    sg=SKIP_GRAM, negative=NEGATIVE_SAMPLING)</span><br><span class="line"><span class="built_in">print</span> (w2v)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Word2Vec&lt;vocab=4937, vector_size=100, alpha=0.025&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Vector for each word</span></span><br><span class="line">w2v.wv.get_vector(<span class="string">&quot;potter&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># array([ 0.04592679,  0.26393083, -0.29759625, -0.51007414,  0.02860732,</span></span><br><span class="line"><span class="comment">#        -0.01302573,  0.3703193 ,  0.14425582, -0.4187037 ,  0.04296769,</span></span><br><span class="line"><span class="comment">#        -0.13030362, -0.30441925, -0.14958233,  0.04964258,  0.14798391,</span></span><br><span class="line"><span class="comment">#        -0.18539314,  0.51730794,  0.01598365, -0.11325987, -0.6307836 ,</span></span><br><span class="line"><span class="comment">#         0.39244524,  0.25232184,  0.29555508, -0.22162063, -0.29100868,</span></span><br><span class="line"><span class="comment">#        -0.22083738, -0.52918744, -0.68654346, -0.09764519,  0.05514489,</span></span><br><span class="line"><span class="comment">#         0.06108054,  0.3587375 , -0.01166064, -0.42530054, -0.05000629,</span></span><br><span class="line"><span class="comment">#         0.45623606, -0.29811206, -0.09037815, -0.0024387 , -0.41930553,</span></span><br><span class="line"><span class="comment">#         0.12495753, -0.1773121 ,  0.19551197,  0.02754493,  0.25369856,</span></span><br><span class="line"><span class="comment">#         0.10022393, -0.38912103, -0.10274333, -0.24544689,  0.00851442,</span></span><br><span class="line"><span class="comment">#         0.26698554, -0.03026148,  0.12343717, -0.07433262,  0.0162609 ,</span></span><br><span class="line"><span class="comment">#         0.15033086,  0.09943663,  0.28371716, -0.26024884, -0.05571229,</span></span><br><span class="line"><span class="comment">#         0.0938114 , -0.00562614, -0.11472147,  0.21217017,  0.12490374,</span></span><br><span class="line"><span class="comment">#         0.34131378,  0.10346038,  0.38650215, -0.44265935, -0.02233333,</span></span><br><span class="line"><span class="comment">#        -0.47005087, -0.28585035,  0.06968105,  0.08989634,  0.22004889,</span></span><br><span class="line"><span class="comment">#        -0.22940454, -0.06248426,  0.089827  , -0.35011858,  0.11977731,</span></span><br><span class="line"><span class="comment">#        -0.06323916,  0.0940324 , -0.31842625,  0.53730965,  0.17043817,</span></span><br><span class="line"><span class="comment">#         0.15869781,  0.40275395,  0.04705542,  0.35397893,  0.00738561,</span></span><br><span class="line"><span class="comment">#         0.21539825,  0.14310665,  0.13341616, -0.0660746 ,  0.42496106,</span></span><br><span class="line"><span class="comment">#         0.09145384,  0.47487733, -0.23636843,  0.00715503,  0.05220298],</span></span><br><span class="line"><span class="comment">#       dtype=float32)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get nearest neighbors (excluding itself)</span></span><br><span class="line">w2v.wv.most_similar(positive=<span class="string">&quot;scar&quot;</span>, topn=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [(&#x27;forehead&#x27;, 0.9045635461807251),</span></span><br><span class="line"><span class="comment">#  (&#x27;pain&#x27;, 0.9014869928359985),</span></span><br><span class="line"><span class="comment">#  (&#x27;mouth&#x27;, 0.8918080925941467),</span></span><br><span class="line"><span class="comment">#  (&#x27;prickling&#x27;, 0.890386164188385),</span></span><br><span class="line"><span class="comment">#  (&#x27;throat&#x27;, 0.8795480728149414)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Saving and loading</span></span><br><span class="line">w2v.wv.save_word2vec_format(<span class="string">&quot;w2v.bin&quot;</span>, binary=<span class="literal">True</span>)</span><br><span class="line">wv = KeyedVectors.load_word2vec_format(<span class="string">&quot;w2v.bin&quot;</span>, binary=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="FastText">FastText</h3><p>当一个词在我们的词汇表中不存在时会发生什么？我们可以分配一个 UNK 标识来表示为未登录词，或者使用<a href="https://radimrehurek.com/gensim/models/fasttext.html" title="FastText">FastText</a>，它使用字符级的n-grams算法来embed单词，这样有助于处理罕见词,拼错的词，以及语料库中不存在但相似的词。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> FastText</span><br><span class="line"></span><br><span class="line"><span class="comment"># Super fast because of optimized C code under the hood</span></span><br><span class="line">ft = FastText(sentences=sentences, vector_size=EMBEDDING_DIM,</span><br><span class="line">              window=WINDOW, min_count=MIN_COUNT,</span><br><span class="line">              sg=SKIP_GRAM, negative=NEGATIVE_SAMPLING)</span><br><span class="line"><span class="built_in">print</span> (ft)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># FastText&lt;vocab=4937, vector_size=100, alpha=0.025&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This word doesn&#x27;t exist so the word2vec model will error out</span></span><br><span class="line">wv.most_similar(positive=<span class="string">&#x27;scarring&#x27;</span>, topn=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># KeyError: &quot;Key &#x27;scarring&#x27; not present in vocabulary&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># FastText will use n-grams to embed an OOV word</span></span><br><span class="line">ft.wv.most_similar(positive=<span class="string">&#x27;scarring&#x27;</span>, topn=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [(&#x27;swimming&#x27;, 0.9938331246376038),</span></span><br><span class="line"><span class="comment">#  (&#x27;howling&#x27;, 0.9927006959915161),</span></span><br><span class="line"><span class="comment">#  (&#x27;dabbing&#x27;, 0.9923058748245239),</span></span><br><span class="line"><span class="comment">#  (&#x27;wriggling&#x27;, 0.9921060800552368),</span></span><br><span class="line"><span class="comment">#  (&#x27;bulging&#x27;, 0.9919766783714294)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Save and loading</span></span><br><span class="line">ft.wv.save(<span class="string">&quot;ft.bin&quot;</span>)</span><br><span class="line">ftwv = KeyedVectors.load(<span class="string">&quot;ft.bin&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="Pretrained-embeddings">Pretrained embeddings</h2><p>我们可以利用上述方法从头开始应用embeddings，也可以利用已经在百万文档上训练过的预训练embeddings。流行的包括<a href="https://www.tensorflow.org/tutorials/text/word2vec" title="Word2Vec">Word2Vec</a>、<a href="https://nlp.stanford.edu/projects/glove/" title="GloVe">GloVe</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Preview of the GloVe embeddings file</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;glove.6B.100d.txt&quot;</span>, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    line = <span class="built_in">next</span>(fp)</span><br><span class="line">    values = line.split()</span><br><span class="line">    word = values[<span class="number">0</span>]</span><br><span class="line">    embedding = np.asarray(values[<span class="number">1</span>:], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">f&quot;word: <span class="subst">&#123;word&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">f&quot;embedding:\n<span class="subst">&#123;embedding&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">f&quot;embedding dim: <span class="subst">&#123;<span class="built_in">len</span>(embedding)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># word: the</span></span><br><span class="line"><span class="comment"># embedding:</span></span><br><span class="line"><span class="comment"># [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141</span></span><br><span class="line"><span class="comment">#   0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384</span></span><br><span class="line"><span class="comment">#  -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464</span></span><br><span class="line"><span class="comment">#  -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155</span></span><br><span class="line"><span class="comment">#  -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021</span></span><br><span class="line"><span class="comment">#   0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531</span></span><br><span class="line"><span class="comment">#   0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559</span></span><br><span class="line"><span class="comment">#  -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243</span></span><br><span class="line"><span class="comment">#   0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514</span></span><br><span class="line"><span class="comment">#   0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044</span></span><br><span class="line"><span class="comment">#   0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212</span></span><br><span class="line"><span class="comment">#  -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148</span></span><br><span class="line"><span class="comment">#  -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215</span></span><br><span class="line"><span class="comment">#  -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459</span></span><br><span class="line"><span class="comment">#   0.8278    0.27062 ]</span></span><br><span class="line"><span class="comment"># embedding dim: 100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load embeddings (may take a minute)</span></span><br><span class="line">glove = KeyedVectors.load_word2vec_format(<span class="string">&quot;glove.6B.100d.txt&quot;</span>, binary=<span class="literal">False</span>, no_header=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># (king - man) + woman = ?</span></span><br><span class="line"><span class="comment"># king - man = ? -  woman</span></span><br><span class="line">glove.most_similar(positive=[<span class="string">&quot;woman&quot;</span>, <span class="string">&quot;king&quot;</span>], negative=[<span class="string">&quot;man&quot;</span>], topn=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [(&#x27;queen&#x27;, 0.7698540687561035),</span></span><br><span class="line"><span class="comment">#  (&#x27;monarch&#x27;, 0.6843381524085999),</span></span><br><span class="line"><span class="comment">#  (&#x27;throne&#x27;, 0.6755736470222473),</span></span><br><span class="line"><span class="comment">#  (&#x27;daughter&#x27;, 0.6594556570053101),</span></span><br><span class="line"><span class="comment">#  (&#x27;princess&#x27;, 0.6520534157752991)]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get nearest neighbors (excluding itself)</span></span><br><span class="line">glove.most_similar(positive=<span class="string">&quot;goku&quot;</span>, topn=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [(&#x27;gohan&#x27;, 0.7246542572975159),</span></span><br><span class="line"><span class="comment">#  (&#x27;bulma&#x27;, 0.6497020125389099),</span></span><br><span class="line"><span class="comment">#  (&#x27;raistlin&#x27;, 0.644360363483429),</span></span><br><span class="line"><span class="comment">#  (&#x27;skaar&#x27;, 0.6316742897033691),</span></span><br><span class="line"><span class="comment">#  (&#x27;guybrush&#x27;, 0.6231325268745422)]</span></span><br></pre></td></tr></table></figure><p>我们可视化一下 king, queen, man, woman 这四个单词的位置关系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reduce dimensionality for plotting</span></span><br><span class="line">X = glove[glove.index_to_key]</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pca_results = pca.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_embeddings</span>(<span class="params">words, embeddings, pca_results</span>):</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        idx = embeddings.key_to_index[word]</span><br><span class="line">        plt.scatter(pca_results[idx, <span class="number">0</span>], pca_results[idx, <span class="number">1</span>])</span><br><span class="line">        plt.annotate(word, xy=(pca_results[idx, <span class="number">0</span>], pca_results[idx, <span class="number">1</span>]))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize</span></span><br><span class="line">plot_embeddings(</span><br><span class="line">    words=[<span class="string">&quot;king&quot;</span>, <span class="string">&quot;queen&quot;</span>, <span class="string">&quot;man&quot;</span>, <span class="string">&quot;woman&quot;</span>], embeddings=glove,</span><br><span class="line">    pca_results=pca_results)</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/tmp/6c1b85445e28b27bfed73fc0d1f7d3ec.png" alt=""></p><p>再看一下，离woman和doctor近，但离man远的词有哪些</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Bias in embeddings</span></span><br><span class="line">glove.most_similar(positive=[<span class="string">&quot;woman&quot;</span>, <span class="string">&quot;doctor&quot;</span>], negative=[<span class="string">&quot;man&quot;</span>], topn=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [(&#x27;nurse&#x27;, 0.7735227942466736),</span></span><br><span class="line"><span class="comment">#  (&#x27;physician&#x27;, 0.7189430594444275),</span></span><br><span class="line"><span class="comment">#  (&#x27;doctors&#x27;, 0.6824328303337097),</span></span><br><span class="line"><span class="comment">#  (&#x27;patient&#x27;, 0.6750683188438416),</span></span><br><span class="line"><span class="comment">#  (&#x27;dentist&#x27;, 0.6726033091545105)]</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>下一篇，我们将进一步介绍Embeddings如何提升我们前篇介绍的CNN分类模型。</p>]]></content>
    
    <summary type="html">
    
      Explore and motivate the need for representation via embeddings.
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 卷积神经网络</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-CNN/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-CNN/</id>
    <published>2023-06-27T09:30:43.000Z</published>
    <updated>2023-07-04T15:10:06.945Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>本文简单示范了如何利用CNN处理NLP任务。</p><p>CNNs的核心就是利用卷积（滑动）操作来提取数据特征的卷积核（aka kernels, filters,weights, etc.)。它们随机初始化但通过参数共享来提取特征。</p><p><img src="//s3.mindex.xyz/tmp/1a58a0c1e58cd3f543995ecee0eb71d4.gif" alt=""></p><h2 id="Set-up">Set up</h2><p>复用<a href="http://neo1989.net/Way2AI/Way2AI-utilities/" title="PyTorch实现神经网络的基本套路">《PyTorch实现神经网络的基本套路》</a> 里介绍的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_seeds</span>(<span class="params">seed=<span class="number">1024</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Set seeds for reproducibility.&quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    touch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)  <span class="comment"># multi-GPU</span></span><br><span class="line"></span><br><span class="line">set_seeds(seed=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">cuda = <span class="literal">True</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span>(torch.cuda.is_available() <span class="keyword">and</span> cuda) <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">torch.set_default_tensor_type(&#123;<span class="string">&quot;cuda&quot;</span>: <span class="string">&quot;torch.cuda.FloatTensor&quot;</span>, <span class="string">&quot;cpu&quot;</span>: <span class="string">&quot;torch.FloatTensor&quot;</span>&#125;.get(<span class="built_in">str</span>(device)))</span><br></pre></td></tr></table></figure><h3 id="Load-data">Load data</h3><p>我们将在<a href="https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset" title="AG News Classification Dataset">AGNews dataset</a> 这个数据集上完成本次学习任务。这是一份来自4个不同新闻分类120k条新闻标题样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load data</span></span><br><span class="line">url = <span class="string">&quot;https://s3.mindex.xyz/datasets/news.csv&quot;</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>)</span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/tmp/5820e01cf3a5f9f93ce85ba8d488647e.png" alt=""></p><h3 id="Preprocessing">Preprocessing</h3><p>首先要做的，是对这些数据进行预处理，手段包括删除停用词、字母小写（英文）、词形还原词干提取、中文分词、正则处理等。</p><p>由于我们的任务是纯英文数据，这里使用英文的通用处理方法。中文任务以后再表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> PorterStemmer</span><br><span class="line"></span><br><span class="line">nltk.download(<span class="string">&quot;stopwords&quot;</span>)</span><br><span class="line">STOPWORDS = stopwords.words(<span class="string">&quot;english&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (STOPWORDS[:<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;]</span></span><br><span class="line"></span><br><span class="line">porter = PorterStemmer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">text, stopwords=STOPWORDS</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Conditional preprocessing on our text unique to our task.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Lower</span></span><br><span class="line">    text = text.lower()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove stopwords</span></span><br><span class="line">    pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;\b(&quot;</span> + <span class="string">r&quot;|&quot;</span>.join(stopwords) + <span class="string">r&quot;)\b\s*&quot;</span>)</span><br><span class="line">    text = pattern.sub(<span class="string">&quot;&quot;</span>, text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove words in parenthesis</span></span><br><span class="line">    text = re.sub(<span class="string">r&quot;\([^)]*\)&quot;</span>, <span class="string">&quot;&quot;</span>, text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Spacing and filters</span></span><br><span class="line">    text = re.sub(<span class="string">r&quot;([-;;.,!?&lt;=&gt;])&quot;</span>, <span class="string">r&quot; \1 &quot;</span>, text)  <span class="comment"># separate punctuation tied to words</span></span><br><span class="line">    text = re.sub(<span class="string">&quot;[^A-Za-z0-9]+&quot;</span>, <span class="string">&quot; &quot;</span>, text)  <span class="comment"># remove non alphanumeric chars</span></span><br><span class="line">    text = re.sub(<span class="string">&quot; +&quot;</span>, <span class="string">&quot; &quot;</span>, text)  <span class="comment"># remove multiple spaces</span></span><br><span class="line">    text = text.strip()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply to dataframe</span></span><br><span class="line">preprocessed_df = df.copy()</span><br><span class="line">preprocessed_df.title = preprocessed_df.title.apply(preprocess)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;<span class="subst">&#123;df.title.values[-<span class="number">1</span>]&#125;</span>\n\n<span class="subst">&#123;preprocessed_df.title.values[-<span class="number">1</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Oil Slips Under \$55 a Barrel</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># oil slips 55 barrel</span></span><br></pre></td></tr></table></figure><h3 id="Split-data">Split data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_val_test_split</span>(<span class="params">X, y, train_size</span>):</span><br><span class="line">    X_train, X_, y_train,y_ = train_test_split(X, y, train_size=train_size, stratify=y)</span><br><span class="line">    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Data</span></span><br><span class="line">X = preprocessed_df[<span class="string">&quot;title&quot;</span>].values</span><br><span class="line">y = preprocessed_df[<span class="string">&quot;category&quot;</span>].values</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (84000,), y_train: (84000,)</span></span><br><span class="line"><span class="comment"># X_val: (18000,), y_val: (18000,)</span></span><br><span class="line"><span class="comment"># X_test: (18000,), y_test: (18000,)</span></span><br><span class="line"><span class="comment"># Sample point: wenger plans buy new keeper → Sports</span></span><br></pre></td></tr></table></figure><h2 id="Label-encoding">Label encoding</h2><p>复用<a href="http://neo1989.net/Way2AI/Way2AI-utilities/" title="PyTorch实现神经网络的基本套路">《PyTorch实现神经网络的基本套路》</a> 里介绍的 LabelEncoder</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Encode</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">label_encoder.fit(y_train)</span><br><span class="line">NUM_CLASSES = <span class="built_in">len</span>(label_encoder)</span><br><span class="line"><span class="built_in">print</span> (label_encoder.class_to_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;&#x27;Business&#x27;: 0, &#x27;Sci/Tech&#x27;: 1, &#x27;Sports&#x27;: 2, &#x27;World&#x27;: 3&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert labels to tokens</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line">y_train = label_encoder.encode(y_train)</span><br><span class="line">y_val = label_encoder.encode(y_val)</span><br><span class="line">y_test = label_encoder.encode(y_test)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: Sports</span></span><br><span class="line"><span class="comment"># y_train[0]: 2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> <span class="built_in">enumerate</span>(counts)&#125;</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># counts: [21000 21000 21000 21000]</span></span><br><span class="line"><span class="comment"># weights: &#123;0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Tokenizer">Tokenizer</h2><p>由于任务要处理的是文本，无法直接送给模型。因此我们定义一个Tokenizer来处理文本数据，目的是将文本序列转化成离散的标记（tokens)，以便后续的处理和分析。这意味着每个token可以映射到一个唯一的索引，这样我们就可以用一个索引数组（向量）来表示文本序列。而一个token可以是一个字符、一个单词、一个词组等等。</p><p>下面是一个示例实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> more_itertools <span class="keyword">import</span> take</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tokenizer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, char_level, num_tokens=<span class="literal">None</span>, pad_token=<span class="string">&quot;&lt;PAD&gt;&quot;</span>, oov_token=<span class="string">&quot;&lt;UNK&gt;&quot;</span>, token_to_index=<span class="literal">None</span></span>):</span><br><span class="line">        self.char_level = char_level</span><br><span class="line">        self.separator = <span class="string">&quot;&quot;</span> <span class="keyword">if</span> self.char_level <span class="keyword">else</span> <span class="string">&quot; &quot;</span></span><br><span class="line">        <span class="keyword">if</span> num_tokens:</span><br><span class="line">            num_tokens -= <span class="number">2</span> <span class="comment"># pad + unk tokens</span></span><br><span class="line">        self.num_tokens = num_tokens</span><br><span class="line">        self.pad_token = pad_token</span><br><span class="line">        self.oov_token = oov_token</span><br><span class="line">        self.token_to_index = token_to_index <span class="keyword">if</span> token_to_index <span class="keyword">else</span> &#123;pad_token: <span class="number">0</span>, oov_token: <span class="number">1</span>&#125;</span><br><span class="line">        self.index_to_token = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.token_to_index.items()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.token_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&lt;Tokenizer(num_tokens=<span class="subst">&#123;<span class="built_in">len</span>(self)&#125;</span>)&gt;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit_on_texts</span>(<span class="params">self, texts</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.char_level:</span><br><span class="line">            texts = [text.split(<span class="string">&quot; &quot;</span>) <span class="keyword">for</span> text <span class="keyword">in</span> texts]</span><br><span class="line">        all_tokens = [token <span class="keyword">for</span> text <span class="keyword">in</span> texts <span class="keyword">for</span> token <span class="keyword">in</span> text]</span><br><span class="line">        counts = Counter(all_tokens).most_common(self.num_tokens)</span><br><span class="line">        self.min_token_freq = counts[-<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> token, count <span class="keyword">in</span> counts:</span><br><span class="line">            index = <span class="built_in">len</span>(self)</span><br><span class="line">            self.token_to_index[token] = index</span><br><span class="line">            self.index_to_token[index] = token</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">texts_to_sequences</span>(<span class="params">self, texts</span>):</span><br><span class="line">        sequences = []</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> texts:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.char_level:</span><br><span class="line">                text = text.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">            sequence = []</span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> text:</span><br><span class="line">                sequence.append(self.token_to_index.get(</span><br><span class="line">                    token, self.token_to_index[self.oov_token]))</span><br><span class="line">            sequences.append(np.asarray(sequence))</span><br><span class="line">        <span class="keyword">return</span> sequences</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sequences_to_texts</span>(<span class="params">self, sequences</span>):</span><br><span class="line">        texts = []</span><br><span class="line">        <span class="keyword">for</span> sequence <span class="keyword">in</span> sequences:</span><br><span class="line">            text = []</span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> sequence:</span><br><span class="line">                text.append(self.index_to_token.get(index, self.oov_token))</span><br><span class="line">            texts.append(self.separator.join([token <span class="keyword">for</span> token <span class="keyword">in</span> text]))</span><br><span class="line">        <span class="keyword">return</span> texts</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, fp</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fp, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;</span><br><span class="line">                <span class="string">&quot;char_level&quot;</span>: self.char_level,</span><br><span class="line">                <span class="string">&quot;oov_token&quot;</span>: self.oov_token,</span><br><span class="line">                <span class="string">&quot;token_to_index&quot;</span>: self.token_to_index</span><br><span class="line">            &#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">cls, fp</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fp, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br></pre></td></tr></table></figure><p>本次实验我们限制tokens的数量为500个(停用词已删除)，其中包括两个占位的。如果您的计算资源足够，可以使用更大的tokens数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tokenize</span></span><br><span class="line">tokenizer = Tokenizer(char_level=<span class="literal">False</span>, num_tokens=<span class="number">500</span>)</span><br><span class="line">tokenizer.fit_on_texts(texts=X_train)</span><br><span class="line">VOCAB_SIZE = <span class="built_in">len</span>(tokenizer)</span><br><span class="line"><span class="built_in">print</span> (tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;Tokenizer(num_tokens=500)&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample of tokens</span></span><br><span class="line"><span class="built_in">print</span> (take(<span class="number">10</span>, tokenizer.token_to_index.items()))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;least freq token&#x27;s freq: <span class="subst">&#123;tokenizer.min_token_freq&#125;</span>&quot;</span>) <span class="comment"># use this to adjust num_tokens</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [(&#x27;&lt;PAD&gt;&#x27;, 0), (&#x27;&lt;UNK&gt;&#x27;, 1), (&#x27;39&#x27;, 2), (&#x27;b&#x27;, 3), (&#x27;gt&#x27;, 4), (&#x27;lt&#x27;, 5), (&#x27;us&#x27;, 6), (&#x27;new&#x27;, 7), (&#x27;oil&#x27;, 8), (&#x27;microsoft&#x27;, 9)]</span></span><br><span class="line"><span class="comment"># least freq token&#x27;s freq: 166</span></span><br></pre></td></tr></table></figure><p>Ok，接下来将我们文本数据全部token化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert texts to sequences of indices</span></span><br><span class="line">X_train = tokenizer.texts_to_sequences(X_train)</span><br><span class="line">X_val = tokenizer.texts_to_sequences(X_val)</span><br><span class="line">X_test = tokenizer.texts_to_sequences(X_test)</span><br><span class="line">preprocessed_text = tokenizer.sequences_to_texts([X_train[<span class="number">0</span>]])[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Text to indices:\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  (preprocessed) → <span class="subst">&#123;preprocessed_text&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  (tokenized) → <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Text to indices:</span></span><br><span class="line"><span class="comment">#   (preprocessed) → ibm wins time talks &lt;UNK&gt; case</span></span><br><span class="line"><span class="comment">#   (tokenized) → [ 31  32  69  26   1 100]</span></span><br></pre></td></tr></table></figure><h2 id="One-hot-encoding">One-hot encoding</h2><p>One-hot编码是一种将离散变量表示为二进制向量的技术。它允许我们以一种模型可以理解的方式来表示数据，并且不受token的实际值的影响。</p><p>举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设我们有个只含5个字符的词表：</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;a&quot;</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">&quot;e&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;i&quot;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">&quot;o&quot;</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;u&quot;</span>: <span class="number">4</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 那么文本 aou 就会表示成一个二维矩阵：</span></span><br><span class="line"><span class="comment"># 列对应着词表，而每一行表示单个token的二进制向量（只在词表对应位置置为1，其他位置为0）</span></span><br><span class="line">[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]]</span><br></pre></td></tr></table></figure><p>我们手动实现一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">to_categorical</span>(<span class="params">seq, num_classes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;One-hot encode a sequence of tokens.&quot;&quot;&quot;</span></span><br><span class="line">    one_hot = np.zeros((<span class="built_in">len</span>(seq), num_classes))</span><br><span class="line">    <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(seq):</span><br><span class="line">        one_hot[i, item] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> one_hot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># One-hot encoding</span></span><br><span class="line"><span class="built_in">print</span> (X_train[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span> (<span class="built_in">len</span>(X_train[<span class="number">0</span>]))</span><br><span class="line">cat = to_categorical(seq=X_train[<span class="number">0</span>], num_classes=<span class="built_in">len</span>(tokenizer))</span><br><span class="line"><span class="built_in">print</span> (cat)</span><br><span class="line"><span class="built_in">print</span> (cat.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [ 31  32  69  26   1 100]</span></span><br><span class="line"><span class="comment"># 6</span></span><br><span class="line"><span class="comment"># [[0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 0.]]</span></span><br><span class="line"><span class="comment"># (6, 500)</span></span><br></pre></td></tr></table></figure><p>接下来需要将我们的数据进行one-hot编码处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert tokens to one-hot</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(tokenizer)</span><br><span class="line">X_train = [to_categorical(seq, num_classes=vocab_size) <span class="keyword">for</span> seq <span class="keyword">in</span> X_train]</span><br><span class="line">X_val = [to_categorical(seq, num_classes=vocab_size) <span class="keyword">for</span> seq <span class="keyword">in</span> X_val]</span><br><span class="line">X_test = [to_categorical(seq, num_classes=vocab_size) <span class="keyword">for</span> seq <span class="keyword">in</span> X_test]</span><br></pre></td></tr></table></figure><h2 id="Padding">Padding</h2><p>由于我们的数据是不定长的新闻标题，而模型能够处理的是相同形状的数据，所以引入padding来预处理数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pad_sequences</span>(<span class="params">sequences, max_seq_len=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Pad sequences to max length in sequence.&quot;&quot;&quot;</span></span><br><span class="line">    max_seq_len = <span class="built_in">max</span>(max_seq_len, <span class="built_in">max</span>(<span class="built_in">len</span>(sequence) <span class="keyword">for</span> sequence <span class="keyword">in</span> sequences))</span><br><span class="line">    num_classes = sequences[<span class="number">0</span>].shape[-<span class="number">1</span>]</span><br><span class="line">    padded_sequences = np.zeros((<span class="built_in">len</span>(sequences), max_seq_len, num_classes))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> <span class="built_in">enumerate</span>(sequences):</span><br><span class="line">        padded_sequences[i][:<span class="built_in">len</span>(sequence)] = sequence</span><br><span class="line">    <span class="keyword">return</span> padded_sequences</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3D sequences</span></span><br><span class="line"><span class="built_in">print</span> (X_train[<span class="number">0</span>].shape, X_train[<span class="number">1</span>].shape, X_train[<span class="number">2</span>].shape)</span><br><span class="line">padded = pad_sequences(X_train[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span> (padded.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># (6, 500) (8, 500) (6, 500)</span></span><br><span class="line"><span class="comment"># (3, 8, 500)</span></span><br></pre></td></tr></table></figure><h2 id="Dataset">Dataset</h2><p>一如上篇文章里介绍的，我们需要把数据放在 Dataset 中，并使用 DataLoader 来有效地创建用于训练和验证的批次。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">FILTER_SIZE = <span class="number">1</span> <span class="comment"># unigram</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, X, y, max_filter_size</span>):</span><br><span class="line">        self.X = X</span><br><span class="line">        self.y = y</span><br><span class="line">        self.max_filter_size = max_filter_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&lt;Dataset(N=<span class="subst">&#123;<span class="built_in">len</span>(self)&#125;</span>)&gt;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        X = self.X[index]</span><br><span class="line">        y = self.y[index]</span><br><span class="line">        <span class="keyword">return</span> [X, y]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">self, batch</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Processing on a batch.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Get inputs</span></span><br><span class="line">        batch = np.array(batch)</span><br><span class="line">        X = batch[:, <span class="number">0</span>]</span><br><span class="line">        y = batch[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pad sequences</span></span><br><span class="line">        X = pad_sequences(X, max_seq_len=self.max_filter_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cast</span></span><br><span class="line">        X = torch.FloatTensor(X.astype(np.int32))</span><br><span class="line">        y = torch.LongTensor(y.astype(np.int32))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_dataloader</span>(<span class="params">self, batch_size, shuffle=<span class="literal">False</span>, drop_last=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">return</span> torch.utils.data.DataLoader(</span><br><span class="line">            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,</span><br><span class="line">            shuffle=shuffle, drop_last=drop_last, pin_memory=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create datasets for embedding</span></span><br><span class="line">train_dataset = Dataset(X=X_train, y=y_train, max_filter_size=FILTER_SIZE)</span><br><span class="line">val_dataset = Dataset(X=X_val, y=y_val, max_filter_size=FILTER_SIZE)</span><br><span class="line">test_dataset = Dataset(X=X_test, y=y_test, max_filter_size=FILTER_SIZE)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Datasets:\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  Train dataset:<span class="subst">&#123;train_dataset.__str__()&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  Val dataset: <span class="subst">&#123;val_dataset.__str__()&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  Test dataset: <span class="subst">&#123;test_dataset.__str__()&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Sample point:\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  X: <span class="subst">&#123;test_dataset[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  y: <span class="subst">&#123;test_dataset[<span class="number">0</span>][<span class="number">1</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Datasets:</span></span><br><span class="line"><span class="comment">#   Train dataset:&lt;Dataset(N=84000)&gt;</span></span><br><span class="line"><span class="comment">#   Val dataset: &lt;Dataset(N=18000)&gt;</span></span><br><span class="line"><span class="comment">#   Test dataset: &lt;Dataset(N=18000)&gt;</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: [[0. 1. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 0.]]</span></span><br><span class="line"><span class="comment">#   y: 1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create dataloaders</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_dataloader = train_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">val_dataloader = val_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">test_dataloader = test_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">batch_X, batch_y = <span class="built_in">next</span>(<span class="built_in">iter</span>(test_dataloader))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Sample batch:\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  X: <span class="subst">&#123;<span class="built_in">list</span>(batch_X.size())&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  y: <span class="subst">&#123;<span class="built_in">list</span>(batch_y.size())&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Sample point:\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  X: <span class="subst">&#123;batch_X[<span class="number">0</span>]&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  y: <span class="subst">&#123;batch_y[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Sample batch:</span></span><br><span class="line"><span class="comment">#   X: [64, 15, 500]</span></span><br><span class="line"><span class="comment">#   y: [64]</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: tensor([[0., 1., 0.,  ..., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 1., 0.,  ..., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.,  ..., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         ...,</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.,  ..., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.,  ..., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.,  ..., 0., 0., 0.]])</span></span><br><span class="line"><span class="comment">#   y: 1</span></span><br><span class="line"><span class="comment"># </span></span><br></pre></td></tr></table></figure><h2 id="CNN">CNN</h2><p>接下来呢要进入本篇的重点，CNN了。</p><h3 id="Inputs">Inputs</h3><p>下面这个简单的示例里，我们随机给出了N个样本，每个样本有8个token，而我们的词表大小是10个。</p><p>也就意味着，我们inputs的形状是 (N, 8, 10)</p><p>但需要注意的是，当我使用PyTorch处理CNN时，通道数需要在第二个维度，也就意味着，在这个例子里，我们的inputs的形状得是 (N, 10, 8)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume all our inputs are padded to have the same num of tokens.</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">max_seq_len = <span class="number">8</span>  <span class="comment"># tokens per input</span></span><br><span class="line">vocab_size = <span class="number">10</span>  <span class="comment"># one-hot size</span></span><br><span class="line">x = torch.randn(batch_size, max_seq_len, vocab_size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;X: <span class="subst">&#123;x.shape&#125;</span>&quot;</span>)</span><br><span class="line">x = x.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;X: <span class="subst">&#123;x.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X: torch.Size([64, 8, 10])</span></span><br><span class="line"><span class="comment"># X: torch.Size([64, 10, 8])</span></span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/tmp/fea49de2fb514aafde1b3545c5163058.png" alt=""></p><h3 id="Filters">Filters</h3><p>在下面的动画中，我们将卷积核和输入简化成2D，以便于可视化，而且实际上值并不总是是0或1，而是任意的浮点数。</p><p><img src="//s3.mindex.xyz/tmp/1a58a0c1e58cd3f543995ecee0eb71d4.gif" alt=""></p><p>现在回到我们的示例数据，单个样本的形状是(8, 10) [max_seq_len, vocab_size]，然后我们考虑用50个形状是(1, 3)的一维卷积来提取数据的特征，由于我们的数据的通道数是10 （num_channels = vocab_size = one_hot_size = 10）, 这边意味着这个卷积核的形状便是 (3, 10, 50) [kernel_size, vocab_size, num_filters]</p><p><img src="//s3.mindex.xyz/tmp/18c83d441855deaf7671fea9f9f26bdc.png" alt=""></p><p>这里有两个关键的概念，步长(stride) 和 填充(padding). 详见下图</p><p><img src="//s3.mindex.xyz/tmp/49dc51e7b89f5215577c01e74a17ce73.png" alt=""></p><p>这里采用一维卷积<a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d" title="Conv1d">Conv1D</a>来处理示例数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convolutional filters (VALID padding)</span></span><br><span class="line">num_filters = <span class="number">50</span> <span class="comment"># num filters</span></span><br><span class="line">filter_size = <span class="number">3</span></span><br><span class="line">stride = <span class="number">1</span></span><br><span class="line">padding = <span class="number">0</span>  <span class="comment"># valid padding (no padding)</span></span><br><span class="line">conv1 = nn.Conv1d(in_channels=vocab_size, out_channels=num_filters,</span><br><span class="line">                  kernel_size=filter_size, stride=stride,</span><br><span class="line">                  padding=padding, padding_mode=<span class="string">&quot;zeros&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;conv: <span class="subst">&#123;conv1.weight.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># conv: torch.Size([50, 10, 3])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Forward pass</span></span><br><span class="line">z = conv1(x)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;z: <span class="subst">&#123;z.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># z: torch.Size([64, 50, 6])</span></span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/tmp/0fcb44386fcdfdd5c7e56e79be18515a.png" alt=""></p><p>如你所见，我们输入数据max_seq_len=8，而经过卷积后的output的长度却是6。如果需要保证长度一致，那么就需要引入padding了。<br>$$<br>\begin{split}<br>W = \frac{W - F + 2P}{S} + 1 \\<br>P = \frac{S(W - 1) - W + F}{2}<br>\end{split}<br>$$</p><p>如果P不是一个整数，考虑向上取整(math.ceil)在右侧填充。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convolutional filters (SAME padding)</span></span><br><span class="line">num_filters = <span class="number">50</span> <span class="comment"># num filters</span></span><br><span class="line">filter_size = <span class="number">3</span></span><br><span class="line">stride = <span class="number">1</span></span><br><span class="line">padding = <span class="number">0</span>  <span class="comment"># valid padding (no padding)</span></span><br><span class="line">conv = nn.Conv1d(in_channels=vocab_size, out_channels=num_filters,</span><br><span class="line">                 kernel_size=filter_size, stride=stride,</span><br><span class="line">                 padding=padding, padding_mode=<span class="string">&quot;zeros&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;conv: <span class="subst">&#123;conv.weight.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># conv: torch.Size([50, 10, 3])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># `SAME` padding</span></span><br><span class="line">padding_left = <span class="built_in">int</span>((conv.stride[<span class="number">0</span>]*(max_seq_len-<span class="number">1</span>) - max_seq_len + filter_size) / <span class="number">2</span>)</span><br><span class="line">padding_right =<span class="built_in">int</span>(math.ceil((conv.stride[<span class="number">0</span>]*(max_seq_len-<span class="number">1</span>) - max_seq_len + filter_size) / <span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;padding: <span class="subst">&#123;(padding_left, padding_right)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># padding: (1, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Forward pass</span></span><br><span class="line">z = conv(F.pad(x, (padding_left, padding_right)))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;z: <span class="subst">&#123;z.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># z: torch.Size([64, 50, 8])</span></span><br></pre></td></tr></table></figure><p>未来我们会探索更高维度的卷积层。包括使用Conv2D来处理3D数据（图像、字符级别文本等），使用Conv3D来处理4D数据（视频、时间序列数据等）</p><h3 id="Pooling">Pooling</h3><p>池化是一种用于简化下游计算的方法，通过将高维特征图总结为较低维特征图来减少冗余信息。在卷积滤波器对输入进行处理后产生的特征映射中，由于卷积和重叠的性质，会存在大量的冗余信息。池化操作可以采用最大值或平均值等方式。下面是一个池化的示例：假设来自卷积层的输出是4x4的特征图，我们使用2x2的最大池化过滤器进行处理。</p><p><img src="//s3.mindex.xyz/tmp/16910ad2e084e2b567c1cfadffcabab4.png" alt=""></p><p>在这个例子里，我们使用<a href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d" title="MaxPool1d">MaxPool1D</a>取一个max值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Max pooling</span></span><br><span class="line">pool_output = F.max_pool1d(z, z.size(<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;Size: <span class="subst">&#123;pool_output.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([64, 50, 1])</span></span><br></pre></td></tr></table></figure><h3 id="Batch-normalization">Batch normalization</h3><p>在构建模型前，需要讨论的最后一个主题便是<a href="https://arxiv.org/abs/1502.03167" title="batch normalization">batch normalization</a>.  它是一种对来自前一层激活的标准化操作，使其均值为0，标准差为1。</p><p>在以前的笔记本中，我们对输入进行标准化，以便模型能够更快地进行优化，并提高学习率。这里采用相同的概念，但我们在重复的前向传递过程中保持标准化的值，以进一步帮助优化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Batch normalization</span></span><br><span class="line">batch_norm = nn.BatchNorm1d(num_features=num_filters)</span><br><span class="line">z = batch_norm(conv(x)) <span class="comment"># applied to activations (after conv layer &amp; before pooling)</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;z: <span class="subst">&#123;z.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># z: torch.Size([64, 50, 6])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Mean and std before batchnorm</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;mean: <span class="subst">&#123;torch.mean(conv(x)):<span class="number">.2</span>f&#125;</span>, std: <span class="subst">&#123;torch.std(conv(x)):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># mean: -0.00, std: 0.59</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Mean and std after batchnorm</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;mean: <span class="subst">&#123;torch.mean(z):<span class="number">.2</span>f&#125;</span>, std: <span class="subst">&#123;torch.std(z):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># mean: -0.00, std: 1.00</span></span><br></pre></td></tr></table></figure><h2 id="Modling">Modling</h2><h3 id="Model">Model</h3><p>可视化一下模型的前向传播.</p><ul><li>首先对输入tokenizer化 (batch_size, max_seq_len)</li><li>然后，one-hot编码 (batch_size, max_seq_len, vocab_size)</li><li>接下来，使用filters（filter_size, vocab_size, num_filter)进行卷积，然后批归一化。这里我们的filters相当于一个n-gram检测器。</li><li>紧跟着，应用max polling，从特征图中提取最相关信息</li><li>再接一个含dropout的全连接层</li><li>最后再一个softmax全连接层以输出最终的类别概率</li></ul><p><img src="//s3.mindex.xyz/tmp/3984d8e8ac31581c8b9525fd221e275c.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">NUM_FILTERS = <span class="number">50</span></span><br><span class="line">HIDDEN_DIM = <span class="number">100</span></span><br><span class="line">DROPOUT_P = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_filters, filter_size,</span></span><br><span class="line"><span class="params">                 hidden_dim, dropout_p, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># COnvolutional filters</span></span><br><span class="line">        self.filter_size = filter_size</span><br><span class="line">        self.conv = nn.Conv1d(</span><br><span class="line">            in_channels=vocab_size, out_channels=num_filters,</span><br><span class="line">            kernel_size=filter_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, padding_mode=<span class="string">&#x27;zeros&#x27;</span>)</span><br><span class="line">        self.batch_norm = nn.BatchNorm1d(num_features=num_filters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FC layers</span></span><br><span class="line">        self.fc1 = nn.Linear(num_filters, hidden_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, channel_first=<span class="literal">False</span>,</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Rearrange input so num_channels is in dim 1 (N, C, L)</span></span><br><span class="line">        x_in, = inputs</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> channel_first:</span><br><span class="line">            x_in = x_in.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Padding for `SAME` padding</span></span><br><span class="line">        max_seq_len = x_in.shape[<span class="number">2</span>]</span><br><span class="line">        padding_left = <span class="built_in">int</span>((self.conv.stride[<span class="number">0</span>]*(max_seq_len-<span class="number">1</span>) - max_seq_len + self.filter_size)/<span class="number">2</span>)</span><br><span class="line">        padding_right = <span class="built_in">int</span>(math.ceil((self.conv.stride[<span class="number">0</span>]*(max_seq_len-<span class="number">1</span>) - max_seq_len + self.filter_size)/<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Conv outputs</span></span><br><span class="line">        z = self.conv(F.pad(x_in, (padding_left, padding_right)))</span><br><span class="line">        z = F.max_pool1d(z, z.size(<span class="number">2</span>)).squeeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FC layer</span></span><br><span class="line">        z = self.fc1(z)</span><br><span class="line">        z = self.dropout(z)</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = CNN(vocab_size=VOCAB_SIZE, num_filters=NUM_FILTERS, filter_size=FILTER_SIZE,</span><br><span class="line">            hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="built_in">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of CNN(</span></span><br><span class="line"><span class="comment">#   (conv): Conv1d(500, 50, kernel_size=(1,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=50, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h3 id="Training">Training</h3><p>接下来，利用到<a href="http://neo1989.net/Way2AI/Way2AI-utilities/" title="PyTorch实现神经网络的基本套路">《PyTorch实现神经网络的基本套路》</a> 里介绍到Trainer类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">LEARNING_RATE = <span class="number">1e-3</span></span><br><span class="line">PATIENCE = <span class="number">5</span></span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Trainer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, device, loss_fn=<span class="literal">None</span>, optimizer=<span class="literal">None</span>, scheduler=<span class="literal">None</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set params</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.device = device</span><br><span class="line">        self.loss_fn = loss_fn</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self.scheduler = scheduler</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">self, dataloader</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Train step.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Set model to train mode</span></span><br><span class="line">        self.model.train()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over train batches</span></span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Step</span></span><br><span class="line">            batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">            inputs, targets = batch[:-<span class="number">1</span>], batch[-<span class="number">1</span>]</span><br><span class="line">            self.optimizer.zero_grad()  <span class="comment"># Reset gradients</span></span><br><span class="line">            z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">            J = self.loss_fn(z, targets)  <span class="comment"># Define loss</span></span><br><span class="line">            J.backward()  <span class="comment"># Backward pass</span></span><br><span class="line">            self.optimizer.step()  <span class="comment"># Update weights</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Cumulative Metrics</span></span><br><span class="line">            loss += (J.detach().item() - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">eval_step</span>(<span class="params">self, dataloader</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Validation or test step.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line">        y_trues, y_probs = [], []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Step</span></span><br><span class="line">                batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">                inputs, y_true = batch[:-<span class="number">1</span>], batch[-<span class="number">1</span>]</span><br><span class="line">                z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">                J = self.loss_fn(z, y_true).item()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Cumulative Metrics</span></span><br><span class="line">                loss += (J - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line">                y_trues.extend(y_true.cpu().numpy())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, np.vstack(y_trues), np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict_step</span>(<span class="params">self, dataloader</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Prediction step.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">        y_probs = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Forward pass w/ inputs</span></span><br><span class="line">                inputs, targets = batch[:-<span class="number">1</span>], batch[-<span class="number">1</span>]</span><br><span class="line">                z = self.model(inputs)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, num_epochs, patience, train_dataloader, val_dataloader</span>):</span><br><span class="line">        best_val_loss = np.inf</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">            <span class="comment"># Steps</span></span><br><span class="line">            train_loss = self.train_step(dataloader=train_dataloader)</span><br><span class="line">            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)</span><br><span class="line">            self.scheduler.step(val_loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Early stopping</span></span><br><span class="line">            <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">                best_val_loss = val_loss</span><br><span class="line">                best_model = self.model</span><br><span class="line">                _patience = patience  <span class="comment"># reset _patience</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                _patience -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> _patience:  <span class="comment"># 0</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Stopping early!&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Logging</span></span><br><span class="line">            <span class="built_in">print</span>(</span><br><span class="line">                <span class="string">f&quot;Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> | &quot;</span></span><br><span class="line">                <span class="string">f&quot;train_loss: <span class="subst">&#123;train_loss:<span class="number">.5</span>f&#125;</span>, &quot;</span></span><br><span class="line">                <span class="string">f&quot;val_loss: <span class="subst">&#123;val_loss:<span class="number">.5</span>f&#125;</span>, &quot;</span></span><br><span class="line">                <span class="string">f&quot;lr: <span class="subst">&#123;self.optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]:<span class="number">.2</span>E&#125;</span>, &quot;</span></span><br><span class="line">                <span class="string">f&quot;_patience: <span class="subst">&#123;_patience&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> best_model</span><br></pre></td></tr></table></figure><p>定义必要的组件，然后开始训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Defince Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(<span class="built_in">list</span>(class_weights.values())).to(device)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer &amp; scheduler</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">    optimizer, mode=<span class="string">&quot;min&quot;</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train module</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, device=device, loss_fn=loss_fn,</span><br><span class="line">    optimizer=optimizer, scheduler=scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">best_model = trainer.train(</span><br><span class="line">    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 1 | train_loss: 0.86713, val_loss: 0.79795, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 2 | train_loss: 0.77799, val_loss: 0.79238, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 3 | train_loss: 0.77053, val_loss: 0.78976, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 4 | train_loss: 0.76625, val_loss: 0.78882, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 5 | train_loss: 0.76305, val_loss: 0.78799, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 6 | train_loss: 0.76027, val_loss: 0.78786, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 7 | train_loss: 0.75813, val_loss: 0.78810, lr: 1.00E-03, _patience: 4</span></span><br><span class="line"><span class="comment"># Epoch: 8 | train_loss: 0.75588, val_loss: 0.78725, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 9 | train_loss: 0.75429, val_loss: 0.78740, lr: 1.00E-03, _patience: 4</span></span><br><span class="line"><span class="comment"># Epoch: 10 | train_loss: 0.75270, val_loss: 0.78747, lr: 1.00E-03, _patience: 3</span></span><br></pre></td></tr></table></figure><h3 id="Evaluaton">Evaluaton</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_metrics</span>(<span class="params">y_true, y_pred, classes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Per-class performance metrics.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">&quot;overall&quot;</span>: &#123;&#125;, <span class="string">&quot;class&quot;</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">&quot;weighted&quot;</span>)</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;precision&quot;</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;recall&quot;</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;f1&quot;</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;num_samples&quot;</span>] = np.float64(<span class="built_in">len</span>(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(classes)):</span><br><span class="line">        performance[<span class="string">&quot;class&quot;</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">&quot;precision&quot;</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">&quot;recall&quot;</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">&quot;f1&quot;</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">&quot;num_samples&quot;</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions</span></span><br><span class="line">test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Determine performance</span></span><br><span class="line">performance = get_metrics(</span><br><span class="line">    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)</span><br><span class="line"><span class="built_in">print</span> (json.dumps(performance[<span class="string">&quot;overall&quot;</span>], indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &quot;precision&quot;: 0.7074944756886696,</span></span><br><span class="line"><span class="comment">#   &quot;recall&quot;: 0.6868333333333333,</span></span><br><span class="line"><span class="comment">#   &quot;f1&quot;: 0.6866617275444412,</span></span><br><span class="line"><span class="comment">#   &quot;num_samples&quot;: 18000.0</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><p>保存一些必要的模型数据，以供后续能够完整的加载和使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save artifacts</span></span><br><span class="line"><span class="built_in">dir</span> = Path(<span class="string">&quot;cnn&quot;</span>)</span><br><span class="line"><span class="built_in">dir</span>.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">label_encoder.save(fp=Path(<span class="built_in">dir</span>, <span class="string">&quot;label_encoder.json&quot;</span>))</span><br><span class="line">tokenizer.save(fp=Path(<span class="built_in">dir</span>, <span class="string">&#x27;tokenizer.json&#x27;</span>))</span><br><span class="line">torch.save(best_model.state_dict(), Path(<span class="built_in">dir</span>, <span class="string">&quot;model.pt&quot;</span>))</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(Path(<span class="built_in">dir</span>, <span class="string">&#x27;performance.json&#x27;</span>), <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    json.dump(performance, indent=<span class="number">2</span>, sort_keys=<span class="literal">False</span>, fp=fp)</span><br></pre></td></tr></table></figure><h3 id="Inference">Inference</h3><p>接下来看看如何利用模型进行新的推理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load artifacts</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">label_encoder = LabelEncoder.load(fp=Path(<span class="built_in">dir</span>, <span class="string">&quot;label_encoder.json&quot;</span>))</span><br><span class="line">tokenizer = Tokenizer.load(fp=Path(<span class="built_in">dir</span>, <span class="string">&#x27;tokenizer.json&#x27;</span>))</span><br><span class="line">model = CNN(</span><br><span class="line">    vocab_size=VOCAB_SIZE, num_filters=NUM_FILTERS, filter_size=FILTER_SIZE,</span><br><span class="line">    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line">model.load_state_dict(torch.load(Path(<span class="built_in">dir</span>, <span class="string">&quot;model.pt&quot;</span>), map_location=device))</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># CNN(</span></span><br><span class="line"><span class="comment">#   (conv): Conv1d(500, 50, kernel_size=(1,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=50, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize trainer</span></span><br><span class="line">trainer = Trainer(model=model, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dataloader</span></span><br><span class="line">text = <span class="string">&quot;China’s economic recovery fades as services, factory activity show weakness&quot;</span></span><br><span class="line">sequences = tokenizer.texts_to_sequences([preprocess(text)])</span><br><span class="line"><span class="built_in">print</span> (tokenizer.sequences_to_texts(sequences))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [&#x27;china economic &lt;UNK&gt; &lt;UNK&gt; services &lt;UNK&gt; &lt;UNK&gt; show &lt;UNK&gt;&#x27;]</span></span><br><span class="line"></span><br><span class="line">X = [to_categorical(seq, num_classes=<span class="built_in">len</span>(tokenizer)) <span class="keyword">for</span> seq <span class="keyword">in</span> sequences]</span><br><span class="line">y_filler = label_encoder.encode([label_encoder.classes[<span class="number">0</span>]]*<span class="built_in">len</span>(X))</span><br><span class="line">dataset = Dataset(X=X, y=y_filler, max_filter_size=FILTER_SIZE)</span><br><span class="line">dataloader = dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line">y_prob = trainer.predict_step(dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span> (label_encoder.decode(y_pred))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [&#x27;Business&#x27;]</span></span><br></pre></td></tr></table></figure><p>推理结果是 “China’s economic recovery fades as services, factory activity show weakness” 这篇文章属于 “Business” 这个分类，符合预期。</p><p>我们来看一下这个case的具体概率分布</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_probability_distribution</span>(<span class="params">y_prob, classes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Create a dict of class probabilities from an array.&quot;&quot;&quot;</span></span><br><span class="line">    results = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, class_ <span class="keyword">in</span> <span class="built_in">enumerate</span>(classes):</span><br><span class="line">        results[class_] = np.float64(y_prob[i])</span><br><span class="line">    sorted_results = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">sorted</span>(</span><br><span class="line">        results.items(), key=<span class="keyword">lambda</span> item: item[<span class="number">1</span>], reverse=<span class="literal">True</span>)&#125;</span><br><span class="line">    <span class="keyword">return</span> sorted_results</span><br><span class="line"></span><br><span class="line"><span class="comment"># Class distributions</span></span><br><span class="line">prob_dist = get_probability_distribution(y_prob=y_prob[<span class="number">0</span>], classes=label_encoder.classes)</span><br><span class="line"><span class="built_in">print</span> (json.dumps(prob_dist, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &quot;Business&quot;: 0.7551461458206177,</span></span><br><span class="line"><span class="comment">#   &quot;World&quot;: 0.23087970912456512,</span></span><br><span class="line"><span class="comment">#   &quot;Sci/Tech&quot;: 0.01362547930330038,</span></span><br><span class="line"><span class="comment">#   &quot;Sports&quot;: 0.0003486045461613685</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>本篇给出了一个使用CNN对文本进行分类的完整示例，有很多细节需要深入学习。</p><p>但无论如何，先跑起来再说，在战斗中学习战斗。</p>]]></content>
    
    <summary type="html">
    
      先上手再说。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT开放函数调用能力 · 好用到震惊！</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-openai-function-calling/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-openai-function-calling/</id>
    <published>2023-06-16T04:06:39.000Z</published>
    <updated>2024-03-18T10:19:42.106Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>ChatGPT已经自带函数调用能力了，本文给了一个简单的示例。</p><h2 id="回顾">回顾</h2><p>笔者曾经在<a href="http://neo1989.net/Notes/NOTE-langchain-3/" title="LangChain | 快速释放LLMs的能力">LangChain</a>系列文章里交代利用LangChain赋予ChatGPT上网的能力。</p><p>然而OpenAI官方在<a href="https://openai.com/blog/function-calling-and-other-api-updates" title="function-calling-and-other-api-updates">June 13, 2023</a>的更新里提出了function calling的能力，可以说在这个方向上直接灭掉了LangChain。</p><p>先看一下官方有哪些更新。<br><img src="//s3.mindex.xyz/blog/Courses/68e512444d547a4c6727d27d049050f4.png" alt=""></p><ul><li>在Chat Completions API中提供了新的函数调用能力</li><li><code>gpt-4</code> 和 <code>gpt-3.5-turbo</code> 模型的小版本迭代</li><li><code>gpt-3.5-turbo</code> 扩展到了4倍（16k）的上下文的能力</li><li>SOTA embeddings 模型降价 75%</li><li><code>gpt-3.5-turbo</code> 降价25%</li><li><code>gpt-3.5-turbo-0301</code> 和 <code>gpt-4-0314</code> 的退役时间</li></ul><p>而最令人激动的，实属 <code>function calling</code></p><h2 id="一个示范">一个示范</h2><p>如下图，依然是用人话要股票信息，能够直接给出df数据（完成函数调用）。<br><img src="//s3.mindex.xyz/blog/Courses/ec1319f72a0c54c58a3d8080a9231041.png" alt=""></p><h2 id="如何实现">如何实现</h2><p>简单到令人发指。</p><p>首先，只需定义functions manifest，如下示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">functions = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;get_stock_a&quot;</span>,</span><br><span class="line">        <span class="string">&quot;description&quot;</span>: <span class="string">&quot;获取指定A股股票一段时间内的量价信息&quot;</span>,</span><br><span class="line">        <span class="string">&quot;parameters&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;object&quot;</span>,</span><br><span class="line">            <span class="string">&quot;properties&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;stock_name&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;description&quot;</span>: <span class="string">&quot;具体的股票名称或代号，如贵州茅台、中国移动&quot;</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">&quot;start_date&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;description&quot;</span>: <span class="string">&quot;开始日期，格式为2023-01-01&quot;</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                 <span class="string">&quot;end_date&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;description&quot;</span>: <span class="string">&quot;结束日期，格式为2023-01-01&quot;</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&quot;required&quot;</span>: [<span class="string">&quot;stock_name&quot;</span>, <span class="string">&quot;start_date&quot;</span>, <span class="string">&quot;end_date&quot;</span>],</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>当然这个是不能乱来的，需要遵循一定的规则，具体需要参考官方<a href="https://swagger.io/specification/" title="OpenAPI Specification">specification</a></p><p>然后，实现你的自定义方法，这个示例就是实现 <code>get_stock_a</code> 方法，以获取指定A股股票的量价数据。这里我不给出具体实现，有兴趣私聊。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_stock_a</span>(<span class="params">stock_name, start_date, end_date</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 获取指定A股股票的量价数据 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;stock_name: <span class="subst">&#123;stock_name&#125;</span>, start_date: <span class="subst">&#123;start_date&#125;</span>, end_date: <span class="subst">&#123;end_date&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>接着，只需在调用Chat Completions API时候，把functions带上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">requests.post(<span class="string">f&quot;<span class="subst">&#123;openai.api_base&#125;</span>/chat/completions&quot;</span>, headers=headers,</span><br><span class="line">    json=&#123;</span><br><span class="line">        <span class="string">&quot;model&quot;</span>: GPT_MODEL,  <span class="comment"># &#x27;gpt-3.5-turbo-0613&#x27; or &#x27;gpt-4-0613&#x27;</span></span><br><span class="line">        <span class="string">&quot;messages&quot;</span>: messages,</span><br><span class="line">        <span class="string">&quot;functions&quot;</span>: functions</span><br><span class="line">    &#125;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>最后，你会看到类似下面这样的返回，完成一点解析和调用的动作，这个事就成了。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;assistant&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;function_call&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;get_stock_a&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;arguments&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&#123;\n  \&quot;stock_name\&quot;: \&quot;招商银行\&quot;,\n  \&quot;start_date\&quot;: \&quot;2023-05-01\&quot;,\n  \&quot;end_date\&quot;: \&quot;2023-06-01\&quot;\n&#125;&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="完整示例">完整示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">GPT_MODEL = <span class="string">&quot;gpt-3.5-turbo-0613&quot;</span></span><br><span class="line"></span><br><span class="line">openai.api_key = <span class="string">&quot;&quot;</span>  <span class="comment"># 你的密钥</span></span><br><span class="line">openai.api_base = <span class="string">&quot;https://api.openai.com/v1&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">messages, functions</span>):</span><br><span class="line">    headers = &#123;<span class="string">&quot;Content-Type&quot;</span>: <span class="string">&quot;application/json&quot;</span>, <span class="string">&quot;Authorization&quot;</span>: <span class="string">f&quot;Bearer <span class="subst">&#123;openai.api_key&#125;</span>&quot;</span>&#125;</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.post(<span class="string">f&quot;<span class="subst">&#123;openai.api_base&#125;</span>/chat/completions&quot;</span>, headers=headers,</span><br><span class="line">            json=&#123;</span><br><span class="line">                <span class="string">&quot;model&quot;</span>: GPT_MODEL,</span><br><span class="line">                <span class="string">&quot;messages&quot;</span>: messages,</span><br><span class="line">                <span class="string">&quot;functions&quot;</span>: functions</span><br><span class="line">            &#125;,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> response.json()[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;message&quot;</span>]</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Unable to generate ChatCompletion response&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Exception: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">functions = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;get_stock_a&quot;</span>,</span><br><span class="line">        <span class="string">&quot;description&quot;</span>: <span class="string">&quot;获取指定A股股票一段时间内的量价信息&quot;</span>,</span><br><span class="line">        <span class="string">&quot;parameters&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;object&quot;</span>,</span><br><span class="line">            <span class="string">&quot;properties&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;stock_name&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;description&quot;</span>: <span class="string">&quot;具体的股票名称或代号，如贵州茅台、中国移动&quot;</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">&quot;start_date&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;description&quot;</span>: <span class="string">&quot;开始日期，格式为2023-01-01&quot;</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                 <span class="string">&quot;end_date&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;description&quot;</span>: <span class="string">&quot;结束日期，格式为2023-01-01&quot;</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&quot;required&quot;</span>: [<span class="string">&quot;stock_name&quot;</span>, <span class="string">&quot;start_date&quot;</span>, <span class="string">&quot;end_date&quot;</span>],</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_stock_a</span>(<span class="params">stock_name, start_date, end_date</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;stock_name: <span class="subst">&#123;stock_name&#125;</span>, start_date: <span class="subst">&#123;start_date&#125;</span>, end_date: <span class="subst">&#123;end_date&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chat_call</span>(<span class="params">message</span>):</span><br><span class="line">    messages = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;不要对函数中应该填入的数值作出自作主张的假设。如果用户的要求不够明确，要求澄清。&quot;</span>&#125;]</span><br><span class="line">    messages.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: message&#125;)</span><br><span class="line">    r = call(messages, functions)</span><br><span class="line"></span><br><span class="line">    fcall = r[<span class="string">&quot;function_call&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">eval</span>(fcall[<span class="string">&quot;name&quot;</span>])(**json.loads(fcall[<span class="string">&quot;arguments&quot;</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">chat_call(<span class="string">&quot;2023年5月1日到6月1日，招商银行的A股量价给我一份&quot;</span>)</span><br></pre></td></tr></table></figure><p>如果你足够幸运，你将看到这样一串文本: “stock_name: 招商银行, start_date: 2023-05-01, end_date: 2023-06-01”</p><h2 id="Ending">Ending</h2><p>不多说了，黄老板已经说过了，“跑起来掠食，或是努力奔跑免得成为掠食者的食物”。</p>]]></content>
    
    <summary type="html">
    
      Function calling capability in the Chat Completions API.
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
</feed>
