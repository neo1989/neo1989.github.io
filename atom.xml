<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>愚苏记</title>
  
  <subtitle>To no avail but try.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://neo1989.net/"/>
  <updated>2024-06-28T07:47:17.861Z</updated>
  <id>https://neo1989.net/</id>
  
  <author>
    <name>Neo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>一个好看的CSS效果</title>
    <link href="https://neo1989.net/HandMades/HANDMADE-css-3d-card-md/"/>
    <id>https://neo1989.net/HandMades/HANDMADE-css-3d-card-md/</id>
    <published>2024-06-28T07:08:10.000Z</published>
    <updated>2024-06-28T07:47:17.861Z</updated>
    
    <content type="html"><![CDATA[<style type="text/css">## {}@media screen and (min-width : 1000px) {#card {vertical-align: middle;position: relative;height: 600px;width: 300px;display: inline-block;left: 30%;margin-top: 40px;}}@media screen and (max-width : 1000px) {#card {vertical-align: middle;position: relative;height: 600px;width: 300px;display: inline-block;margin: 40px 0 0 40px;}}#card img {position: absolute;transition: 0.5s;}#card .cover {z-index: 1;}#card:hover .cover {box-shadow: 0 35px 35px -8px []rgba(0, 0, 0, 0.75) !important;transform: perspective(500px) rotateX(10deg);}#card .hero {z-index: 3;margin-left: 130px;margin-top: 50px;opacity: 0;}#card:hover .hero {border-radius: 0px !important;box-shadow: 0 0 0 0 rgba(0, 0, 0, 0) !important;opacity: 1;transform: perspective(500px) translate3d(0, -50px, 100px);}</style><p><mark>仅支持桌面设备，鼠标hover效果。</mark></p><p id="card"><img src="https://s3.mindex.xyz/blog/HandMades/IMG_3407.jpg" class="cover"><img src="https://s3.mindex.xyz/blog/HandMades/IMG_3407_m.png" class="hero"></p>]]></content>
    
    <summary type="html">
    
      眼前一亮。
    
    </summary>
    
    
      <category term="HandMades" scheme="https://neo1989.net/categories/HandMades/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="CSS" scheme="https://neo1989.net/tags/CSS/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · LLM Agent 简介</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-LLM-Agents/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-LLM-Agents/</id>
    <published>2024-03-29T06:27:42.000Z</published>
    <updated>2024-03-29T16:04:19.104Z</updated>
    
    <content type="html"><![CDATA[<p>设想一款大型语言模型（LLM）应用，它的设计目的是帮助金融分析师解答关于公司业绩的各类问题。如果配备了精心设计的检索增强生成（RAG）流程，分析师就能够轻松回答像“2022财年，X公司的总收入是多少？”这样的问题。对于经验丰富的分析师来说，这些信息可以轻松地从财务报表中提取出来。</p><p>现在请思考这样一个问题：“在2023财年第二季度的财报电话会议中，有哪三个重要的信息值得我们关注？请特别聚焦公司正在构建的技术壁垒。”</p><p>这种问题是金融分析师在编写报告时希望解答的，但是他们需要投入时间来寻找答案。</p><p>我们如何才能找到解决类似上述问题的方法呢？显然，这类信息的获取不仅仅是从财报会议中简单查找得来的。解答这类问题需要精心规划，有针对性的关注，记忆力，运用不同的工具，以及将复杂的问题分解成更简单的子问题。将这些要素融合在一起，我们就得到了所谓的 LLM Agent。</p><h3 id="What-is-an-AI-agent">What is an AI agent?</h3><p>我们可以将其理解为这样一个系统：它能够利用LLM进行问题的推理，构建解决问题的方案，并借助一系列工具来实施这个方案。</p><p>简单来说，这些智能体就是具有复杂推理能力，记忆功能以及执行任务手段的系统。</p><p>这种能力最初在 <a href="https://github.com/Significant-Gravitas/AutoGPT" title="AutoGPT">AutoGPT</a> 或 <a href="https://github.com/yoheinakajima/babyagi" title="BabyAGI">BabyAGI</a> 等项目中被发现，这些项目能够在几乎不需要人为干预的情况下解决复杂问题。为了更深入地理解AI智能体，我们来看一下由LLM驱动的AI智能体应用的一般架构。</p><p>一个智能体主要由以下几个关键部分组成：</p><ul><li>Agent core</li><li>Memory module</li><li>Tools</li><li>Planning module</li></ul><p><img src="//s3.mindex.xyz/blog/Courses/1a8e605eb77cb5ff10188caf9a7b3213.png" alt=""></p><h3 id="Agent-core">Agent core</h3><p>智能体的核心部分是一个集中协调模块，负责管理智能体的基本逻辑和行为特征。你可以把它看作是智能体的“主要决策中心”。</p><p>它可能包含以下定义：</p><ul><li>任务或者目标</li><li>可以执行的tools</li><li>对如何利用各种规划模块的详细解读</li><li>记忆</li><li>所持有的个性特征（可选）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">template = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">GENERAL INSTRUCTIONS</span></span><br><span class="line"><span class="string">Your task is to answer questions. If you cannot answer this question, request a helper or use a tool. Fill with Nil where no tool or helper is requireds.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">AVALIABLE Tools</span></span><br><span class="line"><span class="string">- Search Tool</span></span><br><span class="line"><span class="string">- Math Tool</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">AVALIABLE HELPERS</span></span><br><span class="line"><span class="string">- Decomposition: Breaks Complex Questions down into simpler subparts</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">CONTEXTUAL INFORMATION</span></span><br><span class="line"><span class="string">&lt;No Previous questions asked&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">QUESTIONS</span></span><br><span class="line"><span class="string">How much did the revenue grow between Q1 of 2024 and Q2 of 2024?</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">ANSWER FORMAT</span></span><br><span class="line"><span class="string">&#123;&quot;Tool_Request&quot;: &quot;&lt;FILL&gt;&quot;, &quot;Helper_Request&quot;: &quot;&lt;FILL&gt;&quot;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="Memory-module">Memory module</h3><p>记忆块在 AI 智能体中发挥着至关重要的作用。我们可以将内存模块理解为一种存储设备，它存放着智能体的内部记录以及与用户的互动信息。</p><p>有两种类型的记忆模块:</p><ul><li>Short-term memory: 这是一个记录智能体尝试回答用户单个问题过程中的行为和思维的日志：也就是智能体的“思维轨迹”。</li><li>Long-term memory: 这是一份记录用户与 AI 智能体之间互动和思考的清单，就像一本日志，记录了跨越数周或数月的对话历史。</li></ul><p>记忆的过程并不只依赖于基于语义相似性的检索方式。一般来说，我们会综合考虑语义相似性、信息的重要性、新近程度以及其他特定应用的度量标准，形成一个综合评分。这个评分被用于帮助我们检索到特定的信息。</p><h3 id="Tools">Tools</h3><p>工具可以理解为一系列明确定义的步骤，AI 智能体可以利用它们来完成各种任务。这些工具在很大程度上，就像是为特定目的定制的第三方 API。</p><p>举例来说，AI 智能体可以利用 RAG（Retrieval-Augmented Generation）工具，根据上下文生成恰当的回答；或者使用代码解释器来解决需要编程的复杂问题；它们甚至能通过 API 在互联网上搜寻信息，或者使用一些简单的 API 服务，比如查询天气的 API，或者即时通讯应用的 API。</p><h3 id="Plaining-module">Plaining module</h3><p>解决复杂问题，如分析一组财务报告以解答一个多层次的商业问题，通常需要巧妙的策略。利用 LLM 驱动的 AI 智能体，我们可以通过结合两种技术来应对这种复杂性：</p><ul><li>Task and question decomposition</li><li>Reflection or critic</li></ul><h4 id="任务拆解">任务拆解</h4><p>解答复杂问题或推理信息往往需要我们将其拆解。比如，这样一个问题，“在NVIDIA最近一次的财报电话会议中，有哪三大重要观点？”</p><p>要回答这个问题，我们无法直接从一个小时长的会议记录中直接提取出答案。但是，我们可以将这个问题拆分成多个子问题：</p><ul><li>“会议中讨论了哪些主要的技术变革？”</li><li>“企业面临哪些商业挑战？”</li><li>“财务状况如何？”</li></ul><p>这些问题都可以进一步细分。也就是说，我们需要一个专门的 AI 智能体来引导这个拆解过程。</p><h4 id="反省或批评">反省或批评</h4><p>像 ReAct、Reflexion、Chain of Thought 和 Graph of Thought 这样的技术，都是基于批评或证据的引导框架。它们广泛应用于提升大型语言模型（LLMs）的推理能力和响应效果。同时，这些技术也可以用来优化 AI 智能体（agent）生成的任务执行策略。</p><h3 id="商业应用智能体">商业应用智能体</h3><h4 id="“Talk-to-your-data”-agent">“Talk to your data” agent</h4><p>“与你的数据进行交流”并不是一个容易解决的问题。有很多挑战是直接使用 RAG 管道无法解决的：</p><ul><li>来源文档之间的语义相似度</li><li>如表格那样的复杂数据结构</li><li>缺乏明显的上下文信息（并不是每一块数据都包含其来源的标识）</li><li>用户提出的问题的复杂性</li><li>……等等</li></ul><p>你如何回答这个问题：“2023年第三季度与2024年第一季度之间，数据中心的收入增加了多少？”要解答这个问题，你实际上需要独立解答三个子问题（也就是说，我们需要一个能进行问题分解的模块）：</p><ul><li>2023年第三季度的数据中心收入是多少？</li><li>2024年第一季度的数据中心收入是多少？</li><li>这两个数值之间的差距是多少？</li></ul><p>在这种情况下，你需要一个能访问问题分解模块的 AI 智能体，这个模块能生成子问题并寻找答案，直到解决整个问题。此外，你还需要一个 RAG 工具，用于检索特定信息，以及一些能准确处理子问题的记忆模块。</p><h4 id="智能体集群">智能体集群</h4><p>我们可以将一群 AI 智能体看作是在同一环境中共存、并能够协同解决问题的智能体集合。这样的分布式智能体生态系统，就像是多个“智能”微服务协同工作，共同解决问题。</p><p>像 <a href="https://github.com/joonspk-research/generative_agents" title="Generative Agents">Generative Agents</a> 和 <a href="https://github.com/OpenBMB/ChatDev" title="ChatDev">ChatDev</a> 这样的多智能体环境在社区中备受欢迎。原因何在？因为像 ChatDev 这样的框架可以帮你构建一支包括工程师、设计师、产品管理人员、CEO 以及智能体在内的团队，以极低的成本创建基础软件。流行的游戏，如 “Brick Breaker” 或 “Flappy Bird”，甚至可以以低至50美分的价格制作出原型！</p><p>利用一群 AI 智能体，你可以为诸如经济研究的行为模拟、企业营销活动、物理基础设施的用户体验元素等应用，创建一个数字化的公司、社区，甚至是整个城镇。</p><h4 id="推荐助手">推荐助手</h4><p>推荐系统在互联网中扮演着重要角色。利用 AI 智能体驱动的对话推荐系统，我们可以提供更个性化的服务。</p><p>比如说，在一个电商网站上，有一个 AI 智能体可以帮你比较各种产品，并根据你的需求和选择为你提供推荐。我们甚至可以创建一种全方位的、类似于有礼宾服务的体验，让多个 AI 智能体帮助用户在在线商店中导航。就像选择看哪部电影或预定哪间酒店房间一样，这些体验可以被设计成对话——而不仅仅是类似决策树的对话！</p><h4 id="自定义写作">自定义写作</h4><p>另一种强大的工具是拥有一个可以协助你完成任务的个人 AI 写作助手，例如共同撰写电子邮件，或者帮你准备紧急会议和演讲。常规的写作工具的问题在于，对于不同的受众，我们必须根据他们的特性来定制不同类型的文本。比如说，向投资者的推介必须与团队的演讲用词不同。</p><p>AI 智能体可以利用你过去的工作。然后，你可以让这个智能体将一个它生成的推介塑造成符合你的个人风格，并根据你的特定场景和需求进行定制。这个过程通常对于一般的 LLM 微调来说，细节处理得过于微妙。</p><h4 id="多模态">多模态</h4><p>如果仅仅依赖文本输入，你其实无法真正深入地理解你的数据。所有前文提到的使用场景都可以通过构建能够处理多种输入的多模态智能体来进行增强，比如说，处理图像和音频文件。</p><p>开发社区正在积极探索的领域包括为数据整理、社交图谱以及领域专业知识开发的 AI 智能体，这些都是企业应用的活跃研究领域。</p><h3 id="Challenges">Challenges</h3><ul><li><p><strong>角色适应能力</strong>：LLM 智能体在特定领域内有效执行任务通常需要适配特定角色。针对 LLM 较弱的角色识别能力，可以通过针对性地微调 LLM，使用反映罕见角色或心理特征的数据来增强其性能。</p></li><li><p><strong>长期规划与有限上下文长度</strong>：基于长时间历史的规划是一大挑战，可能导致错误累积，智能体难以自我纠错。同时，LLM 支持的上下文长度有限，这可能制约智能体的功能，比如限制其利用短期记忆的能力。</p></li><li><p><strong>广义的人类价值观对齐</strong>：使智能体与多元化的人类价值观保持一致同样具有挑战性，这在传统 LLM 中也是一个普遍问题。一种可能的解决方法是通过设计先进的提示策略来调整 LLM，以便更好地对齐人类价值。</p></li><li><p><strong>提示的稳定性与可靠性</strong>：LLM 智能体可能涉及设计多个提示来支持不同功能模块，如记忆和规划，而对提示微小变化的高敏感度常导致可靠性问题。整个提示框架的设计使其更易受到稳定性问题的影响。可能的解决办法包括通过反复试验来设计提示元素、自动优化或调整提示，或利用 GPT 自动生成提示。另一个常见问题是“语言幻觉”，LLM 智能体依赖自然语言与外部组件交互，可能因为接收到的冲突信息而产生信息准确性问题。</p></li><li><p><strong>知识界限</strong>：控制 LLM 的知识范围具有挑战性，这直接影响模拟活动的有效性。LLM 内部的知识可能携带偏见或使用用户不熟悉的信息，这在特定环境下可能影响智能体的表现。</p></li><li><p><strong>效率</strong>：LLM 智能体的操作涉及大量请求处理，这可能影响其行动的效率，因其高度依赖于 LLM 的推理速度。同时，在部署多个智能体时，成本也成为了一个需要考虑的因素。</p></li></ul><h3 id="Source">Source</h3><p><a href="https://neo1989.net/Way2AI/Way2AI-langchain-3/" title="LangChain | 快速释放LLMs的能力">LangChain | 快速释放LLMs的能力</a></p><p><a href="https://lilianweng.github.io/posts/2023-06-23-agent/" title="LLM Powered Autonomous Agents">LLM Powered Autonomous Agents</a></p><p><a href="https://github.com/hyp1231/awesome-llm-powered-agent" title="Awesome LLM-Powered Agent">Awesome LLM-Powered Agent</a></p>]]></content>
    
    <summary type="html">
    
      一种利用大语言模型进行复杂任务执行的应用。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="LLM" scheme="https://neo1989.net/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · RAG 综述</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-RAG-Survey/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-RAG-Survey/</id>
    <published>2024-03-20T09:05:44.000Z</published>
    <updated>2024-03-25T09:09:31.984Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>RAG 通过融合外部数据库的知识，提高了模型的准确性和可靠性，尤其在知识密集型任务中更是如此。RAG 允许进行持续的知识更新，并整合特定领域的信息。RAG 将大语言模型的内在知识与外部数据库广大、不断更新的知识库协同融合。</p><p>本文深入探讨了RAG 范式的发展过程，包括 Naive RAG、Advanced RAG 以及 Modular RAG； 仔细剖析了构成 RAG 框架的三大核心部分，包括检索、生成以及增强技术；详细阐述了每个关键部分所采用的最先进技术，深入剖析了 RAG 系统的最新进展；同时，也介绍了评估 RAG 模型的各项指标和基准，以及最新的评估框架；最后，勾勒出了未来的研究方向，包括面临的挑战、多模态的拓展，以及 RAG 基础设施和生态系统的进步。</p><h2 id="Introduction">Introduction</h2><p>大语言模型在处理特定领域或高度专业化的查询时，存在显著的局限性。例如，当查询超出模型的训练数据范围或需要获取最新信息时，模型常常会生成错误的信息，这种现象被称为“幻觉”。这些问题使得在没有额外保护措施的情况下，直接将大语言模型作为黑箱解决方案用于实际生产环境变得不现实。然而，RAG 提供了一种有效的解决方式，它通过将外部数据检索融入生成过程，从而提升了模型提供准确和相关回应的能力。</p><p>具体来说，RAG 的工作流程首先包含一个检索步骤，在回答问题或生成文本之前，大语言模型 (LLMs) 会先向外部数据源发出查询，以获取相关信息。这个过程不仅为接下来的文本生成阶段提供依据，同时也确保了生成的内容基于检索到的证据，这大大增强了输出结果的准确性和相关性。在推理阶段，RAG 能够从知识库中动态检索信息，以解决生成内容中的事实错误，这种现象通常被称为 “幻觉” (hallucinations)。RAG 的引入极大地推动了大语言模型的发展，它已经迅速被采用，并成为了提升聊天机器人能力和使大语言模型更具实用价值的关键技术。</p><p><img src="//s3.mindex.xyz/blog/Courses/47723527970e1cda0cb0e887269fc6fa.png" alt="图1"></p><p>如 图1 所示，RAG 的发展过程可以分为四个明显的阶段。在 2017 年，RAG 刚刚诞生，这个阶段正好与 Transformer 架构的出现同时发生。这个时期，RAG 的主要工作是通过预训练模型（PTM）来学习和获得更多的知识，以此来提升语言模型的性能。在这个阶段，RAG 的主要努力集中在优化预训练的方法上。</p><p>在初始阶段过后，RAG相关研究进入了一段相对平静的时期，直到聊天型GPT的出现。聊天型GPT的问世无疑是一个转折点，它将大语言模型推向了研究的前沿。研究社区开始将关注点转向如何利用大语言模型的能力，以提高模型的控制性并满足日益变化的需求。因此，大部分的RAG研究开始集中在推理上，只有少数研究者专注于微调过程。随着大语言模型的能力的不断提升，尤其是在GPT-4的推出后，RAG技术的发展面貌发生了显著的变化。研究重点开始转向一种混合方法，这种方法结合了RAG和微调的优势，同时，仍有一小部分研究者持续关注预训练方法的优化。</p><p>尽管 RAG 研究取得了飞速的发展，但我们仍然缺乏对该领域的系统性总结和概括，这使得我们难以全面理解 RAG 的技术进步。本次调研的目标就是梳理 RAG 的全过程，并深入探讨其现状和未来发展趋势，我们将通过深入研究大语言模型中的检索增强技术来实现这一目标。</p><p>因此，本文的目标是全面梳理和组织 RAG 的技术原理、发展历程、内容，特别是在大语言模型诞生后的相关方法和应用，以及 RAG 的评估方法和应用场景。我们希望通过对现有 RAG 技术的全面概述和分析，为未来的发展方向提供洞见和展望。本研究的意图是让读者和实践者对大模型和 RAG 有一个深入且系统的理解，清楚地理解检索增强的发展进程和关键技术，明确各种技术的优点和局限以及它们适用的情境，并预测未来可能的发展趋势。</p><p>本文的主要贡献包括：</p><ul><li><p>全面而系统地回顾了最新的 RAG 技术，梳理了其从初级 RAG、高级 RAG 到模块化 RAG 的发展历程。这次回顾将 RAG 研究的更广泛的背景置于大语言模型的发展脉络中。</p></li><li><p>确定并讨论了 RAG 过程中的关键技术，特别是“检索”、“生成器”和“增强”这几个环节，并深入探讨他们的协同作用，明确阐述这些组件如何紧密协作以构建一个连贯和有效的 RAG 框架。</p></li><li><p>为 RAG 构建了一个全面的评估框架，明确了评估的目标和评价指标。本文的比较分析揭示了 RAG 相比于模型微调方法的优势和劣势。此外，本文也预测了 RAG 的未来发展方向，强调了解决当前挑战的可能改进，扩展到多模态环境的可能性，以及其生态系统的发展。</p></li></ul><h2 id="Definition">Definition</h2><p>我们可以通过其工作流程来理解RAG的定义。</p><p>如 图2 所示，这是一个典型的 RAG 应用工作流程。</p><p>假设有用户向 ChatGPT 提问了一个最近引发了大量公众讨论的重大事件。 作为目前最知名且使用最广泛的大语言模型，ChatGPT 受到其预训练数据的限制，对最新的事件并不了解。RAG 就能解决这个问题，它能从外部知识库中检索到最新的文档片段。在这个例子中，RAG 检索到了一些与用户提问相关的新闻文章。接着，这些文章和用户的问题一起，被整合成一个信息丰富的提示，从而使 ChatGPT 能够生成一个更有深度的回答。</p><p>这个例子展示了 RAG 的工作过程，以及它如何通过实时信息检索来提升模型的回答质量。</p><p><img src="//s3.mindex.xyz/blog/Courses/2e656cad5ada2a3fc179e5c211322434.png" alt="图2"></p><p>从技术角度来看，RAG 已经通过各种创新方法得到了丰富，解决了如“应检索什么”、“何时进行检索”以及“如何利用检索到的信息”等核心问题。对于“应检索什么”的问题，研究已经从简单的 Token 和实体检索发展到更复杂的结构，如数据块和知识图，这些研究主要关注检索的精细程度以及数据的结构化程度。粗粒度的检索可以提供更多的信息，但精度较低。而检索结构化的文本虽然可以提供更多的信息，但会牺牲一些效率。对于“何时进行检索”的问题，已经产生了从单次检索到自适应和多次检索的策略。频繁的检索可以带来更多的信息，但效率会降低。至于“如何利用”检索到的数据，已经在模型架构的各个层次（包括输入层、中间层和输出层）中开发了集成技术。尽管在中间层和输出层的集成表现得更有效，但这需要更多的训练，并可能导致效率降低。</p><p>RAG 是一种通过整合外部知识库来增强大语言模型的新范式。它采用了一种协同的策略，将信息检索机制和上下文学习（即 In-Context Learning，ICL）结合起来，以此提升大语言模型的性能。在这个框架下，用户发起的查询会通过搜索算法触发相关信息的检索。然后，这些信息被融入到大语言模型的提示中，为生成过程提供了更丰富的上下文。RAG 的主要优势在于，它避免了为特定任务重新训练大语言模型的需求。开发者可以直接连接一个外部知识库，通过丰富输入来提升模型的输出精度。由于其高实用性和低门槛，RAG 已经成为大语言模型系统中最受欢迎的架构之一，许多对话产品几乎完全基于 RAG 构建。</p><p>RAG 的工作流程可以概括为三个步骤。<br>首先，我们将大量的文本数据（语料库）分割为一小块一小块，然后使用一种叫做编码模型的工具，对这些小块进行标记，好比给每一块都打上了一个独特的标签。<br>接下来，当 RAG 收到一个查询请求时，它会根据这个请求和已经标记好的小块之间的相似度，找出最相关的几块。<br>最后，RAG 会根据这些找出的相关小块，生成一个回答。<br>这个过程就像是在一大堆书中，找出最相关的几本，然后根据这几本书的内容，回答一个问题。这三个步骤构成了 RAG 的基本框架，使得 RAG 能够找到相关信息，并生成与上下文相关的回答。</p><p>接下来，我们会详细介绍 RAG 的研究框架。</p><h2 id="RAG-Framework">RAG Framework</h2><p>尽管 RAG 在成本效益上表现出色，并且超过了原生大语言模型的表现，但它们也存在一些明显的限制。Advanced RAG 和 Modular RAG 的开发就是针对 Naive RAG 中的这些特定短板的改进。</p><h3 id="Naive-RAG">Naive RAG</h3><p>Naive RAG 研究范式代表了最初的方法，这种方法在 ChatGPT 得到广泛采用后不久就开始崭露头角。Naive RAG 遵循一种传统的流程，包括索引、检索和生成。它也被形象地称为 “Retrieve-Read” 模式。</p><h4 id="Indexing">Indexing</h4><p>Indexing 是数据准备的关键环节，这一过程在离线环境中进行，并涉及多个阶段。</p><p>首先，我们需要对原始数据进行索引，这一步骤包括清洗和提取数据，将各种文件格式（如 PDF，HTML，Word，Markdown）转换为标准化的纯文本。为了适应语言模型的上下文处理能力，我们会将这些文本分割成更小、更易于处理的片段，这个过程被称为 “切块”。<br>接着，我们会使用一个嵌入模型将这些片段转换为向量表示，这种嵌入模型的选择考虑了推理效率和模型大小的平衡，这样能在检索阶段方便地进行相似性比较。<br>最后，我们会创建一个索引来存储这些文本片段及其向量嵌入，这些信息以键值对的形式存储，从而实现高效、可扩展的搜索功能。</p><h4 id="Retrieval">Retrieval</h4><p>当系统接收到用户的查询请求后，它会使用与索引阶段相同的编码模型，将输入的信息转化为向量形式。接着，它会计算出这个查询向量与索引库中各个向量片段的相似程度。系统会优先选择出与查询向量最相似的前K个片段。这些片段将作为更丰富的上下文信息，帮助系统更好地回应用户的请求。</p><h4 id="Generation">Generation</h4><p>用户的查询请求和被选中的文档会被整合成一个连贯的指令，大语言模型需要根据这个指令来形成回应。模型回答问题的方式可能会根据具体任务的要求而有所不同，既可以依赖于其内在的参数知识，也可以仅限于使用所提供文档中的信息。在进行连续对话的情况下，任何已有的对话历史都可以被整合到指令中，这样模型就能有效地进行多轮对话交互。</p><h4 id="Naive-RAG-的缺点">Naive RAG 的缺点</h4><p>Naive RAG 技术在“信息检索”、“答案生成”和“信息增强”三个关键环节面临着重大挑战。</p><p>信息检索的质量问题主要体现在精准度低，导致检索到的信息片段对齐不佳，可能出现信息错位或者信息断裂等问题。同时也存在召回率低的情况，即无法检索到所有相关的信息片段，这会阻碍大语言模型（LLMs）生成全面的回应。此外，过时的信息也会使问题变得更加复杂，可能会导致检索结果的准确性降低。</p><p>在答案生成的质量方面，模型可能会产生“幻觉”问题，即生成的答案并未基于所提供的上下文，还有可能出现与上下文无关或者模型输出存在潜在偏见的问题。</p><p>在信息增强的过程中，如何有效地将检索到的段落的上下文与当前的生成任务结合起来也是一大挑战，可能会导致生成的内容不连贯或者不一致。当多个检索到的段落包含相似的信息时，生成的回应可能会出现重复或者过度冗余的问题。</p><p>如何判断多个检索到的段落对生成任务的重要性和相关性也是一个挑战，需要适当权衡每个段落的价值。此外，如何在保证输出一致性的前提下，处理不同的写作风格和语气也是一项重要的任务。</p><p>最后，如果生成模型过度依赖检索到的信息，可能会导致输出的内容只是重复检索到的内容，而没有提供新的价值或合成信息。</p><h3 id="Advanced-RAG">Advanced RAG</h3><p>在提高检索质量上，Advanced RAG 增加了预检索和后检索策略。Advanced RAG 利用滑动窗口技术、细粒度的切割方法，以及元数据等手段，对其索引方式进行了优化。 同时，它也引入了多种方法，以进一步优化检索过程。</p><h4 id="Pre-Retrieval-Process">Pre-Retrieval Process</h4><p><strong>Optimizing Data Indexing</strong> 优化数据索引的目标是提高索引内容的质量。实现这一目标主要涉及五大策略：提升数据的精细度，优化索引的结构，添加元数据，调整数据对齐方式，以及混合检索策略。</p><p>提高数据粒度的目标是为了提升文本的规范性、一致性、真实性和丰富的背景信息，从而进一步提高 RAG 系统的性能。这涉及到删除无关的信息，澄清实体和术语的含义，核实事实的真实性，保持信息的上下文关联性，以及更新过时的文档。</p><p>优化索引结构主要包括调整数据块的大小以便更好地捕获相关的上下文信息，跨越多个索引路径进行查询，以及利用图数据索引中节点间的关系，借助图结构的信息来获取相关的上下文。</p><p>添加元数据信息主要包括将一些元数据（如日期、目的等）融入到数据块中用于过滤，同时也包括整合如章节、参考文献的子部分等元数据，以提高检索的效率。</p><p>对齐优化是一种解决文档间不一致问题的方法，它通过在文档中引入<a href="https://arxiv.org/abs/2305.19912">“假设性问题”</a>，来修正这些对齐的问题和差异。</p><h4 id="Retrieval-2">Retrieval</h4><p>在检索阶段，我们主要通过计算查询内容和信息片段（chunks）之间的相似度来确定最合适的上下文。在这个过程中，嵌入模型扮演着至关重要的角色。在 Advanced RAG 技术中，我们有潜力对这些嵌入模型进行进一步的优化。</p><p><strong>Fine-tuning Embedding</strong> 微调嵌入模型对 RAG 系统检索到的内容的相关性有着重大影响。这个过程包括定制嵌入模型，以提升在特定领域，尤其是在处理持续发展或罕见术语的专业领域中的检索相关性。比如，由 BAAI 开发的 BGE 嵌入模型（例如 BGE-large-EN）就是一种高性能的嵌入模型，可以通过微调来优化检索的相关性。微调所需的训练数据可以用类似 GPT-3.5-turbo 这样的语言模型生成，这种模型可以构造出基于文档片段的问题，然后将这些问题与对应的答案作为微调的配对数据。</p><p><strong>Dynamic Embedding</strong> 动态嵌入能够根据词语在语境中的使用而变化，这与<a href="https://arxiv.org/abs/2004.04906">静态嵌入</a>不同，后者为每个词语分配一个固定的向量。例如，在像 BERT 这样的 Transformer 模型中，同一个词语在不同的语境中可能会有不同的嵌入表示。OpenAI 的 embeddings-ada-02 模型，是基于 GPT 等大语言模型（LLMs）原理构建的一种复杂的动态嵌入模型，能够理解并捕获语境信息。然而，它可能对语境的敏感度并不如最新的全尺寸语言模型，如 GPT-4 那样高。</p><h4 id="Post-Retrieval-Process">Post-Retrieval Process</h4><p>在从数据库中检索出有价值的上下文后，必须将其与查询内容一同输入到大语言模型（LLMs）中，同时也要应对上下文窗口大小限制带来的挑战。如果一次性将所有相关文档全都输入到大语言模型中，可能会超出模型处理的上下文窗口大小限制，导致信息混乱，干扰对关键信息的提取。因此，我们需要对检索到的内容进行进一步的处理，以解决这些问题。</p><p><strong>Re-Ranking</strong></p><p>将检索到的信息重新排序，以将最相关的内容重新定位到提示的边缘，是一项关键策略。这个概念已经在像 LlamaIndex、LangChain 和 HayStack 这样的框架中实现。例如，Diversity Ranker 根据文档的多样性优先进行重新排序，而 LostInTheMiddleRanker 则交替地将最佳文档放在上下文窗口的开头和结尾。此外，像 cohereAI rerank、BGE rerank 和 LongLLMLingua 这样的方法重新计算了相关文本和查询之间的语义相似性，解决了解释基于向量的模拟搜索语义相似性的挑战。</p><p><strong>Prompt Compression</strong></p><p>研究发现，检索文档中的噪声会对 RAG 的性能产生不良影响。在后处理阶段，我们主要关注如何压缩无关的上下文，凸显出关键段落，并减少整体上下文的长度。有些方法，如 Selective Context 和 LLMLingua，使用小型语言模型来计算提示的互信息（即两个变量之间的信息共享程度）或困惑度（用于衡量模型预测的准确性），以此来评估各个元素的重要性。<a href="https://arxiv.org/abs/2310.04408" title="RECOMP">RECOMP</a> 方法则通过训练不同级别的压缩算法来解决这个问题，而 <a href="https://arxiv.org/abs/2310.03025" title="Long Context">Long Context</a> 和 <a href="https://arxiv.org/abs/2310.05029" title="Walking Down the Memory Maze">“Walking in the Memory Maze”</a> 则通过设计总结技术来提高大语言模型对关键信息的识别能力，尤其是在处理大量上下文信息时。</p><h3 id="Modular-RAG">Modular RAG</h3><p>模块化的 RAG 结构进一步拓展了传统的 Naive RAG 框架，提供了更高的灵活性和多样性。它采用了多种方法来提升系统性能，如引入了一个用于相似度检索的搜索模块，并在检索器中采用了微调技术。为了解决特定的问题，研究人员开发了改进的 RAG 模块和迭代方法。模块化的 RAG 范式在 RAG 领域中逐渐成为主流，允许进行序列化的处理流程，或者在多个模块之间进行端到端的训练。图3 展示了三种 RAG 范式之间的关系。然而，模块化 RAG 并不是孤立存在的。Advanced RAG 是模块化 RAG 的一种特化形式，而 Naive RAG 则是 Advanced RAG 的一个特例。这三种范式之间的关系是一种继承和发展的过程。</p><p><img src="//s3.mindex.xyz/blog/Courses/de3edf0ee0b239184c3eae0197716cd4.png" alt="图3"></p><h4 id="New-Modules">New Modules</h4><p><strong>Search Module</strong> 相较于 Naive/Advanced RAG 的相似性检索，搜索模块更针对特定场景进行定制，进一步在额外的语料库上直接进行搜索。这种集成方式是通过大语言模型（LLM）生成的代码，以及如 SQL 或 Cypher 等查询语言和其他自定义工具实现的。进行搜索的数据源可以包括搜索引擎、文本数据、表格数据，以及知识图谱。</p><p><strong>Memory Module</strong> 该模块运用大语言模型（LLM）的记忆功能来引导信息检索。这种方法主要是找出与当前输入最匹配的记忆信息。<a href="https://arxiv.org/abs/2305.02437" title="Self-Memory">Self-Memory</a> 则是通过一个检索增强的生成器，迭代地创建一个无界的记忆池，这个过程中会结合“原始问题”和“双向问题”。通过运用一个能够自我改进，即利用自身输出进行优化的检索增强生成模型，使得在推理过程中，文本的表述更能贴近数据的实际分布。因此，模型会优先使用自身的输出结果，而非<a href="https://arxiv.org/abs/2203.08773">训练数据</a>。</p><p><strong>Fusion</strong> <a href="https://arxiv.org/abs/2402.03367" title="RAG-Fusion">RAG-Fusion</a> 改进了传统搜索系统的某些局限性，它采用一种多角度查询方法。这种方法利用大语言模型（LLM）将用户的搜索请求扩展到多个不同的角度去考虑。这样不仅能够抓住用户明确寻求的信息，还能挖掘出更深层次、更具变革性的知识。在这个过程中，系统会同时对原始的搜索请求和扩展后的请求进行搜索，然后通过智能排序优化搜索结果，再把最佳的搜索结果与新的查询请求配对。这种高级的方法确保搜索结果不仅符合用户明确的需求，还能捕捉到用户可能没有明确表达出来的需求，从而帮助用户发现更深入、更相关的信息。</p><p><strong>Routing</strong> RAG 系统在检索过程中会利用各种不同的资源，这些资源在领域、语言和格式上各不相同，根据具体情况，可以选择交替使用或者合并。查询路由能够根据用户的查询来决定接下来的操作，这些操作可能包括生成摘要、搜索特定的数据库，或者将不同的路径合并成一个响应。此外，查询路由还会选择最适合当前查询的数据存储，可能包括各种不同的资源，如向量存储、图数据库或关系数据库，甚至是索引的层次结构——比如，用于多文档存储的摘要索引和文档块向量索引。查询路由的决策是预定义的，通过调用大语言模型 (LLMs) 来执行，从而将查询指向选定的索引。</p><p><strong>Predict</strong> 不同于直接从数据源进行检索，此模块更倾向于利用大语言模型来生成必要的上下文。相较于直接检索得到的信息，大语言模型生成的内容更能准确地包含我们所需要的、与问题紧密相关的信息。</p><p><strong>Task Adapter</strong> 这个模块致力于让 RAG 在各种下游任务中发挥作用。<a href="https://arxiv.org/abs/2303.08518" title="UPRISE">UPRISE</a> 系统能够自动从预先构建的数据池中提取出用于零样本任务的输入提示，从而增强了不同任务和模型之间的通用性。同时，<a href="https://arxiv.org/abs/2209.11755" title="Promptagator">Promptagator</a> 则运用大语言模型作为少样本查询生成器，根据产生的数据，进一步构建出针对特定任务的检索器。通过发挥大语言模型的泛化能力，我们可以仅用极少的样本就能开发出针对特定任务的端到端检索系统。</p><h4 id="New-Patterns">New Patterns</h4><p>Modular RAG 的架构具有极高的灵活性，可以根据特定问题的需求，调整或替换 RAG 流程中的模块。</p><p>无论是 Naive RAG 还是 Advanced RAG，都可以看作是由一些固定的模块构成。如 图3 所示，Naive RAG 主要由 “检索”（Retrieve）和 “阅读”（Read）模块组成。而 Advanced RAG 在 Naive RAG 的基础上，增加了 “重写”（Rewrite）和 “重新排序”（Rerank）模块。总的来说，Modular RAG 具有更大的多样性和灵活性。</p><p>目前的研究主要探讨两种组织模式。第一种是添加或替换模块，第二种则关注调整模块间的工作流程。这种灵活性使得 RAG 流程可以定制化，从而有效地应对各种任务。</p><p><strong>Adding or Replacing Modules</strong></p><p>引入或替换模块的策略的主要目标是在保持检索-阅读过程的基本架构的同时，融入额外的模块以增强某些特定的功能。<a href="https://arxiv.org/abs/2305.14283">Rewrite-Retrieve-Read</a> 就是一个很好的例子，它在原有的检索-阅读过程中加入了重写环节，形成了一种新的“重写-检索-阅读”的流程。在这个过程中，大语言模型（LLM）的表现被用作一种强化学习的奖励机制，用来驱动重写模块。这样，重写模块就能够对检索的查询进行微调，从而提升读取器在完成下游任务时的性能。</p><p>同样，我们也可以在一些方法中选择性地替换模块，比如在 <a href="https://arxiv.org/abs/2209.10063" title="Generate-Read">Generate-Read</a> 的方法中，大语言模型（LLM）的生成模块就可以替代原有的检索模块。另一个例子是 <a href="https://arxiv.org/abs/2210.01296" title="Recite-Read">Recite-Read</a> 的方法，它将检索的过程从外部数据转移到了模型的权重之中。在这种方法中，大语言模型（LLM）需要首先记住特定任务的信息，然后再产生能够处理知识密集型自然语言处理任务的输出。</p><p><strong>Adjusting the Flow between Modules</strong></p><p>在调整各个模块的运行流程这个领域，人们正在努力增强语言模型和检索模型之间的互动。<a href="https://arxiv.org/abs/2212.14024" title="Demonstrate-Search-Predict">DSP</a> 提出了一种被称为“展示-搜索-预测”的框架，它将上下文学习系统视为一个具体的程序，而不仅仅是完成任务的最终提示，从而能更有效地处理那些需要大量知识的任务。而 <a href="https://arxiv.org/abs/2305.15294" title="Iterative Retrieval-Generation">ITER-RETGEN</a> 方法则利用生成的内容来引导信息检索，在一个不断循环的“检索-阅读-再检索-再阅读”的过程中，实现了“通过检索来增强生成”和“通过生成来增强检索”。这种方法展示了一种新颖的方式，即利用一个模块的输出来提升另一个模块的功能。</p><h4 id="Optimizing-the-RAG-Pipeline">Optimizing the RAG Pipeline</h4><p>检索过程的优化旨在提高 RAG 系统中信息的获取效率和质量。</p><p>当前的研究主要集中在融合各种搜索技术，精化检索步骤，引入认知回溯机制（这是一种根据已获得信息反向调整搜索策略的方法），实行灵活多样的查询策略，以及运用嵌入相似性（这是一种通过比较向量在多维空间中的位置关系来衡量其相似性的方法）。这些方法的共同目标是在 RAG 系统中实现检索效率与上下文信息深度的平衡。</p><p><strong>Hybrid Search Exploration</strong> RAG 系统通过巧妙地融合各类技术，如关键词检索、语义理解和向量匹配，从而提升其性能。这种方式充分利用了每种技术的特长，以适应各种不同的搜索需求和信息类型，保证我们能够始终找到与查询内容高度匹配且包含丰富上下文信息的答案。混合搜索的使用，为我们的检索策略提供了强有力的支持，从而提升了整个 RAG 系统的效果。</p><p><strong>Recursive Retrieval and Query Engine</strong> 递归检索的过程分为两个步骤。首先，在初始阶段，我们会获取一些较小的信息片段，以便抓住关键的语义含义。然后，在后续阶段，我们会向 LLM 提供含有更多上下文信息的较大的信息片段。这种两步式的检索方法旨在在效率和提供丰富的上下文响应之间找到一个平衡点。</p><p><strong>StepBack-prompt</strong> 让大语言模型（LLM）不再只关注具体的实例，而是参与到对更广泛概念和原则的推理中去。实验结果表明，当我们使用这种后向提示的方法时，对各种需要推理的挑战性任务的处理性能有了显著的提高，这也突出了这种方法与 RAG 过程的良好适配性。不仅在生成对后向提示的响应时，我们可以应用这些增强检索的步骤，而且在最终的问答过程中也可以使用。</p><p><strong>Sub-Queries</strong> 根据不同的场景，我们可以采用各种不同的搜索策略。比如，我们可以利用 LlamaIndex 等框架提供的搜索引擎，或者利用树形结构进行搜索，还可以通过向量搜索，或者简单地按顺序搜索数据的各个部分。</p><p><strong>Hypothetical Document Embeddings</strong> HyDE的工作原理基于一个假设，那就是生成的答案在特征空间中可能比直接查询更接近。HyDE利用 LLM 根据查询生成一个假设的答案（即一份文档），然后将这份文档转化为特征向量，并使用这个特征向量来检索与假设文档相似的真实文档。这种方法并不是直接根据查询来寻找特征向量的相似性，而是关注从一个答案到另一个答案的特征向量的相似性。然而，这种方法可能并不总是能得到理想的结果，特别是当语言模型对某个主题不够熟悉时，可能会导致更多的错误。</p><h2 id="Retrieval-3">Retrieval</h2><p>在使用 RAG 的过程中，我们必须要能够从数据源中高效地找到相关的文档，这是至关重要的。然而，构建一个高效的检索器却充满了挑战。<br>在这一部分，我们要探讨三个核心问题：1) 我们如何构建出准确的语义表达？2) 我们应该采用什么方法来确保查询内容和文档内容在语义层面上的一致性？3) 我们如何让检索器的结果更符合大语言模型的处理方式？</p><h3 id="Enhancing-Semantic-Representations">Enhancing Semantic Representations</h3><p>在 RAG 系统中，语义空间的作用非常重要，因为它负责将查询和文档进行多维度的对应关系映射。在这个语义空间中，检索的精确度会直接影响到 RAG 的最终效果。</p><h4 id="Chunk-optimization">Chunk optimization</h4><p>在处理外部文档时，首先我们需要将文档分解成更小的片段，以便提取出更细致的特征，然后将这些特征嵌入到一个代表其语义的模型中。但是，如果嵌入的文本片段过大或者过小，都可能导致我们得到的结果并不理想。因此，找出文档在语料库中最适合的片段大小，对于保证我们检索到的结果既准确又相关，是非常关键的。</p><p>选择合适的信息分块策略需要我们细心考虑多个关键因素，包括索引内容的特性、嵌入模型及其理想的分块大小、用户查询的可能长度和复杂度，以及特定应用对搜索结果的使用方式。例如，我们应根据内容的长度——长或短——来选择分块模型。另外，不同的嵌入模型在处理不同大小的信息块时，表现会有所不同。比如，sentence-transformer 在处理单句时效果更好，而 text-embedding-ada-002 在处理包含256或512个 tokens 的信息块时表现优异。</p><p>另外，用户输入问题的长度和复杂性，以及应用程序的特定需求（如语义搜索或问答），都会影响我们如何选择切分策略（chunking strategy）。这个选择可能直接受到我们选择的 LLMs 的 token 限制，需要我们对切分的块大小（block size）进行调整。实际上，要获取精确的查询结果，我们需要灵活地运用不同的切分策略。并没有一种“一刀切”的最佳策略，只有最适应特定情境的策略。</p><p>最新的研究正在探索各种用于提高检索效率和准确性的块优化技术。其中一种方法使用了类似滑动窗口的技术，通过在多个检索过程中融合全局相关信息，实现了分层检索。另一种策略，被称为 “small2big” 方法，它在初始搜索阶段使用小的文本块，然后在后续阶段向语言模型提供更大的相关文本块进行处理。</p><p>摘要嵌入技术依据文档的摘要（或概述）对检索结果进行优先级排序，能够帮助我们更全面地理解整个文档的上下文。另一方面，元数据过滤技术则是运用文档的元数据来优化过滤流程。还有一种创新的做法，即图索引技术，它将实体和关系转换为图中的节点和连接，这种转换在处理多跳问题时，能够显著提升结果的相关性。</p><p>这些多元化的方法相互结合，已经带来了显著的技术进步，这使得 RAG 在检索结果的质量和整体性能上都有了显著的提升。</p><h4 id="Fine-tuning-Embedding-Models">Fine-tuning Embedding Models</h4><p>确定了合适的信息块大小后，下一个重要步骤就是利用一种叫做“嵌入模型”的技术，把这些信息块和查询问题转化到一个叫做“语义空间”的地方。这个转化过程的好坏至关重要，因为它直接影响了我们的模型理解和表示所有信息的能力。最近的研究中，出现了一些重要的嵌入模型，比如 ‘AngIE’、Voyage 和 ‘BGE’ 等。这些模型已经在大量的信息中进行了预训练。然而，当它们被应用到专业领域时，可能在准确理解特定领域的信息方面会有所不足。</p><p>此外，为了确保模型能够从内容相关性的角度理解用户的查询，对嵌入模型进行针对特定任务的微调是必不可少的。如果一个模型没有经过微调，它可能无法充分满足特定任务的需求。因此，对嵌入模型进行微调对于其后续的实际应用来说至关重要。在微调嵌入模型的方法中，主要有两种不同的思路。</p><p><strong>Domain Knowledge Fine-tuning</strong> 要让嵌入式检索模型准确地反映特定领域的信息，使用特定领域的数据集进行微调是非常重要的。这种过程与常规的语言模型微调有所不同，主要体现在所使用的数据集的特性上。通常，嵌入式检索模型微调的数据集主要包括三个元素：查询，语料库，以及相关文档。模型使用这些查询在语料库中找出相关的文档。然后，通过看模型是否能够在回应查询时检索到这些相关文档来评估模型的效果。数据集的构建，模型的微调，以及评估阶段都有各自的挑战。‘LlamaIndex’ 提供了一套核心的类和函数，目的是为了优化嵌入式检索模型微调的工作流程，使这些复杂的过程变得更加简单。通过创建一个充满领域知识的语料库，并利用 ‘LlamaIndex’ 提供的方法，我们可以有效地微调嵌入式检索模型，使其更好地满足目标领域的特定需求。</p><p><strong>Fine-tuning for Downstream Tasks</strong> 为了提高模型性能，微调嵌入模型以适应各种下游任务是至关重要的一步。在采用 RAG 处理这些任务的领域，已经出现了一些创新的方法，它们通过充分利用 LLMs 的功能来微调嵌入模型。例如，Promptagator 通过使用 LLM 作为生成少样本查询的工具，创建出针对特定任务的检索器，从而解决了在有监督微调中，特别是在数据稀疏的领域中所面临的挑战。另一种方法 <a href="https://arxiv.org/abs/2310.07554" title="LLM-Embedder">LLM-Embedder</a> ，则是利用 LLMs 的功能，为多个下游任务的数据生成奖励信号。检索器的微调过程依赖于两种类型的监督信号：一种是数据集的明确标签，另一种是来自 LLMs 的奖励信号。这种双信号方法可以更有效地进行微调，使得嵌入模型能够更好地适应各种不同的下游应用。</p><p>虽然一些方法通过融入领域知识和特定任务的微调来改进语义的表达，但是，检索器并不总是能与特定的 LLM 达到最佳的配合效果。为了解决这个问题，一些研究者尝试通过使用来自 LLM 的反馈信息，对微调过程进行直接的指导和监督。这种直接监督的目标就是让检索器更好地与 LLM 对齐，从而在后续任务中提升性能表现。</p><h3 id="Aligning-Queries-and-Documents">Aligning Queries and Documents</h3><p>在 RAG 的应用场景中，检索器可能使用一个嵌入模型来同时编码查询和文档，或者为每个部分分别使用独立的模型。还有，用户的初始查询可能由于措辞不够准确或者缺乏足够的语义信息而受到影响。因此，使用户查询的语义空间与文档的语义空间对齐就显得至关重要。</p><h4 id="Query-Rewriting">Query Rewriting</h4><p>查询重写是一种基本的方法，用于使查询与文档的语义保持一致。例如，Query2Doc 和 ITER-RETGEN 等方法利用 LLM ，将原始查询和其他指导信息结合起来，生成一个类似于文档的内容（我们称之为“伪文档”）。HyDE 方法则是通过文本提示来构建查询向量，生成一个“假设文档”，以此来抓住核心的模式。还有一个叫做 RRR 的框架，它改变了传统的检索和阅读顺序，将重心放在了查询重写上。STEP-BACKPROMPTING 则让 LLM 能够进行基于高级概念的抽象推理和检索。此外，多查询检索方法利用 LLM 同时生成并执行多个搜索查询，这对于解决那些包含多个子问题的复杂问题具有明显的优势。</p><h4 id="Embedding-Transformation">Embedding Transformation</h4><p>除了像查询重构这样的大体策略之外，还有一些更为精细的技巧，这些技巧是专门为了改进嵌入转换而设计的。LlamaIndex 就是一个很好的例子，它引入了一种可以接在查询编码器后的适配器模块。这个适配器模块的作用是帮助进行微调，从而优化查询嵌入的表示方式，使其能更好地映射到与目标任务更为吻合的潜在空间中。</p><p><a href="https://arxiv.org/abs/2305.19912" title="SANTA">SANTA</a>  应对了将查询内容与结构化的外部文档进行匹配的挑战，尤其是在处理结构化和非结构化数据之间的不兼容性时。它通过两种预训练策略，提升了检索器对结构化信息的敏感性：首先，通过利用结构化和非结构化数据之间的内在关联，以此为基础进行对比学习的预训练，使模型更具结构化数据感知能力；其次，通过执行一种名为 “蒙版实体预测” 的策略。这种策略采用了以实体为中心的遮蔽方法，鼓励语言模型预测并填充被遮蔽的实体信息，从而加深对结构化数据的理解。</p><p>SANTA 解决了将查询内容与结构化的外部文档进行匹配的问题，尤其是在处理结构化和非结构化数据之间的不同之处时。该方法通过两种预训练策略，提升了检索器对结构化信息的识别能力：首先，通过利用结构化和非结构化数据之间的内在关联，以此为基础进行对比学习的预训练，使模型更具结构化数据感知能力；其次，通过执行一种名为 “蒙版实体预测” 的策略。这种策略采用了以实体为中心的遮蔽方法，鼓励语言模型预测并填充被遮蔽的实体信息，从而加深对结构化数据的理解。</p><h3 id="Aligning-Retriever-and-LLM">Aligning Retriever and LLM</h3><p>在 RAG 流程中，仅仅通过各种技术提升检索的命中率，并不一定能改善最终的效果，因为检索到的文档可能并不能满足 LLMs 的具体要求。因此，本节将介绍两种方法，目的是使检索的结果更好地适应大型语言模型的偏好。</p><h4 id="Fine-tuning-Retrievers">Fine-tuning Retrievers</h4><p>有很多研究利用来自大型语言模型（LLM）的反馈信号来改进检索模型。</p><p>例如，<a href="https://arxiv.org/abs/2305.17331" title="AAR">AAR</a> 通过使用编解码器结构，为预训练的检索器引入了监督信号。这是通过识别语言模型偏好的文档，这些文档是通过 FiD 交叉注意力得分来确定的。接着，检索器会通过硬负样本采样和标准交叉熵损失进行微调。最后，改进后的检索器可以直接用于提升未接触过的目标语言模型，从而提高目标任务的性能。此外，有研究建议，大型语言模型可能更倾向于关注可读性强，而非信息丰富的文档。</p><p><a href="https://arxiv.org/abs/2301.12652" title="REPLUG">REPLUG</a> 利用搜索工具和大型语言模型（LLM）来估算搜索到的文档的可能性，并通过计算这些可能性之间的差异（专业术语为 KL 散度）进行训练。这种直接而有效的训练方式，通过使用语言模型（LM）作为指导，提高了搜索模型的性能，同时还避免了需要使用复杂的&quot;交叉关注&quot;技术。</p><p>UPRISE 也采用预设的 LLM 来调整搜索提示器的性能。这个大型语言模型和搜索提示器都接收同样的输入——提示和输入的配对信息，然后利用大型语言模型给出的分数来指导搜索提示器的训练，就好像大型语言模型是给数据打标签的人一样。</p><p>另外，<a href="https://arxiv.org/abs/2208.03299" title="Atlas">Atlas</a> 提出了四种策略来优化和提升 AI 模型的学习效果：</p><ul><li>注意力蒸馏（Attention Distillation）。这个方法是通过使用 LLM（大型语言模型）在处理信息时产生的&quot;注意力分数&quot;，来帮助模型提取和学习知识。</li><li>EMDR2。这个策略是运用一种叫做&quot;期望最大化&quot;的算法，将搜寻到的信息（文档）作为一种&quot;隐藏的变量&quot;来训练模型。</li><li>困惑度蒸馏（Perplexity Distillation）。这个方法直接使用模型在生成信息（token）过程中的&quot;困惑度&quot;（一种衡量模型预测准确性的指标）来进行训练。</li><li>LOOP。这个策略提出了一种新的优化函数，它考虑到在删除某些信息（文档）后，对模型预测结果的影响。这种方法提供了一种高效的训练策略，使模型能更好地适应特定的任务。</li></ul><p>这些策略的目标是提升检索器与 LLM 的协同效应，从而提高检索的效果，并更准确地回答用户的问题。</p><h4 id="Adapters">Adapters</h4><p>微调模型可能会遇到一些挑战，比如需要通过 API 来实现功能集成，或者需要应对由于本地计算资源有限带来的一些限制。因此，有些方法会选择引入一个外部适配器，以帮助模型的调整和优化。</p><p><a href="https://arxiv.org/abs/2310.18347" title="PRCA">PRCA</a> 通过一个上下文提取阶段和一个受奖励驱动的阶段来训练适配器。然后，它使用一种基于 token 的自回归策略来优化检索器的输出。这种基于 token 的过滤方法利用交叉注意力得分来有效地过滤 token，只选择得分最高的输入 token。RECOMP 则引入了用于生成摘要的提取式和生成式压缩器。这些压缩器要么选择相关的句子，要么合成文档信息，为多文档查询创建定制的摘要。</p><p>另外，<a href="https://arxiv.org/abs/2305.04757" title="PKG">PKG</a> 提出了一种创新的方法，通过指令微调将知识集成到透明的模型中。在这种方法中，检索模块直接被替换，以根据查询生成相关的文档。这种方法有助于解决在微调过程中遇到的困难，并提高模型的性能。</p><h2 id="Generation-2">Generation</h2><p>RAG 的核心部分是它的生成器，这个生成器的任务是把检索到的信息转换成流畅且连贯的文本。和传统的语言模型不同，RAG 的生成器通过融合检索到的数据，使其在准确性和相关性上有所提升。在 RAG 中，生成器接收的输入不仅包括了常规的上下文信息，还包括通过检索器获取的相关文本段落。这种全面的输入让生成器能更深入地理解问题的上下文，从而产生更丰富、更贴近上下文的回应。</p><p>此外，生成器在生成内容时，会受到检索到的文本的指引，以确保生成的内容与检索的信息保持一致。多样化的输入数据使得在生成阶段的工作变得更有针对性，所有的努力都是为了让大型模型更好地适应从查询和文档中获取的输入数据。在接下来的小节中，我们将通过深入探讨后检索处理和微调的各个方面，来详细介绍生成器的功能。</p><h3 id="Post-retrieval-with-Frozen-LLM">Post-retrieval with Frozen LLM</h3><p>在 LLM 的领域，许多研究都选择使用像 GPT-4 这样的成熟模型，它们可以利用自身丰富的内部知识，系统地整合各种文档中检索到的信息。</p><p>然而，这些大型模型也面临着一些挑战，例如上下文长度的限制，以及对冗余信息的过度敏感。为了应对这些问题，一些研究开始将注意力转向了如何在信息检索后进行更好的处理。</p><p>后检索处理主要涉及对检索器从大型文档数据库中获取的相关信息进行处理、过滤或优化。其主要目标是提升检索结果的质量，使其更符合用户的需求或后续的任务。我们可以将其视为对在检索阶段获取的文档进行的再次处理。在后检索处理中，常见的操作包括信息压缩和结果的重新排序。</p><h4 id="Information-Compression">Information Compression</h4><p>检索器擅长从海量的知识库中寻找相关信息，但管理检索文档中众多的信息则是一项挑战。当前的研究正在试图通过扩大 LLM 的上下文长度来应对这个问题。然而，现有的大型模型在处理上下文限制方面仍存在困难。因此，在某些情况下，我们需要对信息进行压缩。信息压缩在降低噪声，解决上下文长度限制，以及提升生成效果方面具有重要的作用。</p><p>PRCA 通过训练一个能够提取关键信息的模块来应对这个问题。在处理上下文信息的阶段，只要给这个模块提供一个输入文本 $S_{input}$ ，它就能生成一个输出序列 $C_{extracted}$。这个输出序列就是对输入文档中的上下文信息的精炼和提炼。整个训练过程的目标就是让 $C_{extracted}$ 尽可能接近实际的上下文信息 $C_{truth}$。</p><p>类似地，RECOMP 也采取了一个相似的策略，它通过对比学习（Contrastive Learning）训练了一个能够压缩信息的模块。在这个训练过程中，每个训练数据点都包含一个正样本和五个负样本。在整个训练过程中，这个编码模块会通过最小化对比损失（Contrastive Loss）来进行学习和优化。</p><p>另一项研究选择了一种不同的策略，旨在通过减少文档数量来提高模型的答案准确性。<a href="https://arxiv.org/abs/2303.08559" title="filter-then-rerank">filter-then-rerank paradigm</a> 巧妙地结合了大型语言模型（LLMs）和小型语言模型（SLMs）的优势。在这个模型中，SLMs 起到过滤的作用，而 LLMs 则负责重新排序。研究发现，让 LLMs 对 SLMs 所识别出的复杂样本进行重新排序，可以在各种需要从大量信息中提取关键内容的任务（IE, Information Extraction）中获得显著的改进。</p><h4 id="Reranking">Reranking</h4><p>重排模型对于优化检索器获取的文档集的质量起着至关重要的作用。当语言模型需要处理更多的上下文信息时，其性能通常会有所下降，而重排模型能有效地解决这个问题。其核心思想是重新整理文档的顺序，将最相关的内容优先放在前面，从而控制了文档的总数量。这种方法不仅解决了在检索过程中需要处理更多信息（“上下文窗口扩展”）的挑战，而且还提高了检索的效率和反应速度。</p><p>重排模型在信息检索过程中起到了关键的双重作用，既像一个调整者，优化处理过程，又像一个提炼者，精炼信息的质量。这样，它就能为接下来的语言模型处理提供更有效、更精确的输入信息。</p><p>一种叫做上下文压缩的技术被引入重排序过程中，以提供更精确的检索信息。这种方法就像用一个筛子，把单个文档的冗余内容筛掉，只保留最重要的信息。最终的目标是在搜索结果中展示最相关的信息，让读者能看到更精确、更有针对性的内容。</p><h3 id="Fine-tuning-LLM-for-RAG">Fine-tuning LLM for RAG</h3><p>在 RAG 模型中，优化生成器是极其重要的一环。生成器的职责是将检索得到的信息转化为相关的文本，这些文本就构成了模型的最终输出。通过优化生成器，我们希望生成的文本不仅流畅自然，而且能够有效地利用检索到的文档，以更好地满足用户的查询需求。</p><p>在传统的 LLM 生成任务中，我们通常只需输入一个查询。然而，RAG 的独特之处在于，它不仅接受查询作为输入，还将检索器找到的各种文档（无论是结构化的还是非结构化的）纳入输入。这些额外的信息对模型的理解有着重大影响，特别是对于规模较小的模型。因此，我们需要对模型进行微调，使其能够适应同时包含查询和检索文档的输入。在将输入提供给经过微调的模型之前，我们通常需要对检索器找到的文档进行后检索处理。值得注意的是，对 RAG 中生成器的微调方法与对 LLM 的一般微调方法是一致的。</p><p>接下来，我们将简要介绍一些涉及数据（无论是格式化的还是非格式化的）和优化函数的典型研究。</p><h4 id="General-Optimization-Process">General Optimization Process</h4><p>在通用的优化过程中，训练数据通常由输入和输出的配对构成，目标是训练模型在接收到输入 $x$ 时能够产生对应的输出 $y$。在 Self-Memory 的研究中，我们采用了传统的训练流程，也就是在给定输入 $x$ 的情况下，检索出相关的文档 $z$，然后在将输入 $x$ 和文档 $z$ 组合后，模型就能够生成输出 $y$。Self-Memory 使用了两种常见的微调模式，分别被称为联合编码器（Joint-Encoder）和双重编码器（Dual-Encoder）。</p><p>在联合编码器（Joint-Encoder）范式中，我们使用一个基于编码器-解码器的标准模型。在这里，编码器首先将输入进行编码，然后解码器通过注意力机制，将编码的结果结合起来，以自回归的方式生成 tokens。另一方面，在双重编码器（Dual-Encoder）范式中，系统设置了两个独立的编码器，每个编码器分别对输入（查询，上下文）和文档进行编码。然后，解码器会按顺序对生成的输出进行双向交叉注意力处理。这两种架构都以 Transformer 作为基础构件，并通过优化负对数似然损失来进行训练。</p><h4 id="Utilizing-Contrastive-Learning">Utilizing Contrastive Learning</h4><p>在为语言模型准备训练数据的过程中，我们通常会生成输入和输出的配对数据。这种传统方式可能导致一种称为&quot;暴露偏差&quot;的问题，也就是说，模型只在单一的正确输出样本上进行训练，这限制了模型接触和学习各种可能输出引用的机会 。这个限制可能会影响模型在实际应用中的性能，因为模型可能会过度拟合训练集中的特定样本，从而降低其在不同上下文中的泛化能力。</p><p>为了缓解&quot;暴露偏差&quot;，<a href="https://arxiv.org/abs/2305.18846" title="SURGE">SURGE (SUbgraph Retrieval-augmented GEneration)</a> 提出了采用图文对比学习的方法。这种方法包含一个对比学习目标，鼓励模型生成一系列可能且连贯的回应，这些回应超越了训练数据中的样本。这种策略对于减少过拟合和提升模型的泛化能力至关重要。</p><p>在处理涉及结构化数据的检索任务时，SANTA 框架采用了三阶段的训练方法，旨在有效地捕捉数据的结构特性和语义差异。在训练的初始阶段，主要关注的是检索器，通过使用&quot;对比学习&quot;（一种通过比较不同样本来提升模型学习效果的方法）来优化查询和文档的嵌入表示。</p><p>接下来，在生成器的初级训练阶段，我们采用一种名为“对比学习”（Contrastive Learning）的方法，它的目标是将结构化的数据与其对应的非结构化文档描述相匹配。在生成器训练的后期阶段，<a href="https://arxiv.org/abs/1905.07129">模型开始重视实体的语义（entity semantics）在理解文本数据中的重要性</a>。这个过程首先从结构化数据中找出实体，然后在输入给生成器的数据中，将这些实体进行隐藏或者说“遮罩”（masking），这样模型就可以试图预测这些被隐藏的部分，从而更好地理解和学习数据的内在结构。</p><p>训练的过程中，模型会学习如何利用上下文信息来重新构建被隐藏的实体。这一步骤有助于模型理解文本数据的内在结构和含义，也帮助模型更好地匹配结构化数据中的相关实体。最终的优化目标是让语言模型学会准确地恢复被隐藏的部分，这样可以进一步提升模型对实体语义的理解。</p><h2 id="Augmentation-in-RAG">Augmentation in RAG</h2><p>这部分内容主要围绕三个关键方面展开：增强阶段，增强数据的来源以及增强过程。这些方面揭示了对 RAG 发展至关重要的关键技术。RAG 的核心组件的分类在 图4 中进行了展示。</p><p><img src="//s3.mindex.xyz/blog/Courses/ec7ce0716d2010d7ddd953c4d967eaaf.png" alt="图4：RAG 的核心组件分类"></p><h3 id="RAG-in-Augmentation-Stages">RAG in Augmentation Stages</h3><p>RAG 是一项知识密集的任务，它在语言模型训练的预训练、微调和推理阶段，融合了多种技术方法。</p><h4 id="Pre-training-Stage">Pre-training Stage</h4><p>在预训练阶段，研究人员已经研究了如何通过基于检索的策略来强化开放领域问答（Question Answering）的预训练模型（PTMs）。REALM 模型采用了一种结构化、可解释的知识嵌入方法，在遮蔽语言模型（MLM）框架内，将预训练和微调构建为“先检索再预测”的工作流程。</p><p><a href="https://arxiv.org/abs/2112.04426" title="RETRO">RETRO</a> 利用检索增强技术进行大规模的从零开始的预训练，成功地减少了模型的参数数量，同时在模型的困惑度上超越了标准的 GPT 模型。RETRO 的独特之处在于，它增加了一个额外的编码器，专门处理从外部知识库中检索得到的实体的特征，这是在 GPT 模型的基础架构上进行的创新。</p><p>Atlas 在预训练和微调阶段都将检索机制整合进了 T5 的框架中。它使用预训练好的 T5 来初始化编码器-解码器语言模型，同时使用预训练好的 Contriever 作为密集型检索器，从而提升了处理复杂语言建模任务的效率。</p><p>此外，COG 推出了一种创新的文本生成方式，仿佛是从已有的文本集合中拷贝文字片段。通过高效的向量搜索工具，COG 能够计算并索引出具有上下文含义的文本片段表示，这种方法在诸如问答系统和领域适应性等应用中，相比 RETRO 显示出更强的性能。</p><p>缩放定律（scaling laws）的出现，为模型参数的大规模增长注入了动力，使得自回归模型成为了主流。研究人员正在将 RAG 方法应用到更大的预训练模型中，RETRO++ 就是这种趋势的一个例证，它在扩大模型参数的同时，保证甚至提升了性能。</p><p>实际的研究结果强烈表明，无论是在文本生成的质量、事实的准确性，还是在减少生成文本的有害内容以及提高任务处理能力等方面，都有了明显的提升，尤其在需要大量知识支持的应用中，如开放领域的问答系统。这些成果表明，将检索机制融入到自回归语言模型的预训练过程中，是一个非常有潜力的研究方向，这种方法将复杂的检索技术与大规模语言模型相结合，可以实现更精确、更高效的语言生成。</p><p>增强预训练带来了许多优势，例如，它可以创建出一种强大的基础模型，该模型在困惑度、文本生成质量和特定任务的性能方面，都超过了标准的 GPT 模型，而且使用的参数更少。这种方法在处理知识密集型任务上表现出色，并且可以通过在专门的语料库上进行训练，来方便地开发出特定领域的模型。</p><p>尽管如此，这种方法也面临着一些挑战，比如需要大量的预训练数据集和资源，而且随着模型大小的增加，更新频率也会降低。但即便存在这些难题，这种方法在增强模型的韧性方面却展现出了显著的优势。一旦完成训练，增强检索的模型就可以摆脱对外部库的依赖，从而提升生成速度和运行效率。这种方法所揭示出的潜在优势，使其成为人工智能和机器学习领域中持续研究和创新的热门话题。</p><h4 id="Fine-tuning-Stage">Fine-tuning Stage</h4><p>RAG 和 Fine-tuning 是增强 LLMs 的强大工具，它们的结合可以满足更多特定场景的需求。一方面，Fine-tuning 可以帮助我们检索出具有独特风格的文档，从而实现更好的语义表达，并缩小查询和文档之间的差距，这样就能确保检索器的输出更适应当前的场景。另一方面，Fine-tuning 可以满足我们进行风格化和有针对性的调整的生成需求。而且，Fine-tuning 还可以用来调整检索器和生成器，以提高模型的协同效果。</p><p>微调检索器的主要目标是利用语料库直接优化嵌入模型，从而提升语义表示的质量。通过反馈信号，我们可以更好地将检索器的能力与 LLM 的偏好相对应，实现二者的协同效果。针对特定的下游任务对检索器进行微调，可以提高其适应性 。同时，引入任务无关的微调策略，旨在提升检索器在多任务场景中的通用性。</p><p>微调生成器可以让输出结果更加具有风格和个性化。一方面，这种方法可以让我们更好地适应不同的输入数据格式。例如，我们可以微调 LLM 以适应知识图谱的结构，文本对的结构，以及其他特定的结构。另一方面，通过建立指令性的数据集，我们可以让 LLM 生成特定格式的内容。比如，在适应性或迭代式的检索场景中，我们可以微调 LLM 以生成能够帮助我们确定下一步行动时机的内容。</p><p>通过同时调整 AI 智能体的&quot;检索器&quot;（用于从大量信息中找到相关的部分）和&quot;生成器&quot;（用于根据检索到的信息产生回答），我们可以提高 AI 的适应性，并避免&quot;过拟合&quot;（即 AI 过度适应训练数据，而在面对新的数据时表现不佳）。但是，这种同时调整的方法也会增加计算资源的消耗。RA-DIT 提供了一种轻量级的解决方案，它是一个双向调整框架，可以有效地为任何 LLMs 添加检索功能。这种&quot;增强检索的调整方法&quot;可以更新 LLM，使其更有效地利用检索到的信息，并忽视那些可能干扰其判断的无关内容。</p><p>尽管微调具有不少优点，但它也有其局限性，比如需要专为 RAG 微调设计的数据集，以及对大量计算资源的需求。然而，在这个阶段，我们可以根据特定的需求和数据格式来定制模型，这可能相比预训练阶段能更有效地利用资源，同时还能微调模型的输出风格。</p><p>总的来说，微调阶段对于让 RAG 模型更好地适应特定任务是至关重要的，它能够优化检索器和生成器的性能。尽管存在资源和数据集需求的挑战，但这个阶段仍能提高模型对各种任务的灵活性和适应性。因此，策略性地微调 RAG 模型是开发高效且有效的检索增强系统的一个关键环节。</p><h4 id="Inference-Stage">Inference Stage</h4><p>在 RAG 模型中，推理阶段极为关键，它需要大量地与 LLMs 进行交互。传统的 RAG 方法，也就是我们所说的 Naive RAG，这一阶段会利用检索到的内容来引导生成过程。</p><p>为了克服 Naive RAG 的局限性，一些先进的技术在推理阶段引入了更富有上下文的信息。DSP 框架采用了一种复杂的交换机制，可以在 Frozen LLMs 和检索模型（RMs）之间交换自然语言文本，这样就可以丰富上下文，从而改善生成的结果。PKG 方法则为大型语言模型配备了一个知识引导模块，这个模块可以在不改变模型参数的情况下检索相关的信息，使得模型能够执行更复杂的任务。<a href="https://arxiv.org/abs/2311.06595" title="CREA-ICL">CREA-ICL</a> 通过同步检索跨语言知识来增强上下文，而 <a href="https://arxiv.org/abs/2210.01296" title="RECITE">RECITE</a> 则通过直接从大型语言模型中抽取段落来生成上下文。</p><p>在推理阶段，我们可以看到一些方法进一步优化了 RAG 过程，这些方法主要应用于需要多步推理的任务。例如，<a href="https://arxiv.org/abs/2310.05149" title="ITRG">ITRG</a> 采用迭代式的检索信息方法，以便找出正确的推理路径，从而更好地适应不同的任务。<a href="https://arxiv.org/abs/2305.15294" title="ITER-RETGEN">ITER-RETGEN</a> 则采用了一种循环策略，将信息检索和信息生成步骤交替进行，一会儿利用检索的信息来增强生成，一会儿又利用生成的信息来增强检索。对于那些不需要大量知识的任务，<a href="https://arxiv.org/abs/2305.17653" title="PGRA">PGRA</a> 提出了一个两阶段的框架，首先使用一个通用的信息检索器，然后使用一个由提示引导的重新排序器来选择和优先考虑证据。相比之下，<a href="https://arxiv.org/abs/2212.10509" title="IRCoT">IRCoT</a> 则将 RAG 与思维链 (CoT) 的方法结合，交替进行由 CoT 引导的信息检索与由检索信息引导的 CoT 过程，这显著提升了 GPT-3 在各种问答任务中的表现。</p><p>总的来说，这些在推理阶段的增强手段提供了一种轻量级且经济高效的选择。它们能够借助预训练模型的能力，而无需再进行额外的训练。其主要优势在于，我们可以在保持 LLM 参数不变的同时，提供与特定任务需求紧密相关的信息。然而，这种方法也有其局限性，它需要精细的数据处理和优化，并受到基础模型固有能力的制约。为了更有效地应对各种任务需求，人们通常会将这种方法与一些步骤优化策略结合起来，如分步推理（step-wise reasoning）、迭代式检索（iterative retrieval）以及自适应检索策略（adaptive retrieval strategies）。</p><h3 id="Augmentation-Source">Augmentation Source</h3><p>RAG 模型的效能受到增强数据源选择的深度影响。不同类型的知识和数据需要应用不同的处理方法。这些数据源通常被划分为非结构化数据、结构化数据，以及 LLM 生成的内容。图5 展示了一颗技术树，描绘了各种代表性的 RAG 研究及其不同的数据增强方向。技术树的叶节点以三种不同的颜色标识，分别代表利用非结构化数据、结构化数据，以及由 LLM 生成的内容进行数据增强。从图中可以清楚地看出，最初的数据增强主要依赖非结构化数据，如纯文本。后来，这种方法逐渐扩展，开始利用结构化数据（如知识图谱）进行增强以进一步提高效能。最近，越来越多的研究开始利用由 LLM 自身生成的内容进行检索和数据增强，这是一个明显的研究趋势。</p><p><img src="//s3.mindex.xyz/blog/Courses/31d82ae1f4b367a4e0ec4257d8309d2b.png" alt="图5"></p><h4 id="Augmented-with-Unstructured-Data">Augmented with Unstructured Data</h4><p>非结构化文本通常从各种语料库中收集，如用于微调大型模型的提示数据，以及跨语言数据。检索的单位可以多种多样，包括 Token（如 <a href="https://arxiv.org/abs/1911.00172" title="kNN-LMs">kNN-LMs</a>），短语（如 <a href="https://arxiv.org/abs/2307.06962" title="CoG">CoG</a>）以及文档的段落。需要注意的是，单位越细，虽然可以提高检索的精度，但同时也会增加检索的复杂性。</p><p><a href="https://arxiv.org/abs/2305.06983" title="FLARE">FLARE, Forward-Looking Active REtrieval augmented generation</a> 提出了一种主动检索方法，该方法在大型语言模型生成低概率词汇时触发。它创建一个临时的句子来进行文档检索，然后利用检索到的上下文信息重新生成句子，以预测接下来的句子。而 RETRO 则是利用前一段内容（chunk）来在同一级别中检索最近的相邻内容，结合前一段内容的上下文，以此来引导下一段内容的生成。为了保持因果性，生成下一个内容块 $C_i$ 时，只会利用前一个内容块 $N(C_{i−1})$ 的最近邻居，而不是 $N(C_i)$。</p><h4 id="Augmented-with-Structured-Data">Augmented with Structured Data</h4><p>结构化的数据，例如知识图谱（KGs），能够提供高质量的上下文信息，减少模型产生的错误预测。<a href="https://arxiv.org/abs/2305.14322" title="RET-LLMs">RET-LLMs</a> 会从过去的对话中提取信息，构建出一个知识图谱，这个图谱可以作为一个内存库在未来被模型引用。<a href="https://arxiv.org/abs/2305.18846" title="SUGRE">SUGRE</a> 则是利用了图神经网络（GNNs）技术，通过编码知识图谱的相关部分，确保模型生成的文本与从知识图谱中检索到的信息一致，这个过程是通过多模态对比学习实现的。<a href="https://arxiv.org/abs/2308.11761" title="KnowledGPT">KnowledGPT</a> 则是通过生成针对知识库（KB）的搜索查询，并将查询结果存储起来，以此来增强 RAG 模型的知识丰富度和上下文相关性。</p><h4 id="LLMs-Generated-Content-in-RAG">LLMs-Generated Content in RAG</h4><p>为了解决 RAG 中外部辅助信息的限制问题，一些研究开始尝试挖掘 LLMs 内部的知识。例如，<a href="https://arxiv.org/abs/2310.05002" title="SKR">SKR</a> 这个方法，它可以将问题分为“已知”和“未知”两类，并根据分类结果有选择性地增强信息检索的效果。另一种方法叫做 <a href="https://arxiv.org/abs/2209.10063" title="GenRead">GenRead</a>，它用 LLM 生成器取代了传统的信息检索器，结果发现，由 LLM 生成的上下文信息往往能提供更精确的答案，这是因为这些上下文信息更符合因果语言建模预训练目标的要求。还有一种叫做 Self-Memory 的方法，它能够通过一个增强了检索功能的生成器，迭代地创建一个无界的记忆池，并用一个记忆选择器选择出能对原始问题提供双重问题视角的输出，以此来自我增强生成模型的能力。<br>这些方法都突显了在 RAG 中，如何创新性地利用各种数据源，以提高模型的性能和完成任务的效果。</p><h3 id="Augmentation-Process">Augmentation Process</h3><p>在 RAG 领域，常规的做法往往是通过一次检索后进行生成，但这样可能导致效率不高。一个被称为 “lost in the middle”（中途迷失）的问题就是这样产生的，当一次检索得到的内容过于冗余，可能会淡化或与重要信息产生冲突，从而降低生成的质量。更进一步，对于那些需要多步推理的复杂问题来说，这种一次性的检索往往无法提供足够广泛的信息，因此显得力不从心。</p><p>如 图5  所示，为了应对这些挑战，现代研究提出了几种改进检索过程的方法，包括迭代检索、递归检索和自适应检索。迭代检索让模型可以进行多轮检索，从而提升获取信息的深度和相关性。递归检索则是将一次检索的结果作为下一次检索的输入，这有助于更深入地探索相关信息，尤其是在处理复杂或多步骤的查询时。递归检索常被应用于需要逐步逼近最终答案的场景，比如学术研究、法律案例分析或者某些类型的数据挖掘任务中。而自适应检索则提供了一种动态的调节机制，可以根据各种任务和上下文的具体需求来调整检索过程。</p><h4 id="Iterative-Retrieval">Iterative Retrieval</h4><p>RAG 模型中，我们采用了一种叫做“迭代检索”的技术。这项技术的工作方式是，根据最初的查询以及已经生成的文本，反复地收集相关的文档，以此为大型语言模型（LLM）构建一个更加丰富的知识库。实践证明，这种技术可以通过在多次检索中引入更多的上下文参考信息，有效提高模型生成答案的稳定性。但是，这种方法也有其局限性，比如可能会出现语义的断裂，或者积累了太多的无关信息。这是因为我们通常会依赖于一串 n 个 tokens 来划分生成的文本和检索到的文档之间的界限。</p><p>为了应对特定的数据场景，我们采用了递归检索和多跳检索这两种技术。递归检索使用一种结构化的索引，按照层次结构处理和检索数据。这可能包括在基于摘要进行检索之前，先对文档或长篇PDF的部分内容进行概括。接着，在文档内部进行第二次检索，进一步优化搜索结果，这就体现了这个过程的递归特性。与此不同，多跳检索的目标是深入挖掘图结构的数据源，提取出相互关联的信息。</p><p>另外，有些方法把检索和生成的步骤融合在一起。ITER-RETGEN 采用了一种协同策略，对于需要复制特定信息的任务，它并行地运用了“检索增强生成”和“生成增强检索”。这个模型将解决输入任务所需的内容作为检索相关知识的上下文基础，这反过来又有助于在接下来的迭代中生成更优质的回应。</p><h4 id="Recursive-Retrieval">Recursive Retrieval</h4><p>递归检索是一种在信息检索和自然语言处理中常用的手段，它能有效提升搜索结果的深度和相关度。这种方法的核心在于，我们会根据前一次搜索得到的结果，反复优化我们的搜索问题。递归检索的目标，就是通过这样的反馈循环，逐步找到最相关的信息，从而提升我们的搜索体验。IRCoT 采用了一种名为 “思维链条” 的策略来引导搜索过程，并会根据搜索结果对这个 “思维链条” 进行优化。而 <a href="https://arxiv.org/abs/2310.14696" title="Tree of Clarifications">Tree of Clarifications</a> 则会创建一个 “澄清树”，这是一种系统性的方法，用于优化查询中不明确的部分。在一些复杂的搜索场景中，比如用户一开始并不完全清楚自己需要什么，或者他们正在寻找的信息非常专业或微妙，这种方法就会显得特别有用。递归检索的特性允许它能够不断学习和适应用户的需求，往往能提高用户对搜索结果的满意度。</p><h4 id="Adaptive-Retrieval">Adaptive Retrieval</h4><p>FLARE 和 <a href="https://arxiv.org/abs/2310.11511" title="Self-RAG">Self-RAG</a> 等自适应检索方法优化了 RAG 框架，使大型语言模型（LLMs）能主动决定何时以及获取何种内容进行检索，从而提高了信息获取的效率和相关性。这些方法是更大趋势的一部分，其中 LLMs 在其操作中都表现出主动判断的能力，这在 AutoGPT、Toolformer 和 Graph-Toolformer 等模型智能体中有所体现。以 Graph-Toolformer 为例，它将检索过程分解为若干个步骤，LLMs 在其中主动使用检索器，运用 Self-Ask 技术，并通过少样本提示发起搜索查询。这种主动态度使得 LLMs 能在需要的时候进行搜索，就像智能体使用工具一样。</p><p>WebGPT 通过融入强化学习框架，使得 GPT-3 模型能够在生成文本时自我运用搜索引擎。它使用一些特别设定的 Token （代表特定操作的标记）来完成这一流程，这些 Token 使得模型可以进行搜索引擎查询、结果浏览和参考文献引用等动作，从而借助外部搜索引擎进一步增强了 GPT-3 的功能。</p><p>Flare 通过监测生成过程中的置信度来自动化调整检索的时机，这个置信度是通过生成内容的概率来反映的。当这个概率低于某个设定的阈值时，Flare 会启动检索系统，收集相关的信息，从而优化检索的流程。</p><p>Self-RAG 引入了一种特别的 Token，我们叫它“反射 Token”，这让模型具有了自我审视其输出的能力。这些 Token 分为两类：“检索”和“评价”。模型可以自行决定何时启动检索，或者，我们也可以设定一个预设阈值来触发检索过程。在检索阶段，生成器会在多段文本中进行“片段级别”的 beam 搜索，目的是找出最连贯的文本序列。而“评价”分数则用来更新各个子区的分数，我们可以在推理过程中灵活地调整这些权重，以便根据需要调整模型的行为。Self-RAG 的设计避免了需要额外的分类器或依赖于自然语言推理（NLI）模型，这使得决定何时启动检索机制的过程更加简洁，并提升了模型在生成准确回应时的自主判断能力。</p><p><img src="//s3.mindex.xyz/blog/Courses/ed615ec174d8671515a77b90ebe27e5f.png" alt="图6"></p><p>由于使用越来越广泛，LLM（大型语言模型）的优化已经受到了大量的关注。诸如提示设计（prompt engineering）、微调（Fine-Tuning，FT）和 RAG 等技术各有其独特的特性，这些特性在 图6 中进行了直观的展示。尽管提示设计方法是利用模型的内在能力，但在优化 LLMs 的过程中，我们通常需要同时采用 RAG 和 FT 方法。在选择 RAG 和 FT 之间，应根据具体场景的需求以及每种方法本身的特性进行决定。下表 中提供了 RAG 和 FT 的详细对比。</p><table><thead><tr><th style="text-align:center">特征</th><th style="text-align:center">RAG</th><th style="text-align:center">FT</th></tr></thead><tbody><tr><td style="text-align:center">知识更新</td><td style="text-align:center">直接更新检索的知识库，可以使得信息始终保持最新状态，从而避免频繁的重新训练，这使得它非常适用于动态的数据环境中。</td><td style="text-align:center">存储静态数据，需要对知识和数据更新进行再训练。</td></tr><tr><td style="text-align:center">外部知识</td><td style="text-align:center">对于利用外部资源方面表现出色，特别是在访问文档或其他结构化/非结构化数据库的应用上更是得心应手。</td><td style="text-align:center">可以用于将预训练得到的外部知识与LLM 进行整合，但对于频繁变化的数据源可能并不实用。</td></tr><tr><td style="text-align:center">数据处理</td><td style="text-align:center">只需要进行极少量的数据处理操作。</td><td style="text-align:center">其效果高度依赖于高质量数据集，如果数据集有限，可能并不会带来显著的性能提升。</td></tr><tr><td style="text-align:center">模型定制</td><td style="text-align:center">虽然主要关注信息检索和整合外部知识，但可能无法充分定制模型的行为或写作风格。</td><td style="text-align:center">可以根据特定的语调或术语，调整LLM的行为、写作风格或者特定领域的知识。</td></tr><tr><td style="text-align:center">可解释性</td><td style="text-align:center">可以将响应追溯到特定的数据源，从而提高其可解释性和可追溯性。</td><td style="text-align:center">就像黑盒子一样，并不总能明白模型为何会以某种特定的方式作出反应，这导致了其可解释性相对较低。</td></tr><tr><td style="text-align:center">计算资源</td><td style="text-align:center">它依赖于计算资源来支持相关的检索策略和数据库技术。另外，还需要维护和更新外部数据源的集成。</td><td style="text-align:center">我们需要准备和整理高质量的训练数据集，明确微调的目标，并提供相应的计算资源。</td></tr><tr><td style="text-align:center">延迟要求</td><td style="text-align:center">这个过程会涉及到数据检索，可能会导致处理延迟增加。</td><td style="text-align:center">微调过后的语言模型可以直接做出响应，无需进行数据检索，这样可以降低延迟。</td></tr><tr><td style="text-align:center">减少幻觉</td><td style="text-align:center">由于每个答案都是基于检索到的证据，因此在本质上，这种方法不容易出现无据可依的推测。</td><td style="text-align:center">通过针对特定领域的数据进行训练，可以帮助减少模型的误判，但是当面对不熟悉的输入时，模型可能仍会产生误判。</td></tr><tr><td style="text-align:center">道德和隐私问题</td><td style="text-align:center">从外部数据库存储和检索文本的过程中，会引发道德和隐私方面的问题。</td><td style="text-align:center">如果训练数据中含有敏感内容，可能会触发一些关于道德和隐私的担忧。</td></tr></tbody></table><h3 id="RAG-vs-Fine-Tuning">RAG vs Fine-Tuning</h3><p>RAG 就好比给模型提供了一本可以精准检索信息的教科书，对于特定的查询来说非常完美。另一方面，FT 就好比一个学生随着时间的积累逐渐吸收和掌握知识，更擅长于复制特定的结构、样式或格式。FT 可以通过强化模型的基础知识、调整输出和教授复杂的指令，提升模型的性能和效率。然而，对于引入新知识或迅速应对新的使用场景，FT 的表现并不理想。</p><p>RAG 和 FT 这两种方法并不是互斥的，实际上它们可以相互补充，从不同的角度增强模型的能力。在某些情况下，同时应用这两种方法可能会带来最优的性能。涉及 RAG 和 FT 的优化过程可能需要经过多次迭代才能取得满意的结果。</p><h2 id="RAG-Evaluation">RAG Evaluation</h2><p>在自然语言处理（NLP）领域，由于 RAG 的快速进步和广泛使用，对 RAG 模型的评估已经成为 LLM 社区研究的重点。这种评估的核心目标是深入理解 RAG 模型，并优化其在各种应用场景中的表现。</p><p>历史上，对 RAG 模型的评估主要关注的是它们在特定的下游任务中的表现。这些评估使用的是适应当前任务的既定度量标准。例如，对问题回答的评估可能会依赖 EM 和 F1 分数，而对事实核查任务的评估通常会以准确性作为主要的度量标准。像 <a href="https://arxiv.org/abs/2308.10633" title="RALLE">RALLE</a> 这样的工具，也是基于这些特定任务的度量标准来设计的，以便自动评估 RAG 的应用。然而，针对 RAG 模型特性的独立评估研究却显得匮乏，只有少数相关的研究。</p><p>接下来的部分，我们将从特定任务的评估方法和指标转向，去综合分析各类研究文献中的独特观点。这个探索过程包括 RAG 模型评估的目标，评估模型的各个要素，以及用于评估的基准和工具。我们的目标是提供一个全面的 RAG 模型评估概述，明确指出那些专门针对这些先进的生成系统独特特性的评估方法</p><h3 id="Evaluation-Targets">Evaluation Targets</h3><p>RAG 模型的评估主要关注两个关键部分：检索模块和生成模块。这样的划分方式确保了我们能全面评估提供的上下文的质量和生成的内容的质量。</p><h4 id="Retrieval-Quality">Retrieval Quality</h4><p>对检索质量的评估是决定 RAG 检索模块所获取的上下文信息有效性的关键。我们通常会借用搜索引擎、推荐系统以及信息检索系统中的标准度量方法来衡量 RAG 检索模块的表现。例如，<a href="https://docs.cloud.deepset.ai/docs/experiments-and-metrics">我们常用的度量标准有命中率（Hit Rate）、平均倒数排名（MRR）和归一化折损累积增益（NDCG）</a>。</p><h4 id="Generation-Quality">Generation Quality</h4><p>生成质量的评估主要关注生成器是否能从检索到的上下文中生成连贯且相关的答案。这种评估可以根据内容的性质进行分类：无标签内容和有标签内容。对于无标签内容，评估的重点是生成的答案是否忠实于原文、是否相关且无误导性。相反，对于有标签内容，我们主要关注模型生成的信息的准确性。另外，无论是对检索质量还是生成质量的评估，都可以通过手动或<a href="https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG" title="auto-eval">自动</a>的方式进行。</p><h3 id="Evaluation-Aspects">Evaluation Aspects</h3><p>当前对 RAG 模型的评估方法强调三个主要的质量指标和四项关键技能，这些因素共同影响着 RAG 模型两个主要目标的评估：检索和生成。</p><h4 id="Quality-Scores">Quality Scores</h4><p>质量评分包括上下文的相关性、答案的忠实度以及答案的适应性。这些评分指标从不同角度对 RAG 模型在信息检索和生成过程中的效率进行了评估。</p><p>上下文相关性是评估检索到的上下文是否精准、具体，它确保了内容的相关性，并降低了处理无关信息所产生的额外成本。</p><p>答案的忠实度保证了生成的答案能够真实反映检索到的上下文，保持信息的连贯性，避免产生自相矛盾的情况。</p><p>答案的相关性要求生成的答案直接针对提出的问题，有效地解决了核心询问。</p><h4 id="Required-Abilities">Required Abilities</h4><p>RAG 的评估也涵盖了四种显示其适应性和效率的能力：抗噪声能力、负面拒绝、信息整合能力和反事实鲁棒性。这些能力对于模型在面对各种挑战和复杂情境下的性能表现至关重要，它们将直接影响质量评分。</p><p>&quot;抗噪声能力&quot;用以评估模型处理那些与问题相关但又缺乏实质性信息的噪声文档的能力。</p><p>&quot;负面拒绝&quot;评估了模型在面对检索不到解答问题所需知识的文档时，选择不做回应的判断力。</p><p>&quot;信息整合&quot;则衡量了模型在从众多文档中提炼并融合信息，以解答复杂问题的能力。</p><p>至于&quot;反事实鲁棒性&quot;，则测试了模型在面对文档中的已知错误信息时，即便被告知可能存在误导，也能够识别并忽视这些不准确信息的能力。</p><p>上下文的相关性和对噪声的鲁棒性对于评估检索质量至关重要，而回答的准确性、回答的相关性、在无法回答时选择不回答的判断力、信息的整合能力，以及对于文档中已知错误信息的识别和忽视能力，都对评估生成质量起着重要的作用。</p><p>下表 总结了每个评估方面的具体指标。需要明白的是，这些从相关工作中得出的指标，尽管是传统的测量方法，但并不意味着它们已经形成了一种成熟或标准化的方式来量化 RAG 的评估方面。此外，尽管本文未包含，但在一些评估研究中，也开发出了专门针对 RAG 模型的特性的定制指标。</p><p><img src="//s3.mindex.xyz/blog/Courses/33aac527b2c8cac60cca8325d7b23a54.png" alt=""></p><h3 id="Evaluation-Benchmarks-and-Tools">Evaluation Benchmarks and Tools</h3><p>这一部分详述了 RAG 模型的评估框架，包括基准测试和自动评估工具。这些工具提供定量指标，不仅可以衡量 RAG 模型的性能，更能帮助我们更好地理解模型在各个评估方面的能力。一些知名的基准，如 <a href="https://arxiv.org/abs/2309.01431" title="RGB">RGB</a> 和 <a href="https://arxiv.org/abs/2311.08147" title="RECALL">RECALL</a>，主要是用来评估 RAG 模型的基本能力。与此同时，一些最新的自动工具，如 <a href="https://arxiv.org/abs/2309.15217" title="RAGAS">RAGAS</a>、<a href="https://arxiv.org/abs/2311.09476" title="ARES">ARES</a> 和 <a href="https://github.com/truera/trulens" title="TruLens">TruLens</a>，利用 LLMs 来确定质量分数。这些工具和基准一起构成了一个健壮的框架，用于系统性地评估 RAG 模型，如 下表 所总结的。</p><p><img src="//s3.mindex.xyz/blog/Courses/3223d20139581f95e1d3484c2b75bc18.png" alt=""></p><h2 id="Future-Prospects">Future Prospects</h2><h3 id="Future-Challenges-of-RAG">Future Challenges of RAG</h3><p>尽管 RAG 技术已有显著进步，但仍有一些挑战需要深入研究：</p><p><strong>上下文长度</strong>。RAG 的效果受到大型语言模型（LLMs）的上下文窗口大小的限制。如何在信息不足的过短窗口和信息稀释的过长窗口之间找到平衡，这是至关重要的。而随着当前不断扩大 LLM 上下文窗口的努力，如何让 RAG 适应这些变化，这是个重要的研究课题。</p><p><strong>鲁棒性</strong>。在信息检索过程中，如果出现噪声或矛盾信息，可能会严重影响 RAG 的输出质量。这种情况就像是“错误的信息有时比没有信息还糟糕”。如何提高 RAG 对这种敌对或反事实输入的抵抗力，这已经成为一个研究的热点，并已成为衡量性能的关键指标。</p><p><strong>混合方法（RAG+FT）</strong>。将 RAG 与微调结合起来正在成为一种主导策略。如何最佳地集成 RAG 和微调——无论是顺序的、交替的，还是通过端到端联合训练——以及如何充分利用参数化和非参数化的优势，这些都是当前研究的重要领域。</p><p><strong>拓宽 LLM 的应用领域</strong>。在 RAG 系统中，LLM 不仅被用于生成最终答案，也被用于信息检索和结果评估。如何进一步挖掘 LLM 在 RAG 系统中的潜力，已经成为了一个日益重要的研究方向。</p><p><strong>规模效应</strong>。虽然 LLM 的规模效应（即模型规模越大，性能越好的现象）已经得到了确认，但这种效应是否适用于 RAG 系统，仍然是一个未知数。<a href="https://arxiv.org/abs/2304.06762">初步的研究</a> 已经开始探讨这个问题，但 RAG 模型的参数数量仍然远远落后于 LLM。有一种特别引人关注的可能性，那就是 “反向规模效应”，即在某些情况下，较小的模型可能会表现得比大模型还要好，这个问题值得进一步研究。</p><p><strong>RAG 的实际应用</strong>。由于 RAG 系统的实用性强，且能够满足工程需求，因此在实际应用中得到了广泛的采用。然而，如何提高信息检索的效率，如何在大型知识库中更准确地找到相关的文档（这个过程被称为 “召回”），以及如何确保数据安全，防止 LLM 无意中泄露文档的来源或元数据，这些都是需要解决的重要工程问题。</p><h4 id="Modality-Extension-of-RAG">Modality Extension of RAG</h4><p>RAG 已经突破了其最初的基于文本的问答边界，开始处理多种形式的模态数据。这种扩展推动了各种领域的 RAG 概念集成到创新的多模态模型中：</p><p><strong>图像</strong>。<a href="https://arxiv.org/abs/2211.12561" title="RA-CM3">RA-CM3</a> 是一种开创性的多模态模型，能够检索并生成文本和图像。<a href="https://arxiv.org/abs/2301.12597" title="BLIP-2">BLIP-2</a> 利用冻结的图像编码器与大型语言模型（LLM）一起，实现了高效的视觉语言预训练，使得图像到文本的零样本（zero-shot）转换成为可能。<a href="https://arxiv.org/abs/2210.03765" title="Visualize Before You Write">“Visualize Before You Write”</a> 的方法使用图像生成来引导语言模型的文本生成，在开放式文本生成任务中展现出了潜力。</p><p><strong>音频和视频</strong>。<a href="https://arxiv.org/abs/2210.08174" title="GSS">GSS</a> 方法通过检索并拼接音频片段，实现了将机器翻译的数据转化为语音翻译的数据。<a href="https://arxiv.org/abs/2301.02736" title="UEOP">UEOP</a> 通过结合外部的、离线的语音转文本策略，实现了端到端自动语音识别的重大进步。此外，基于 KNN 的注意力融合策略利用音频嵌入和与之语义相关的文本嵌入来提升 ASR 的性能，从而加速了领域适应。<a href="https://arxiv.org/abs/2302.14115" title="Vid2Seq">Vid2Seq</a> 则通过在语言模型中加入专门的时间标记，有助于预测事件的边界和文本描述，生成统一的输出序列。</p><p><strong>代码</strong>。<a href="https://dl.acm.org/doi/abs/10.1109/ICSE48619.2023.00205" title="RBPS">RBPS</a> 在处理小规模学习任务上表现出色，它能通过对代码进行编码和频率分析，找出与开发者目标最匹配的代码示例。这种方法在如生成测试断言（test assertion generation）和修复程序错误（program repair）等任务中已展现出其强大的效果。对于结构化知识处理，<a href="https://arxiv.org/abs/2305.13269" title="CoK">CoK</a> 方法会首先从知识图谱中找出与输入问题相关的信息，然后将这些信息作为提示融入到输入中，这样做能显著提升在知识图谱问答任务中的表现。</p><h3 id="Ecosystem-of-RAG">Ecosystem of RAG</h3><h4 id="Downstream-Tasks-and-Evaluation">Downstream Tasks and Evaluation</h4><p>RAG 展现出了巨大的潜力，它能够利用大量的知识库，使语言模型有能力处理复杂的查询并生成详细的回答。实践证明，RAG 在各种实际应用中表现出色，包括回答开放式问题和验证事实的真实性。采用 RAG 不仅可以提高回答的准确性和相关性，还能增加回答的多样性和深度。<br>RAG 在多个领域的可扩展性和通用性值得我们深入研究，尤其是在医学、法律和教育等专业领域。在这些领域，RAG 可能有助于降低训练成本，并相比传统的微调方法，提高专业领域知识问答的性能。<br>同时，我们需要优化 RAG 的评估框架，以最大化其在各种任务中的效能和实用性。这需要我们开发更精细化的度量和评估工具，能够衡量上下文的相关性、内容的创新性和避免造成伤害等方面。<br>此外，提高 RAG 驱动模型的可解释性也是我们的重要目标。这将使用户能够理解模型生成回答的理由，从而在使用 RAG 应用时提升信任感和透明度。</p><h4 id="Technical-Stack">Technical Stack</h4><p>RAG 生态系统的发展受到其技术栈进步的深刻影响。随着 ChatGPT 的出现，像 LangChain 和 LLamaIndex 这样的关键工具迅速走红，它们提供了丰富的 RAG 相关 API，并在大型语言模型（LLM）领域变得必不可少。尽管新兴的技术栈在功能上可能不如 LangChain 和 LLamaIndex 丰富，但它们通过提供专门的服务而获得了区别。例如，Flowise AI 优先考虑低代码策略，让用户可以通过直观的拖放界面部署包括 RAG 在内的 AI 应用。其他如 HayStack，Meltano，和 Cohere Coral 的技术也因其对该领域的独特贡献而受到关注。</p><p>除了专注于 AI 的提供商，传统的软件和云服务提供商也在扩展他们的服务范围，包括专注于 RAG 的服务。例如，Verba 是 Weaviate 设计的一款专为个人助理应用程序设计的工具，而 Amazon 的 Kendra14 是一个智能企业搜索服务，它允许用户使用内置连接器浏览各种内容库。</p><p>在 RAG 技术领域的演变过程中，我们可以看到一个明显的趋势，即向不同的专化方向发展，包括：</p><ol><li>定制化。根据特定需求定制 RAG。</li><li>简化。使 RAG 更易于使用，从而降低初学者的学习难度。</li><li>专业化。优化 RAG 以更有效地适应生产环境的需求。</li></ol><h2 id="Conclusion">Conclusion</h2><p><img src="//s3.mindex.xyz/blog/Courses/b758594171be86db55d0071885972bb8.png" alt="图7"></p><p>如 图7 所示，本文总结凸显了 RAG 通过整合语言模型的参数化知识和外部知识库的大量非参数化数据，对提升 LLM 能力作出了重大贡献。我们的调查展示了 RAG 技术的演变以及其对知识密集型任务的影响。我们的分析在 RAG 框架内划分出三个发展范式：Naive RAG、Advanced RAG 和 Modular RAG，每一个都在前者的基础上取得了进一步的提升。Advanced RAG 范式在 Naive 方法的基础上，通过引入复杂的架构元素，如查询重写、块重新排列和提示摘要，实现了超越。这些创新导向了一个更为细致和模块化的架构，提高了 LLM 的性能和可解释性。RAG 与微调和强化学习等其他 AI 方法的技术集成，进一步扩展了其能力。在内容检索方面，一种结合结构化和非结构化数据源的混合方法正逐渐兴起，为检索过程提供了更多元的内容。在 RAG 框架内的前沿研究正在探索新的概念，如从 LLM 中进行自我检索和信息检索的动态时间。</p><p>尽管 RAG 技术已经取得了显著的进步，但在提高其鲁棒性和处理大量上下文信息的能力方面，仍有大量的研究机会。RAG 的应用领域也正在向多模态扩展，适应解读和处理各种数据形式，比如图像、视频和代码。这种扩展展现了 RAG 对 AI 实际应用的重大影响，吸引了学术和工业界的广泛关注。RAG 生态系统的发展趋势在于 RAG 为中心的 AI 应用的增加和相关工具的持续开发。然而，随着 RAG 应用领域的不断扩大，我们迫切需要改进评估方法以跟上其发展步伐。确保性能评估的准确性和代表性，对于全面理解 RAG 对 AI 研究和开发社区的贡献至关重要。</p><h2 id="Source">Source</h2><p><a href="https://arxiv.org/abs/2312.10997" title="RAG for LLMs: A Survey">RAG for LLMs: A Survey</a></p>]]></content>
    
    <summary type="html">
    
      Retrieval Augmented Generation System.
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="RAG" scheme="https://neo1989.net/tags/RAG/"/>
    
      <category term="LLM" scheme="https://neo1989.net/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Server Sent Event (SSE) with Go</title>
    <link href="https://neo1989.net/HandMades/HANDMADE-sse-with-go/"/>
    <id>https://neo1989.net/HandMades/HANDMADE-sse-with-go/</id>
    <published>2024-03-18T09:36:00.000Z</published>
    <updated>2024-03-19T05:36:04.317Z</updated>
    
    <content type="html"><![CDATA[<p>SSE 提供了一种简单而高效的方式，它可以在服务器和网页客户端之间建立一个单向连接，这样服务器就能够实时地发送更新信息，而无需不断地进行数据请求。</p><span id="more"></span><h2 id="理解-Server-Sent-Events-SSE">理解 Server-Sent Events (SSE)</h2><p>Server-Sent Events（服务器发送事件）是 HTML5 规范的一部分，它允许服务器通过一条持久的单一连接向 Web 客户端推送（发送）数据。与 WebSocket（全双工通信协议）不同，后者支持全双工（双向）通信，而服务器发送事件更适合于需要从服务器到客户端的单向通信的场景。</p><h3 id="SSE-如何工作">SSE 如何工作</h3><p>SSE 依赖于客户端的 EventSource API，它让浏览器能够与服务器端点建立一个持久的连接。一旦建立了连接，服务器就可以将事件以简单的文本数据的形式（通常是 “text/event-stream” 格式）发送到客户端。然后，客户端的 JavaScript 可以处理这些事件，并实时刷新网页。</p><h3 id="SSE-的优点">SSE 的优点</h3><ul><li>SSE 使用 HTTP 协议，现有的服务器软件都支持。WebSocket 是一个独立协议。</li><li>SSE 属于轻量级，使用简单；WebSocket 协议相对复杂。</li><li>SSE 默认支持断线重连，WebSocket 需要自己实现。</li><li>SSE 一般只用来传送文本，二进制数据需要编码后传送，WebSocket 默认支持传送二进制数据。</li><li>SSE 支持自定义发送的消息类型。</li></ul><h3 id="Implementing-Server-Sent-Events-with-Go">Implementing Server-Sent Events with Go</h3><p>如果我们想在 Go 应用程序中实施 SSE，就需要建立一个 HTTP 端点供客户端连接。下面是一份详细的步骤指南，还配有一些代码示例：</p><h4 id="Step-1-Create-a-Basic-HTTP-Server">Step 1: Create a Basic HTTP Server</h4><p>首先，我们来使用 net/http 包搭建一个基础的 Go HTTP 服务器。这是一个示例：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line">    <span class="string">&quot;net/http&quot;</span></span><br><span class="line">    <span class="string">&quot;time&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    http.HandleFunc(<span class="string">&quot;/events&quot;</span>, eventsHandler)</span><br><span class="line">    http.ListenAndServe(<span class="string">&quot;:8080&quot;</span>, <span class="literal">nil</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这个示例中，我们设定了一个 HTTP 服务器，它会在 8080 端口进行监听，且设立了一个唯一的端点，“/events”，这个端点将负责处理 SSE 连接。</p><h4 id="Step-2-Implement-the-SSE-Handler">Step 2: Implement the SSE Handler</h4><p>下一步，我们需要实现 SSE 的处理器。这个处理器的作用是向已连接的客户端发送事件。这是一个示例：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">eventsHandler</span><span class="params">(w http.ResponseWriter, r *http.Request)</span></span> &#123;</span><br><span class="line">    <span class="comment">// Set CORS headers to allow all origins. You may want to restrict this to specific origins in a production environment.</span></span><br><span class="line">    w.Header().Set(<span class="string">&quot;Access-Control-Allow-Origin&quot;</span>, <span class="string">&quot;*&quot;</span>)</span><br><span class="line">    w.Header().Set(<span class="string">&quot;Access-Control-Expose-Headers&quot;</span>, <span class="string">&quot;Content-Type&quot;</span>)</span><br><span class="line"></span><br><span class="line">    w.Header().Set(<span class="string">&quot;Content-Type&quot;</span>, <span class="string">&quot;text/event-stream&quot;</span>)</span><br><span class="line">    w.Header().Set(<span class="string">&quot;Cache-Control&quot;</span>, <span class="string">&quot;no-cache&quot;</span>)</span><br><span class="line">    w.Header().Set(<span class="string">&quot;Connection&quot;</span>, <span class="string">&quot;keep-alive&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Simulate sending events (you can replace this with real data)</span></span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">10</span>; i++ &#123;</span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">2</span> == <span class="number">0</span> &#123;</span><br><span class="line">            fmt.Fprintf(w, <span class="string">&quot;event: notice\n&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        fmt.Fprintf(w, <span class="string">&quot;id: %d\n&quot;</span>, i)</span><br><span class="line">        fmt.Fprintf(w, <span class="string">&quot;data: %s\n&quot;</span>, fmt.Sprintf(<span class="string">&quot;Event %d&quot;</span>, i))</span><br><span class="line">        fmt.Fprintf(w, <span class="string">&quot;retry: 10000\n&quot;</span>)</span><br><span class="line">        fmt.Fprintf(w, <span class="string">&quot;\n\n&quot;</span>)</span><br><span class="line">        fmt.Fprintf(w, <span class="string">&quot;: \n\n&quot;</span>)</span><br><span class="line">        time.Sleep(<span class="number">2</span> * time.Second)</span><br><span class="line">        w.(http.Flusher).Flush()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Simulate closing the connection</span></span><br><span class="line">    closeNotify := w.(http.CloseNotifier).CloseNotify()</span><br><span class="line">    &lt;-closeNotify</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这段代码中，我们设定了响应头以指明 SSE 的数据类型是事件流（ text/event-stream ），禁止了缓存，并且保证了连接的持久性。接着我们使用了一个循环来模拟发送事件。</p><p>事件流是一个简单的文本流，仅支持 UTF-8 格式的编码。每条消息以一个空行作为分隔符。</p><p>在规范中为消息定义了 4 个字段：</p><p><strong>event</strong> 消息的事件类型。客户端收到消息时，会在当前的 EventSource 对象上触发一个事件，这个事件的名称就是这个字段的值，如果消息没有这个字段，客户端的 EventSource 对象就会触发默认的 message 事件。</p><p><strong>id</strong> 这条消息的 ID。客户端接收到消息后，会把这个 ID 作为内部属性 Last-Event-ID，在断开重连 成功后，会把 Last-Event-ID 发送给服务器。</p><p><strong>data</strong> 消息的数据字段。 客户端会把这个字段解析为字符串，如果一条消息有多个 data 字段，客户端会自动用换行符 连接成一个字符串。</p><p><strong>retry</strong> 指定客户端重连的时间。只接受整数，单位是毫秒。如果这个值不是整数则会被自动忽略。</p><p>一个很有意思的地方是，规范中规定以冒号开头的消息都会被当作注释，一条普通的注释（:\n\n）对于服务器来说只占 5 个字符，但是发送到客户端上的时候不会触发任何事件，这对客户端来说是非常友好的。所以注释一般被用于维持服务器和客户端的长连接。</p><h4 id="Step-3-Handle-SSE-on-the-Client-Side">Step 3: Handle SSE on the Client Side</h4><p>在客户端，你可以使用 JavaScript 来打开一个 SSE 连接并处理接收到的事件。以下是一个简单的 HTML 和 JavaScript 代码片段：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">title</span>&gt;</span>SSE Example<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;sse-data&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">            <span class="keyword">const</span> eventSource = <span class="keyword">new</span> <span class="title class_">EventSource</span>(<span class="string">&#x27;http://localhost:8080/events&#x27;</span>);</span></span><br><span class="line"><span class="language-javascript">            <span class="keyword">const</span> dataElement = <span class="variable language_">document</span>.<span class="title function_">getElementById</span>(<span class="string">&#x27;sse-data&#x27;</span>);</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">            eventSource.<span class="property">onmessage</span> = <span class="keyword">function</span>(<span class="params">event</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                dataElement.<span class="property">innerHTML</span> += <span class="string">&#x27;message: &#x27;</span> + event.<span class="property">data</span> + <span class="string">&#x27;&lt;br&gt;&#x27;</span>;</span></span><br><span class="line"><span class="language-javascript">            &#125;;</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">            eventSource.<span class="title function_">addEventListener</span>(<span class="string">&quot;notice&quot;</span>, <span class="function"><span class="params">e</span> =&gt;</span> &#123;</span></span><br><span class="line"><span class="language-javascript">                dataElement.<span class="property">innerHTML</span> += <span class="string">&#x27;notice: &#x27;</span> + event.<span class="property">data</span> + <span class="string">&#x27;&lt;br&gt;&#x27;</span>;</span></span><br><span class="line"><span class="language-javascript">            &#125;, <span class="literal">false</span>)</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">            eventSource.<span class="title function_">addEventListener</span>(<span class="string">&#x27;error&#x27;</span>, <span class="function"><span class="params">e</span> =&gt;</span> &#123;</span></span><br><span class="line"><span class="language-javascript">                <span class="keyword">if</span> (e.<span class="property">target</span>.<span class="property">readyState</span> === <span class="title class_">EventSource</span>.<span class="property">CLOSED</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                    dataElement.<span class="property">innerHTML</span> += <span class="string">&#x27;Disconnected&#x27;</span> + <span class="string">&#x27;&lt;br&gt;&#x27;</span>;</span></span><br><span class="line"><span class="language-javascript">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (e.<span class="property">target</span>.<span class="property">readyState</span> === <span class="title class_">EventSource</span>.<span class="property">CONNECTING</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                    dataElement.<span class="property">innerHTML</span> += <span class="string">&#x27;Connecting...&#x27;</span> + <span class="string">&#x27;&lt;br&gt;&#x27;</span>;</span></span><br><span class="line"><span class="language-javascript">                &#125;</span></span><br><span class="line"><span class="language-javascript">            &#125;, <span class="literal">false</span>);</span></span><br><span class="line"><span class="language-javascript">        </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这个 HTML 页面使用 EventSource API 连接到 “/events” 端点，并将接收到的事件添加到一个 <code>&lt;div&gt;</code> 元素中。</p><p><strong>EventSource</strong> 从父接口 <a href="https://developer.mozilla.org/zh-CN/docs/Web/API/EventTarget">EventTarget</a> 中继承了属性和方法，其内置了 3 个 <a href="https://developer.mozilla.org/zh-CN/docs/Web/API/EventHandler">EventHandler</a> 属性、2 个只读属性和 1 个方法：</p><p><strong>EventHandler 属性</strong><br>EventSource.onopen 在连接打开时被调用。<br>EventSource.onmessage 在收到一个没有 event 属性的消息时被调用。<br>EventSource.onerror 在连接异常时被调用。</p><p><strong>只读属性</strong><br>EventSource.readyState 一个 unsigned short 值，代表连接状态。可能值是 CONNECTING (0), OPEN (1), 或者 CLOSED (2)。<br>EventSource.url 连接的 URL。</p><p><strong>方法</strong><br>EventSource.close() 关闭连接</p><h4 id="Step-4-Run-the-Go-Application">Step 4: Run the Go Application</h4><p>打开一个网页浏览器，导航到你创建的 HTML 页面，然后编译并运行你的 Go 应用程序。你应该能看到实时事件在网页上显示。</p><p><img src="//s3.mindex.xyz/blog/Courses/f1ab76d00596a0d18162cd41f5506829.png" alt=""></p><h2 id="SSE使用注意事项">SSE使用注意事项</h2><h3 id="SSE-如何保证数据完整性">SSE 如何保证数据完整性</h3><p>客户端在每次接收到消息时，会把消息的 id 字段作为内部属性 Last-Event-ID 储存起来。</p><p>SSE 默认支持断线重连机制，在连接断开时会 触发 EventSource 的 error 事件，同时自动重连。再次连接成功时 EventSource 会把 Last-Event-ID 属性作为请求头发送给服务器，这样服务器就可以根据这个 Last-Event-ID 作出相应的处理。</p><p>这里需要注意的是，id 字段不是必须的，服务器有可能不会在消息中带上 id 字段，这样子客户端就不会存在 Last-Event-ID 这个属性。所以为了保证数据可靠，我们需要在每条消息上带上 id 字段。</p><h3 id="减少开销">减少开销</h3><p>在 SSE 的草案中提到，“text/event-stream” 的 MIME 类型传输应当在静置 15 秒后自动断开。在实际的项目中也会有这个机制，但是断开的时间没有被列入标准中。</p><p>为了减少服务器的开销，我们也可以有目的的断开和重连。</p><p>简单的办法是服务器发送一个 关闭消息并指定一个重连的时间戳，客户端在触发关闭事件时关闭当前连接并创建 一个计时器，在重连时把计时器销毁 。</p><h3 id="浏览器兼容">浏览器兼容</h3><p><a href="https://developer.mozilla.org/zh-CN/docs/Web/API/EventSource#%E6%B5%8F%E8%A7%88%E5%99%A8%E5%85%BC%E5%AE%B9%E6%80%A7">向下兼容</a>：早些时候，为了实现数据实时更新最常见的方法就是轮询。</p><p>轮询是以一个固定频率向服务器发送请求，服务器在有 数据更新时 返回新的数据，以此来管理数据的更新。这种轮询的方式不但开销大，而且更新的效率和频率有关，也不能达到及时更新的目的。</p><p>接着便出现了长轮询的方式：客户端向服务器发送请求之后，服务器会暂时把请求挂起，等到有数据更新时再返回最新的数据给客户端，客户端在接收到新的消息后再向服务器发送请求。与常规轮询的不同之处是：数据可以做到实时更新，可以减少不必要的开销。</p><p>这里有一个「选择长轮询还是常规轮询？」的命题，长轮询是不是总比常规轮询占有优势？我们可以从带宽占用的角度分析，如果一个程序数据更新太过频繁，假设每秒 2 次更新，如果使用长轮询的话每分钟要发送 120 次 HTTP 请求。如果使用常规轮询，每 5 秒发送一次请求的话， 一分钟才 20 次，从这里看，常规轮询更占有优势。</p><p>长轮询和 SSE 最关键的区别在于，每一次数据更新都需要一次 HTTP 请求。和 WebSocket 还有 SSE 一样，长轮询也会 占用一个 socket。在数据更新效率上和 SSE 差不多，一有数据更新就能检测到。加上所有浏览器都支持，是一个不错的 SSE 替代方案。</p><h2 id="Conclusion">Conclusion</h2><p>服务器发送事件（Server-Sent Events，简称 SSE）是一种简洁且高效的技术，可以用来在网页应用中实现实时通信。通过使用 Go 语言，我们可以方便地创建一个 SSE 服务器，这个服务器可以向客户端推送实时更新，从而为用户提供流畅且高效的实时体验。无论是用于展示实时通知，更新数据仪表盘，还是其他任何用途，SSE 都是你网页开发工具集中的重要组成部分。</p><h2 id="Reference">Reference</h2><p><a href="https://www.ruanyifeng.com/blog/2017/05/server-sent_events.html">Server-Sent Events 教程</a><br><a href="https://softwaremill.com/sse-vs-websockets-comparing-real-time-communication-protocols/">SSE vs WebSockets</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SSE 提供了一种简单而高效的方式，它可以在服务器和网页客户端之间建立一个单向连接，这样服务器就能够实时地发送更新信息，而无需不断地进行数据请求。&lt;/p&gt;
    
    </summary>
    
    
      <category term="HandMades" scheme="https://neo1989.net/categories/HandMades/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · Prompt Engineering 综述</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-A-Systematic-Survey-of-Prompt-Engineering-in-LLM/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-A-Systematic-Survey-of-Prompt-Engineering-in-LLM/</id>
    <published>2024-03-14T12:52:32.000Z</published>
    <updated>2024-03-20T09:03:10.574Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Prompt-Engineering">Prompt Engineering</h2><p>按应用领域进行分类，简要概述提示技术的发展历程，从最初的零样本提示技术，一直到现在的最新进展。</p><h3 id="New-Tasks-Without-Extensive-Training">New Tasks Without Extensive Training</h3><h4 id="Zero-Shot-Prompting">Zero-Shot Prompting</h4><p>零样本提示为我们如何利用大语言模型（LLM）提供了全新的视角。这种技术无需依赖大量的训练数据，而是通过精心设计的提示，引导模型去完成前所未有的任务。具体的说，模型在提示中接收到了任务的描述，但并没有标注数据来训练特定的输入-输出映射。然后，模型就会利用自身已有的知识，根据给定的提示为新任务生成预测。</p><p><img src="//s3.mindex.xyz/blog/Courses/bd6a40576eb18f2c0337d0ec2145ee19.png" alt=""></p><h4 id="Few-Shot-Prompting">Few-Shot Prompting</h4><p>少样本引导是通过提供一些输入-输出示例，帮助模型理解特定任务的一种方法，这与零样本引导（不提供任何示例）有所不同。即便只提供少数几个高质量的示例，也已经能够在一定程度上提升模型在复杂任务上的表现。</p><p>然而，少样本引导需要额外的 Token 来包含这些示例，对于更长的文本输入来说，可能会带来一些处理上的困难。<br>此外，提示示例的选取和组织方式对模型的行为有着显著影响，例如模型可能会倾向于选择使用频率较高的词汇，这种偏见可能会影响少样本引导的结果。</p><p>尽管少样本引导能够增强处理复杂任务的能力，特别是在像 GPT-3 这样的大型预训练模型中，但是，精心设计的引导策略对于实现最佳性能和减少模型的无意识偏见至关重要。</p><p><img src="//s3.mindex.xyz/blog/Courses/144b4f22e7e1afc18d1e55de3e7bd4f1.png" alt=""></p><h3 id="Reasoning-and-Logic">Reasoning and Logic</h3><h4 id="Chain-of-Thought-CoT-Prompting">Chain-of-Thought (CoT) Prompting</h4><p><a href="https://arxiv.org/abs/2201.11903" title="CoT">CoT</a> 提示能够更有效地引导大语言模型产生结构化且深思熟虑的回应。我们通过一系列的实验，展示了CoT提示的独特优势，强调了它能够引导大语言模型按照逻辑链条进行推理的能力。这种方式使得模型的回应展现出对给定提示更深入的理解。例如，对于一个需要多步推理的数学文字题目，CoT提示能够呈现出整个推理过程和最终答案，这仿佛就像人类如何将问题分解为逻辑中间步骤一样。</p><p><img src="//s3.mindex.xyz/blog/Courses/cc23793ec5adae2dc2c1a38b23fff338.png" alt=""></p><h4 id="Automatic-Chain-of-Thought-Auto-CoT-Prompting">Automatic Chain-of-Thought (Auto-CoT) Prompting</h4><p>“Let’s think step by step”， 自动引导大语言模型（LLMs）形成推理链。<a href="https://arxiv.org/abs/2210.03493" title="Auto-CoT">Auto-CoT</a> 注意到在单独生成的推理链中可能会出现错误，因此采取了多样化采样的策略以提高模型的鲁棒性。它会提出各种各样的问题，并为每个问题生成多个不同的推理链，从而形成一个最终的示例集。这种自动化的多样化采样策略可以最大限度地减少错误，同时提高了少样本（few-shot）学习的效果，免去了手动制作推理链的繁重工作。</p><p><img src="//s3.mindex.xyz/blog/Courses/697f3251c2e47a8c25d89b959fa931eb.png" alt=""></p><h4 id="Self-Consistency">Self-Consistency</h4><p>与常用的“贪心解码”（每一步都选择最可能的选项）相比，“自我一致性”的解码策略能在使用CoT（Chain-of-Thought，逐步推理）技术指导大语言模型时，更好地提升推理性能。对于那些存在多个可能解决路径的复杂推理任务，&quot;自我一致性&quot;策略能从语言模型的解码器中采样出多样化的推理链条。接着，它通过对这些采样链条进行统计处理（即“边缘化”），来确定最具一致性的最终答案。这种方法的优势在于，对于需要深入分析的问题，通常存在更多的推理路径，这种多样性正是我们找到解决方案的关键。</p><p><img src="//s3.mindex.xyz/blog/Courses/6e31e14eab13341dd9f693aedb1ef816.png" alt=""></p><h4 id="Logical-Chain-of-Thought-LogiCoT-Prompting">Logical Chain-of-Thought (LogiCoT) Prompting</h4><p>大语言模型 (LLMs) 要解决各种领域的复杂多步问题，具备逻辑推理能力是至关重要的。现有的方法，比如 CoT 提示，虽然倡导逐步推理，但在验证机制上却不够有效。</p><p><a href="https://arxiv.org/abs/2305.12147" title="LogiCoT">LogiCoT</a> 采用了&quot;反证法&quot;的思想，对模型生成的每一步推理进行验证，并在发现错误时提供有针对性的反馈进行修正。通过这样一种&quot;思考-验证-修正&quot;的循环过程，LogiCoT 能有效地减少模型在推理过程中的逻辑错误和误导性信息。</p><p><img src="//s3.mindex.xyz/blog/Courses/9ba28acd4ee5b6197afe5c5d428e5bca.png" alt=""></p><h4 id="Chain-of-Symbol-CoS-Prompting">Chain-of-Symbol (CoS) Prompting</h4><p>大语言模型常常在处理涉及复杂空间关系的任务时遇到困难，因为它们主要依赖于自然语言，这可能导致歧义和偏见。</p><p><a href="https://arxiv.org/abs/2305.10276" title="CoS">CoS</a> 使用简洁的符号来替代自然语言。CoS 的优势在于：它可以提供清晰、简洁的提示，增强大语言模型的空间推理能力，并提高人类对模型的理解。然而，CoS 也面临着一些挑战，比如如何扩展和泛化，如何与其他技术集成，以及如何解释基于符号的大语言模型的推理过程等。</p><p><img src="//s3.mindex.xyz/blog/Courses/fa8d491e924cb02ff8449d58d3a1a483.png" alt=""></p><h4 id="Tree-of-Thoughts-ToT-Prompting">Tree-of-Thoughts (ToT) Prompting</h4><p><a href="https://arxiv.org/abs/2305.10601" title="ToT">ToT</a> 通过构建一个包含中间推理步骤的树形结构，来拓展链式思维（CoT）提示的方法，这些步骤被称为 “思维”。每一个&quot;思维&quot;都代表一段有条理的语言序列，指向最终的解决方案。这样的结构让大语言模型能够通过评估每个&quot;思维&quot;在解决问题上的进展，进行更深入的推理。思维树结合了模型生成和评估&quot;思维&quot;的能力，以及广度优先或深度优先等搜索算法。这使得模型能在推理链中进行系统性的探索，预先扩展可能有希望的解决方向，同时在找到错误的解决方案时能够回溯。</p><p><img src="//s3.mindex.xyz/blog/Courses/408297ef014c50ff725acd01558eb70c.png" alt=""></p><h4 id="Graph-of-Thoughts-GoT-Prompting">Graph-of-Thoughts (GoT) Prompting</h4><p><a href="https://arxiv.org/abs/2308.09687" title="GoT">GoT</a>  对传统的顺序方法进行了改进，使其更好地对应人类思维的非线性特性。这个框架支持动态的交互，回溯和评估各种想法，允许从不同的分支中整合和组合思维，打破了思维树的线性结构。它的主要贡献在于，将推理过程模拟成一个有向图，并提供了一个带有多种转换操作的模块化架构。这个框架被视为一种灵活且动态的语言模型提示方式，能够捕捉人类思维过程的复杂性，并提升模型的能力。</p><p><img src="//s3.mindex.xyz/blog/Courses/1dd475a4bcf9855715799415add7466a.png" alt=""></p><h4 id="System-2-Attention-S2A-Prompting">System 2 Attention (S2A) Prompting</h4><p>在基于 Transformer 的大语言模型 (LLM) 中，柔性的关注机制可能会过度考虑无关的信息，这对于生成有效的 token 不利。</p><p><a href="https://arxiv.org/abs/2311.11829" title="S2A">S2A</a> 利用大语言模型的推理能力，通过重新整理输入的上下文，只关注与任务相关的信息。S2A 采用了两步过程，通过重塑上下文和生成更精确的回应，来提高其关注能力和回应质量。S2A 的效果在各种任务中得到了验证，包括回答基于事实的问题，生成长篇文章，以及解决数学文字题目。</p><p><img src="//s3.mindex.xyz/blog/Courses/bbed833d5b6ad0cb192d38cfc5176047.png" alt=""></p><h4 id="Thread-of-Thought-ThoT-Prompting">Thread of Thought (ThoT) Prompting</h4><p><a href="https://arxiv.org/abs/2311.08734" title="ThoT">ThoT</a> 的设计灵感来自人类的认知过程，它能有条不紊地分析大量的信息，将这些信息划分为易于处理的小部分。这个过程分为两个阶段，首先，LLM 会概括和审查每一段信息，然后再精炼这些信息，以便给出最后的回应。ThoT 的灵活性体现在它可以作为一个多功能的插件，用于增强不同模型和提示方法的推理能力。</p><p><img src="//s3.mindex.xyz/blog/Courses/685c878ed68c513cb9025a8428563eb7.png" alt=""></p><h4 id="Chain-of-Table-Prompting">Chain-of-Table Prompting</h4><p>像 CoT、PoT 和 ToT 这样的方法主要是通过自由形式的文本或代码来进行推理，但在处理包含大量数据和复杂结构的表格时，却遭遇了一些困难。</p><p><a href="https://arxiv.org/abs/2401.04398" title="Chain-of-Table">Chain-of-Table</a> 的核心思想是通过动态地在表格上生成并执行常见的 SQL 或 DataFrame 操作，实现步骤之间的逐步推理。这一过程的反复迭代可以改进中间的推理结果，从而提升大语言模型的预测能力，使其能够通过形象的逻辑推理链条进行预测。</p><p><img src="//s3.mindex.xyz/blog/Courses/a10c977d23baa955352761769ce584f2.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/708189f5f2c23f49cf287e871674f3da.png" alt=""></p><h3 id="Reduce-Hallucination">Reduce Hallucination</h3><h4 id="Retrieval-Augmented-Generation-RAG">Retrieval Augmented Generation (RAG)</h4><p>大语言模型 (LLMs) 已经带来了文本生成的革命性变化，但其依赖有限、静态的训练数据，使得在需要外部知识的任务中，准确的响应成为了难题。传统的提示方法并不能解决这个问题，因为它需要耗费大量的重新训练。</p><p><a href="/tags/RAG/" title="RAG">RAG</a> 为我们提供了新的解决方案，它将信息检索巧妙地融入到提示过程中。RAG 能够分析用户的输入，制定出精准的查询，并在预先构建的知识库中查找相关的资源。检索到的信息片段被纳入到原始的提示中，为其提供了丰富的上下文背景。这样增强后的提示，使大语言模型能够生成具有创新性和准确性的响应。RAG 的敏捷性突破了静态限制，对于需要实时知识的任务，它无疑是一种改变游戏规则的技术。</p><p><img src="//s3.mindex.xyz/blog/Courses/2b7824dc44fda311ea348ff4af2ecca2.png" alt=""></p><h4 id="ReAct-Prompting">ReAct Prompting</h4><p>与以往将推理和行动分别处理的研究不同，<a href="https://arxiv.org/abs/2210.03629" title="ReAct">ReAct</a> 让大语言模型（LLMs）有能力同时进行推理过程和特定任务的行动生成。这种交织的过程增强了推理和行动之间的协同效应，使模型在处理异常情况时能够更好地引导、追踪和更新行动计划。ReAct 被广泛应用于各种语言处理和决策制定任务中，并在与当前SOTA相比较的基线测试中展现出了显著的优势。尤其值得一提的是，在问题回答（HotpotQA）和事实验证（Fever）任务中，ReAct 通过与简单的维基百科API的交互，有效地解决了错误传播和产生不真实信息的问题，从而产生了更易于理解的任务解决路径。</p><p><img src="//s3.mindex.xyz/blog/Courses/d398a9d0ea408794f3bb35f78d76a2c3.png" alt=""></p><h4 id="Chain-of-Verification-CoVe-Prompting">Chain-of-Verification (CoVe) Prompting</h4><p><a href="https://arxiv.org/abs/2309.11495" title="CoVe">CoVe</a> 包含四个步骤：模型首先生成初步的回答，然后提出一些验证问题以检查自己的回答，接着独立地解答这些问题，最后根据验证结果修正并产生最终的回答。这种经过深思熟虑的多步骤验证方式，提升了大语言模型的逻辑推理能力，使其即使在面对矛盾信息时也能减少错误。CoVe 的设计理念是模拟人类的验证过程，以此提高大语言模型输出的连贯性和精确性。在列表问题、问答和长篇生成等实验中，CoVe 成功地在保证事实准确性的同时，减少了虚构现象。通过提出具有针对性的验证问题，模型能更好地发现并纠正自身的不准确之处。</p><p><img src="//s3.mindex.xyz/blog/Courses/f77ebfaff72cd60616c655ed8356dd12.png" alt=""></p><h4 id="Chain-of-Note-CoN-Prompting">Chain-of-Note (CoN) Prompting</h4><p>检索增强型语言模型（Retrieval-augmented language models，RALMs）的设计初衷是为了提升大型语言模型（Large Language Models）的能力，通过融合外部知识，以减少模型在生成过程中产生的不真实信息。然而，这些检索到的信息并非总是可靠的，有可能会引导模型产生错误的回应。常规的 RALMs 在评估自身知识是否充足时，往往会遇到困难，尤其是在缺乏足够信息时，这些模型通常无法给出“我不知道”的答案。</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For standard RALM</span></span><br><span class="line">Task Description:</span><br><span class="line">The primary objective is <span class="built_in">to</span> briefly answer <span class="keyword">a</span> specific question.</span><br><span class="line"></span><br><span class="line"><span class="comment"># For RALM with CON</span></span><br><span class="line">Task Description:</span><br><span class="line"><span class="number">1.</span> Read <span class="keyword">the</span> given question <span class="keyword">and</span> Wikipedia passages <span class="built_in">to</span> gather relevant information.</span><br><span class="line"><span class="number">2.</span> Write reading notes summarizing <span class="keyword">the</span> key points <span class="built_in">from</span> these passages.</span><br><span class="line"><span class="number">3.</span> Discuss <span class="keyword">the</span> relevance <span class="keyword">of</span> <span class="keyword">the</span> given question <span class="keyword">and</span> Wikipedia passages.</span><br><span class="line"><span class="number">4.</span> If some passages are relevant <span class="built_in">to</span> <span class="keyword">the</span> given question, provide <span class="keyword">a</span> brief  answer based <span class="keyword">on</span> <span class="title">the</span> <span class="title">passages</span>.</span><br><span class="line"><span class="number">5.</span> If no passage is relevant, directly provide answer <span class="keyword">without</span> considering <span class="keyword">the</span> passages.</span><br></pre></td></tr></table></figure><p><a href="https://arxiv.org/abs/2311.09210" title="CoN">CoN</a> 能够系统地评估文档的相关性，强调关键且可靠的信息，过滤掉无关的内容，从而使得模型的回应更加精确，更具上下文相关性。<br>CoN 不仅是一个提示模板，而且还包含了一个经过微调的可以记笔记模型。因此CoN可以看作是RAG和Fine-Tuning的结合。</p><p><img src="//s3.mindex.xyz/blog/Courses/4512f010ca40405423106942c10baa58.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/8b5f22339915adbdfc67dde8e70ca6a6.png" alt=""></p><h4 id="Chain-of-Knowledge-CoK-Prompting">Chain-of-Knowledge (CoK) Prompting</h4><p><a href="https://arxiv.org/abs/2305.13269" title="CoK">CoK</a>  从人类解决问题的方式中获得灵感，将复杂的任务系统地分解成一系列有序的步骤。这个过程首先是全面的推理准备阶段，建立问题的上下文，并对问题进行框架化。然后，它进入动态知识适应阶段，从各种来源如内部知识库、外部数据库以及给定的提示中，精心收集相关的证据。</p><p><img src="//s3.mindex.xyz/blog/Courses/c7f7c5ea582e777b188323ff20acba7e.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/df831c882f4df1aabade839c782c0716.png" alt=""></p><h3 id="User-Interface">User Interface</h3><h4 id="Active-Prompting">Active Prompting</h4><p><a href="https://arxiv.org/abs/2302.12246">Active-Prompting</a> 引入了一种新的机制，可以确定哪些问题对于注释最具影响力。这种方法借鉴了基于不确定性的主动学习的思想，通过使用各种度量标准来描述不确定性，并选择最具不确定性的问题进行注释。</p><p><img src="//s3.mindex.xyz/blog/Courses/c689e6ee19708f997f565ad61fa60f73.png" alt=""></p><h3 id="Fine-Tuning-and-Optimization">Fine-Tuning and Optimization</h3><h4 id="Automatic-Prompt-Engineer-APE">Automatic Prompt Engineer (APE)</h4><p><a href="https://arxiv.org/abs/2211.01910" title="APE">APE</a> 通过动态地生成和选择对特定任务最有影响力的提示，从而克服了传统手工设计的、固定不变的提示的缺点。这种巧妙的方法会分析用户的输入，制定一系列可能的指令，然后利用强化学习来挑选出最佳的提示。这种提示能够根据不同的上下文环境进行实时调整，提高了模型的适应性。</p><p><img src="//s3.mindex.xyz/blog/Courses/fec997cd64d3cad3f8161207bf98bb7a.png" alt=""></p><h3 id="Knowledge-Based-Reasoning-and-Generation">Knowledge-Based Reasoning and Generation</h3><h4 id="Automatic-Reasoning-and-Tool-use-ART">Automatic Reasoning and Tool-use (ART)</h4><p><a href="https://arxiv.org/abs/2303.09014" title="ART">ART</a> 赋予了大语言模型通过多步骤过程进行推理，以及无缝地利用外部专业知识的能力，从而使其能够应对复杂问题，超越了简单的文本生成。</p><p>ART通过整合专业知识和计算工具，打开了大语言模型的多功能性，使其的输出能与现实世界紧密联系。这使得大语言模型能够在科学研究、数据分析，甚至决策支持等多元领域发挥作用。</p><p>ART不仅超越了传统的提示技术，还通过结构化程序自动化了推理步骤，从而消除了手工制作的需要。其动态的工具整合能力确保了与外部工具的顺畅协作，可以暂停生成过程以融入外部工具的输出结果，然后无缝地恢复生成流程。在一些具有挑战性的基准测试（例如Big-Bench和MMLU）上，实验证明了ART的有效性，其表现甚至超过了传统的提示技术，有时甚至能够达到手工制作示例的效果。</p><p><img src="//s3.mindex.xyz/blog/Courses/736ea666e264e7bb729505c7454b60d6.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/e286a415d47aba340dd72ee42f93bc7a.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/65deeb80598eff36a0d403e7c745a0bc.png" alt=""></p><h3 id="Improving-Consistency-and-Coherence">Improving Consistency and Coherence</h3><h4 id="Contrastive-Chain-of-Thought-CCoT-Prompting">Contrastive Chain-of-Thought (CCoT) Prompting</h4><p>传统用于大语言模型的CoT提示方法往往忽视了一个关键环节：从错误中吸取教训。</p><p>而 <a href="https://arxiv.org/abs/2311.09277" title="CCoT">CCoT</a> 不仅提供了正确的推理示例，还展示了错误的推理过程。试想你在探索一张地图，既有明确的正确路径，也标出了需要避开的误区，这就是CCoT带来的优势！</p><p><img src="//s3.mindex.xyz/blog/Courses/2342eec6fbabec308efa898da263f8b2.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/614cb77a1ae49769c1ce1a48fc895bfd.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/9d0aa48a1d217f14449a7cc30fb93a21.png" alt=""></p><h3 id="Managing-Emotions-and-Tone">Managing Emotions and Tone</h3><h4 id="Emotion-Prompting">Emotion Prompting</h4><p><a href="https://arxiv.org/abs/2307.11760">EmotionPrompt</a> 解决了大语言模型理解情绪线索能力的不确定性。他们从心理学研究中汲取灵感，探索语言对人类表现的影响，将11个情绪刺激语句融入到提示中，旨在提升大语言模型的情绪智能。实验结果显示，这些情绪刺激语句的加入，能够有效地融入到模型的运作中，从而在各种任务中显著提升大语言模型的表现。</p><p><img src="//s3.mindex.xyz/blog/Courses/ac0b8274fbc10014e730d3bb9f460c08.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/ba723c819c6506283f3ba9720a3d91aa.png" alt=""></p><h3 id="Code-Generation-and-Execution">Code Generation and Execution</h3><h4 id="Scratchpad-Prompting">Scratchpad Prompting</h4><p>尽管基于 Transformer 的语言模型在生成基础编程任务的代码上表现卓越，但在涉及到需要精确逻辑推理的复杂、多步骤算法计算中，它们却面临挑战。</p><p><a href="https://arxiv.org/abs/2112.00114" title="Scratchpad Prompting">Scratchpad</a> 更注重任务设计，而非模型的修改，使得模型能在给出最终答案之前，生成一系列的中间计算步骤。</p><p><img src="//s3.mindex.xyz/blog/Courses/badc6033f1cde07372bd7a5d51b50149.png" alt=""></p><h4 id="Program-of-Thoughts-PoT-Prompting">Program of Thoughts (PoT) Prompting</h4><p>语言模型在解决数学表达式时的表现并不理想，主要原因在于它们容易出现算术错误，无法处理复杂的方程，且在表达大量迭代过程时效率低下。</p><p><a href="https://arxiv.org/abs/2211.12588" title="PoT">PoT</a> 倡导在计算步骤中使用外部的编程语言解释器，以提高语言模型的数值推理能力。</p><p><img src="//s3.mindex.xyz/blog/Courses/c7a6067d672639aad0ece1593402d05e.png" alt=""></p><h4 id="Structured-Chain-of-Thought-SCoT-Prompting">Structured Chain-of-Thought (SCoT) Prompting</h4><p><a href="https://arxiv.org/abs/2305.06599" title="SCoT">SCoT</a> 通过将程序结构（如序列、分支和循环结构）融入推理步骤，从而提升了大语言模型在生成结构化源代码方面的表现。这种方法明确地引导大语言模型从源代码的角度思考需求，相比于 CoT 提示，其在代码生成方面的效果得到了显著提升。</p><p><img src="//s3.mindex.xyz/blog/Courses/336065b71cdb63da883607248c7fe400.png" alt=""></p><h4 id="Chain-of-Code-CoC-Prompting">Chain-of-Code (CoC) Prompting</h4><p><a href="https://arxiv.org/abs/2312.04474" title="CoC">CoC</a> 通过利用编写代码的方式来改善LM的推理能力，适用于逻辑和语义任务。CoC 鼓励LMs把语义子任务格式化为灵活的伪代码，这样就可以让解释器捕捉到未定义的行为，并通过一个被称为&quot;LM模拟器&quot;（LMulator）的工具来模拟这些行为。</p><p><img src="//s3.mindex.xyz/blog/Courses/920be294f8d31bcb2dfc99c9b48f718a.png" alt=""></p><h3 id="Optimization-and-Efficiency">Optimization and Efficiency</h3><h4 id="Optimization-by-Prompting-OPRO">Optimization by Prompting (OPRO)</h4><p><a href="https://arxiv.org/abs/2309.03409" title="OPRO">OPRO</a> 使用自然语言的提示，根据问题的描述，逐步生成解决方案，从而使得快速适应不同任务和个性化优化过程成为可能。通过在诸如线性回归和旅行商问题这样的经典问题上的案例研究，展示了LLMs在优化问题上的巨大潜力。此外，OPRO还探索了如何优化提示，以在自然语言处理任务中最大化准确性，这也突显出LLMs的敏感性。</p><p><img src="//s3.mindex.xyz/blog/Courses/fc2a4795b407fb610709971983ea9db8.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/fece14e6124c739fb7768d5ef7379ca6.png" alt=""></p><h3 id="Understanding-User-Intent">Understanding User Intent</h3><h4 id="Rephrase-and-Respond-RaR-Prompting">Rephrase and Respond (RaR) Prompting</h4><p><a href="https://arxiv.org/abs/2311.04205" title="RaR">RaR</a> 让大语言模型有能力在一次提示中对问题进行重新表述和扩展，从而提高了模型理解问题和回答问题的准确性。他们还开发了一种两步骤的 RaR 变种，这种变种结合了重新表述和回应的大语言模型，显著提高了各种任务的性能。研究强调，相比于人类随意提出的问题，重新表述的问题能够增强语义的清晰度，解决问题本身的模糊性。</p><p><img src="//s3.mindex.xyz/blog/Courses/11b29a315244c8ee5370bfe49edf1078.png" alt=""><br><img src="//s3.mindex.xyz/blog/Courses/e586e329ea1017532ef7dc13886d48e6.png" alt=""></p><h3 id="Metacognition-and-Self-Reflection">Metacognition and Self-Reflection</h3><h4 id="Take-a-Step-Back-Prompting">Take a Step Back Prompting</h4><p><a href="https://arxiv.org/abs/2310.06117" title="Step Back Prompting">Step Back</a> 方法包含两个步骤，即“抽象”和“推理”的整合。通过广泛的实验，将“Step Back”应用于PaLM-2L在STEM（科学、技术、工程和数学）、知识问答和多跳推理等各种推理密集型任务中，实验结果表明，这种方法能显著提升模型的推理能力。</p><p><img src="//s3.mindex.xyz/blog/Courses/0117ebc52d00cbb97b43e4cbfc1d4b1b.png" alt=""></p><h2 id="Conclusion">Conclusion</h2><p>提示工程未来的发展潜力巨大，元学习和混合提示架构等新兴趋势预示着它的能力将得到进一步提升。然而，我们在发展的同时，必须高度重视道德问题，强调负责任的开发和部署，确保其能够积极地融入我们的生活中。</p><h2 id="Source">Source</h2><p><a href="https://arxiv.org/abs/2402.07927" title="A Systematic Survey of Prompt Engineering">A Systematic Survey of Prompt Engineering</a></p>]]></content>
    
    <summary type="html">
    
      在人工智能领域，&quot;提示工程&quot;正成为一种变革性的力量，它释放了大语言模型（LLMs）的巨大潜力。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="LLM" scheme="https://neo1989.net/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 如何构建企业级 RAG 系统</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-How-to-Build-an-Enterprise-RAG-System/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-How-to-Build-an-Enterprise-RAG-System/</id>
    <published>2024-03-06T05:34:01.000Z</published>
    <updated>2024-03-07T12:31:58.896Z</updated>
    
    <content type="html"><![CDATA[<h1>Background</h1><p>在深入探讨 RAG 架构之前，可以先回顾一下前篇，在构建 RAG 系统的最新研究里提出的<a href="/Theses/THESIS-Seven-Failure-Points-When-Engineering-a-RAG-System/">七个常见的失败的地方</a>。</p><p>这里简单罗列一下，分别是：</p><ul><li>Missing Content</li><li>Missed the Top Ranked Documents</li><li>Not in Context - Consolidation strategy Limitations</li><li>Not Extracted</li><li>Wrong Format</li><li>Incorrect Specificity</li><li>Incomplete</li></ul><h1>How to</h1><p>下面的 RAG 系统架构图可以帮助我们了解每个组件在系统中的位置以及其具体使用方式。</p><p><img src="//s3.mindex.xyz/blog/Courses/a873343d21d57c34bef56b4fa638119e.gif" alt=""></p><p>接下来我们将详细探讨每个组件的设计需求和作用，以及建立这些组件的最佳实践。</p><h2 id="User-authentication">User authentication</h2><p>一切从这里开始 —— 我们系统的第一环节！</p><p>在用户开始与聊天机器人交互之前，我们需要出于多种原因进行用户身份验证。身份验证不仅可以保证系统的安全，还能提供个性化的服务，这对于企业级系统来说是至关重要的。</p><h3 id="Access-Control">Access Control</h3><p>认证机制确保只有经过授权的用户才能访问系统。它有助于控制谁可以与系统进行交互，以及他们可以执行哪些操作。</p><h3 id="Data-Security">Data Security</h3><p>保护敏感数据的重要性不言而喻。通过用户认证，我们可以防止未经授权的人员访问机密信息，从而避免数据泄露和非法数据操作。</p><h3 id="User-Privacy">User Privacy</h3><p>认证机制能确保只有用户本人可以访问其个人信息和账户详情，从而保护了用户的隐私。这一点对于建立用户的信任非常关键。</p><h3 id="Legal-Compliance">Legal Compliance</h3><p>很多地区和行业都有相关的法规，要求各类组织实行严格的用户认证制度，以保护用户的数据和隐私。遵循这些规定，可以帮助避免法律纠纷和可能的罚款。</p><h3 id="Accountability">Accountability</h3><p>认证机制能保证行为的责任归属，因为它将系统内的各种操作与特定的用户账户关联起来。这种机制对于审计和追踪用户活动非常重要，可以帮助我们识别和处理任何安全事件或者可疑行为。</p><h3 id="Personalization-and-Customization">Personalization and Customization</h3><p>身份验证的功能在于让系统能够识别每一个独立的用户，从而实现对用户体验的个性化和定制化。这其中可能包括为用户量身定制的内容、偏好设置和个人设置。</p><p>利用 <a href="https://www.youtube.com/watch?v=vqAirwfYgrY">AWS Cognito</a>，<a href="https://www.youtube.com/watch?v=vBUk293QSKY">Firebase Authentication</a> 这类服务，你可以轻松地在移动设备和网页应用中实现用户的注册和身份验证。</p><h2 id="Input-guardrail-输入护栏">Input guardrail 输入护栏</h2><p>我们必须防止用户输入可能含有有害信息或私人信息的内容。</p><p>最近的研究显示，<a href="https://llm-attacks.org/">Jailbreak LLMs</a> 其实并不困难。在这种情况下，输入护栏就显得尤为重要了。接下来，我们一起来看看在哪些场景下我们需要使用护栏。</p><h3 id="Anonymization">Anonymization</h3><p>输入防护机制可以将个人身份信息（Personal Identifiable Information, PII）如姓名、地址或联系方式进行匿名化处理或者删除。这样做有助于保护用户隐私，防止恶意行为导致敏感信息的泄露。</p><h3 id="Restrict-substrings">Restrict substrings</h3><p>禁止使用可能被利用进行 SQL 注入、跨站脚本（XSS）或其他类型注入攻击的特定字符序列或模式，能够防止安全漏洞的产生或者阻止不良行为的发生。</p><h3 id="Restrict-topics">Restrict topics</h3><p>为了限制与可能不适当、冒犯或违反社区规定的特定主题相关的讨论或输入，对涉及仇恨言论、歧视或者不适当的内容进行筛选是非常重要的。</p><h3 id="Restrict-code">Restrict code</h3><p>我们必须防止注入可执行代码，因为这可能会破坏系统的安全，或者引发所谓的“代码注入攻击”。</p><h3 id="Restrict-language">Restrict language</h3><p>我们需要确保文本输入是用正确的语言或脚本编写的，以避免在处理过程中产生潜在的误解或错误。</p><h3 id="Detect-prompt-injection">Detect prompt injection</h3><p>我们需要采取措施，防止有人试图注入误导性或有害的提示，这些提示可能会操纵系统，或者以我们无法预料的方式影响大语言模型的行为。</p><h3 id="Limit-tokens">Limit tokens</h3><p>我们需要对用户输入的Token或字符数量设定一个上限，这样可以防止系统资源被耗尽，同时也能防止所谓的“拒绝服务攻击”。</p><h3 id="Detect-toxicity">Detect toxicity</h3><p>我们需要采取措施，比如实施&quot;毒性过滤器&quot;，这样就能识别并阻止那些包含有害或者辱骂性语言的输入。</p><p>为了保护你的 RAG 系统不受这些问题的影响，你可以使用 Meta 公司的 <a href="https://towardsdatascience.com/safeguarding-your-rag-pipelines-a-step-by-step-guide-to-implementing-llama-guard-with-llamaindex-6f80a2e07756">Llama Guard</a> 工具。你既可以自己来托管这个工具，也可以选择使用像 <a href="https://aws.amazon.com/blogs/machine-learning/llama-guard-is-now-available-in-amazon-sagemaker-jumpstart/">Sagemaker</a> 这样的托管服务。但是，请不要期待它能完美地检测出所有的有毒内容。</p><h2 id="Query-rewriter">Query rewriter</h2><p>当用户的提问成功通过了我们设置的防护机制后，我们就会把这些问题交给一个叫做“问题重塑器”的工具。有时候，用户提出的问题可能模糊不清，或者我们需要更多的上下文信息才能更好地理解他们的真正意图。这时，“问题重塑”就能派上用场。这项技术的主要作用是转换和优化用户的问题，使其更清晰、更精准，更能反映出用户的真实需求。下面，我们就来介绍一些最常用的“问题重塑”技巧。</p><h3 id="Rewrite-based-on-history">Rewrite based on history</h3><p>在这种方法中，系统会借助用户的提问历史来理解对话的背景，并优化后续的提问。我们来看一个关于信用卡咨询的例子。</p><p>提问历史：</p><p>“你有多少信用卡？”</p><p>“铂金卡和金卡每年需要支付费用吗？”</p><p>“比较一下两者的特性。”</p><p>我们需要根据用户的提问历史来理解对话的背景，判断出用户的意图以及各个问题之间的关联，然后生成一个符合这个对话背景的新问题。</p><p>重塑后的问题：“比较一下铂金卡和金卡的特性。”</p><h3 id="Create-subqueries">Create subqueries</h3><p>处理复杂的问题时，有时候会遇到信息检索的困难。为了解决这个问题，我们可以将一个大的问题分解成若干个更具体的小问题，这样就可以更准确地找到回答这些问题所需的相关信息。这种方法被 LlamaIndex 称为<a href="https://docs.llamaindex.ai/en/stable/examples/query_engine/sub_question_query_engine.html">&quot;子问题查询引擎&quot;</a>。</p><p>举个例子，如果我们要比较&quot;铂金信用卡和黄金信用卡的特性&quot;，系统会为每种卡片生成一个子问题，每个子问题都专注于原始问题中提到的一种卡片。</p><p>重写的子查询：</p><ul><li>“铂金信用卡有哪些特性？”</li><li>“黄金信用卡有哪些特性？”</li></ul><h3 id="Create-similar-queries">Create similar queries</h3><p>为了更大概率地找到正确的文档，我们会根据用户的输入生成类似的查询。这样做的目的是为了解决在语义匹配或词汇匹配中存在的检索限制。</p><p>比如，当用户询问信用卡的特性时，系统会产生相关的查询。我们会运用同义词、相关词语或者特定领域的知识，以生成更符合用户查询意图的问题。</p><p>生成的相似查询：</p><p>“我想了解白金信用卡” -&gt; “告诉我白金信用卡有哪些优点。”</p><h2 id="Encoder">Encoder</h2><p>当我们拥有原始查询和重构后的查询，我们会将它们转化为向量以便于检索。在构建你的 RAG 系统时，选择何种编码器可能是最重要的决策。下面我们来探讨为什么会这样，以及在选择文本编码器时需要考虑哪些因素。</p><h3 id="Leveraging-MTEB-benchmarks">Leveraging MTEB benchmarks</h3><p>如果你想全面评估编码器的能力，那么 <a href="https://huggingface.co/spaces/mteb/leaderboard">‘Massive Text Embedding Benchmark’</a>（MTEB）是你的首选参考资源。这个基准测试能够帮助你根据向量维度、平均检索性能和模型大小，进行深入的编码器选择。虽然 MTEB 提供了许多有价值的信息，但我们需要对其结果保持一定的怀疑，因为并没有一种万能的评估基准，而且模型的训练数据具体信息可能并未全部公开。</p><p>MTEB 不仅提供了对 OpenAI、Cohere 和 Voyager 等热门嵌入的性能洞察，还揭示出某些开源模型的性能与前述模型相当。然而，这些结果只能作为一个大概的参考，可能无法准确预测这些嵌入在你特定领域的具体表现。因此，<strong>在最终确定编码器选择之前，对你的数据集进行全面的评估是至关重要的</strong>，这也凸显了定制化评估方法的重要性。</p><h3 id="Custom-evaluation">Custom evaluation</h3><p>编码器不一定能够始终保持最佳表现，特别是在处理敏感信息的时候。因此，<a href="https://www.rungalileo.io/blog/mastering-rag-improve-performance-with-4-powerful-metrics">自定义的评估</a>方式在这种情况下就显得尤为关键。以下是三种进行自定义评估的方法。</p><h4 id="Evaluation-by-annotation">Evaluation by annotation</h4><p>创建专门的数据集，并通过注释的方式获取到&quot;金标签&quot;。完成注释后，可以利用诸如平均倒数排名（Mean Reciprocal Rank，MRR）和归一化折扣累积增益（Normalized Discounted Cumulative Gain，NDCG）等检索指标，来定量地评估不同编码器的性能。</p><h4 id="Evaluation-by-model">Evaluation by model</h4><p>采用和注释方式相似的数据生成流程，但是将大语言模型（LLM）或者跨编码器用作评估工具。这样可以在所有编码器之间建立相对的排名。然后，对排名前三的编码器进行人工评估，就能得到精确的性能指标。</p><h4 id="Evaluation-by-clustering">Evaluation by clustering</h4><p>尝试使用多种聚类技术，并在不同的 Silhouette 分数（这是一种评估聚类效果的指标，反映簇内向量的相似性）下分析聚类的数据量（也就是我们所说的“覆盖范围”）。你可以试验一些算法，比如 HDBSCAN，调整它们的参数以达到最佳的聚类效果。这种基于聚类的评估方法能够深入揭示数据点的分布和分组情况，从而帮助我们选择能够满足特定测评指标的编码器。</p><h3 id="Consideration-Of-Selecting-A-Text-Encoder">Consideration Of Selecting A Text Encoder</h3><p>当你在挑选编码器时，你需要在自行开发的编码器和公共可用的编码器之间做出选择。你可能会被自行开发的编码器的易用性所吸引，但是在这两种选择之间，有一些具体的取舍需要考虑。这个决定至关重要，因为它将决定你的系统的性能和响应速度。</p><h4 id="Querying-cost">Querying cost</h4><p>在语义搜索中，要确保用户体验的流畅，关键在于嵌入式 API 服务必须始终可用。OpenAI 和其他类似的服务提供商提供了可靠的 API，这就避免了你需要自行管理服务器的问题。然而，如果选择开源模型，就需要根据模型的大小和响应速度的需求，投入一定的开发工作。较小的模型（参数量最多为 110M）可以使用 CPU 实例进行部署，而更大的模型可能需要使用 GPU 来满足响应速度的要求。</p><h4 id="Indexing-cost">Indexing cost</h4><p>建立语义搜索需要对文档进行索引，这一过程需要投入一定的成本。索引和查询过程是由同一编码器完成的，因此，索引的成本大小取决于我们选择的编码器服务。为了便于服务的重置或将索引转移到其他的向量数据库，我们建议将嵌入向量单独存储起来。如果忽略了这个步骤，就可能需要重新计算这些相同的嵌入向量，这无疑会增加不必要的工作量。</p><h4 id="Storage-Cost">Storage Cost</h4><p>对于需要索引数以百万计的向量的应用来说，向量数据库的存储成本成为了一个重要的考虑因素。存储成本与向量的维度成正比，也就是说，维度越高，存储成本就越高。例如，OpenAI 的这种 1526 维的嵌入向量就会产生最大的存储成本。要估算存储成本，我们可以计算每个文档中的平均单元数（即短语或句子的数量），然后据此进行推算。</p><h4 id="Language-Support">Language Support</h4><p>如果你想让系统支持非英语的语言，你有两种选择：一是使用能够处理多种语言的编码器，二是结合使用翻译系统和专门处理英语的编码器</p><h4 id="Search-latency">Search latency</h4><p>语义搜索的延迟与嵌入的维度线性增长。为了最小化延迟，选择低维度的嵌入是更好的。</p><h4 id="Privacy">Privacy</h4><p>在诸如金融和医疗等对数据隐私要求极高的敏感领域，使用像 OpenAI 这样的服务可能会面临一些挑战。</p><h2 id="Document-ingestion">Document ingestion</h2><p>文档摄取系统负责管理数据的处理和保存。在建立索引的过程中，每一个文档都会被切分成小块，然后通过嵌入模型将这些小块转化为嵌入向量。接下来，这些原始的小块和对应的嵌入向量都会被存储在数据库中进行索引。下面，我们来详细了解一下文档摄取系统的各个组成部分。</p><h3 id="Document-parser">Document parser</h3><p>文档解析器在处理各种文档格式中显得尤为重要，它能有效地从这些文件中提取出结构化的信息。这不仅包括处理可能含有图像和表格的 PDF 文件，还有其他各种格式的文件。</p><h4 id="Document-formats">Document formats</h4><p>文档解析器需要能够熟练处理各种文档格式，比如 PDF、Word、Excel 等，以保证在处理不同类型文档时的灵活性。这包括识别和处理文档中嵌入的内容，如超链接、多媒体元素或注释，从而能够完整地展现文档的内容。</p><h4 id="Table-recognition">Table recognition</h4><p>识别并从文档中的表格提取数据对于维护信息的结构化非常关键，尤其是在报告或研究论文中。从表格中提取元数据，如表头、行列信息，有助于我们更好地理解文档的组织结构。对于这样的任务，像 <a href="https://huggingface.co/spaces/nielsr/tatr-demo">Table Transformer</a> 这样的模型可能会很有用。</p><h4 id="Image-recognition">Image recognition</h4><p>我们在文档中的图像上应用 OCR 技术，以便识别和提取文本信息，这样就可以将这些信息纳入索引，方便后续的查找和使用。</p><h4 id="Metadata-extraction">Metadata extraction</h4><p>元数据是关于文档的附加信息，不包括在主要内容中。这些信息包括作者、创建日期、文档类型、关键词等。元数据不仅提供了有价值的上下文，帮助我们更好地组织文档，还可以通过考虑元数据属性来提升搜索结果的相关性。我们可以使用自然语言处理和光学字符识别技术（NLP/OCR）来提取这些元数据，并将它们作为特殊字段与文档一起进行索引。</p><h2 id="Chunker">Chunker</h2><p>你如何对长篇文本进行分词可以决定你的词嵌入（embeddings）的质量和搜索系统的性能。如果划分得过小，可能无法回答某些问题；如果划分得过大，那么结果可能会包含多余的、无关的信息（也就是“噪声”）。你可以利用 <a href="https://www.youtube.com/watch?v=qaPMdcCqtWk">summarisation</a> 技术来减少这种噪声，同时也能降低文本的大小、编码的成本和存储的成本。</p><p>划分文本（也称为“分块”）是一个重要但常被忽视的话题。它可能需要类似于特征工程（一种从原始数据中提取有用特征的过程）的专业知识。举个例子，对于 Python 代码库的分块，可能会根据像 def/class 这样的前缀进行划分。</p><p><img src="//s3.mindex.xyz/blog/Courses/94ac6c938c06bc88b4f3511a33418754.png" alt=""></p><h2 id="Indexer">Indexer</h2><p>索引器，正如你可能已经猜到的，负责创建文档的索引，这是一种有结构的数据结构（试着快速重复说这句话三次吧…）。<br>索引器使得搜索和检索操作变得更为高效。<strong>高效的索引对于快速和准确地检索文档至关重要。</strong> 它的工作包括将文档的分块或 Token 映射到它们在文档集合中的对应位置。索引器在文档检索中执行了一些重要的任务，如创建索引以及添加、更新或删除文档。</p><p>作为 RAG 系统的核心组成部分，索引器面临着各种可以影响系统整体效率和性能的挑战和问题。</p><h3 id="Scalability-issues">Scalability issues</h3><p>随着文档数量的增长，保持索引的高效和快速变得越来越具有挑战性。当系统在处理大量文档时遇到困难，可能会引发可扩展性的问题，从而导致索引和检索的速度变慢。</p><h3 id="Real-time-index-updates">Real-time index updates</h3><p>让索引保持实时更新是一项颇具挑战性的任务，特别是在那些文件频繁增加、修改或删除的系统中。我们需要让实时 API 和索引更新机制能够无缝衔接，同时又不牺牲系统的性能，这无疑是一项持续的挑战。</p><h3 id="Consistency-and-atomicity">Consistency and atomicity</h3><p>在面对同时发生的多个文档更新或修改时，要保证操作的连贯性和不可分割性是一项复杂的任务。我们需要确保，即使在同时发生的各种改动中，索引的更新也能够保持数据的正确性。这就需要我们进行精心的设计和实施。</p><h3 id="Optimizing-storage-space">Optimizing storage space</h3><p>对大量文档进行索引可能需要大量的存储空间。如何在保证索引依然易于访问和高效响应的同时，优化存储空间，是一项持续的挑战，尤其是在需要考虑存储成本的情况下。</p><h3 id="Security-and-access-control">Security and access control</h3><p>我们必须实施适当的安全措施和访问控制，以防止未经授权的索引修改。确保只有经过授权的用户或程序才能进行CURD等操作，这样可以帮助我们保护文档库的完整性。</p><h3 id="Monitoring-and-maintenance">Monitoring and maintenance</h3><p>定期对索引器的健康状况和性能进行监控是必不可少的。要发现诸如索引失败、资源瓶颈或过时的索引等问题，我们需要完善的监控和维护流程，以保证系统能够持续稳定运行。</p><p>这些都是软件工程中一些具有挑战性但广为人知的问题，我们可以通过遵循优秀的软件设计实践来应对这些挑战。</p><h2 id="Data-storage">Data storage</h2><p>鉴于我们需要处理各种类型的数据，每种类型的数据都需要有专门的存储空间。深入理解每种存储类型的特点和使用场景是非常关键的。</p><h3 id="Embeddings">Embeddings</h3><p>数据库类型: SQL/NoSQL</p><p>将文档的嵌入信息单独存储，可以让我们在不必为整个文档集合重新计算这些嵌入信息的情况下，迅速地更新索引。而且，这种嵌入信息的存储方式同时也起到了备份的作用，即使面临系统故障或更新，也能保住那些关键信息不丢失。</p><h3 id="Documents">Documents</h3><p>数据库类型: NoSQL</p><p>把文档以原始的形式存储下来对于长期保存是至关重要的。这种原始的形式是各种处理步骤的基石，比如建立索引、解析文本和检索信息。同时，保持文档的原始形式为未来对系统的升级提供了便利，因为原始文档始终保持完好，需要时可以重新进行处理。</p><h3 id="Chat-history">Chat history</h3><p>数据库类型: NoSQL</p><p>对于 RAG 系统来说，存储聊天历史是支持其对话功能的关键。通过存储聊天历史，系统能够获取到用户过去的查询、响应和偏好信息，进而根据用户的特定上下文，对未来的交互进行个性化调整。这些历史数据是一份宝贵的资源，能够为我们的机器学习系统提供改进的研究依据。</p><h3 id="User-feedback">User feedback</h3><p>数据库类型: SQL/NoSQL</p><p>用户反馈是通过 RAG 应用中的各种交互方式系统化地收集的。在大多数大语言模型（LLM）系统中，用户可以通过点赞/点踩、星级评价和文本反馈等方式进行反馈。这些用户反馈形成了一份宝贵的信息库，它们反映了用户的体验和感受，为我们不断优化和提升系统提供了重要依据。</p><h2 id="Vector-database">Vector database</h2><p>作为 RAG 中关键的信息检索部件，向量数据库对于语义搜索的实现至关重要。然而，为了避免可能的问题，我们需要谨慎选择这个部件。在这个选择过程中，有几个关于 <a href="https://vdbs.superlinked.com/">向量数据库</a> 的因素需要我们来考量。下面，让我们一起来探讨其中的一些。</p><h3 id="Recall-vs-Latency">Recall vs. Latency</h3><p>在向量数据库中，我们需要在提高召回率（即获取相关结果的能力）与减少延迟（即返回结果的速度）之间找到平衡。不同的索引技术，如 <a href="https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#flat-indexes">Flat</a>、<a href="https://weaviate.io/blog/ann-algorithms-vamana-vs-hnsw">HNSW</a>（Hierarchical Navigable Small World）、<a href="https://www.pinecone.io/learn/series/faiss/product-quantization/">PQ</a>（Product quantization）、<a href="https://erikbern.com/2015/09/24/nearest-neighbor-methods-vector-models-part-1">ANNOY</a> 和 <a href="https://zilliz.com/learn/DiskANN-and-the-Vamana-Algorithm">DiskANN</a>，在追求速度和召回率之间做出了不同的<a href="https://ann-benchmarks.com/">trade-offs</a>。你可以通过对自己的数据和查询进行基准测试，从而做出更明智的决策。</p><h3 id="Cost">Cost</h3><p>采用云原生数据库和托管解决方案，通常是根据你的数据存储量和查询频率来收费。这种模式适合数据量大的组织，可以帮助他们规避基础设施的投入成本。在选择这种模式时，需要考虑的关键因素包括：预测数据集的增长速度，团队的技术能力，数据的敏感程度，以及理解选择云托管解决方案会带来的成本影响。</p><p>另一方面，自行托管数据库可以让组织对自己的基础设施有更多的控制权，可能还能降低一些成本。但是，这也意味着组织需要负责管理和维护这些基础设施，包括考虑其可扩展性，安全性和定期更新的问题。</p><h3 id="Insertion-speed-vs-Query-speed">Insertion speed vs. Query speed</h3><p>在数据处理中，插入速度和查询速度的平衡至关重要。你需要寻找那些能应对高速数据流插入需求的供应商。然而，对于大多数组织而言，更应优先考虑的是查询速度。在系统高负载时，你需要评估向量的插入速度和查询的延迟时间，以便做出明智的选择。</p><h3 id="In-memory-vs-On-disk-index-storage">In-memory vs. On-disk index storage</h3><p>在内存存储和硬盘存储之间做选择，需要权衡速度和成本。内存存储虽然速度快，但有些场景下可能需要存储的向量数据量超过了内存的容量。像内存映射文件这样的技术，可以在不影响搜索速度的前提下，扩大向量的存储规模。像 DiskANN 中的 Vamana 这样的新型索引技术，也承诺提供高效的超内存索引服务。</p><h3 id="Full-Text-search-vs-Vector-Hybrid-search">Full-Text search vs. Vector Hybrid search</h3><p>仅仅依赖向量搜索可能无法满足企业级应用的需求。另一方面，混合式搜索，它融合了密集型和稀疏型的方法，可能需要更多的实现工作。典型的实现方式包括：建立一个密集型向量索引，一个稀疏型的倒排索引，并加入一个重新排序的步骤。在 <a href="https://www.pinecone.io/learn/hybrid-search-intro/">Pinecone</a>, <a href="https://weaviate.io/blog/hybrid-search-fusion-algorithms">Weaviate</a> 和 <a href="https://www.elastic.co/blog/improving-information-retrieval-elastic-stack-hybrid">Elasticsearch</a> 这些工具中，我们可以通过一个名为 alpha 的参数来调整密集型元素和稀疏型元素之间的平衡。</p><p><img src="//s3.mindex.xyz/blog/Courses/c5d2cdcbacf199bedbab9dc15aacea48.png" alt=""></p><h3 id="Filtering">Filtering</h3><p>在现实的搜索场景中，我们经常需要根据元数据的特性进行筛选。虽然在搜索之前进行筛选看似直接有效，但这样可能会遗漏一些相关的搜索结果。如果筛选的属性在整个数据集中所占比例较小，那么在搜索后进行筛选可能会遇到问题。像 <a href="https://weaviate.io/developers/weaviate/concepts/prefiltering">Weaviate</a> 这样的搜索工具采用了自定义的过滤方式，它先进行预筛选，然后结合倒排索引片段和 HNSW 索引片段进行高效的语义搜索。</p><h2 id="Techniques-for-improving-retrieval">Techniques for improving retrieval</h2><p>近期的研究揭示，<a href="https://arxiv.org/abs/2302.00093">LLMs可能会被无关的信息轻易分散注意力</a>，且当上下文信息过多（如检索到的前K个文档）时，由于其注意力模式的特性，可能会<a href="https://arxiv.org/abs/2307.03172">遗失部分上下文信息</a>。因此，寻找相关且多样化的文档以优化信息检索过程变得尤为关键。接下来，我们将探讨一些已被证实能有效提升信息检索效率的技术。</p><h3 id="Hypothetical-document-embeddings-HyDE">Hypothetical document embeddings (HyDE)</h3><p>我们可以采用 HyDE 技术来应对检索性能不佳的问题，特别是在处理那些可能让信息查找变得困难的短语或不匹配的查询时。HyDE 的独特之处在于，它利用像 GPT 这样的模型生成了一些&quot;假设性文档&quot;。这些文档虽然可能包含虚构或不准确的细节，但却能捕捉到重要的模式。然后，一个智能文本编码器将这些假设性文档转化为向量嵌入。相较于直接嵌入查询，这种方式更能有效地在文档集合中找到与其相似的实际文档。</p><p>通过实验，我们发现 HyDE 的效果优于其他先进的方法，因此，它是提升 RAG 系统性能的有效工具。</p><h3 id="Query-routing">Query routing</h3><p>在处理多个索引时，查询路由显示出其优势，它能将查询精准地指向最相关的索引，从而实现有效的信息检索。这种方法通过确保每次查询都能找到最适合的索引，优化了信息检索的准确性和速度。</p><p>在企业搜索的场景中，数据从各种来源进行索引，如技术文档、产品文档、任务和代码仓库等，此时查询路由就显得尤为重要。比如，如果用户正在搜索与特定产品功能相关的信息，查询可以被精准地指向包含产品文档的索引，从而提高搜索结果的准确性。</p><h3 id="Reranker">Reranker</h3><p>当编码器的信息检索效果不尽如人意，无法提供最佳质量时，我们会使用一种名为 <a href="https://medium.com/@abul.aala.fareh/different-reranking-techniques-in-llamaindex-6a56ed1f30a3">reranker</a> 的工具来优化文档的排名。现在，一种常见的做法是在交叉编码器的环境中使用如 <a href="https://huggingface.co/BAAI/bge-large-en-v1.5">BGE-large</a> 这样的开源单一编码器 Transformer。近期，一些只使用解码器的新方法，如 <a href="https://arxiv.org/abs/2309.15088">RankVicuna</a>、<a href="https://arxiv.org/abs/2304.09542">RankGPT</a> 和 <a href="https://arxiv.org/abs/2312.02724">RankZephyr</a>，进一步提升了重新排序器的性能。</p><p>引入重新排序器确实有其优点，它能减少大语言模型在生成响应时的错误预测 ( <a href="https://www.rungalileo.io/blog/deep-dive-into-llm-hallucinations-across-generative-tasks">LLM 幻觉</a> )，同时也能提升系统在处理非专业领域问题时的泛化能力。然而，这也伴随着一些挑战。复杂的重新排序器可能会因为计算负载过大而导致响应延迟，这对于需要实时反馈的应用来说可能是个问题。此外，部署高级的重新排序器可能会消耗大量的资源，因此需要仔细权衡性能提升和资源利用之间的关系。</p><h3 id="Maximal-Marginal-Relevance-MMR">Maximal Marginal Relevance (MMR)</h3><p>MMR（最大边缘相关性）是一种设计用来提升搜索结果多样性的方法，避免结果的重复性。MMR 的关注点并不仅仅在于找到最相关的搜索结果，而是在相关性和多样性之间寻找平衡。这就像在聚会上为朋友介绍新朋友。首先，根据朋友的喜好，找到最匹配的人。然后，再找一个和前者有些不同的人。这个过程会一直持续，直到达到了预定的介绍人数。通过这种方式，MMR 确保呈现的搜索结果既丰富多样，又高度相关，尽可能地减少了重复性。</p><h3 id="Autocut">Autocut</h3><p>Weaviate 的 autocut 功能是设计用来通过检测得分接近的搜索结果群组来限制返回的搜索结果数量。它的工作方式是通过分析搜索结果的得分，并找出其中的显著变化，这种变化可能意味着搜索结果从高度相关转变为相对不那么相关。</p><p>比如，我们有一个搜索任务，返回的对象距离值如下：</p><p>[0.1899, 0.1901, 0.191, 0.21, 0.215, 0.23]。</p><p>使用 Autocut 后，我们得到的结果是：</p><ul><li>autocut-1: [0.1899, 0.1901, 0.191]</li><li>autocut-2: [0.1899, 0.1901, 0.191, 0.21, 0.215]</li><li>autocut-3: [0.1899, 0.1901, 0.191, 0.21, 0.215, 0.23]</li></ul><h3 id="Recursive-retrieval">Recursive retrieval</h3><p><img src="//s3.mindex.xyz/blog/Courses/98f56b7139fa65b965494ac3f17644fc.png" alt=""></p><p>递归检索，也被称为“小块到大块”的检索技术，它在检索过程中处理小的文本块，同时返回大的父级上下文供语言模型进行信息整合。小的文本块可以提高检索的精确度，而大的文本块则为语言模型提供了更丰富的上下文信息。这种顺序化的过程首先关注那些信息密集的小单位，以提高检索的准确性，然后将这些小单位有效地与它们的大的父级上下文关联起来，以便进行信息整合。</p><h3 id="Sentence-window-retrieval">Sentence window retrieval</h3><p>检索过程选取一个句子，并返回该句子所在的一段上下文。这种基于句子的上下文检索方式（<a href="https://docs.llamaindex.ai/en/latest/examples/node_postprocessor/MetadataReplacementDemo.html">Sentence window retrieval</a>）确保我们得到的信息不仅准确，而且与其上下文紧密相关，从而提供了关于主要句子周围的全面信息。</p><h2 id="Generator">Generator</h2><p>既然我们已经讨论了所有的检索组件，那么接下来我们来谈谈生成器。它需要我们仔细考虑和权衡，主要是在自我托管的推理部署和私有 API 服务之间做出选择。这本身是一个非常大的话题，我会尽量简明扼要地介绍，避免让你感到信息过载。</p><h3 id="API-considerations">API considerations</h3><p>在评估用于大语言模型（LLMs）的 API 服务器时，我们必须优先考虑那些能确保顺畅集成和强大性能的功能。一个设计精良的 API 不仅应能轻松启动各种流行的大语言模型，还需要考虑到诸如生产环境的准备就绪、安全防护，以及对生成内容的<a href="https://www.rungalileo.io/blog/5-techniques-for-detecting-llm-hallucinations">幻觉检测</a>等关键因素。值得一提的是，<a href="https://github.com/huggingface/text-generation-inference">HuggingFace 的 TGI 服务</a>就很好地实现了这些原则，它提供了一套全面的功能集。接下来，让我们一起来理解一下大语言模型服务器中最常用且重要的一些功能。</p><h4 id="Performance">Performance</h4><p>一个高效的 API 必须优先考虑性能，以满足各种用户的需求。Tensor 并行性（Tensor parallelism）是一个显著的特性，它能够在多个 GPU 上加速推理过程，从而提高整体的处理速度。此外，连续批处理（continuous batching）的设计可以处理更多的请求，从而提高了系统的总体吞吐量，使系统响应更快，扩展性更强。引入了量化技术（quantization），特别是 bitsandbytes 和 GPT-Q，这进一步优化了 API，使其在各种使用场景中都能提供更高的效率。利用优化过的 Transformer（Transformer）代码，可以在最常用的架构上顺利进行推理，确保了系统的高效运行。</p><h4 id="Generation-quality-enhancers">Generation quality enhancers</h4><p>要提升生成内容的质量，API 需要融入能够改变输出结果的特性。其中，logits 处理器（包括温度缩放（temperature scaling）、top-p、top-k 以及重复惩罚（repetition penalty）等功能）让用户可以按照自己的需求定制输出结果。另外，停止序列（stop sequences）赋予用户对生成过程的控制权，使他们能够更好地管理和优化内容生成的过程。对于检测生成内容是否偏离实际（幻觉检测，hallucination detection）非常重要的 log 概率，作为另一层优化手段，能确保生成的内容与预期的上下文保持一致，避免产生误导性的信息。</p><h4 id="Security">Security</h4><p>API 的安全性至关重要，尤其是在处理大语言模型（LLMs）和企业级应用场景时。Safetensors 的权重加载功能（Safetensors weight loading）就是一个关键的特性，它可以防止未经授权的篡改模型参数，从而确保模型安全地部署。此外，引入水印技术（watermarking）也增强了安全性，使得在使用大语言模型的过程中可以进行追踪和负责任管理。</p><h4 id="User-experience">User experience</h4><p>在提升用户体验方面，Token 流式传输显得至关重要，它能实现无缝的交互效果。通过运用服务器发送事件（Server-Sent Events, SSE）技术，我们可以优化 Token 的流式传输，从而提升 API 的实时反馈效能，使得用户获得更流畅、更富交互性的使用体验。这种方式保证了用户能够分步接收到 AI 生成的内容，从而提升了大语言模型（LLM）的整体用户参与度和易用性。</p><h3 id="Self-hosted-inference">Self-hosted inference</h3><p>如果我们选择自行托管推理服务，就需要在像 AWS、GCP 或 Azure 这样的云服务平台上部署大语言模型（LLM）。在此过程中，服务器的选择（例如 TGI、Ray 或 FastAPI）变得至关重要，因为这将直接影响到系统的性能和成本。在选择过程中，我们需要考虑到计算效率、部署的便利性，以及所选服务器与大语言模型之间的兼容性。</p><p>衡量大语言模型（LLM）的推理性能是非常关键的，像 ‘<a href="https://github.com/ray-project/llmperf-leaderboard">Anyscale 的 LLMPerf 排行榜</a>’ 这样的评比工具就显得无比重要。它根据关键的性能指标，如首个 Token 的响应时间（TTFT），Token 之间的延迟（ITL）和成功率，来对提供推理服务的公司进行排名。对于评估托管模型的各种特性，负载测试和<a href="https://www.rungalileo.io/blog/mastering-rag-8-scenarios-to-test-before-going-to-production">正确性测试</a>都是必不可少的。</p><p>在新的研究方法中，<a href="https://predibase.com/blog/lorax-the-open-source-framework-for-serving-100s-of-fine-tuned-llms-in">Predibase 的 LoRAX</a> 提出了一种创新的方式，能高效地运行经过微调的大语言模型（LLM）。它成功地解决了如何利用共享的 GPU 资源来同时运行多个经过微调的模型的挑战。</p><h3 id="Private-API-services">Private API services</h3><p>像 OpenAI、Fireworks、Anyscale、Replicate、Mistral、Perplexity 和 Together 这些公司所提供的大语言模型（LLM）API 服务，为我们呈现了多种不同的部署策略。我们必须要深入理解这些服务的特性、价格模型以及<a href="https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation">大语言模型的性能指标</a>。比如说，OpenAI 采用的是基于 Token 的定价方式，它对输入和输出的 Token 进行了区分，这可能会对使用 API 的总体成本产生重大影响。当我们在进行私有 API 服务和自托管大语言模型的<a href="https://www.rungalileo.io/blog/mastering-rag-improve-performance-with-4-powerful-metrics">成本比较</a>时，必须要考虑到 GPU 的成本、使用情况以及可扩展性等问题。对于一些人来说，速度限制可能会成为一个制约因素。</p><h2 id="Prompting-techniques-for-improving-RAG">Prompting techniques for improving RAG</h2><p>有许多方法可以用来提升 RAG 的输出质量。在 ‘<a href="https://www.rungalileo.io/blog/mastering-rag-llm-prompting-techniques-for-reducing-hallucinations">Reducing Hallucinations</a>’ 这篇文章中，我们对五种最有效的方法进行了深入的探讨。许多新的技巧的性能甚至超过了 CoT （Chain-of-Thought）。而且，你还可以将这些技巧组合起来，以尽可能减少生成的幻想内容。</p><p><img src="//s3.mindex.xyz/blog/Courses/186b9f8e41cd2168627090767f4a4d57.png" alt=""></p><h2 id="Output-guardrail">Output guardrail</h2><p>输出防护机制的功能与其对应的输入防护机制相似，但专门用于检测生成内容中可能出现的问题。其主要聚焦于识别 <a href="https://www.rungalileo.io/blog/mastering-rag-8-scenarios-to-test-before-going-to-production">RAG 评估</a>过程中可能出现的虚构信息、竞品的提及，以及可能对品牌造成的潜在伤害。其目标是防止生成与品牌价值观不符的不准确或道德上有争议的信息。通过积极监控和分析生成的内容，这个防护机制确保生成的内容在事实上的准确，道德上的合规，并与品牌的指南保持一致。</p><h2 id="User-feedback-2">User feedback</h2><p>一旦生成并提供了输出，获取用户的正反馈是非常有助于我们的。用户反馈对于不断优化 RAG 系统的运行机制至关重要，这是一个持续不断的过程，而非一次性的任务。这不仅涵盖了如重新索引和实验重复运行等常规的自动化任务，也包括系统化地整合用户的反馈，以实现系统的大幅度提升。</p><p>对系统改进影响最大的手段在于积极解决底层数据中存在的问题。RAG 系统应该包含一个处理用户反馈并推动持续改进的迭代工作流程。</p><h3 id="User-interaction-and-feedback-collection">User interaction and feedback collection</h3><p>用户通过与 RAG 系统的交互，使用如 👍/ 👎或星级评分等功能来反馈他们的使用体验。这些多元化的反馈方式为我们提供了一份关于系统性能的宝贵资料，它记录了用户的实际体验和感受。</p><h3 id="Issue-identification-and-diagnostic-inspection">Issue identification and diagnostic inspection</h3><p>收集完反馈后，团队可以进行深入的分析，找出可能性能不佳的问题。这个过程包括检查检索的资源，仔细分析，以确定问题出在哪一环节——是检索过程、生成过程，还是底层数据源。</p><h3 id="Data-improvement-strategies">Data improvement strategies</h3><p>一旦发现问题，尤其是那些源于数据本身的问题，团队可以策略性地制定提升数据质量的计划。这可能包括修复不完整的信息，或者重新整理结构混乱的内容。</p><h3 id="Evaluation-and-testing-protocols">Evaluation and testing protocols</h3><p>在进行了数据改进后，系统需要对之前性能不佳的查询进行<a href="https://www.rungalileo.io/blog/mastering-rag-8-scenarios-to-test-before-going-to-production">严格的重新评估</a>。这些评估的结果可以被系统化地融入到测试套件中，以确保我们能够持续地根据实际交互进行审查和优化。</p><p>通过让用户积极参与这个全面的反馈循环，RAG 系统不仅能够解决通过自动化流程发现的问题，还能够充分利用用户的丰富体验。</p><h2 id="Observability">Observability</h2><p>构建一个 RAG 系统的工作并不仅仅在于将其投入生产。即使我们已经设置了强大的防护措施并且有高质量的<a href="https://www.rungalileo.io/blog/optimizing-llm-performance-rag-vs-finetune-vs-both">微调</a>数据，但是一旦模型投入生产，就需要进行持续的监控。对于生成式 AI 应用来说，除了常规的度量指标，如延时和成本，还需要对大语言模型进行<a href="https://docs.rungalileo.io/galileo/llm-studio/llm-monitor">特定的观察</a>，以便检测和纠正诸如产生不真实的输出、处理超出其训练领域的查询，以及链式处理出现的问题等。接下来，让我们来了解一下大语言模型观察的关键因素。</p><h3 id="Prompt-analysis-and-optimization">Prompt analysis and optimization</h3><p>识别与输入提示相关的问题，并利用实时的生产数据进行迭代改进，使用强大的评估机制来检测并处理像 AI 产生不真实的“幻觉”这样的问题。</p><h3 id="Traceability-in-LLM-applications">Traceability in LLM applications</h3><p>从 Langchain 和 LlamaIndex 等常见框架中获取大语言模型（LLM）的运行轨迹，以便对输入提示和处理步骤进行调试。</p><h3 id="Information-retrieval-enhancement">Information retrieval enhancement</h3><p>对 RAG 参数进行故障排查和评估，以优化对大语言模型（LLM）性能至关重要的信息提取过程。</p><h3 id="Alerting">Alerting</h3><p>如果系统运行出现异常，比如错误增多、响应延迟增高或者 AI 产生不真实的“幻觉”，你将会收到警报。</p><p>实时监控是观察生产环境中应用程序性能、运行状态和整体健康状况的关键。要密切关注服务等级协议（SLA）的执行情况，并设置警报系统，以便及时处理任何偏离正常的情况。通过分析使用模式和资源消耗，有效地跟踪运行大语言模型（LLM）应用的相关成本，以助你进行成本优化。</p><h2 id="Caching">Caching</h2><p>对于规模化运营的公司来说，成本可能会成为一个阻碍。在这种情况下，缓存是一种极好的节省资金的策略。缓存的过程包括将输入提示和对应的回应存储在数据库中，以便在后续使用时进行检索。这种策略性的缓存机制使得大语言模型（LLM）应用具备三个独特的优势，可以更快速、更经济地产生响应。</p><h3 id="Enhanced-production-inference">Enhanced production inference</h3><p>缓存技术能够使生产过程中的模型预测更快速、更经济。通过使用已缓存的响应，某些查询请求可以实现近乎零延迟，从而优化了用户的使用感受。</p><h3 id="Accelerated-development-cycles">Accelerated development cycles</h3><p>在开发阶段，缓存的应用极大地方便了我们，因为它避免了我们对于同样的提示反复调用 API 的必要。这使得开发周期变得更快、更省成本。</p><h3 id="Data-storage-2">Data storage</h3><p>拥有一个全面存储所有提示的数据库，可以极大地简化大语言模型的微调过程。借助存储的提示-回应对，我们能更高效地基于已积累的数据进行模型优化。</p><p>如果你真的想要提升效率，可以使用 <a href="https://github.com/zilliztech/GPTCache">GPTCache</a> 来为精确匹配和相似匹配实现缓存。它提供了一些重要的指标，如缓存命中率、延迟和召回率，这些指标能够帮助我们了解缓存的性能，从而进行持续的优化，确保达到最佳的效率。</p><h2 id="Multi-tenancy-多租户">Multi-tenancy 多租户</h2><p>在 SaaS 模型中，经常需要处理多租户的情况，这就需要我们在便捷性和保护用户隐私之间找到平衡。对于 RAG 系统的多用户环境，我们的目标是打造一个既能高效检索信息，又能尊重每个用户数据隐私的系统。简单来说，就是要保证每个用户与系统的交互都是独立的，确保系统只处理与当前用户相关的信息。</p><p>实现这样的多用户环境的一种简单方法就是利用元数据。当我们向系统添加文档时，我们会在元数据中加入特定的用户信息。这样，每个文档就与特定的用户建立了关联。当用户进行检索时，系统会利用这些元数据进行过滤，只展示与当前用户相关的文档。然后，系统会进行智能检索，找到对当前用户最重要的信息。这种方式避免了不同用户的私人信息混淆，保证了每个人的数据安全和隐私。</p><p>学习<a href="https://blog.llamaindex.ai/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b">如何使用 Llamaindex 实现多租户</a>。</p><h1>Conclusion</h1><p>我们应当认识到，构建一个健壮且可扩展的企业级 RAG 系统，需要精细地协调各个相互关联的组件。无论是用户认证，输入限制，查询重写，编码，文档摄取，还是像向量数据库和生成器这样的检索组件，每一步都在决定着系统的性能。</p><p>在这个 RAG 系统不断变革的领域里，我们希望这篇实用的指南能为开发者和领导者提供实际可行的见解！</p><h1>Source</h1><p><a href="https://www.rungalileo.io/blog/mastering-rag-how-to-architect-an-enterprise-rag-system">How To Architect An Enterprise RAG System</a></p>]]></content>
    
    <summary type="html">
    
      Mastering RAG.
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="RAG" scheme="https://neo1989.net/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>必读论文 · 7 FPs When Engineering a RAG System (2024)</title>
    <link href="https://neo1989.net/Theses/THESIS-Seven-Failure-Points-When-Engineering-a-RAG-System/"/>
    <id>https://neo1989.net/Theses/THESIS-Seven-Failure-Points-When-Engineering-a-RAG-System/</id>
    <published>2024-02-29T05:56:11.000Z</published>
    <updated>2024-03-29T10:26:35.851Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract">Abstract</h2><p>随着软件工程师在应用程序中不断引入一种名为检索增强生成（Retrieval Augmented Generation, RAG）的策略，以实现语义搜索功能，RAG 系统的应用也日趋广泛。RAG 系统的核心在于寻找与搜索问题在语义上相匹配的文档，并将这些文档交给大语言模型（LLM），如 ChatGPT，依靠 LLM 来抽取准确的答案。RAG 系统旨在：a) 减少大语言模型产生错误回应的风险，b) 为生成的回答关联上来源和参考文献，以及 c) 避免对文档进行元数据标注的必要。但是，RAG 系统也存在局限性，这些局限性源于信息检索系统本身的缺陷以及对大语言模型的依赖性。在本文中，我们通过三个不同领域：研究、教育和生物医学的案例研究，分析了 RAG 系统的失败经验。我们总结了这些案例中的教训，并提出了设计 RAG 系统时应考虑的七个潜在失败点。从我们的工作中得出两个主要的结论是：1) RAG 系统的验证只能在实际运行中才能进行，以及 2) RAG 系统的稳健性是通过不断的迭代优化而形成的，而非在最初设计时就能完全预设的。</p><h2 id="Introduction">Introduction</h2><p>大语言模型（LLMs）的最新进展，包括 ChatGPT，为软件工程师提供了构建新型人机交互（HCI）解决方案的新工具，能够帮助他们完成复杂的任务，概括文档，解答特定文件中的问题，以及创造新的内容。不过，LLMs 在处理最新知识或企业数据库中的特定领域知识方面还存在不足。目前有两种解决策略：a) 对 LLMs 进行微调（继续用特定领域的资料训练 LLM），这需要管理或提供一个经过微调的 LLM；或 b) 使用基于检索的增强生成（RAG）系统，这种系统依赖 LLMs 利用现有的（可扩展的）知识资料来生成答案。这两种方法在数据隐私与安全、可扩展性、成本、所需技能等方面各有利弊。本文将重点讨论 RAG 系统。</p><p>检索增强生成（RAG）系统提出了一个具有吸引力的解决方案来应对这一挑战。它将检索机制与大语言模型（LLMs）的生成功能相结合，能够生成与上下文相关、准确且最新的信息。RAG 系统融合了信息检索和大语言模型的生成才能。其检索部分致力于从数据仓库中寻找与用户问题相关的信息，而生成部分则利用这些检索到的信息作为背景，以产生针对用户问题的答案。RAG 系统的一个重要应用是，它使得所有非结构化的信息都能被索引和查询，这大大缩短了开发周期，无需构建知识图谱，同时也减少了数据整理和清洗的工作量。</p><p>软件工程师在构建 RAG 系统时，首先需要对获取的不同格式的领域知识进行预处理，并将其作为工件存放。接着，他们要将这些经过处理的信息储存在适当的数据仓库（例如向量数据库）中，选择或者整合一种合适的查询与工件匹配策略，对找到的工件进行排序，并通过调用大语言模型（LLMs）的 API，结合用户的查询和相关的上下文文档。目前，关于构建 RAG 系统的新技术不断涌现，但要了解这些技术如何适用于特定的应用场景以及它们的实际效果如何，还需要进一步的探索。</p><p>在本研究中，我们通过三个案例研究分享了所学到的经验和七个导致失败的关键因素。本文旨在为实践者提供参考，并为 RAG 系统的研究发展指明方向。据我们所知，这是首次就构建鲁棒 RAG 系统面临的挑战提供实证分析。随着大语言模型（LLMs）技术的不断进步，软件工程领域的专家们需要负起责任，分享如何构建基于 LLMs 的鲁棒系统的知识。这项工作对于提升 RAG 系统的鲁棒性来说，是一个重要的进展。</p><p>本项研究的主要问题包括：</p><ul><li><p>在构建 RAG 系统时，我们会遇到哪些失败的环节？(第5节) 为了探究这些潜在的失败环节，我们以 BioASQ 数据集为基础开展了一项实证研究。该实验包含了15,000篇文档和1,000组问题与答案。我们首先对所有文档建立索引，随后利用 GPT-4 运行查询并记录生成的答案。接下来，我们用 OpenAI 的评估工具对所有问题与答案进行了验证。我们对所有出现差异的案例、被标记为错误的案例以及随机选取的标记为正确的案例进行了手动审查，以此来分析并识别出现问题的模式。</p></li><li><p>当我们要构建一个 RAG 系统时，有哪些关键因素需要我们仔细考虑呢？在这里，我们将分享从三个 RAG 系统实施案例中吸取的宝贵经验。这些案例不仅展示了我们在实施过程中遇到的挑战，也揭示了我们从中获取的深刻洞察。</p></li></ul><p>这项研究带来的主要贡献包括：</p><ul><li>我们罗列了 RAG 系统中可能出现的各种失败点 (FP)。</li><li>我们分享了三个 RAG 系统实施案例的实践经验，其中两个案例的系统现正于 Deakin 大学运行。</li><li>我们根据从三个案例研究中吸取的经验，为 RAG 系统的研究指明了一条新的道路。</li></ul><h2 id="Related-Work">Related Work</h2><p>RAG是指在预训练和推理阶段，利用文档来提升大语言模型的性能。但是，要知道，训练这样的模型需要大量的计算资源、数据准备时间，因此，如果能直接使用未经训练或微调的 RAG，无疑是一种极具吸引力的方案。然而，当我们试图使用大语言模型来提取信息时，便会遭遇一些挑战，比如处理长篇文本的性能问题。</p><p>最近的一项调查显示，大语言模型在 RAG 流程中被广泛应用，包括检索器、数据生成、重写器和阅读器。我们的研究从软件工程的角度出发，旨在深入探讨工程师在实践中可能遇到的问题，以及为了实现当前最先进的 RAG 系统，需要进行哪些软件工程的研究。</p><p>近期有研究对 RAG 系统进行了基准测试，但并未关注在实施过程中可能出现的问题。软件工程研究已经探讨了 RAG 系统在代码相关任务中的应用。然而，RAG 系统的应用范围远超软件工程任务。本文从实践者的角度出发，补充了现有的研究，揭示了在实施 RAG 系统过程中可能遇到的挑战。</p><p>RAG 系统中产生的错误和失败与其他信息检索系统有许多相同之处，如 1) 缺乏查询重写的评价标准，2) 文档的重新排名，以及 3) 高效的内容概括。这些问题，我们的研究结果已经证实。而与大语言模型的语义和生成特性相关的部分，如评估事实准确性，便是 RAG 系统所独有的挑战。</p><h2 id="Retrieval-Augmented-Generation">Retrieval Augmented Generation</h2><p>随着 ChatGPT、Claude 和 Bard 等大语言模型服务的广泛应用，人们开始探索将它们作为问答系统的可能性。虽然这些系统的表现令人瞩目，但也存在两个根本性的问题：1) &quot;幻觉&quot;问题 —— 即大语言模型生成的答案看似正确，实则错误；2) &quot;无法控制&quot;问题 —— 除了通过精心设计的提示，我们无法直接指导或修改模型的输出内容。为了解决这些问题，人们设计了 RAG 系统，这是一种信息检索方法，旨在克服直接使用大语言模型所带来的限制。</p><p>RAG 的工作方式是，首先将用自然语言表达的查询问题转化为嵌入，以在大量文档中进行语义搜索。在找到相关的文档后，这些文档会被送到一个大语言模型中，由模型生成答案。如 图1 所示，RAG 系统主要包含两个步骤：一是建立索引，二是进行查询。更多的细节可以参考相关的研究调查报告。</p><p><img src="//s3.mindex.xyz/blog/Theses/7822f2a103869fb54aa6f83968687c27.png" alt="图1：创建一个检索增强生成（RAG）系统，需要进行索引和查询两个步骤。通常情况下，索引步骤在系统开发阶段完成，而查询步骤则在系统实际运行时进行。在我们的研究中，我们找到的可能导致系统失败的环节，都在图表中用红色框进行了标注。同时，所有必须完成的步骤，也都在图表中用下划线进行了标识。"></p><h3 id="Index-Process">Index Process</h3><p>在 RAG 系统中，我们采用一种称为 “嵌入” 的技术来帮助检索系统工作。“嵌入” 是一种压缩了的文档语义表示方式，可以想象成是一个由数字组成的向量。在建立索引的过程中，我们会把每个文档切分成更小的片段，然后用一个特殊的模型将这些片段转化成 “嵌入”。这些原始的片段和它们对应的 “嵌入” 会被一起存储在数据库中。在这个过程中，软件工程师需要做出一些设计决策，比如如何合理地切分文档，以及每个片段应该有多大。如果切分的片段太小，某些问题可能就无法得到完整的答案；而如果片段太大，那么生成的答案可能会包含一些无关的信息。</p><p>不同类型的文档在处理和分块策略上有不同的需求。例如，对于视频内容，我们需要一个转录系统来提取音频并在编码前将其转换为文本（详见 <a href="#Cognitive-Reviewer">Cognitive Reviewer</a> ）。此外，选择何种嵌入方式也至关重要，因为更改嵌入策略需要重新对所有分块进行索引。在选择嵌入方式时，我们应根据其在语义上检索正确答案的能力来决定。这个过程会受到分块大小、预期的问题类型、内容的结构以及应用领域的影响。</p><h3 id="Query-Process">Query Process</h3><p>查询过程是在实时运行中进行的。首先，我们将自然语言形式的问题转化为一般性的查询。为了让这个查询更具普遍性，我们使用了大语言模型，这使得我们可以在新的查询中加入更多的上下文信息，比如之前的聊天历史。然后，我们根据新的查询计算出一个嵌入，用于在数据库中寻找相关的文档。我们会使用如余弦相似度这样的相似性方法来检索最相似的 top k 个文档（向量数据库有一些技术，如倒排索引，可以加速检索过程）。直观来说，与查询在语义上更接近的块更有可能包含我们需要的答案。</p><p>检索到的文档将被重新排序，以便让含有答案的文档块尽可能地排在前面。接下来的阶段是“整合器”（Consolidator），它负责处理这些文档块。这个步骤的存在是为了解决大语言模型面临的两个主要限制：1）Token 的数量限制；2）处理速度的限制。像 OpenAI 这样的服务会对输入的文本量设定一个上限。这就限制了我们可以用于提取答案的文档块的数量，因此我们需要一个策略来精简并链接这些文档块，以便从中获取答案。这些在线服务还会限制在一定时间内可以使用的 Token 数量，这就限制了系统的响应速度。因此，软件工程师在设计 RAG 系统时需要考虑这些权衡。</p><p>在 RAG 流程的最后阶段，答案会从生成的文本中被提取出来。在这个阶段，我们需要从输入的问题中筛选出有用的信息，同时遵循一些特定的格式要求，比如把答案列成一个选项列表，然后生成最后的输出结果。要实现 RAG 系统，我们需要设计多种不同的问题和答案处理方式。这样做可以确保我们能得到和特定领域相关的问题。通过使用 LLM 从文本中实时提取答案，我们可以开发出一些新的应用领域，比如实时的问题回答系统。不过，RAG 系统的测试是非常困难的，因为我们没有现成的数据可以用来测试。我们只能通过生成一些模拟的数据，或者先行试运行系统来进行实验性的测试。</p><h2 id="Case-Studies">Case Studies</h2><p>本研究通过三个案例研究，探讨了在 RAG 系统的实施过程中可能遇到的挑战。表 1 展示了每个案例研究的概述。BioASQ 案例研究的所有脚本、数据，以及每个失败环节的示例，都已在网上公开。由于涉及到保密问题，另外两个案例研究并未包含在内。</p><p><img src="//s3.mindex.xyz/blog/Theses/920285f4374640c3cea4bf404a8cdf66.png" alt="表1：这是本文所提到的 RAG 案例研究的概要。"></p><h3 id="Cognitive-Reviewer">Cognitive Reviewer</h3><p>Cognitive Reviewer 是一款 RAG 系统，旨在帮助研究人员分析科学文档。研究人员可以设定研究问题或目标，并上传一系列相关的研究论文。接着，系统会根据设定的目标对所有文档进行排序，供研究人员进行人工审阅。此外，研究人员还可以直接向系统提出关于所有文档的问题。目前，Deakin University 的博士生们正在使用 Cognitive Reviewer 来支持他们的文献综述工作。Cognitive Reviewer 在运行时进行索引处理，并依赖于一个强大的数据处理流程来处理上传的文档，也就是说，在开发阶段无法进行质量控制。此系统还采用了一种排名算法来对上传的文档进行排序。</p><h3 id="AI-Tutor">AI Tutor</h3><p>AI 导师是一个 RAG 系统，学生可以向系统提出关于课程的问题，答案则基于学习资料。学生可以通过查看答案来源的列表来核实答案。AI 导师通过整合到 Deakin 大学的学习管理系统，对包括 PDF 文档、视频和文本文件在内的所有内容进行编码和索引。在这个编码和索引的过程中，我们使用深度学习模型 Whisper 对视频进行转录，然后将其分解成可管理的部分。AI 导师是在 2023 年 8 月到 11 月期间开发的，用于一个在 2023 年 10 月 30 日开始的拥有 200 名学生的课程试点项目。我们的目标是分享实施过程中的经验教训，并在试点项目结束时分享后续的发现。这个 RAG 流程包括一种可以简化用户查询的重写功能。我们设计了一个聊天界面，其中用户和 AI 导师之间之前的对话被用作每个问题的上下文。重写功能会考虑这种上下文，重新构造用户的查询，以解决像 “请进一步解释这个概念” 这样的模糊请求。</p><h3 id="Biomedical-Question-and-Answer">Biomedical Question and Answer</h3><p>在之前的案例中，我们主要研究的是内容规模较小的文件。为了进一步探索大规模数据的问题，我们使用了 BioASQ 数据集，构建了一个 RAG 系统。BioASQ 数据集由生物医学专家编制，包含了问题、相关文档链接和答案，答案的形式可能是yes/no、文本摘要、事实或者列表。我们从这个数据集中下载了 4017 篇开放获取的文档，并提出了 1000 个问题。所有的文档都经过了索引处理，并在 RAG 系统中进行了问题提问。接着，我们运用 OpenAI 实现的 OpenEvals 技术对生成的问题进行了评估。在所有生成的问题中，我们手动审查了 40 个问题，以及所有被 OpenEvals 标记为不准确的问题。我们发现，在这个领域，自动评估的结果通常比人类评估者更为保守。但是，需要注意的是，BioASQ 是一个特定领域的数据集，而进行评审的并非专家。也就是说，大语言模型可能在某些方面比非专家更有见解。</p><h2 id="Failure-Points-of-RAG-Systems">Failure Points of RAG Systems</h2><p>通过我们的案例研究，我们发现了一系列即将在下文中详述的问题。接下来的部分，我们将探讨这样一个研究问题：在构建 RAG 系统过程中，都有哪些可能导致失败的环节？</p><ul><li><p><strong>Missing Content</strong> 首个问题出现在，当我们提出一个无法从现有文档中找到答案的问题时。在最好的情况下，RAG 系统会回应：“对不起，我无法回答这个问题。”然而，对于那些与内容相关，但是并没有明确答案的问题，系统可能会被误导，产生错误的回答。</p></li><li><p><strong>Missed the Top Ranked Documents</strong>  问题的答案其实在文档中，但由于排名不够高，没有被返回给用户。理论上，系统会对所有文档进行排名，然后在后续步骤中使用。但实际上，系统只会返回排名 top k 的文档，其中 k 是基于性能选择的一个值。</p></li><li><p><strong>Not in Context - Consolidation strategy Limitations</strong> 虽然包含答案的文档已经从数据库中检出，但却未能被纳入用于生成答案的上下文中。这种情况通常发生在数据库返回大量文档，并进行了整合处理以提取答案的情况下</p></li><li><p><strong>Not Extracted</strong> 虽然答案确实存在于上下文中，但大语言模型却未能抽取出正确的答案。这种情况通常发生在上下文中存在过多的干扰信息或者信息之间存在矛盾的时候。</p></li><li><p><strong>Wrong Format</strong> 问题需要以特定的格式（如表格或列表）提取信息，但大语言模型并未按照这个要求执行。</p></li><li><p><strong>Incorrect Specificity</strong> 虽然用户可以从响应中获取答案，但答案可能过于笼统或者过于详细，无法满足用户的实际需求。这种情况通常出现在 RAG 系统的设计者对某个问题有特定的预期结果，比如教师对学生的期望。在这种情况下，我们需要提供的不仅仅是答案，还应包含具体的教育内容。另外，当用户不清楚如何准确提问，或者提问过于宽泛时，也可能导致返回的答案过于模糊或过于详细。</p></li><li><p><strong>Incomplete</strong> 虽然不完整的答案并不算是错误，但却可能遗漏了一些原本可以从上下文中获取的信息。比如，如果一个问题是“文档 A、B 和 C 中都涵盖了哪些关键点？”，这种情况下，将问题分开来逐一提问可能会是一个更好的策略。</p></li></ul><h2 id="Lessons-and-Future-Research-Directions">Lessons and Future Research Directions</h2><p><img src="//s3.mindex.xyz/blog/Theses/18513d54285f06b15a9a5d8fb6b3e3cb.png" alt="表2：从三个案例研究中我们汲取了宝贵的经验，这对未来RAG的实施提供了重要的参考"></p><p>我们从三个案例研究中学到的教训已在 表2 中列出。对于研究问题&quot;在构建 RAG 系统时，有哪些关键考虑因素？&quot;，我们的发现如下：根据我们的总结，我们找到了几个与 RAG 有关的潜在研究领域，具体如下：</p><h3 id="Chunking-and-Embeddings">Chunking and Embeddings</h3><p>文档分块看似简单，然而分块的质量以多种方式影响检索过程，尤其是通过影响块的嵌入，进而影响块与用户查询的相似性和匹配。有两种分块的方式：一种是基于启发式的分块（通过使用标点符号，段落结束等方式），另一种是语义分块（利用文本中的语义信息来确定块的开始和结束）。进一步的研究应探讨这两种方法之间的权衡，以及它们对关键下游过程，如嵌入和相似性匹配的影响。一个系统性的评估框架，对分块技术在诸如查询相关性和检索准确性等指标上的效果进行比较，将对这个领域有所贡献。</p><p>嵌入是另一个热门的研究领域，包括为多媒体和多模态块（如表格、图形、公式等）生成嵌入。通常在系统开发过程中或新文档被索引时，会创建一次块嵌入。查询预处理在很大程度上影响了 RAG 系统的性能，尤其是在处理负面或模糊查询时。我们需要进一步研究架构模式和方法，以应对嵌入的固有限制（匹配质量是领域特定的）。</p><h3 id="RAG-vs-Finetuning">RAG vs Finetuning</h3><p>大语言模型（LLMs）因其大量的训练数据和在发布前对模型进行的微调，被视为优秀的全球模型。但是，这些模型是通用的（可能并不了解你特定领域的具体知识），并且知识库并不是最新的（存在知识截止日期）。微调和 RAG 提供了两种可能的定制方式，每种方式都有其独特的权衡。微调需要收集内部数据集来调整和训练大语言模型。然而，你所有的数据都会被集成到模型中，你需要解决安全/隐私问题（谁能访问什么）。此外，随着基础模型本身的改进或者你获得新的数据添加到模型中，你需要再次进行微调。另一方面，RAG 系统似乎提供了一个实用的解决方案，允许你根据需要划分你的数据，并只把相关的数据片段纳入到上下文中，让大语言模型从这些上下文中生成答案。这方便了用新的文档持续更新知识，并且也给了用户对哪些数据片段可访问的控制权。然而，对于数据片段的嵌入、检索和上下文融合的最优策略，仍然是研究的热点。未来的研究应系统地比较微调和 RAG 的各种因素，包括准确性、延迟、运行成本和鲁棒性。</p><h3 id="Testing-and-Monitoring-RAG-systems">Testing and Monitoring RAG systems</h3><p>对于 RAG 系统，其软件工程的最佳实践仍在探索阶段。软件测试和测试用例的生成是其中需要进一步优化的领域。RAG 系统需要的问题和答案通常是特定于应用的，在对非结构化文档进行索引时，这些问题和答案往往无法直接获取。目前有一些新的研究尝试使用大语言模型从多个文档中生成问题。然而，如何生成与特定领域相关的、符合实际情况的问题和答案，这仍然是一个未解决的问题。</p><p>一旦获得了合适的测试数据，我们还需要质量度量标准来帮助工程师进行质量取舍。使用大语言模型的成本高昂，同时也带来了延迟问题，每次新版本的发布都会改变其性能特性。这种特性在机器学习系统中已经被研究过，但是针对基于大语言模型的系统（如RAGs）所需的适应性策略（如果有的话）尚未实施。另一种思路是将自适应系统的理念融入到 RAG 系统的监控和调整中，其他机器学习应用的初步工作已经开始尝试这种方法。</p><h2 id="CONCLUSION">CONCLUSION</h2><p>RAG 系统是一种革新性的信息检索技术，它巧妙地运用了大语言模型（LLM）。现在，软件工程师越来越多地通过实施语义搜索或参与新的代码相关任务，来与 RAG 系统进行互动。本文分享了我们在进行三个案例研究的过程中，包括对15000份文档和1000个问题的实证研究，所得到的宝贵经验与发现。这些发现为实践者提供了实施 RAG 系统时可能面临的挑战，为他们指明了方向。我们还对 RAG 系统的未来研究方向进行了探讨，包括：1) 如何进行分块和嵌入，2) RAG 与微调的关系，以及 3) 如何进行测试和监控。随着研究的深入，大语言模型将会拥有更多对工程师和研究人员有价值的新功能。本文是首次从软件工程的角度对 RAG 系统进行深入的探讨。</p><h2 id="Source">Source</h2><p><a href="https://arxiv.org/abs/2401.05856">Seven Failure Points When Engineering a Retrieval Augmented Generation System</a></p>]]></content>
    
    <summary type="html">
    
      Retrieval Augmented Generation System.
    
    </summary>
    
    
      <category term="Theses" scheme="https://neo1989.net/categories/Theses/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="RAG" scheme="https://neo1989.net/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>必读论文 · Attention Is All You Need (2017)</title>
    <link href="https://neo1989.net/Theses/THESIS-Attention-is-All-You-Need/"/>
    <id>https://neo1989.net/Theses/THESIS-Attention-is-All-You-Need/</id>
    <published>2024-02-26T09:11:53.000Z</published>
    <updated>2024-02-28T04:25:03.166Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract">Abstract</h2><p>主流的序列转换模型都建立在复杂的循环神经网络或卷积神经网络之上，它们都包含编码器和解码器。而且，最优秀的模型还会通过注意力机制（attention mechanism）将编码器和解码器连接起来。我们提出了一种新的网络架构——Transformer，它完全依赖于注意力机制，而不再需要复杂的循环和卷积过程。在两项机器翻译任务的实验中，这种模型在质量上表现出色，同时具有更高的并行性，训练时间也大大缩短。在WMT 2014年的英德翻译任务中，我们的模型达到了28.4的<a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>分数，比现有的最好成绩（包括集成模型）提高了2个以上的BLEU分数。在WMT 2014年的英法翻译任务中，我们的模型在八个GPU上训练3.5天后，创造了41.8的SOTA BLEU新纪录，这只是最好模型训练成本的一小部分。我们还证明，无论是在大规模训练数据还是有限训练数据的情况下，Transformer都能成功应用于英语句法解析任务，从而证明了其良好的泛化性能。</p><h2 id="Introduction">Introduction</h2><p>循环神经网络，尤其是长短期记忆（Long Short-Term Memory）和门控循环神经网络（Gated Recurrent Neural Networks），在序列建模和转换问题上已经成为SOTA方法，这些问题包括语言建模和机器翻译。自那时起，大量的研究工作不断尝试突破循环语言模型和编码-解码架构的技术限制。</p><p>循环模型通常会根据输入和输出序列中的符号位置进行计算。它们会生成一系列的隐藏状态 $h_t$ ，这个状态是由前一状态 $h_{t-1}$ 和当前位置 $t$ 的输入共同决定的。这种固有的顺序性使得在单个训练样例中进行并行化变得不可能，这在处理长序列时尤其关键，因为内存限制使得无法在多个样例间进行批处理。最近的研究通过使用分解技巧和条件计算，不仅大大提升了计算效率，而且在后者的情况下，模型的性能也得到了提升。然而，顺序计算的基本限制仍然无法被克服。</p><p>注意力机制已经成为各种任务中序列建模和转导模型的关键部分，它可以在不考虑输入或输出序列中的距离的情况下，模拟出数据间的依赖关系。然而，除少数情况外，这种注意力机制通常都是与循环网络一起使用的。</p><p>在这项工作中，我们提出了 Transformer，这是一种新的模型架构，它不再依赖循环网络，而是完全依赖于注意力机制来处理输入和输出之间的全局依赖关系。Transformer 允许更高效的并行处理，只需在八个 P100 GPU 上训练短短十二小时，就能达到前所未有的翻译质量。</p><h2 id="Background">Background</h2><p>‘Extended Neural GPU’，‘ByteNet’ 和 ‘ConvS2S’ 都秉承了减少顺序计算的设计理念，它们都采用卷积神经网络作为基础模块，能够并行处理所有输入和输出的隐藏表示。然而，在这些模型中，如果想要建立两个任意输入或输出位置之间的关系，所需的计算步骤会随着位置间距离的增加而增加，‘ConvS2S’ 是线性增长，而 ‘ByteNet’ 则是对数增长。这使得模型在学习远距离位置间的关系时面临更大的困难。相比之下，Transformer 将这个问题简化为固定数量的操作，尽管这样做会因为平均注意力加权位置而降低有效的分辨率，但我们在第3.2节中介绍的多头注意力机制可以有效地解决这个问题。</p><p>自注意力，有时也被称为内部注意力，是一种将单一序列中不同位置的信息进行关联，以此来计算出序列的表征的注意力机制。自注意力已经在许多任务中取得了成功，这些任务包括理解阅读内容、进行抽象的总结、理解文本的内在含义，以及学习与具体任务无关的句子表征。</p><p>端到端记忆网络是基于循环注意力机制构建的，而不是依赖于序列对齐的循环机制。这种网络已经在简洁语言的问题回答和语言模型构建任务上展现出了优秀的性能。</p><p>据我们所知，Transformer 是首个完全依赖自注意力来计算其输入和输出表示的转换模型，它并未使用任何序列对齐的 RNN 或卷积。在接下来的部分，我们将详细介绍 Transformer，阐述自注意力的重要性，并探讨它相较于 ‘ICLR (2016)’，‘Neural machine translation in linear time (2017)’ 和 ‘Convolutional seq2seq learning (2017)’ 等模型的优势。</p><h2 id="Model-Architecture">Model Architecture</h2><p>大部分具有竞争力的神经序列转换模型都采用了编码器-解码器的结构。在此结构中，编码器将一个符号表示的输入序列 $(x_1, …, x_n)$ 映射到一个连续表示的序列 $z = (z_1, …, z_n)$。给定 $z$，解码器便逐个生成输出序列 $y = (y_1, …, y_n)$ 的符号。在每一步中，模型都是自回归的，即在生成下一个符号时，会利用之前生成的符号作为额外的输入。</p><p>Transformer 模型采用了如 图1 所示的结构，其中编码器和解码器部分都采用了堆叠的自注意力机制和逐点的全连接层。</p><p><img src="//s3.mindex.xyz/blog/Theses/d64b985edbb3a6d5a521cda18f3ab35f.png" alt="图1：Transformer - 模型架构。"></p><h3 id="Encoder-and-Decoder-Stacks">Encoder and Decoder Stacks</h3><p><strong>Encoder</strong>: 编码器由 $N = 6$ 个完全相同的层组成。每一层都由两个子层构成：第一个子层是多头自注意力机制，第二个子层是简单的位置相关的全连接前馈网络。我们采用了一种设计，即每个子层的输出都会与其输入进行残差连接，然后进行层归一化处理，即 $LayerNorm(x + Sublayer(x))$，其中 $Sublayer(x)$ 是子层自身的功能。为了实现这种残差连接，模型中所有的子层以及嵌入层都会输出维度为 $d_{model} = 512$ 的结果。</p><p><strong>Decoder</strong>: 解码器的构造与编码器类似，也是由 6 个完全相同的层组成。但在每一层中，解码器除了拥有编码器的两个子层外，还增加了第三个子层，这个子层的作用是对编码器所有层的输出进行多头自注意力处理。就像编码器一样，我们在每个子层的输入和输出之间添加了残差连接，并进行层归一化处理。为了防止解码器的自注意力子层处理时，后面的位置能看到前面的信息，我们对其进行了修改。通过这种方式，加上输出嵌入向后偏移一个位置，我们可以确保在预测第 $i$ 个位置的输出时，只能使用位置 $i$ 之前的已知输出。</p><h3 id="Attention">Attention</h3><p>注意力函数可以理解为，它接收一个查询和一组键值对，然后生成一个输出结果。这个查询、键、值以及输出结果都可以被视为向量。输出结果实际上是值向量的加权和，而每个值的权重则是由查询和相应键的匹配程度决定的。</p><p><img src="//s3.mindex.xyz/blog/Theses/77f1cfcd5ed58276e8d605972e6ce520.png" alt="图2：（左图）缩放的点积注意力；（右图）多头注意力，有多个并行运行的注意力层构成"></p><h4 id="Scaled-Dot-Product-Attention">Scaled Dot-Product Attention</h4><p>我们将这种特定的注意力机制称为“缩放点积注意力”（见 图2）。输入由维度为 $d_k$ 的查询项（queries）和键（keys），以及维度为 $d_v$ 的数值（values）组成。我们首先计算各query与所有keys的点积，然后各自除以 $\sqrt{d_k}$，最后通过 softmax 函数得出各个数值的权重。</p><p>在实际应用中，我们会同时对一组查询进行注意力函数的计算，这些查询被整合成一个矩阵 Q。同样，键和值也被整合成矩阵 K 和 V。我们按照以下方式计算得出输出矩阵：<br>$$<br>Atteention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}}) V<br>$$</p><p>最常用的两种注意力机制是加性注意力和点积（乘性）注意力。点积注意力与我们所使用的算法几乎一样，只不过有一个 $\frac{1}{\sqrt{d_k}}$ 的缩放因子。加性注意力则是通过一个单隐藏层的前馈网络来计算相似度函数。虽然这两种方法在理论上的复杂度差不多，但在实际应用中，<strong>点积注意力因为能够利用高度优化的矩阵乘法代码，所以运行速度更快，也更节省存储空间</strong>。</p><p>对于较小的 $d_k$ 值，这两种注意力机制的表现类似，但是当 $d_k$ 值增大时，如果不进行缩放处理，加性注意力的表现会优于点积注意力。我们推测，<strong>对于较大的 $d_k$ 值，点积的结果可能会变得非常大，这会使 $softmax$ 函数进入到梯度极小的区域。为了抵消这种影响，我们将点积的结果乘以 $\frac{1}{\sqrt{d_k}}$ 进行缩放</strong>。</p><h4 id="Multi-Head-Attention">Multi-Head Attention</h4><p>我们发现，直接对 $d_m$ 维的键（keys）、值（values）和查询（queries）进行单一的自注意力（attention）处理不如将它们线性地投影到 $d_k$、$d_k$ 和 $d_v$ 维度更有利。这种线性投影我们会进行 $h$ 次，每次都使用不同的学习到的线性投影。然后，我们在每个投影后的键、值和查询上并行地执行自注意力函数，得到 $d_v$  维的输出值。这些输出值会被连接起来，然后再次投影，得到最终的结果，这个过程在 图2 中有详细的展示。</p><p>多头注意力（Multi-head attention）的设计使模型能够在不同的位置，同时关注不同的信息表示子空间。而如果只使用单一的注意力机制，这种能力就会被平均化操作所抑制。</p><p>$$<br>MultiHead(Q, K, V) = Concat(head_1, …, head_h) W^O \\<br>where \enspace head_i = Attention(QW_{i}^Q, KW_{i}^K, VW_{i}^V)<br>$$</p><p>在这里，所说的投影其实就是参数矩阵。其中 $W_{i}^Q \in \mathbb{R}^{d_{model} \times d_k}$，$W_{i}^K \in \mathbb{R}^{d_{model} \times d_k}$，$W_{i}^V \in \mathbb{R}^{d_{model} \times d_v}$，$W_{i}^O \in \mathbb{R}^{d_{model} \times hd_v}$。</p><p>在我们的研究中，我们采用了 $h = 8$ 个并行的注意力层，也就是我们所说的8个&quot;头&quot;。对于这8个&quot;头&quot;，我们都设定了 $d_k = d_v = \frac{d_{model}}{h} = 64$。由于每个&quot;头&quot;的数据规模缩小了，所以总的计算成本与使用全数据规模的单一注意力层差不多。</p><h4 id="Applications-of-Attention-in-our-Model">Applications of Attention in our Model</h4><p>Transformer 在三种不同的方式中使用了多头注意力：</p><ul><li><p>在“编码器-解码器注意力”层中，查询来自前一个解码器层，而记忆键和值来自编码器的输出。这使得解码器中的每个位置都能关注输入序列中的所有位置。这模仿了序列到序列模型中典型的编码器-解码器注意力机制，如 ‘Google’s neural machine translation system, (2016)’, ‘Neural machine translation by jointly learning to align and translate (2014)’, ‘Convolutional seq2seq learning (2017)’</p></li><li><p>编码器由多个自注意力层组成。在每一个自注意力层里，用于生成注意力分数的键（keys）、值（values）和查询（queries）都源自同一处，即编码器前一层的输出。这样的设计使得编码器当前层的每一个元素都能够接收并处理前一层所有元素的信息。</p></li><li><p>解码器内部的自注意力层同样使得每个位置能够接收到该位置之前所有位置的信息。为了维持解码器的自回归特性，我们必须防止信息从右向左流动。这一目标是通过在缩放点积注意力机制中进行操作实现的：我们会对softmax函数的输入进行遮蔽处理，即将那些不应该被当前位置所接收的信息的权重设置成极小值（负无穷），从而有效地切断了这些不合规则的信息连接。</p></li></ul><h3 id="Position-wise-Feed-Forward-Networks">Position-wise Feed-Forward Networks</h3><p>除了自注意力（Self-attention）子层之外，我们的编码器和解码器的每一层还包括一个独立且对每个位置都相同处理的全连接前馈神经网络。这个网络由两个全连接层（linear transformations）构成，在两层之间使用了ReLU激活函数。</p><p>$$<br>FFN(x) = max(0, xW_1 + b_1)W_2 + b_2<br>$$</p><p>尽管不同位置上的线性变换是一致的，但从一层到另一层，它们会使用不同的参数集。换一种方式来说，就像是使用了卷积核尺寸为 $1$ 的两个卷积操作。模型的输入和输出维度 $d_{model} = 512$，而内部层的维度 $d_{ff} = 2048$。</p><h3 id="Embeddings-and-Softmax">Embeddings and Softmax</h3><p>就像其他的序列转换模型一样，我们利用学习得到的嵌入技术（embeddings）将输入的Token和输出的Token转化为维度为 $d_{model}$ 的向量。我们也采用常规的线性变换学习方法和softmax函数，来把解码器的输出转换成预测下一个Token可能性的概率值。在我们的模型中，我们采用了一个与’Using the output embedding to improve language models (2016)'相似的方法，即在两个嵌入层和softmax之前的线性变换中共享相同的权重矩阵。在嵌入层，我们还会将这些权重与 $\sqrt{d_{model}}$ 相乘。</p><h3 id="Positional-Encoding">Positional Encoding</h3><p>因为我们的模型既不使用递归结构也不使用卷积操作，为了使模型能够理解序列中Token的顺序信息，我们必须提供关于Token在序列中的相对或绝对位置的信息。为了做到这一点，在编码器和解码器的最底层，我们向输入的嵌入数据中添加了“位置编码”。这些位置编码的维度（$d_{model}$）与嵌入数据的维度相同，使得它们可以直接相加。至于位置编码的选择，既有通过学习得到的，也有固定不变的多种方案。</p><p>在这项研究中，我们采用了频率各异的正弦与余弦函数：</p><p>$$<br>PE_{(pos, 2i)} = sin(\frac{pos}{1000^{2i/d_{model}}}) \\<br>PE_{(pos, 2i+1)} = cos(\frac{pos}{1000^{2i/d_{model}}})<br>$$</p><p>在这个公式中，$pos$ 表示序列中的位置，$i$ 代表维度。 换句话说，位置编码的每个维度都对应域一个正弦波形。这些波形的波长按几何数级递增，范围 从 $2\pi$ 到 $10000 \cdot 2\pi$。我们选择这种函数是基于一个假设：它能够让模型轻松地根据 Token 之间的相对位置进行自注意力学习。因为无论偏移量 k 如何变化，位置编码 $PE_{pos+k}$ 都能够用位置编码 $PE_{pos}$ 的线性组合来表示。这样一来，模型就能够通过简单的数学变换来识别和处理序列中 Token 的位置关系。</p><p>我们还试验了采用学习型的位置嵌入（learned positional embeddings）方法，结果显示两种方法得出的成果相差无几（参见 表3 第（E）行）。我们选择了基于正弦波的方法，因为这可能使模型能够处理超出训练时序列长度的数据。</p><h2 id="Why-Self-Attention">Why Self-Attention</h2><p>在这一部分，我们将自注意力层与循环层和卷积层进行了比较。这些层通常用于处理符号表示的变长序列（$x_1$, …, $x_n$）到另一个等长序列（$z_1$, …, $z_n$）的映射问题，其中 $x_i, z_i \in \mathbb{R}^d$ 空间中的向量，比如这样的映射在典型的序列转换编码器或解码器的隐藏层中非常常见。我们选择使用自注意力的动机是基于三个我们所期望的特性。</p><p>在考量每一层的运算效率时，我们首先要评估的是该层的总体计算复杂度。 其次，我们会考察计算任务中可以并行处理的部分，这部分的大小可以通过必须顺序执行的操作次数来衡量。</p><p>第三个考量因素是网络内部处理长程数据依赖时信号传递的路径长度。在众多序列转换任务中，学习这种长程依赖关系是一个核心挑战。能否有效学习这种依赖关系，关键在于信号在网络中传递的路径有多长，无论是前向传递还是后向传递。简而言之，输入与输出序列之间任意两点的路径越短，网络学习长程依赖就越容易。因此，我们也对比了由不同类型的层构成的网络中，任意两个输入和输出位置之间的最大路径长度。</p><p><img src="//s3.mindex.xyz/blog/Theses/84163f518d756c1a1ca877da25396d5b.png" alt="表1：对于不同类型的层，其最大路径长度、每个层的复杂度以及最小序列操作数量各不相同。这里，n 代表序列的长度，d 代表特征维度，k 是卷积核的大小，而 r 则是自注意力限制区域的大小。"></p><p>如 表1 所展示的，自注意力层能够通过一系列固定数量的操作，将所有位置相互连接起来，相比之下，递归层（recurrent layer）则需要 $O(n)$ 次操作。在计算复杂度上，自注意力层在序列长度 $n$ 小于其表示空间的维度 $d$ 的情况下，比递归层更为高效，而这种情况在当前机器翻译中的 SOTA 模型所使用的句子表示，如 <strong>word-piece</strong> 和 <strong>byte-pair</strong> 表示法中非常普遍。为了在处理极长序列的任务时提高计算效率，自注意力可以被限制在仅考虑输入序列中，围绕各自输出位置的大小为 $r$ 的局部邻域内。这种做法会导致最大路径长度变为 $O(n/r)$ 。我们计划在未来的研究中进一步探索这一方法。</p><p>在我们的模型中，单层卷积网络由于核宽度 $k$ 小于序列长度 $n$，不能直接将每个输入位置与输出位置完全连接起来。如果使用连续的核，要实现完全连接需要叠加大约 $n/k$ 层卷积网络；而采用膨胀卷积的话，大约需要 $log_k(n)$ 层。这样会使得网络内任意两点间的最长路径长度增加。一般来说，卷积层的计算成本比循环层要高，大概高出 $k$ 倍。不过，采用可分离卷积可以大幅降低计算复杂度，降至 $O(k · n · d + n · d^2)$。即便核宽度 $k$ 等于 n，可分离卷积的计算复杂度也仅相当于自注意力层和点对点前馈网络层相结合的复杂度，这正是我们模型所采用的策略。</p><p>另外，自注意力还有一个额外好处，那就是能够促成模型的可解释性更强。我们对模型中的注意力分布进行了详细检查，并在附录中展示并讨论了若干例子。我们发现，不仅单独的注意力头确实学会了执行不同的任务，而且许多注意力头的行为似乎还与句子的句法和语义结构密切相关。</p><h2 id="Training">Training</h2><p>本节将详细介绍我们的模型训练方案。</p><h3 id="Training-Data-and-Batching">Training Data and Batching</h3><p>我们使用了包含约450万个句子对的标准 ‘WMT 2014 English-German’ 数据集进行模型训练。这些句子通过字节对编码（byte-pair encoding）方法进行处理，形成了一个大约含有37000个 Token 的共用词汇表，用于源语言和目标语言。在英语-法语的训练中，我们采用了规模更大的 ‘WMT 2014 English-French’ 数据集，它包含了3600万句子，并且使用了包含32000个词片（word-piece）的词汇表来分割 Token。为了提高训练效率，我们按照句子的大致长度将它们分组打包，每个训练批次都包含了一组句子对，这些句子对总共大约包含25000个源语言 Token 和25000个目标语言 Token。</p><h3 id="Hardware-and-Schedule">Hardware and Schedule</h3><p>我们在一台搭载了8块 NVIDIA P100 GPUs 的计算机上对模型进行了训练。基础模型采用论文中详述的超参数，每个训练步骤耗时约0.4秒。基础模型总共训练了100,000步，持续了12小时。至于我们的大型模型（详见 表3 最后一行），每步训练时间为1.0秒。这些大型模型训练了300,000步，用时3.5天。</p><h3 id="Optimizer">Optimizer</h3><p>我们选用了 Adam 优化器，并设置参数 $β_1 = 0.9, β_2 = 0.98$ 以及 $ε = 10^{-9}$。在整个训练过程中，我们按照特定的公式调整了学习率</p><p>$$<br>l_{rate} =  d_{model}^{-0.5} \cdot min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})<br>$$</p><p>这意味着在训练的最初阶段（前 4000 步），我们会逐步增加学习速率，之后则根据训练步数增长的平方根逐渐减慢增速。这里的“逐步增加”是指学习速率会按照一个固定的规律线性上升，而“逐渐减慢增速”则是随着步数的增加，增加的幅度会越来越小。</p><h3 id="Regularization">Regularization</h3><p>在训练过程中，我们使用了三种不同的正则化手段来防止模型过拟合：</p><p><strong>Residual Dropout</strong>  在将每个子层的输出与其输入相加并进行规范化处理之前，我们会使用 Dropout 技术。此外，在编解码器的嵌入向量和位置编码相加的部分，我们也采用了 Dropout。对于基础模型，我们设置的 $P_{dropout} = 0.1$。</p><p><strong>Label Smoothing</strong> 在训练过程中，我们使用了$ε_{ls} = 0.1$的标签平滑技术。尽管这样做会增加模型的预测困难度（即提高了困惑度），使得模型的预测更加保守，但它实际上提升了模型的准确率和BLEU评分。</p><h2 id="Result">Result</h2><h3 id="Machine-Translation">Machine Translation</h3><p><img src="//s3.mindex.xyz/blog/Theses/3d4e0e9e97745a2a482c9d4068d2cc63.png" alt="表2：在将英语翻译成德语和法语的 newstest2014 测试中，Transformer 模型的表现超越了以往最先进的模型，取得了更高的 BLEU 评分，而且所需的训练成本仅为过去模型的一小部分。"></p><p>在WMT 2014的英德翻译任务上，我们的大型Transformer模型（在表2中标记为Transformer (big)）的性能超越了此前所有公布过的最好模型（包括那些集成模型），其BLEU评分高达28.4，刷新了此前的最佳成绩。该模型的具体配置详见 表3 的最后一栏。该模型的训练仅用了8个P100 GPU花费了3.5天。甚至我们的基础版模型也超越了所有先前公布的模型，而且训练成本远低于其他任何有竞争力的模型。</p><p><img src="//s3.mindex.xyz/blog/Theses/8cdbbb740de70e749ed7562ec46484d6.png" alt="表3：Transformer 模型的不同版本在英德翻译的开发测试集 newstest2013 上的表现各有差异。未列出的值与基础模型相同。所有的性能指标都是针对这个特定的测试集。我们根据字节对编码（Byte-Pair Encoding, BPE）计算的困惑度是基于pre_wordpiece，因此与通常基于单词计算的困惑度不同，不应直接比较二者。"></p><p>在 WMT 2014 的英法翻译任务中，我们的大型 Transformer 模型取得了 41.0 的 BLEU 分数，不仅超过了此前所有公开的单模型记录，而且其训练成本还不足先进模型的四分之一。这个专为英法翻译训练的 Transformer（大型）模型，采用了 $P_{dropout} = 0.1$，相较于常规的 0.3 有所降低。</p><p>在基础模型的训练中，我们采用了一个特殊的方法：将最后 5 次保存的模型参数（每 10 分钟保存一次）进行平均，以此得到最终的单一模型。而对于大型模型，我们则平均了最后 20 次的保存点。在模型推理时，我们使用了大小为 4 的 beam search 技术，并设置了序列长度惩罚因子（$α = 0.6$），这些超参数都是在开发数据集上通过多次试验确定的。此外，我们把模型生成文本的最大长度限制在 $输入文本长度 + 50 个词$ 以内，不过如果能够提前得出结论，我们也会尽早结束生成过程。</p><p>表2 汇总了我们的研究成果，并把我们的翻译质量和训练成本与文献中报道的其他模型架构做了对比。我们估计了训练一个模型所需要的浮点运算量，这是通过计算训练时间、使用的 GPU 数目以及每个 GPU 的单精度浮点运算能力的平均值来得出的。</p><h3 id="Model-Variations">Model Variations</h3><p>为了评判 Transformer 各部分的重要性，我们对基础模型进行了多种修改，并在开发集 newstest2013 上测试了这些改动对英译德翻译性能的影响。我们采用了之前章节描述的 Beam Search 策略，但没有进行 checkpoint 的平均处理。相关的结果展示在 表3 中。</p><p>在 表3 的行 (A) 中，我们调整了 Transformer 中的注意力机制的头数 (attention heads) 和对应键 (key) 与值 (value) 的维数，正如<a href="#Multi-Head-Attention">Multi-Head Attention</a> 节所详述，同时确保整体的计算量保持不变。虽然仅使用一个头的注意力机制的翻译质量比最优参数配置低了 0.9 BLEU 分，但过多的头数也会导致翻译质量的降低。</p><p>在表 3 的行（B）中，我们发现当减少了用于计算注意力的关键参数大小 $d_k$  后，模型的性能有所下降。这暗示了评估模型各部分之间的相互关系并非易事，可能需要一个比简单的点乘运算更为复杂的计算方式来提升效果。进一步地，从行（C）和（D）的数据可以看出，一如所料，模型规模越大，其表现通常越好；而且，使用dropout技术能有效防止模型过度学习特定数据的问题。在行（E）中，我们尝试将模型中用于捕捉不同位置信息的正弦波形位置编码替换成了通过学习得到的位置嵌入，结果显示，新模型的表现与原始模型相差无几。</p><h3 id="English-Constituency-Parsing">English Constituency Parsing</h3><p>为了探究 Transformer 是否能够适应其他种类的任务，我们对其在英文成分句法分析（一种分析句子结构的任务）上的表现进行了实验。这项任务具有其独特的挑战性：它要求输出结果具有严格的结构性，并且输出的长度通常会显著超过输入的长度。而且，传统的循环神经网络（RNN）基于序列的模型在数据量不大的情况下，还未能达到最先进（SOTA）的水平。</p><p>我们在Penn Treebank的大约有4万条WSJ数据集上训练了一个有4层且模型维度 $d_{model} = 1024$ 的 Transformer。同时，我们还尝试了半监督学习的方式，利用了大约 1700 万句的高质量语料库，包括 BerkleyParser 语料库。在 ‘WSJ only’ 训练中，我们使用了 16000 个Token的词汇表；而在半监督学习中我们使用了32000个Token的词汇表。</p><p>在开发数据集上，我们只进行了有限的实验来确定 dropout 设置（这包括自注意力和残差连接的设置，详见第 5.4 节）、学习速率和 beam 宽度，所有其他的参数设置都遵循了原先用于英语到德语翻译的基本模型。在模型进行推断时，我们把最大输出长度设定为输入长度加上 300。无论是在只使用WSJ数据集的实验还是在半监督学习的设定中，我们都采用了 21 的 beam 宽度和 0.3 的长度惩罚系数（α）。</p><p>如 表4 所示，尽管我们的模型没有进行特定任务的细致调优，但它的表现出乎意料地出色，其结果超过了除了递归神经网络语法（Recurrent Neural Network Grammar，RNN Grammar）以外的所有以往公布过的模型。</p><p><img src="//s3.mindex.xyz/blog/Theses/84f649a70e83efb38b0934d80efc1adb.png" alt="表4：Transformer 模型在英文成分句法分析方面表现出了良好的适应性和准确度。"></p><p>与基于循环神经网络（RNN）的序列对序列模型不同，Transformer 在只使用包含 4 万句子的华尔街日报（WSJ）训练集进行训练时，其性能甚至超过了 Berkeley-Parser。</p><h2 id="Conclusion">Conclusion</h2><p>在这篇文章中，我们首次介绍了 Transformer 模型，这是一个创新的序列转录模型，它完全依赖于注意力机制（attention mechanisms），用多头自注意力（multi-headed self-attention）技术替代了传统编解码器结构中常用的循环网络层。</p><p>在翻译任务中，Transformer 的训练速度显著快于那些基于循环或卷积层的结构。在 WMT 2014 的英德和英法翻译挑战中，我们的模型都达到了新的行业最高水平（SOTA）。特别是在英德翻译任务中，我们的最佳模型的表现甚至超越了此前所有公开的模型集合。</p><p>我们对注意力机制模型的未来发展前景感到非常期待，并打算将其应用于更多领域。我们的计划是将 Transformer 应用到文本以外的其他类型的数据输入和输出，例如图像、音频和视频，并且研究能够有效处理这些大型数据的局部和有限制的注意力机制。此外，我们还致力于研究如何让内容生成变得更加非线性，这也是我们的研究目标之一。</p><p>我们用来训练和评估模型的代码可以在 <a href="https://github.com/tensorflow/tensor2tensor">github</a> 找到。</p><h2 id="Source">Source</h2><p><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></p>]]></content>
    
    <summary type="html">
    
      Transformer.
    
    </summary>
    
    
      <category term="Theses" scheme="https://neo1989.net/categories/Theses/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="NLP" scheme="https://neo1989.net/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>必读论文 · BERT (2019)</title>
    <link href="https://neo1989.net/Theses/THESIS-bert/"/>
    <id>https://neo1989.net/Theses/THESIS-bert/</id>
    <published>2024-02-22T13:44:02.000Z</published>
    <updated>2024-02-26T09:39:49.614Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract">Abstract</h2><p>我们要介绍的是一种新的语言表示模型，名为 BERT ，一种基于Transformer双向编码表征器。与近期的语言表现模型不同，BERT 的设计目标是从未经标注的文本中预训练深度双向表示（deep bidirectional representations），这意味着它在所有层级上同时考虑左右两侧的上下文信息。因此，只需对预训练的 BERT 模型增加一层输出层进行微调，就能构建出适用于各种任务的SOTA模型，如问题回答和语言推断，而无需对原有结构进行大量的任务相关的改动。</p><p>BERT 的设计理念十分简洁，而且在实践中展现出强大的性能。它在十一个自然语言处理任务中刷新了最佳成绩，这包括将 GLUE 的得分提升到了 80.5%（相较于之前提升了 7.7 个百分点），将 MultiNLI 的准确率提升到 86.7%（相较于之前提升了 4.6%），将 SQuAD v1.1 的问题回答测试 F1 分数提升到 93.2（相较于之前提升了 1.5 点），以及将 SQuAD v2.0 的测试 F1 分数提升到 83.1（相较于之前提升了 5.1 点）</p><h2 id="Introduction">Introduction</h2><p>预训练的语言模型已经被证明在提升许多自然语言处理任务的性能方面具有显著效果。这包括一些句子级别的任务，如自然语言推理和改写，这些任务试图通过深入分析句子来预测它们之间的关系。还有一些是Token级别的任务，如命名实体识别和问题回答，这些任务要求模型能够在Token级别给出精确的输出结果。</p><p>目前，我们有两种策略可以将预训练的语言表示应用到下游任务中：一种是基于特征的方法，另一种是微调方法。基于特征的方法，比如ELMo，会使用一种特别设计的架构，这种架构将预训练的表示作为额外的特征。而微调方法，比如生成预训练Transformer（OpenAI GPT），只引入极少的针对特定任务的参数，并通过对所有预训练参数进行微调来训练下游任务。这两种策略在预训练阶段有着共同的目标，都是通过单向语言模型来学习和掌握通用的语言表达方式。</p><p>我们认为，现有的技术方法限制了预训练模型的潜力，特别是在进行模型微调的过程中。最主要的限制在于，传统的语言模型都是单向的，这就限制了我们在预训练阶段可以选择的架构类型。举个例子，在 OpenAI GPT 中，作者们采用了一种从左到右的架构，其中每个单词（Token）只能在 Transformer 的自注意力（self-attention）层中关注到其前面的单词。对于句子级别的任务，这样的限制并不是最理想的，而且在将微调的方法应用于例如问答这类需要处理单词级别的任务时，这样的限制可能会带来很大的问题，因为在这种情况下，能够融合来自前后两个方向的上下文信息是非常关键的。</p><p>在本论文中，我们提出了一种名为BERT的方法，这是一种基于Transformer的双向编码表示，以此改进了微调方法。BERT采用了一种受Cloze(完形填空)任务启发的预训练策略，称为&quot;掩码语言模型&quot;（MLM），以解决之前提到的单向性问题。在掩码语言模型中，我们会随机遮挡输入中的部分tokens，然后尝试仅根据其上下文来预测这些被遮挡的词的原始词汇id。与传统的自左向右的语言模型预训练不同，MLM能够同时考虑左右两个方向的上下文，这使得我们可以预训练一个深度的双向Transformer。除了掩码语言模型，我们还引入了一种&quot;下一句预测&quot;的任务，以便同时预训练文本对的表示。</p><p>我们的论文主要贡献如下：</p><ul><li><p>我们强调了双向预训练在语言表示中的重要性。与 ‘Radford et al. (2018)’ 采用的单向语言模型预训练不同，BERT 通过使用掩码语言模型来实现深度双向表示的预训练。这与 ‘Peters et al.(2018a)’ 他们通过简单组合独立训练的左向右和右向左的语言模型的方法形成了鲜明对比。</p></li><li><p>我们发现，预训练的表示能够减少对大量精细设计的任务特定架构的依赖。BERT 是首个基于微调的表示模型，它在众多的句子级别和 token 级别的任务上都达到了SOTA性能，超过了许多专为特定任务设计的架构。</p></li><li><p>BERT 提升了十一个 NLP 任务的 SOTA 水平。此<a href="https://github.com/google-research/bert">链接</a>可以找到相关的代码和预训练模型。</p></li></ul><h2 id="Related-Work">Related Work</h2><p>预训练通用语言表达的研究历史悠久，本节我们将简单回顾一下这个领域中最常用的一些方法。</p><h3 id="基于特征无监督的方法">基于特征无监督的方法</h3><p>学习广泛适用的词表征是几十年来的研究热点，涵盖了非神经网络模型和神经网络模型。预训练的词嵌入是现代自然语言处理系统的重要组成部分，它们相比从零开始训练的词嵌入，可以带来显著的性能提升。在预训练词嵌入向量的过程中，我们通常采用了从左到右的语言模型预测目标，以及在上下文中区分正确和错误词汇的目标。</p><p>这些技术已经被扩展应用到更大的语言单位上，如句子或者段落的嵌入表示。在训练句子表示的过程中，以往的研究主要采用了几种方法：一种是通过目标函数对候选的下一句进行排序；另一种是在给定前一句的表示后，从左到右生成下一句的单词；还有一种是利用去噪自动编码器派生的目标函数。</p><p>ELMo及其前身在另一维度上拓展了传统的词嵌入研究。它们从左向右和右向左的语言模型中提取出对上下文敏感的特征。每个Token的上下文表示是由其左向右和右向左的表示拼接而成。ELMo通过将上下文词嵌入与特定任务的架构相结合，成功地提升了一些主要NLP任务（包括问题回答，情感分析，和命名实体识别）的SOTA水平。‘Melamud et al. (2016)’ 提出了一种新的学习上下文表示的方法，他们设计了一个任务，通过长短期记忆网络（LSTMs）来预测左右上下文中的一个词。这种方法在某种程度上与ELMo类似，它们的模型都是基于特征的，而不是深度双向的。‘Fedus et al. (2018)’ 证明使用Cloze任务可以有效地提升文本生成模型的稳定性和鲁棒性。</p><h3 id="基于微调的无监督方法">基于微调的无监督方法</h3><p>就像基于特征的方法一样，这个方向的初步工作只是从未标记的文本中预训练了词嵌入参数。</p><p>近期，一种新型的句子或文档编码器开始受到关注，它能生成具有上下文信息的 Token 表示，并且这种编码器可以通过无标签的文本进行预训练，然后在有监督的下游任务中进行微调。这种方法的优点在于，我们只需要学习少量的参数。正是因为这个优势，OpenAI 的 GPT 在 GLUE 基准的许多句子级任务中创下了前所未有的 SOTA 成果。而且，从左到右的语言建模和自动编码器等目标已经被用于这种模型的预训练。</p><h3 id="从有监督数据中进行迁移学习">从有监督数据中进行迁移学习</h3><p>一些研究已经证明，从大规模数据集的监督任务，如自然语言推理和机器翻译，进行迁移学习是有效的。在计算机视觉领域，研究也展示了大型预训练模型在迁移学习中的重要性，一种有效的策略就是微调那些使用 ImageNet 数据集预训练过的模型。</p><h2 id="BERT">BERT</h2><p>本节将详细介绍BERT及其实现过程。我们的框架分为两个步骤：预训练和微调。在预训练阶段，模型会在各种预训练任务上对无标签数据进行训练。微调阶段，首先使用预训练的参数对BERT模型进行初始化，然后利用下游任务的标签数据对所有参数进行微调。尽管所有的下游任务都使用同样的预训练参数进行初始化，但每个任务都会有自己单独的经过微调的模型。图 1 中的问答示例将在本节中作为持续进行的示例进行讲解。</p><p><img src="//s3.mindex.xyz/blog/Theses/65ee5b6bf0176abff3884c1c91aeca99.png" alt="图1: BERT 的预训练和微调过程的总览。除了输出层外，预训练和微调阶段使用的网络架构是一样的。同样的预训练模型参数被用于初始化不同下游任务的模型。在微调阶段，所有的参数都会被进一步调整。[CLS] 是一个特殊的符号，我们在每个输入样例的开始位置添加它，而 [SEP] 则是一个用于分隔不同部分的特殊 Token。"></p><p>BERT的一个独特特点是，无论是在哪种任务中，其架构都保持一致。预训练阶段的架构和最终应用于下游任务的架构之间的差别极其微小。</p><ul><li><p><strong>Model Architecture</strong></p><p>BERT 的模型架构是一种多层双向 Transformer 编码器，这种架构基于 ‘Vaswani 等人 (2017)’ 的原始实现，并已经在 ‘tensor2tensor’ 库中公开。由于 Transformer 的使用已经非常普遍，且我们的实现几乎与原始的完全一致，因此我们不会详细描述这个模型架构的背景。如需进一步了解，读者可以参考 ‘Vaswani 等人 (2017)’ 的文章，或者阅读像是 “The Annotated Transformer” 这样的优秀教程。</p><p>在我们的研究中，我们以 L 来表示层数，以 H 来表示隐藏层的大小，以 A 来表示自注意力头的数量。主要的性能结果我们将以两种模型规模来展示：‘BERT_BASE’ (L=12, H=768, A=12, 总参数量=110M) 和 ‘BERT_LARGE’ (L=24, H=1024, A=16, 总参数量=340M)。</p><p>我们选择 BERT_BASE 的原因是为了与 OpenAI GPT 的模型大小做出比较。然而，最关键的区别在于，BERT Transformer 采用了双向自注意力机制（即每个 Token 可以关注到其左右两侧的上下文），而 GPT Transformer 则采用了受限的自注意力机制（即每个 Token 只能关注到其左侧的上下文）。</p></li><li><p><strong>Input/Output Representations</strong></p><p>为了让 BERT 能够处理各种下游任务，我们设计了一种输入表示方式，这种方式可以在一个 Token 序列中清晰地表示出单个句子，也可以表示出两个有关联的句子（例如，⟨ Question, Answer ⟩）。在我们的研究中，“句子”可以是一段连续的文本片段，而不仅限于实际的语言学意义上的句子。“序列”则是指输入给BERT的 Token序列，这可能是一个独立的句子，或者是两个句子组合在一起。</p><p>我们采用了WordPiece嵌入方法，其词汇表规模为30000个Token。每个序列的第一个Token总是一个特殊的分类Token（[CLS]）。对应这个Token的最后一个隐藏状态被用作分类任务中的序列总体表示。我们会将一对句子组合成一个单独的序列。我们通过两种方式来区分这些句子。首先，我们使用一个特殊的Token（[SEP]）来分隔它们。其次，我们为每个Token添加一个训练得到的嵌入，用来标识它是属于句子A还是句子B。如图 1 所示，我们将输入的嵌入记做E，[CLS]标记的最后一个隐向量记做C，以及第 i 个输入token的最后一个隐向量记做 $ T_i $ 。</p><p>对于一个特定的token，我们通过将对应的token、分段、和位置的嵌入相加，来构建它的输入表示。图2即为这种构建方式的可视化展示。</p></li></ul><p><img src="//s3.mindex.xyz/blog/Theses/51c91502fae0b24682733616c21a384c.png" alt="图2：BERT 的输入表示方法。输入嵌入是 token 嵌入、分段嵌入和位置嵌入的总和。"></p><h3 id="Pre-training-BERT">Pre-training BERT</h3><p>我们的方法与 ‘Peters et al. (2018a)’ 和 ‘Radford et al. (2018)’ 的方法有所不同，他们采用的是传统的顺序（从左至右或从右至左）语言模型进行 BERT 的预训练。而我们则选择使用两种特定的无监督学习任务来预训练 BERT，这些任务将在本节中详细介绍。你可以在图 1 的左侧部分看到这个预训练步骤的示意图。</p><ul><li><p><strong>Task #1: Masked LM</strong></p><p>直观上看，深度双向模型必然比单向模型（从左到右或从右到左）或者是简单地将一个从左到右的模型和一个从右到左的模型拼接在一起的方法更强大。然而，遗憾的是，我们通常只能以从左到右或从右到左的方式来训练标准的条件语言模型，因为如果允许模型同时考虑两个方向的信息，每个单词就能间接地“看到”自己，这就使得模型可以轻易地在多层次的上下文中预测出目标单词。</p><p>为了训练出深度的双向表示，我们采用了一种简单的方法：随机遮蔽输入中的一部分 tokens，然后预测这些被遮蔽的 tokens。我们把这个过程称为 “遮蔽语言模型”（Masked LM，简称 MLM），在学术界，这种任务有时也被称为 “填空任务”（Cloze task）。在这个过程中，对应被遮蔽 tokens 的最后一层隐藏向量会被输入到一个 softmax 函数中，这个函数会输出一个覆盖整个词汇表的概率分布，这与传统的语言模型是一样的。在我们的所有实验中，我们会随机遮蔽每个序列中 15% 的所有 WordPiece tokens（词片 tokens）。与 “降噪自编码器”（Denoising Auto-encoders）不同的是，我们只预测被遮蔽的词，而不是重构整个输入。</p><p>尽管这种方法让我们可以得到一个预训练的双向模型，但它也带来了一个问题，那就是在预训练和微调阶段出现了不匹配，因为在微调阶段并没有使用 [MASK] token。为了解决这个问题，我们在“遮蔽”单词时，并不总是用 [MASK] token 来替换。在生成训练数据时，我们会随机选取 15% 的 token 位置进行预测。如果选中了第 i 个 token，那么我们会这样替换它：80% 的情况下用 [MASK] token 替换，10% 的情况下用随机的 token 替换，剩下 10% 的情况下保持原样。然后，我们会用交叉熵损失函数来预测 $T_i$ 的原始 token。</p></li><li><p><strong>Task #2: Next Sentence Prediction (NSP)</strong></p><p>如问题回答（QA）和自然语言推理（NLI）等许多重要的下游任务，都是基于理解两个句子之间的关系，而这种关系并非能直接通过语言模型获取。为了训练出能理解句子关系的模型，我们采用了预训练方法，针对一个“下一句预测任务”，该任务可以简单地从任何单语语料库中生成。具体来说，当我们为每个预训练样本选择句子 A 和句子 B 时，有 50% 的概率 B 是真正紧跟在 A 后面的句子（我们标记为“IsNext”），另外 50% 的概率 B 是从语料库中随机抽取的句子（我们标记为“NotNext”）。如图 1 所示，C 被用于下一句预测（NSP）。尽管它很简单，但我们在第 5.1 节证明，这种预训练方式对于 QA 和 NLI 这两种任务都大有裨益。</p><p>NSP 任务与 ‘Jernite et al. (2017)’ 和 ‘Logeswaran and Lee (2018)’ 的研究中使用的目标表示学习方法密切相关。但是，在以前的研究中，只有句子嵌入被应用到下游任务中，而 BERT 则将所有参数应用于初始化最终任务的模型参数。</p></li><li><p><strong>Pre-training data</strong></p><p>我们的预训练过程大致遵循了现有的语言模型预训练方法。在预训练语料库的选择上，我们使用了 BooksCorpus（8亿词）和英文维基百科（25亿词）。对于维基百科，我们只提取了文本段落，忽略了列表、表格和标题。与使用如’Billion Word Benchmark’这样的打乱的句子级别语料库不同，选择文档级别的语料库以提取长的连贯序列是非常关键的。</p></li></ul><h3 id="Fine-tuning-BERT">Fine-tuning BERT</h3><p>BERT的微调过程相当直观，这得益于Transformer中的自注意力机制，它使得BERT能够处理许多下游任务，无论这些任务是涉及单个文本还是文本对，只需要更改相应的输入和输出即可。对于涉及文本对的应用，常见的做法是先独立地对每段文本进行编码，然后再应用双向交叉注意力，这种做法可以参见 ‘Parikh et al. (2016)’ 和 ‘Seo et al. (2017)’ 的研究。然而，BERT采用了自注意力机制，将这两个阶段融为一体。也就是说，通过自注意力机制对连接后的文本对进行编码，实际上已经包含了两个句子之间的双向交叉注意力。</p><p>对于每项任务，我们只需将特定任务的输入和输出接入到 BERT 中，并进行全面的参数微调。在输入端，预训练时用到的句子 A 和句子 B 在不同任务中有不同的对应关系，例如：（1）在改述任务中，它们对应于一对待改述的句子；（2）在蕴含(entailment)任务中，它们对应于假设和前提两部分；（3）在问答任务中，它们对应于问题和答案段落；（4）在文本分类或序列标记任务中，它们对应于一段文本和一个空标记。在输出端，token 的表示形式会被用于 token 级别的任务，如序列标记或问答，而 [CLS] 的表示形式会被用于分类任务，如蕴含或情感分析。</p><p>相较于预训练，微调的计算资源和时间消耗更少。无论是在单个 Cloud TPU 上，还是使用 GPU，只需最多一小时或几小时，就可以重现本文中所有的结果，前提是使用完全相同的预训练模型。关于任务特定的详细内容，我们已在第4节的对应小节中进行了描述。</p><h3 id="Experiments">Experiments</h3><p>在本节中，我们将为大家展示 BERT 在 11 个自然语言处理任务上的微调成果。</p><h4 id="GLUE">GLUE</h4><p>General Language Understanding Evaluation（GLUE）基准测试是汇集了多种自然语言理解任务的一项测试。</p><p>为了对 GLUE 进行微调，我们将输入序列（无论是单个句子还是句子对）处理为第3节所描述的形式，并使用与第一个输入 token ([CLS]) 相对应的最终隐藏向量 $ C \in \mathbb{R}^{H} $ 作为整体的表达形式。微调过程中新引入的唯一参数是分类层的权重 $ W \in \mathbb{R}^{K * H}$，其中 K 是标签的数量。我们用 C 和 W 来计算常规的分类损失，如 $ log(softmax(CW^{T}))$。</p><p><img src="//s3.mindex.xyz/blog/Theses/7f0168120059ef69a4c7b0a8ea572fe6.png" alt="表1：GLUE 测试结果。"></p><p>如 表1 所示，BERT_BASE 和 BERT_LARGE 在所有任务中均表现优秀，相较于先前的最先进技术，平均准确度分别提高了4.5% 和 7.0%。值得注意的是，除了注意力掩蔽(masking)的差异，BERT_BASE 和 OpenAI GPT 在模型架构上几乎完全相同。在最大且被广泛报告的 GLUE 任务 MNLI 中，BERT 实现了4.6%的绝对准确度提升。在官方 GLUE 排行榜10中，BERT_LARGE 的得分为80.5，而在本文写作时，OpenAI GPT 的得分为72.8。</p><p>BERT_LARGE 在所有任务中都显著优于 BERT_BASE，尤其在训练数据非常少的任务中表现更为出色。我们将在第5.2节更深入地探讨模型大小对性能的影响。</p><h4 id="SQuAD-v1-1">SQuAD v1.1</h4><p>斯坦福问答数据集（SQuAD v1.1）包含了10万对由众包产生的问题和答案。给定一个问题和一个包含答案的维基百科段落，任务就是要在段落中预测出答案文本的位置。</p><p>如图 1 所示，在问答任务中，我们将输入的问题和段落打包成一个序列，其中问题使用 A 嵌入，段落使用 B 嵌入。在模型的微调阶段，我们引入了两个特殊的向量，一个叫做&quot;起始向量&quot; S，另一个叫做&quot;结束向量&quot; E。这两个向量的作用是帮助我们计算出单词i作为答案开始部分的概率。通过计算 $T_i$ 和 $S$ 的点积，然后在段落中所有单词上进行 softmax 运算转化成概率。最后我们得到的概率就表示这个单词作为答案开始部分的可能性。</p><p>公式表示为 $ P_i = \frac{e^{S · T_i}}{\sum_{j}{e^{S · T_j}}}$ 。</p><p>对于答案的结束部分，我们也使用一种类似的计算方法。这个方法会给从位置 i 到位置 j 的一段文字打分， 定义为 $S · T_i + E · T_j$，其中 $j ≥ i$ 的最高得分范围被用作预测的答案。训练目标是正确的开始和结束位置的对数似然之和。我们进行了 3 轮的微调，学习率为 5e-5，批量大小为 32。</p><p>表2 展示了领先的排行榜成绩以及顶级已发布系统的表现。SQuAD 排行榜上的最佳成绩并未提供最新的公开系统描述，并且在训练他们的系统时，可以使用任何公开的数据。因此，我们在自己的系统中通过先在 TriviaQA 上进行模型微调（fine-tuning），再在 SQuAD 上进行微调，以实现适当的数据增强策略。</p><p><img src="//s3.mindex.xyz/blog/Theses/2196b4faa7855fb90e7be4193ca78081.png" alt="表2：SQuAD 1.1 的结果。BERT ensemble是有7套系统组成，使用了不同的预训练阶段的checkpoints和微调的seeds"></p><p>我们表现最好的系统在集成模式下，F1 分数比排行榜顶部的系统高出1.5个点，作为单一系统，也高出1.3个F1分数。事实上，我们的单一 BERT 模型在 F1 分数上甚至超过了顶级的集成系统。即使没有 TriviaQA 的微调数据，我们的 F1 分数也只会下降0.1-0.4个点，但仍然以很大的优势领先于所有现有的系统。</p><h4 id="SQuAD-v2-0">SQuAD v2.0</h4><p>SQuAD 2.0 任务在 SQuAD 1.1 问题定义的基础上进行了扩展，允许在给定的段落中可能不存在短答案，这使得问题更具现实性。</p><p>为了完成这项任务，我们采用了一种简单的策略，将SQuAD v1.1的BERT模型进行了扩展。对于那些没有答案的问题，我们将其答案的开始和结束都定位在 [CLS] Token 上。也就是说，答案的开始和结束的可能位置被扩展，包括了 [CLS] Token 的位置。在进行预测时，我们会比较无答案范围的得分 ($s_{null} = S·C + E·C$) 和最佳非空答案范围的得分 ($s_{\hat{i},j} = max_{j \geq i} S·T_i + E·T_j$)。只有当 $s_{null} &gt; s_{\hat{i},j} + \tau$时，我们才会预测出一个非空的答案。这个阈值 $τ$ 是我们在开发数据集上通过最大化 F1 分数来选择的。对于这个模型，我们没有使用 TriviaQA 的数据进行训练。我们进行了两个周期的微调，使用的学习率为5e-5，批量大小为48。</p><p><img src="//s3.mindex.xyz/blog/Theses/9046b856a7ab91a6243235b6c92dede8.png" alt="表3：SQuAD 2.0 的结果。我们排除了所有包含 BERT 作为一部分的项目。"></p><p>我们在 表3 中展示了与先前的排行榜成绩和已发表的顶级研究相比的结果，但并未包括那些使用 BERT 作为组成部分的系统。我们发现，与之前最好的系统相比，F1 分数提高了 5.1。</p><h3 id="SWAG">SWAG</h3><p>Situations With Adversarial Generations (SWAG) 数据集包含了 113k 个句子配对填充示例，这些示例被用于评估基于实际常识的推理能力。给定一个句子，任务是在四个选项中选择最有可能的接续。</p><p>在对 SWAG 数据集进行微调时，我们构造了四个输入序列，每个序列包含给定的句子（A）和一个可能的延续（B）的连接。唯一引入的任务特定参数是一个向量，其与 [CLS] token 表示 C 的点积代表每个选项的得分，该得分通过 softmax 层进行归一化。我们对模型进行了3轮微调，学习率为2e-5，批量大小为16。结果在 表4 中展示。BERT_LARGE 的表现超过了 ESIM+ELMo (作者的基线) 27.1%，超过了 OpenAI GPT 8.3%。</p><p><img src="//s3.mindex.xyz/blog/Theses/8432ea7f4389fd068a343d25edca5776.png" alt="表4：SWAG 的开发集和测试集的准确率。人类的表现是通过在 SWAG 论文中报告的 100 个样本来衡量的。"></p><h3 id="Ablation-Studies">Ablation Studies</h3><p>在这一部分，我们通过消融实验，探讨了 BERT 的多个关键要素，以便更好地理解它们各自的重要性。</p><h4 id="Effect-of-Pre-training-Tasks">Effect of Pre-training Tasks</h4><p>我们通过评估两个使用与 BERT_BASE 完全相同的预训练数据、微调方案和超参数的预训练目标，展示了 BERT 的深度双向性的重要性:</p><p><strong>No NSP</strong>: 一种双向模型，该模型使用 “masked LM”（MLM）进行训练，但不包括 “next sentence prediction”（NSP）任务。</p><p><strong>LTR &amp; No NSP</strong>: 一个只考虑左侧上下文的模型，它采用的是标准的从左到右（Left-to-Right，LTR）语言模型训练方式，而不是被遮蔽的语言模型（Masked Language Model，MLM）。在微调阶段，我们也坚持了这种“只看左边”的规则，因为如果不这样做，预训练和微调阶段的处理方式就会不一致，这可能导致模型在实际任务中的表现下降。另外，这个模型在预训练阶段并没有执行“下一句预测”（Next Sentence Prediction，NSP）任务。这个模型可以直接和 OpenAI 的 GPT 进行比较，不过我们使用了更大的训练数据集，我们自己的输入数据表示方法，以及我们自己的微调策略。</p><p>我们首先研究了 NSP 任务带来的影响。在 表5 中，我们展示了去掉 NSP 在 QNLI、MNLI 和 SQuAD 1.1 上显著降低了性能。接下来，我们通过比较“No NSP”和“LTR &amp; No NSP”来评估训练双向表示的影响。LTR 模型在所有任务上的表现都比 MLM 模型差，其中在 MRPC 和 SQuAD 上的下降尤为显著。</p><p>对于 SQuAD 任务，我们可以直观地理解，一个从左到右（LTR）的模型在预测单个字符（token）时表现会很差，因为这些字符级别的隐含状态缺乏右侧的上下文信息。为了尽可能地提升这个从左到右的系统，我们在其上增加了一个随机初始化的双向长短期记忆网络（BiLSTM）。这确实在 SQuAD 任务上取得了显著的改善，但是与预训练的双向模型相比，其结果仍然相差甚远。而且，BiLSTM 在 GLUE 任务上的表现反而下降了。</p><p>我们意识到，也可以像 ELMo 那样，分别训练从左到右（LTR）和从右到左（RTL）的模型，然后将每个字符（token）的表示形式作为这两个模型的结合。但是，这种方式存在以下问题：(a) 它的成本是单个双向模型的两倍；(b) 对于像问答（QA）这样的任务，这种方式并不直观，因为从右到左的模型无法根据问题来决定答案；© 它的能力不如深度双向模型，因为深度双向模型可以在每一层都同时使用左右两侧的上下文信息。</p><h4 id="Effect-of-Model-Size">Effect of Model Size</h4><p>在这一部分，我们研究了模型规模对微调任务精度的影响。我们训练了多个BERT模型，这些模型在层数、隐藏单元和注意力头的数量上有所不同，但在超参数和训练流程上，它们与前文描述的保持一致。</p><p>如 表6 所示，我们给出了在选定的GLUE任务上的结果。在这个表格中，我们给出了在进行5次随机微调后，开发集（Dev Set）准确度的平均值。我们可以看到，模型规模更大的BERT模型能在所有四个数据集上都带来准确度的显著提升，即使是在只有3600个标注训练样本，与预训练任务大相径庭的MRPC数据集上也是如此。同时，令人惊讶的是，我们在已经相对于现有研究来说规模较大的模型基础上，还能取得如此显著的提升。例如，'Vaswani et al. (2017)'探索的最大的Transformer模型是(L=6, H=1024, A=16)，编码器的参数有100M，而我们在文献库中找到的最大的Transformer模型是(L=64, H=512, A=2)，参数有235M。相比之下，BERT_BASE模型包含了110M参数，BERT_LARGE模型包含了340M参数。</p><p><img src="//s3.mindex.xyz/blog/Theses/c46e2f0f3b153c2c018d6a32ca759608.png" alt="表6：对BERT模型大小进行消融实验。#L = 层数的数量；#H = 隐藏层大小；#A = 注意力头的数量。“LM (ppl)”是保留训练数据的被遮蔽的语言模型困惑度。"></p><p>大家都知道，如果我们增加模型的规模，将会在大规模任务，如机器翻译和语言建模，上取得持续的改进。这一点可以通过查看 表6 中展示的预留训练数据的语言模型复杂度得到证实。然而，我们认为，这是第一项有力地证明，只要模型得到了充分的预训练，即使在非常小规模的任务上，极大地增加模型规模也能带来显著改进的研究。在 ‘Peters et al. (2018b)’ 的研究中，他们针对预训练的双向语言模型大小从两层增加到四层的影响，展示了混合的结果。而 ‘Melamud et al. (2016)’ 则提到，将隐藏维度从 200 增加到 600 有所帮助，但是进一步增加到 1000 并未带来更多的改进。这两项研究都采用了基于特征的方法。我们的假设是，当模型直接在下游任务上进行微调，并且只使用极少量的随机初始化的附加参数时，即使下游任务数据非常少，任务特定的模型也能从更大，更具表达力的预训练表示中受益。</p><h4 id="Feature-based-Approach-with-BERT">Feature-based Approach with BERT</h4><p>我们迄今为止展示的所有BERT结果都是采用了微调策略，也就是在预训练模型的基础上增加了一个简单的分类层，并在具体任务上对所有参数进行了统一的微调。然而，基于特征的方法，也就是从预训练模型中提取出固定的特征，也有其独特的优点。首先，不是所有的任务都能被Transformer编码器架构轻松地表达出来，因此有时需要添加特定于任务的模型架构。其次，一种有效的计算策略是先预计算一次训练数据的复杂表示，然后在这个表示的基础上使用更经济的模型进行多次实验。</p><p>在这一部分，我们将 BERT 模型应用于 CoNLL-2003 命名实体识别 (NER) 任务，以此来对比两种不同的方法。在为 BERT 模型提供输入的过程中，我们采用了一种能够保留字母大小写的 WordPiece 模型，并且还包括了数据所能提供的最大的文档环境信息。遵循常规的做法，我们将这个任务设定为一个标记任务，但是在模型的输出部分，我们并未使用条件随机场（CRF）层。在对 NER 标签集进行 token 级别的分类时，我们选择了第一个子 token 的表示作为输入。</p><p>为了探究微调方法的影响，我们采用了一种基于特征的方法：从 BERT 的一个或多个层中提取激活值，而不对 BERT 的任何参数进行微调。这些上下文嵌入被作为输入送入一个随机初始化的两层 768 维的 BiLSTM，然后再进入分类层。</p><p><img src="//s3.mindex.xyz/blog/Theses/f31d3b1d7cf7187e0aba53f8323dbf12.png" alt="表7："></p><p>结果见 表7。BERT_LARGE 的表现与最先进的方法相媲美。表现最好的方法是将预训练的 Transformer 模型中最顶层的四个隐藏层的 token 表示进行拼接，这仅比微调整个模型的 F1 分数低 0.3。这证明了 BERT 对于微调和基于特征的方法都是有效的。</p><h2 id="Conclusion">Conclusion</h2><p>近期的研究发现，通过使用语言模型进行迁移学习，且在预训练阶段引入大量无监督数据，可以显著提升语言理解系统的性能。特别地，这种方法甚至使得资源匮乏的任务也能从深度单向模型（Deep Unidirectional Models）中获益。我们的主要贡献在于，我们将这些研究成果推广到了深度双向模型（Deep Bidirectional Models），使得同一个预训练模型能够成功应对各种各样的自然语言处理（NLP）任务。</p><h2 id="Source">Source</h2><p><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>]]></content>
    
    <summary type="html">
    
      Bidirectional Encoder Representations from Transformers.
    
    </summary>
    
    
      <category term="Theses" scheme="https://neo1989.net/categories/Theses/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="NLP" scheme="https://neo1989.net/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>好好说话</title>
    <link href="https://neo1989.net/Notes/NOTE-communication/"/>
    <id>https://neo1989.net/Notes/NOTE-communication/</id>
    <published>2024-02-18T08:24:01.000Z</published>
    <updated>2024-02-18T13:52:30.585Z</updated>
    
    <content type="html"><![CDATA[<h3 id="乔哈里视窗">乔哈里视窗</h3><p>将沟通的信息比作一个窗子，它被分为4个区域：公开区、隐蔽区、盲目区、未知区，人的有效沟通就是这四个区域的有机融合。</p><p>公开区：自己知道，他人知道；往往关系越近、共同信息知晓越多的两个人公开区域会越大。<br>盲点区：自己不知道，他人知道；在很多情况下，我们都会有盲点区，自身的学识、所处的环境等都会造成我们与对方所了解的信息不对等。<br>隐秘区：自己知道，他人不知道；造成隐蔽区的原因在于我们能不好意思跟其他人说、忘了对其他人说、没沟通清楚造成误解、误以为别人知道等。<br>未知区：自己不知道，他人不知道；</p><h3 id="沟通漏斗">沟通漏斗</h3><ul><li>你心里想的，100%</li><li>你说出来的，80%</li><li>别人听到的，60%</li><li>别人听懂的，40%</li><li>别人执行的，20%</li></ul><h3 id="FIRE-接收模型">FIRE 接收模型</h3><p>首先注意到<strong>事实</strong>，并对这些事实<strong>进行解读</strong>，根据解读的结果，经历情绪的<strong>反应</strong>，期望得到想要的<strong>结果</strong></p><p>FIRE模型能帮助我们认清哪些事实，哪些是主观想法。它最大的好处就是，当你听到一个难以入耳的反馈时，它能帮助你平静理性的展开分析，从而得到一个有效的解决方案。</p><p>F-事实：确实存在的事情，具体、公正、客观、不带感情色彩。事实是真实谈话的基础<br>I-解读：事件发生，我们依据经验对事实进行解读，得出这一事实的目的或意义<br>R-反应：根据解读结果，我们会产生相应的情绪反应<br>E-结果：经历情绪反应后，我们就会期望某种结果</p><h3 id="PREP-沟通法则">PREP 沟通法则</h3><p>先说<strong>结论</strong>，再说<strong>原因</strong>，接着用<strong>事例</strong>辅助证明观点。最后重复<strong>结论</strong>，构成一次完整的表述。</p><p>P-结论：结论先行，沟通中的黄金法则。抛出明确的观点，让对方清楚接下来的谈话是围绕哪个结论展开<br>R-依据：通过有效的数据，有力的事实验证结论的可靠性。<br>E-事例：用事例引起倾听者的共情和想象<br>P-重述结论：强化双方对结论的共识</p><h3 id="STAR-叙事模型">STAR 叙事模型</h3><p>S-情境：所发生的事情的背景，在所阐述的事实中所发生的背景情况<br>T-任务：在背景环境下所承担的角色及所执行的任务，以及要达成的目标<br>A-行动：在任务当中如何去操作执行任务的，重点是行动过程<br>R-结果：付诸行动，完成任务所达到的效果，一般是可量化的指标</p><h3 id="SCQRTV-方案模型">SCQRTV 方案模型</h3><p>该模型符合人对事物的体验路径，与大脑思考策略执行的逻辑契合度极高，能够让倾听者更容易接受你的想法。</p><p>体验路径：感官 - 情感 - 思考 - 行为 - 识别<br>思考路径：是什么 - 怎么了 - 为什么 - 怎么办 - 结果如何<br>执行路径：分析 - 判断 - 推理 - 决策 - 评估</p><p>S-情境：客观地描述时间情境，明确问题<br>C-冲突：提出与现实相违背的内容的疑问<br>R-原因：分析事件的原因与动机<br>T-策略：解决问题的方法，进行决策<br>V-价值：产生价值，创造价值</p><h3 id="FFC-赞美法则">FFC 赞美法则</h3><p>赞美对方时，先说出<strong>内心的感受</strong>，然后陈述你的奇特感受和<strong>客观事实</strong>，最后将被赞美人和同类让<strong>进行对比</strong>，让对方认为就是这样</p><p>F-感受：从细节出发，说出内心的感受。<br>F-事实：陈述带给你这种感受的客观事实。<br>C-对比：与相似的人/事进行对比，突出对方的优点。</p><h3 id="RIDE-说服模型">RIDE 说服模型</h3><p>利用人们心理<strong>趋利避害</strong>的天然潜意识，通过暴露风险，阐述利益，继而引入差异性及影响，来说服他人接受你的观点</p><p>R-风险： 不采纳方案会带来的风险<br>I-利益：采纳方案带来的利益，从之前的方案抛出风险，降低对方的预期，继而引入利益点，提高预期，会使对方更好地接受<br>D-差异：事实举例说明自身建议与其他方案的差异之处<br>E-影响：方案本身所能带来的负面影响。太完美的东西反而不真实，小缺点瑕不掩瑜</p><h3 id="GROW-模型">GROW 模型</h3><p><strong>用提问代替说教</strong>，这样可以引导对方<strong>自己找到</strong>问题的解决方法。有效避免对方产生<strong>抵触情绪</strong>。</p><p>G-目标：明确要实现的目标，你想要什么？<br>R-现状：分析当前状况、聚焦目标。现在是什么情况？<br>Q-方案：找出可供选择的方案。有哪些方案？<br>W-意愿：强化意愿。你将要从什么动作开始？</p><h3 id="电梯演讲">电梯演讲</h3><p>麦肯锡认为，一般人们只能记住一二三，记不住四五六，所以凡是要归纳在3条以内。</p><p>Hook 吸引：通过现状、存在的问题、现状产生的原因、解决方案等吸引对方注意力。<br>Mutual Benefit 给利：提出具体解决方案，让对方意识到你的价值存在。<br>Call to Action 收网：指出上述方法的依据及理由，并留下联系方式。</p><h3 id="Tips">Tips</h3><p>明确双方所表达的语义是否一致，掌握好措辞；若不一致需及时了解，打破双方误解。<br>结论先行，也就是PREP核心思想。<br>阐述事情先讲背景，再讲内容。<br>在沟通之前，明确此次沟通的目的，了解所沟通对象的诉求。<br>不论沟通对象是谁，结构化表达，能够让对方更好地理解你说的话。若当面沟通无法阐述清楚，则预先打个腹稿或者以文字形式输出。<br>切莫自己一顿疯狂输出，也记得倾听对方的想法、方案。<br>区分哪些是真正的事实，哪些是对方/自己解读后的观点，基于客观事实和行为模式是寻求真相谈话的基础。<br>沟通方式、沟通模型仅仅只是招式，最重要的还是<strong>沟通的内容</strong></p>]]></content>
    
    <summary type="html">
    
      沟通和执行相差甚远的时候，多半是沟通模式出了问题
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="SoftSkills" scheme="https://neo1989.net/tags/SoftSkills/"/>
    
      <category term="Wisdom" scheme="https://neo1989.net/tags/Wisdom/"/>
    
  </entry>
  
  <entry>
    <title>18 Emotional Equations</title>
    <link href="https://neo1989.net/Notes/NOTE-18-Emotional-Equations/"/>
    <id>https://neo1989.net/Notes/NOTE-18-Emotional-Equations/</id>
    <published>2024-01-30T13:50:25.000Z</published>
    <updated>2024-02-18T13:53:59.519Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Dealing-with-Difficult-Times">Dealing with Difficult Times</h2><h3 id="绝望">绝望</h3><p>绝望 = 苦难 - 意义 （Despair = Suffing - Meaning）</p><p>绝望就是当承受的苦难变得毫无意义时的结果。</p><p>在一个人的低估时期，比如弗兰克（《活出生命的意义》的作者）被投入集中营时，苦难实际上是一个常量，所以为了降低绝望感，最好是把注意力转移到寻求更多的意义上。</p><h3 id="失望">失望</h3><p>失望 = 期望 - 现实（Disappointment = Expectations - Reality）</p><p>失望是你的期望和现实之间的差距。</p><p>应对失望的方法之一是，在最初试图改变某些事情的结果时保持乐观，然后随着时间的推移，一旦力有不逮就调低期望来振作精神，以对抗潜在的负面结果。另一种应对机制是稀释现实的重要性。问问自己，这个结果对你来说究竟有多重要，真的重要吗？</p><h3 id="遗憾">遗憾</h3><p>遗憾 = 失望 + 责任感 （Regret = Disappointment +  Responsibility）</p><p>遗憾是对不幸的个人选择而导致的结果的不满。即没能力改变现实的失望，再被个人选择的责任感放大。遗憾中有期望和向往，这是一种更加成熟的情绪，失望与遗憾的主要区别就在于你的责任感。</p><h3 id="猜忌">猜忌</h3><p>猜忌 = 不信任 / 自尊 （Jealousy = Mistrust / Self-esteem）</p><p>有两个关键因素影响着人们的猜忌心理：你不信任的程度和你的自尊心。</p><p>猜忌是害怕失去原本属于自己的东西，常在爱恨情仇中出现。妒忌则是见到别人拥有你想有的东西时感到的挫折。爱情中的猜忌通常包含三个主体：你、你的爱人和你的情敌。而妒忌一般只涉及你自己和拥有你想要的东西的那个人。猜忌中隐藏着对失去的害怕而妒忌中隐藏着对得到的期待。</p><h3 id="妒忌">妒忌</h3><p>妒忌 = (傲慢 + 虚荣) / 慈悲（Envy = (Pride + Vanity) / Kindness）</p><p>妒忌和自我觉知有很大的关联。妒忌和自恋师承一脉。</p><p>一个自我觉知膨胀的人会很傲慢和虚荣。可以直接反制妒忌的美德是慈悲或者慷慨的精神，当极度妒忌某人时，注入一些慈悲心将会帮助我们淡化不满并让它转化为动力，比如让自己更努力工作去获得类似的成就，或者转变为认同，比如走出仔细，国懿仲沉浸在为他人感到开心的更加广阔的人生。</p><h3 id="焦虑">焦虑</h3><p>焦虑= 不确定性 * 无力感 （Anxiety = Uncertainty * Powerlessness）</p><p>这个方程中存在两个变量：你不知道的（不确定性）和你不能控制的（无力感）。通常二者是相互影响的：你感觉越不确定，你就会感觉越无力。</p><p>因为感觉无力会使人精神衰弱，所以这个方程运用乘法产生一个指数的结果。然而如果你能改变其中一个变量，减小到接近零，你就可以明显降低你的焦虑。</p><p>一般来说，少一点焦虑不只是会让你感觉好一点，还会让你更好地对待生活。对某件事情十分确定却感到无力去改变它时，我们是会感觉很不自在但你更多是会逆来顺受，而不会感到焦虑。同样，感觉不确定却充满力量，意味着你对任何迎面而来的失去都有应对的勇气和信心，那也意味着你的焦虑消除了。</p><h2 id="Getting-the-Most-out-of-Your-Work-life">Getting the Most out of Your Work life</h2><h3 id="使命">使命</h3><p>使命 = 快乐 / 痛苦 （Calling = Pleasure / Pain）</p><p>你越是生活在使命中，就能感受到快乐而忽略掉痛苦。<br>你越是生活在使命中，快乐就越能主宰痛苦。</p><h3 id="工作狂">工作狂</h3><p>工作狂 = 你在逃避什么？/ 你为何而活？（Workaholism = What are you running from? / What are you living for?）</p><p>工作狂是上瘾的一种形式，简单来说，各种成瘾都和我们在逃避什么相关。<br>通常我们会沉醉于哪些能够改善我们心情的事物（包括工作），一定程度上是因为我们迫切需要逃脱那些正在吞噬我们的情绪或恐惧。在你剥下任何成瘾症的表层情绪后，你会发现一些隐藏的共同情绪，比如自卑、不可爱、羞耻以及诸如害怕亲密、失败甚至成功等各种恐惧。</p><p>从“工作——休息——工作”循环中抽离出来，反思你究竟为什么而活，反思在我们的生命中，还有哪些细微的愉快感受值得我们更多的关注与投入，能够帮助我们从失衡的状态拉回来，从一种习惯性行为的舒服中得到释放。</p><h3 id="心流">心流</h3><p>心流 = 技能 / 挑战 （Flow = Skill / Challenge）</p><p>当参加有组织」有方向和有明确目标的活动时，我们所具备的技能能应对所感知到的挑战，达到平衡状态时，就会处于一种心流状态。<br>能让我们体验到心流的地方之一就是在工作中。在工作中达到心流状态意味着你将所有情绪投入到某项能够是你达到巅峰表现的活动中了。<br>进入心流状态与失去自我意识有关。这不是说你得迷失自我，更不是值你昏迷不醒。是意味着，有那么亦可，你意识不到你自己的存在。平时我们是在对自己的意识、脑子里想什么，以及绳梯感觉如何都极度清醒的状态中生活的。但是当我们处于心流状态时，我们暂时切断了自己与这些感受的联系，让我们去拓宽“我们是谁”和“我们能做什么”的观念。</p><h3 id="好奇心">好奇心</h3><p>好奇心 = 惊奇 + 敬畏 （Curiosity = Wonder + Awe）</p><p>好奇先通常是没有预设目标的行为，它是大脑的养分。有大量证据显示，好奇心好似血管内的血液，是一种让我们永保青春、不可或却并给予生命肯定的情绪。<br>好奇心的两个组成部分是：惊奇和敬畏。纯粹的好奇心蕴含着高度的敬意和公平，我们只有满怀惊奇与敬畏，才会乐于去学习、去爱、去犯错和去生活。心理学家托德·卡什丹将好奇心比作“成长的引擎”。</p><h2 id="Defining-Who-You-Are">Defining Who You Are</h2><h3 id="真实性">真实性</h3><p>真实性 = 自我觉知 * 勇气 （Autheniticity = (Self-Awwareness) * Courage ）</p><p>要认识到你自己需要两种基本工具，把你的真实自我从大理石中雕琢出来，它们就是自我觉知的勇气。两者都极为重要。没有勇气的自我觉知意味着你认识了自己，但是其他人并没有认识你。有勇气却缺乏自我觉知的人会表现得装腔作势。因此这是一个乘法运算，而不是简单的加法。</p><h3 id="自恋">自恋</h3><p>$自恋 = 特权 * {(自尊心)}^2 $ （Narcissism = Entitlement * (self-esteem)**2）</p><p>自恋的人自尊心强到极致，一个有正常自尊心的人会关心别人，而自恋者则太过关注自己，以至于与周围的人断了关联，只会关注他人是如何看待或者服务自己的。<br>自尊心极强的人还会产生特权思想，当想着自己很独特时，你会相信自己有权享受特殊待遇。</p><h3 id="正直">正直</h3><p>正直 = 真实 * 无形 * 可靠 （Integrity = Autheniticity * Invisibility * Reliability ）</p><p>有三个情绪变量可以组合成正直，这就是真实、无形和可靠。<br>正直是“完整”的内在感觉，就是那种我们身上的特点合而为一的感觉。正直不需要观众，一个正直的人就算没有人在注意，也会做正当的事。 可靠包括兼容性、忠诚，还有言行一致。无论你面临什么样的情况，正直都能够跟你的价值观进行可靠的结合。如果你看到了一个人的正直，那么他很有可能在践行你所钦佩的生活和做人的方式。</p><h2 id="Finding-Contentment">Finding Contentment</h2><h3 id="幸福">幸福</h3><p>幸福 = 拥有的 / 想要的 （Happiness = Wanting what you have / Having what you want）</p><p>成功和幸福经常被混为一谈，但是成功是策略上的最大化和最优化，而幸福则倾向于对现状的知足和感恩。心理学家在临床研究中表明，幸福感当中最重要的一部分是表达和感受恩典。该调查同时表明，哪些“策略上的最大化者”（受成功激励的人）的幸福感远远不及那些“知足者”。</p><p>幸福的人更关注“舒适的生活”而不是“更好的生活”。</p><h3 id="喜悦">喜悦</h3><p>喜悦 = 爱 - 恐惧 （Joy = Love - Fear）</p><p>寻求爱可能毫无所得，但寻求喜悦可能却是获得爱的一种方式。爱就像一种电流，我们可以选择关闭或者打开，这一切只是基于我们是否愿意生活在黑暗的恐惧之中，只要我们生活在爱中，喜悦的情绪就会缓缓流入。</p><h3 id="活力">活力</h3><p>活力 = 积极的频率 / 消极的频率 （Thriving = Frequency of Positive / Frequency of Negative）</p><p>活力是积极频率与消极频率的比，且要大于等于3.</p><p>巴西的社会学家 Marcial Losada 和心理学的领军人物 芭芭拉·弗雷德里克森 进行了一场关于积极情绪的实验，实验表明，一旦积极情绪与消极情绪的比例达到3:1，人们就会感到充满希望，情绪高昂，这个比例被心理学家称为“洛萨达比例”或“洛萨达线”。</p><h3 id="信仰">信仰</h3><p>信仰 = 信念 / 智力 （Faith = Belif / Intellect）</p><p>我们所有人都会信仰或者信赖某些事物，无神论者也一样，信仰存在于他们的思想中、本质中、宇宙中，以及所有事物中。<br>信仰和信念经常被当作同义词，但其实前者是一种可能没有明显证据的信赖，而后者则混杂有经验证据。越理性智力水平越高，对于某些事物的信仰就越弱，这就是很多无神论者有些可能会认为信仰宗教的人没有智力的原因。<br>信仰并不嫩解开我们所有的疑惑，有时可能只是让我们愉快地忽视了问题的存在。但是信仰能够给我们一定程度的信心，更加自由地感受生活。想萨姆·哈里斯这样的无神论者也承认：“信仰能让我们当中的很多人平静地承担起生命中的苦楚，而这是我们在纯理性的世界中力有不逮的。”<br>多项事实证明，在很多情况下，药用安慰剂能够发挥很大作用，部分好似因为我们相信的到了某种能够治愈我们的药而产生的信仰。</p><h3 id="智慧">智慧</h3><p>智慧 = 经历的开方 （$Wisdom = \sqrt{Experience}$）</p><p>智慧就是简化生活的复杂性，将分散的东西集中到核心。<br>察觉到规律并将其简化为普遍真理，然后创造成简单的认知模式，在无意识情况下运用到待人处世中。智慧之美在于它的简单。<br>大多数研究人员关于智慧的研究都一致地显示出，智慧的一个品质便是『经历』。这也是为什么我们会认为那些已经有几十年生活经历的人会比年轻人更加智慧的原因。</p><h2 id="Reference">Reference</h2><p><a href="https://medium.com/@hashim.alzain/emotional-equations-book-summary-6348cc98ab81">Emotional Equations</a></p>]]></content>
    
    <summary type="html">
    
      保持平静的简单秘诀。
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="SoftSkills" scheme="https://neo1989.net/tags/SoftSkills/"/>
    
      <category term="Health" scheme="https://neo1989.net/tags/Health/"/>
    
      <category term="Wisdom" scheme="https://neo1989.net/tags/Wisdom/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · Building Advanced RAG</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-Building-Advanced-RAG/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-Building-Advanced-RAG/</id>
    <published>2024-01-24T12:01:43.000Z</published>
    <updated>2024-02-29T14:24:02.398Z</updated>
    
    <content type="html"><![CDATA[<p><img src="//s3.mindex.xyz/blog/Courses/fa53038bd6116d6d7897b88d8b3e59b7.png" alt="A comprehensive RAG CheatSheet detailing motivations for RAG as well as techniques and strategies for progressing beyond Basic or Naive RAG builds."></p><h2 id="Basic-RAG">Basic RAG</h2><p>主流的 RAG 主要涉及从外部知识库召回文档，并将这些文档连同用户的查询一起传递给大语言模型，以此生成回应。<br>也就是说，RAG 包含了 <strong>召回部分</strong>、<strong>外部知识库</strong> 以及 <strong>生成部分</strong> 三个组成部分。</p><p>LlamaIndex Basic RAG 示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> SimpleDirectoryReader, VectorStoreIndex</span><br><span class="line"></span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">documents = SimpleDirectoryReader(input_dir=<span class="string">&quot;...&quot;</span>).load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># build VectorStoreIndex that takes care of chunking documents</span></span><br><span class="line"><span class="comment"># and encoding chunks to embeddings for future retrieval</span></span><br><span class="line">index = VectorStoreIndex.from_documents(documents=documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The QueryEngine class is equipped with the generator</span></span><br><span class="line"><span class="comment"># and facilitates the retrieval and generation steps</span></span><br><span class="line">query_engine = index.as_query_engine()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your Default RAG</span></span><br><span class="line">response = query_engine.query(<span class="string">&quot;A user&#x27;s query&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="Success-Requirements-for-RAG">Success Requirements for RAG</h2><p>要想让一个RAG被认定为成功（即能为用户的问题提供有用且相关的答案），实际上有两个核心要求：</p><ul><li><strong>召回</strong> 必须是用户查询最相关的文档。</li><li><strong>生成</strong> 必须能够充分利用召回的文档来有效的回答用户查询。</li></ul><h2 id="Advanced-RAG">Advanced RAG</h2><p>一旦我们明确了成功的标准，就可以说，构建先进的 RAG 主要是要运用更精细的技术和策略（应用于召回或生成组件），以确保最终达到这些标准。<br>进一步说，我们可以把这些精细的技术分为两类：一类是独立解决两大成功要求中的一项（或多或少）的技术，另一类则是同时应对这两大要求的技术。</p><h3 id="召回">召回</h3><p>接下来，我们将简述几种更高级的技术，这些技术能帮助我们实现第一个要求：</p><h4 id="Chunk-Size-Optimization">Chunk-Size Optimization</h4><p>因为大语言模型的上下文长度有限，所以在构建外部知识库时，我们需要将文档切分成多个部分。如果切分的部分过大或过小，都可能给生成组件带来问题，从而导致生成的回答不准确。</p><p>LlamaIndex Chunk Size Optimization <a href="https://github.com/run-llama/llama_index/blob/main/docs/examples/param_optimizer/param_optimizer.ipynb">示例</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> ServiceContext</span><br><span class="line"><span class="keyword">from</span> llama_index.param_tuner.base <span class="keyword">import</span> ParamTuner, RunResult</span><br><span class="line"><span class="keyword">from</span> llama_index.evaluation <span class="keyword">import</span> SemanticSimilarityEvaluator, BatchEvalRunner</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Perform hyperparameter tuning as in traditional ML via grid-search</span></span><br><span class="line"><span class="comment">### 1. Define an objective function that ranks different parameter combos</span></span><br><span class="line"><span class="comment">### 2. Build ParamTuner object</span></span><br><span class="line"><span class="comment">### 3. Execute hyperparameter tuning with ParamTuner.tune()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Define objective function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">objective_function</span>(<span class="params">params_dict</span>):</span><br><span class="line">    chunk_size = params_dict[<span class="string">&quot;chunk_size&quot;</span>]</span><br><span class="line">    docs = params_dict[<span class="string">&quot;docs&quot;</span>]</span><br><span class="line">    top_k = params_dict[<span class="string">&quot;top_k&quot;</span>]</span><br><span class="line">    evals_qs = params_dict[<span class="string">&quot;eval_qs&quot;</span>]</span><br><span class="line">    ref_response_strs = params_dict[<span class="string">&quot;ref_response_strs&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># build RAG pipeline</span></span><br><span class="line">    index = _build_index(chunk_size, docs)  <span class="comment"># hlper function</span></span><br><span class="line">    query_engine = index.as_query_engine(similarity_top_k=top_k)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform inference with RAG pipeline on a privoded questions `eval_qs`</span></span><br><span class="line">    pred_response_objs = get_responses(</span><br><span class="line">        eval_qs, query_engine, show_progress=true</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform evaluations of predictions by comparing them to reference</span></span><br><span class="line">    <span class="comment"># response `ref_response_strs`</span></span><br><span class="line">    evaluator = SemanticSimilarityEvaluator(...)</span><br><span class="line">    eval_batch_runner = BatchEvalRunner(</span><br><span class="line">        &#123;<span class="string">&quot;semantic_similarity&quot;</span>: evaluator&#125;, workers=<span class="number">2</span>, show_progress=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    eval_results = eval_batch_runner.evaluate_responses(</span><br><span class="line">        evals_qs, responses=pred_response_objs, reference=ref_response_strs</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get semantic similarity metric</span></span><br><span class="line">    mean_score = np.array(</span><br><span class="line">        [r.score <span class="keyword">for</span> r <span class="keyword">in</span> eval_results[<span class="string">&quot;semantic_similarity&quot;</span>]]</span><br><span class="line">    ).mean()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> RunResult(score=mean_score, params=params_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Build ParamTuner object</span></span><br><span class="line">param_dict = &#123;<span class="string">&quot;chunk_size&quot;</span>: [<span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>]&#125;</span><br><span class="line">fixed_param_dict = &#123;</span><br><span class="line">    <span class="string">&quot;top_k&quot;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">&quot;docs&quot;</span>: docs,</span><br><span class="line">    <span class="string">&quot;evals_qs&quot;</span>: evals_qs[:<span class="number">10</span>],</span><br><span class="line">    <span class="string">&quot;ref_response_strs&quot;</span>: ref_response_strs[:<span class="number">10</span>],</span><br><span class="line">&#125;</span><br><span class="line">param_tuner = ParamTuner(</span><br><span class="line">    param_fn=objective_function,</span><br><span class="line">    param_dict=param_dict,</span><br><span class="line">    fixed_param_dict=fixed_param_dict,</span><br><span class="line">    show_progress=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Execute hyperparameter search</span></span><br><span class="line">results = param_tuner.tune()</span><br><span class="line">best_result = results.best_run_result</span><br><span class="line">best_chunk_size = results.best_run_result.params[<span class="string">&quot;chunk_size&quot;</span>]</span><br></pre></td></tr></table></figure><h4 id="Structured-External-Knowledge">Structured External Knowledge</h4><p>在复杂的情况下，我们可能需要构建一个比基本向量索引更有结构的外部知识库，这样才能在处理有明显区别的外部知识源时，进行递归召回或者路径召回。</p><p>LlamaIndex Recursive Retrieval <a href="https://docs.llamaindex.ai/en/stable/examples/retrievers/recursive_retriever_nodes.html">示例</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> SimpleDirectoryReader, VectorStoreIndex</span><br><span class="line"><span class="keyword">from</span> llama_index.node_parser <span class="keyword">import</span> SentenceSplitter</span><br><span class="line"><span class="keyword">from</span> llama_index.schema <span class="keyword">import</span> IndexNode</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Build a recursive retriever that retrieves using small chunks</span></span><br><span class="line"><span class="comment">### but passes associated larger chunks to the generation stage</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">documents = SimpleDirectoryReader(</span><br><span class="line">    input_file=<span class="string">&quot;...&quot;</span></span><br><span class="line">).load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># build parent chunks via NodeParser</span></span><br><span class="line">node_parser = SentenceSplitter(chunk_size=<span class="number">1024</span>)</span><br><span class="line">base_nodes = node_parser.get_nodes_from_documents(documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define smaller child chunks</span></span><br><span class="line">sub_chunk_sizes = [<span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line">sub_node_parsers = [</span><br><span class="line">    SentenceSplitter(chunk_size=c, chunk_overlap=<span class="number">20</span>) <span class="keyword">for</span> c <span class="keyword">in</span> sub_chunk_sizes</span><br><span class="line">]</span><br><span class="line">all_nodes = []</span><br><span class="line"><span class="keyword">for</span> base_node <span class="keyword">in</span> base_nodes;</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> sub_node_parsers:</span><br><span class="line">        sub_nodes = n.get_nodes_from_documents([base_node])</span><br><span class="line">        sub_inodes = [</span><br><span class="line">            IndexNode.from_text_node(sn, base_node.node_id) <span class="keyword">for</span> sn <span class="keyword">in</span> sub_nodes</span><br><span class="line">        ]</span><br><span class="line">        all_nodes.extend(sub_inodes)</span><br><span class="line">    <span class="comment"># also add original node to node</span></span><br><span class="line">    original_node = IndexNode.from_text_node(base_node, base_node.node_id)</span><br><span class="line">    all_nodes.append(original_node)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define a VectorStoreIndex with all of the nodes</span></span><br><span class="line">vector_index_chunk = VectorStoreIndex(</span><br><span class="line">    all_nodes, service_context=service_context</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define a VectorStoreIndex with all of the nodes</span></span><br><span class="line">vector_index_chunk = VectorStoreIndex(</span><br><span class="line">    all_nodes, service_context=service_context</span><br><span class="line">)</span><br><span class="line">vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build RecursiveRetriever</span></span><br><span class="line">all_node_dict = &#123;n.node_id: n <span class="keyword">for</span> n <span class="keyword">in</span> all_nodes&#125;</span><br><span class="line">retriever_chunk = RecursiveRetriever(</span><br><span class="line">    <span class="string">&quot;vector&quot;</span>,</span><br><span class="line">    retriever_dict=&#123;<span class="string">&quot;vector&quot;</span>: vector_retriever_chunk&#125;,</span><br><span class="line">    node_dict=all_nodes_dict,</span><br><span class="line">    verbose=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build RetrieverQueryEngine using recursive_retriever</span></span><br><span class="line">query_engine_chunk = RetrieverQueryEngine.from_args(</span><br><span class="line">    retriever_chunk, service_context=service_context</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># perform inference with advanced RAG (i.e. query engine)</span></span><br><span class="line">response = query_engine_chunk.query(</span><br><span class="line">    <span class="string">&quot;Can you tell me about the key concepts for safety finetuning&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="Other-useful-links">Other useful links</h4><p>我们提供了一些指南，展示了在复杂情况下如何应用其他高级技术来确保准确的召回。以下是其中一些选定的链接：</p><ul><li><a href="https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine.html">Building External Knowledge using Knowledge Graphs</a></li><li><a href="https://docs.llamaindex.ai/en/stable/examples/vector_stores/elasticsearch_auto_retriever.html">Performing Mixed Retrieval with Auto Retrievers</a></li><li><a href="https://docs.llamaindex.ai/en/stable/examples/retrievers/simple_fusion.html">Building Fusion Retrievers</a></li><li><a href="https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding.html">Fine-tuning Embedding Models used in Retrieval</a></li><li><a href="https://docs.llamaindex.ai/en/stable/examples/query_transformations/HyDEQueryTransformDemo.html">Transforming Query Embeddings (HyDE)</a></li></ul><h3 id="生成">生成</h3><p>与上一节类似，我们提供了一些高级技术的示例，这些技术的目的是确保召回到的文档能够很好地对齐LLM的生成器。</p><h4 id="Information-Compression">Information Compression</h4><p>大语言模型（LLM）不仅受到上下文长度的限制，而且如果召回到的文档中含有太多的无关信息（也就是噪声），可能会使生成的回答质量下降。</p><p>LlamaIndex Information Compression <a href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/LongLLMLingua.html">示例</a> ：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> SimpleDirectoryReader, VectorStoreIndex</span><br><span class="line"><span class="keyword">from</span> llama_index.query_engine <span class="keyword">import</span> RetrieverQueryEngine</span><br><span class="line"><span class="keyword">from</span> llama_index.postprocessor <span class="keyword">import</span> LongLLMLinguaPostprocessor</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Define a Postprocessor object, here LongLLMLinguaPostprocessor</span></span><br><span class="line"><span class="comment">### Build QueryEngine that uses this Postprocessor on retrieved docs</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Postprocessor</span></span><br><span class="line">node_postprocessor = LongLLMLinguaPostprocessor(</span><br><span class="line">    instruction_str=<span class="string">&quot;Given the context, please answer the final question&quot;</span>,</span><br><span class="line">    target_token=<span class="number">300</span>,</span><br><span class="line">    rank_method=<span class="string">&quot;longllmlingua&quot;</span>,</span><br><span class="line">    additional_compress_kwargs=&#123;</span><br><span class="line">        <span class="string">&quot;condition_compare&quot;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&quot;condition_in_question&quot;</span>: <span class="string">&quot;after&quot;</span>,</span><br><span class="line">        <span class="string">&quot;context_budget&quot;</span>: <span class="string">&quot;+100&quot;</span>,</span><br><span class="line">        <span class="string">&quot;reorder_context&quot;</span>: <span class="string">&quot;sort&quot;</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define VectorStoreIndex</span></span><br><span class="line">documents = SimpleDirectoryReader(input_dir=<span class="string">&quot;...&quot;</span>).load_data()</span><br><span class="line">index = VectorStoreIndex.from_documents(documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define QueryEngine</span></span><br><span class="line">retriever = index.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br><span class="line">retriever_query_engine = RetrieverQueryEngine.from_args(</span><br><span class="line">    retriever, node_postprocessor=[node_postprocessor]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Used your advanced RAG</span></span><br><span class="line">response = retriever_query_engine.query(<span class="string">&quot;A user&#x27;s query&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="Result-Re-Rank">Result Re-Rank</h4><p>大语言模型（LLM）存在被称为“<a href="https://arxiv.org/abs/2307.03172">中间迷失</a>”的现象，即模型主要关注输入提示的两端。因此，在将召回到的文档传递给生成部分之前，对它们进行重新排序是有帮助的。</p><p>LlamaIndex Re-Ranking For Better Generation <a href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/CohereRerank.html">示例</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> SimpleDirectoryReader, VectorStoreIndex</span><br><span class="line"><span class="keyword">from</span> llama_index.postprocessor.cohere_rerank <span class="keyword">import</span> CohereRerank</span><br><span class="line"><span class="keyword">from</span> llama_index.postprocessor <span class="keyword">import</span> LongLLMLinguaPostprocessor</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Define a Postprocessor object, here CohereRerank</span></span><br><span class="line"><span class="comment">### Build QueryEngine that uses this Postprocessor on retrieved docs</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Build CohereRerank post retrieval processor</span></span><br><span class="line">api_key = os.environ[<span class="string">&quot;COHERE_API_KEY&quot;</span>]</span><br><span class="line">cohere_rerank = CohereRerank(api_key=api_key, top_n=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build QueryEngine (RAG) using the post processor</span></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;...&quot;</span>).load_data()</span><br><span class="line">index = VectorStoreIndex.from_documents(documents=documents)</span><br><span class="line">query_engine = index.as_query_engine(</span><br><span class="line">    similarity_top_k=<span class="number">10</span>,</span><br><span class="line">    node_postprocessor=[cohere_rerank],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your advanced RAG</span></span><br><span class="line">response = query_engine.query(</span><br><span class="line">    <span class="string">&quot;What did Sam Altman do in this essay?&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="召回-生成">召回&amp;生成</h3><p>在这一部分，我们研究了一些高级方法，这些方法利用召回和生成的相互配合，旨在提高召回效果，同时生成出更精确的对用户查询的回应。</p><h4 id="Generator-Enhanced-Retrieval">Generator-Enhanced Retrieval</h4><p>这些方法利用大语言模型的内在推理能力，在进行召回之前优化用户的查询，以便更准确地找出生成有用回应所需的信息。</p><p>LlamaIndex Generator-Enhanced Retrieval <a href="https://docs.llamaindex.ai/en/stable/examples/query_engine/flare_query_engine.html">示例</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> llama_index.query_engine <span class="keyword">import</span> FLAREInstructQueryEngine</span><br><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> (</span><br><span class="line">    VectorStoreIndex,</span><br><span class="line">    SimpleDirectoryReader,</span><br><span class="line">    ServiceContext,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Build a FLAREInstructQueryEngine which has the generator LLM play</span></span><br><span class="line"><span class="comment">### a more active role in retrieval by prompting it to elicit retrieval</span></span><br><span class="line"><span class="comment">### instructions on what it needs to answer the user query.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Build FLAREInstructQueryEngine</span></span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;...&quot;</span>).load_data()</span><br><span class="line">index = VectorStoreIndex.from_documents(documents=documents)</span><br><span class="line">index_query_engine = index.as_query_engine(similarity_top_k=<span class="number">2</span>)</span><br><span class="line">service_context = ServiceContext.from_defaults(llm=OpenAI(model=<span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line">flare_query_engine = FLAREInstructQueryEngine(</span><br><span class="line">    query_engine=index_query_engine,</span><br><span class="line">    service_context=service_context,</span><br><span class="line">    max_iterations=<span class="number">7</span>,</span><br><span class="line">    verbose=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your advanced RAG</span></span><br><span class="line">response = flare_query_engine(</span><br><span class="line">    <span class="string">&quot;Can you tell me about the author&#x27;s trajectory in the startup world?&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="Iterative-Retrieval-Generator-RAG">Iterative Retrieval-Generator RAG</h4><p>在一些复杂的场景下，我们可能需要进行多步骤的推理，才能给出对用户查询的有用且相关的回答。</p><p>LlamaIndex Iterative Retrieval-Generator <a href="https://docs.llamaindex.ai/en/stable/examples/evaluation/RetryQuery.html#retry-query-engine">示例</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.query_engine <span class="keyword">import</span> RetryQueryEngine</span><br><span class="line"><span class="keyword">from</span> llama_index.evaluation <span class="keyword">import</span> RelevancyEvaluator</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Build a RetryQueryEngine which performs retrieval-generation cycles</span></span><br><span class="line"><span class="comment">### until it either achieves a passing evaluation or a max number of </span></span><br><span class="line"><span class="comment">### cycles has been reached</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Build RetryQueryEngine</span></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;...&quot;</span>).load_data()</span><br><span class="line">index = VectorStoreIndex.from_documents(documents=documents)</span><br><span class="line">base_query_engine = index.as_query_engine()</span><br><span class="line">query_response_evaluator = RelevancyEvaluator() <span class="comment"># evaluator to critique retrieval-generation cycles</span></span><br><span class="line"></span><br><span class="line">retry_query_engine = RetryQueryEngine(</span><br><span class="line">    base_query_engine, query_response_evaluator</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your advanced rag</span></span><br><span class="line">retry_response = retry_query_engine(<span class="string">&quot;A user&#x27;s query&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="RAG的评估指标">RAG的评估指标</h2><p>对 RAG 系统进行评估，无疑是非常重要的。在<a href="https://arxiv.org/pdf/2312.10997.pdf">《Retrieval-Augmented Generation for Large Language Models: A Survey》</a>中，作者们提出了7个评估指标，这些指标在CheetSheet右上角部分有所体现。</p><p>llama-index 库包含了一些评估抽象，还集成了对 RAGAs 的支持，以帮助开发者从这些评估指标的角度，理解他们的 RAG 系统在多大程度上达到了预期的成功要求。下面，我们列举了一些精选的评估笔记本指南:</p><ul><li><a href="https://docs.llamaindex.ai/en/latest/examples/evaluation/answer_and_context_relevancy.html">Answer Relevancy and Context Relevancy</a></li><li><a href="https://www.notion.so/LlamaIndex-Platform-0754edd9af1c4159bde12649c184c8ef?pvs=21">Faithfulness</a></li><li><a href="https://github.com/run-llama/llama_index/blob/main/docs/examples/evaluation/retrieval/retriever_eval.ipynb">Retrieval Evaluation</a></li><li><a href="https://docs.llamaindex.ai/en/stable/examples/evaluation/batch_eval.html">Batch Evaluations with BatchEvalRunner</a></li></ul><h2 id="Reference">Reference</h2><p>希望在你阅读完这篇博客文章后，能有更多的信心和准备，去运用这些精妙的技术来打造先进的 RAG 系统！</p><p><a href="https://blog.llamaindex.ai/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b">A Cheat Sheet and Some Recipes For Building Advanced RAG</a></p>]]></content>
    
    <summary type="html">
    
      一份全面的 RAG 速查手册，详细阐述了选择 RAG 的理由，以及如何运用技巧和策略，超越基础或初阶的 RAG 构建。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="RAG" scheme="https://neo1989.net/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>正则表达式</title>
    <link href="https://neo1989.net/CheatSheet/CHEATSHEET-Regular-Expressions/"/>
    <id>https://neo1989.net/CheatSheet/CHEATSHEET-Regular-Expressions/</id>
    <published>2024-01-20T08:56:50.000Z</published>
    <updated>2024-01-26T16:29:50.084Z</updated>
    
    <content type="html"><![CDATA[<p>正则表达式用于描述文本模式，因此可以用来检测文本中是否存在特定模式，从更长的字符串中提取出子字符串，或者对文本进行一些调整。正则表达式可以非常简洁，用来描述特定的单词，也可以更复杂一些，用来查找像 URL 中的顶级域名这样的不明确的字符模式。</p><p>本文是基于Python的解析引擎。</p><h2 id="定义">定义</h2><ul><li><p>原字符：原字符是正则表达式中最基础的元素。它直接对应你写下的字符。比如，如果你想要代表一个 “r”，你直接写 r 就可以了。</p></li><li><p>特殊字符：特殊字符用于告诉正则表达式引擎，接下来的字符有特殊的含义。我们通常在特殊字符前面加一个 \ ，它们可以用来表示一行的开头，一行的结尾，或匹配任何单独的字符。</p></li><li><p>字符集：字符集是用来告诉正则表达式引擎，去寻找一组字符中的任意一个。它由 [ 和 ] 表示，你想要寻找的字符就放在这两个括号之间。</p></li><li><p>捕获组：捕获组是由一对圆括号表示的。它们可以让你把多个正则表达式归为一组，然后对这个组应用其他正则表达式的功能，比如量词（后面会提到）。</p></li></ul><h2 id="锚点">锚点</h2><p>锚点用于定位字符的前后位置。</p><table><thead><tr><th style="text-align:center">语法</th><th style="text-align:center">描述</th><th style="text-align:center">示例</th><th style="text-align:center">示例可匹配</th><th style="text-align:center">示例不可匹配</th></tr></thead><tbody><tr><td style="text-align:center">^</td><td style="text-align:center">匹配行首</td><td style="text-align:center">^r</td><td style="text-align:center"><strong>r</strong>abbit <br/> <strong>r</strong>accoon</td><td style="text-align:center">parrot <br/> ferret</td></tr><tr><td style="text-align:center">$</td><td style="text-align:center">匹配行尾</td><td style="text-align:center">t$</td><td style="text-align:center">rabbi<strong>t</strong> <br/> roo<strong>t</strong></td><td style="text-align:center">trap <br/> nice</td></tr><tr><td style="text-align:center">\A</td><td style="text-align:center">匹配行首</td><td style="text-align:center">\Ar</td><td style="text-align:center"><strong>r</strong>abbit <br/> <strong>r</strong>accoon</td><td style="text-align:center">parrot <br/> ferret</td></tr><tr><td style="text-align:center">\Z</td><td style="text-align:center">匹配行尾</td><td style="text-align:center">t\Z</td><td style="text-align:center">rabbi<strong>t</strong> <br/> roo<strong>t</strong></td><td style="text-align:center">trap <br/> nice</td></tr><tr><td style="text-align:center">\b</td><td style="text-align:center">匹配词首或词尾</td><td style="text-align:center">\bfox\b</td><td style="text-align:center">the <strong>fox</strong> ran</td><td style="text-align:center">foxskin</td></tr><tr><td style="text-align:center">\B</td><td style="text-align:center">匹配非空格字符中间的字符</td><td style="text-align:center">\Bee\B</td><td style="text-align:center">b<strong>ee</strong>f</td><td style="text-align:center">tree</td></tr></tbody></table><h2 id="匹配字符类型">匹配字符类型</h2><p>你可以根据字符的类型进行匹配，比如字母、数字等，而不仅仅是特定的字符。</p><table><thead><tr><th style="text-align:center">语法</th><th style="text-align:center">描述</th><th style="text-align:center">示例</th><th style="text-align:center">示例可匹配</th><th style="text-align:center">示例不可匹配</th></tr></thead><tbody><tr><td style="text-align:center">.</td><td style="text-align:center">除了换行之外的所有内容</td><td style="text-align:center">c.e</td><td style="text-align:center"><strong>cle</strong>an <br/> <strong>che</strong>ap</td><td style="text-align:center">acert <br/> cent</td></tr><tr><td style="text-align:center">\d</td><td style="text-align:center">匹配数字</td><td style="text-align:center">\d</td><td style="text-align:center"><strong>6060</strong> ~ <strong>228</strong> <br/> <strong>2</strong>b</td><td style="text-align:center">two <br/> +*+</td></tr><tr><td style="text-align:center">\D</td><td style="text-align:center">匹配非数字</td><td style="text-align:center">\D</td><td style="text-align:center">6060 <strong>~</strong> 228 <br/> 2<strong>b</strong></td><td style="text-align:center">12 <br/> 333</td></tr><tr><td style="text-align:center">\w</td><td style="text-align:center">匹配单词字符</td><td style="text-align:center">\wello\w</td><td style="text-align:center"><strong>hello呀</strong></td><td style="text-align:center">hell呀</td></tr><tr><td style="text-align:center">\W</td><td style="text-align:center">匹配非单词字符</td><td style="text-align:center">hell\W</td><td style="text-align:center"><strong>hell🎸</strong>no</td><td style="text-align:center">hello</td></tr><tr><td style="text-align:center">\s</td><td style="text-align:center">匹配空白</td><td style="text-align:center">hell\s</td><td style="text-align:center">‘<strong>hell\t</strong>no’</td><td style="text-align:center">hello</td></tr><tr><td style="text-align:center">\S</td><td style="text-align:center">匹配非空白</td><td style="text-align:center">hell\S</td><td style="text-align:center"><strong>hello</strong></td><td style="text-align:center">hell no</td></tr><tr><td style="text-align:center">\元字符</td><td style="text-align:center">对元字符进行转义以匹配元字符</td><td style="text-align:center">\.\.\.</td><td style="text-align:center">no <strong>…</strong> no</td><td style="text-align:center">world</td></tr></tbody></table><h2 id="字符集合">字符集合</h2><p>字符集合是一组或一系列的字符。</p><table><thead><tr><th style="text-align:center">语法</th><th style="text-align:center">描述</th><th style="text-align:center">示例</th><th style="text-align:center">示例可匹配</th><th style="text-align:center">示例不可匹配</th></tr></thead><tbody><tr><td style="text-align:center">[xy]</td><td style="text-align:center">匹配指定的任意字符</td><td style="text-align:center">r[ea]</td><td style="text-align:center">g<strong>ra</strong>y <br/> g<strong>re</strong>at</td><td style="text-align:center">grip <br/> groot</td></tr><tr><td style="text-align:center">[x-y]</td><td style="text-align:center">匹配一段连续的字符</td><td style="text-align:center">[a-e]</td><td style="text-align:center"><strong>a</strong>m<strong>be</strong>r <br/> <strong>b</strong>r<strong>a</strong>n<strong>d</strong></td><td style="text-align:center">fox <br/> zoo</td></tr><tr><td style="text-align:center">[^xy]</td><td style="text-align:center">匹配指定之外的字符</td><td style="text-align:center">r[ea]</td><td style="text-align:center">g<strong>ri</strong>p <br/> g<strong>ro</strong>ot</td><td style="text-align:center">gray <br/> great</td></tr><tr><td style="text-align:center">[^-]</td><td style="text-align:center">匹配指定的元字符</td><td style="text-align:center">4[\^\.]\d</td><td style="text-align:center"><strong>4.2</strong> <br/> <strong>4^3</strong></td><td style="text-align:center">44 <br/> 33</td></tr></tbody></table><h2 id="重复">重复</h2><p>你可以找出重复出现的字符，而不仅仅是单个的字符。</p><table><thead><tr><th style="text-align:center">语法</th><th style="text-align:center">描述</th><th style="text-align:center">示例</th><th style="text-align:center">示例可匹配</th><th style="text-align:center">示例不可匹配</th></tr></thead><tbody><tr><td style="text-align:center">x*</td><td style="text-align:center">匹配0个或多个</td><td style="text-align:center">ar*o</td><td style="text-align:center">cac<strong>ao</strong> <br/> c<strong>arro</strong>t</td><td style="text-align:center">arugula <br/> artichoke</td></tr><tr><td style="text-align:center">x+</td><td style="text-align:center">匹配1个或多个</td><td style="text-align:center">re+</td><td style="text-align:center">g<strong>ree</strong>n <br/> t<strong>ree</strong></td><td style="text-align:center">trap <br/> ruined</td></tr><tr><td style="text-align:center">x?</td><td style="text-align:center">匹配0个或1个</td><td style="text-align:center">ro?a</td><td style="text-align:center"><strong>roa</strong>st <br/> <strong>ra</strong>nt</td><td style="text-align:center">root <br/> rear</td></tr><tr><td style="text-align:center">x{m}</td><td style="text-align:center">匹配m次</td><td style="text-align:center">\we{2}\w</td><td style="text-align:center"><strong>deer</strong> <br/> <strong>seer</strong></td><td style="text-align:center">red <br/> enter</td></tr><tr><td style="text-align:center">x{m,}</td><td style="text-align:center">匹配m次或更多</td><td style="text-align:center">2{3,}4</td><td style="text-align:center"><strong>2222224</strong></td><td style="text-align:center">224</td></tr><tr><td style="text-align:center">x{m,n}</td><td style="text-align:center">匹配m到n之间的次数</td><td style="text-align:center">2{2,3}4</td><td style="text-align:center"><strong>224</strong> <br/> <strong>2224</strong></td><td style="text-align:center">24 <br/> 22224</td></tr><tr><td style="text-align:center">x+?</td><td style="text-align:center">尽可能少地匹配，懒惰模式</td><td style="text-align:center">re+?</td><td style="text-align:center">t<strong>re</strong>eeeee</td><td style="text-align:center">trout</td></tr></tbody></table><h2 id="捕获，选择与反向引用">捕获，选择与反向引用</h2><p>如果你想从一段字符串中提取特定的部分，你可以进行捕获操作，甚至可以给你捕获的这些部分命名。</p><table><thead><tr><th style="text-align:center">语法</th><th style="text-align:center">描述</th><th style="text-align:center">示例</th><th style="text-align:center">示例可匹配</th><th style="text-align:center">示例不可匹配</th></tr></thead><tbody><tr><td style="text-align:center">(x)</td><td style="text-align:center">捕获一个模式</td><td style="text-align:center">(iss)+</td><td style="text-align:center">M<strong>ississ</strong>ippi <br/> m<strong>iss</strong>ed</td><td style="text-align:center">mist <br/> persist</td></tr><tr><td style="text-align:center">(?:x)</td><td style="text-align:center">匹配但不捕获</td><td style="text-align:center">(?:ab)(cd)</td><td style="text-align:center">ab<strong>cd</strong></td><td style="text-align:center">accd</td></tr><tr><td style="text-align:center">(?P&lt;name&gt;x)</td><td style="text-align:center">捕获且命名</td><td style="text-align:center">(?P&lt;a&gt;\d)(?P&lt;b&gt;\d)\d*</td><td style="text-align:center"><strong>1325</strong> <br/> a: 1 <br/> b: 3</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">(x</td><td style="text-align:center">y)</td><td style="text-align:center">匹配多种可能的模式</td><td style="text-align:center">(re</td><td style="text-align:center">ba)</td></tr><tr><td style="text-align:center">\n</td><td style="text-align:center">引用之前的捕获，其中 n 是组索引，从 1 开始编号</td><td style="text-align:center">(b)(\w*)\1</td><td style="text-align:center"><strong>blob</strong> <br/> <strong>brib</strong>e</td><td style="text-align:center">bear <br/> bring</td></tr><tr><td style="text-align:center">(?P=name)</td><td style="text-align:center">引用已命名的捕获</td><td style="text-align:center">(?P&lt;a&gt;5)(\d*)(?P=a)</td><td style="text-align:center"><strong>51245</strong> <br/> <strong>55</strong></td><td style="text-align:center">523</td></tr></tbody></table><h2 id="前瞻后顾">前瞻后顾</h2><p>你可以设定一些特定的字符必须在你的匹配项前后出现，但这些字符并不会被包含进匹配结果中。</p><table><thead><tr><th style="text-align:center">语法</th><th style="text-align:center">描述</th><th style="text-align:center">示例</th><th style="text-align:center">示例可匹配</th><th style="text-align:center">示例不可匹配</th></tr></thead><tbody><tr><td style="text-align:center">(?=x)</td><td style="text-align:center">预览接下来的字符，但不把它们纳入匹配结果中</td><td style="text-align:center">an(?=an)</td><td style="text-align:center">b<strong>an</strong>ana</td><td style="text-align:center">band</td></tr><tr><td style="text-align:center">(?!x)</td><td style="text-align:center">预览下一个字符以避免匹配</td><td style="text-align:center">ai(?!n)</td><td style="text-align:center">f<strong>ai</strong>l</td><td style="text-align:center">faint</td></tr><tr><td style="text-align:center">(?&lt;=x)</td><td style="text-align:center">检查前面的字符以进行匹配，但不会把这些字符纳入匹配结果中</td><td style="text-align:center">(?&lt;=tr)a</td><td style="text-align:center">tr<strong>a</strong>il</td><td style="text-align:center">tail</td></tr><tr><td style="text-align:center">(?&lt;!x)</td><td style="text-align:center">查看前面的字符以避免匹配</td><td style="text-align:center">(?!tr)a</td><td style="text-align:center">be<strong>a</strong>r</td><td style="text-align:center">trail</td></tr></tbody></table><h2 id="Reference">Reference</h2><p><a href="https://www.datacamp.com/cheat-sheet/regular-expresso">Regular Expressions</a></p>]]></content>
    
    <summary type="html">
    
      能极大地提升我们对文本数据的处理能力。
    
    </summary>
    
    
      <category term="CheatSheet" scheme="https://neo1989.net/categories/CheatSheet/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="NLP" scheme="https://neo1989.net/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 语义搜索</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-Semantic-Search/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-Semantic-Search/</id>
    <published>2024-01-10T04:21:28.000Z</published>
    <updated>2024-01-13T06:55:59.489Z</updated>
    
    <content type="html"><![CDATA[<p>语义搜索通过理解搜索查询的内容来提高搜索准确性。与传统搜索引擎不同，传统搜索引擎仅根据词法匹配查找文档，而语义搜索还可以找到同义词。</p><h3 id="背景">背景</h3><p>语义搜索的基本思想是将语料库中的所有条目（无论是句子、段落还是文档）都嵌入到一个向量空间中。</p><p>在搜索时，查询会被嵌入到相同的向量空间中，然后从语料库中找到与查询最接近的嵌入。这些嵌入应该与查询具有高度语义重叠。</p><p><img src="//s3.mindex.xyz/blog/Courses/a88a6ad41612242bacf9371252618da4.png" alt=""></p><h3 id="对称与非对称">对称与非对称</h3><p>对称语义搜索是指你的查询和语料库中的条目长度大致相同，并且具有相同数量的内容。一个例子是: 通过搜索类似的问题 “如何在线学习 Python？” ，你想找到一个像“如何在网络上学习 Python？”这样的条目。对于对称任务，你可能可以在语料库中翻找到查询和对应的条目。</p><p>非对称语义搜索是指你通常有一个简短的查询（例如一个问题或一些关键词），你想找到一个更长的段落来回答查询。像 “什么是 Python” 的查询，你想找到段落“Python 是一种解释型、高级且通用的编程语言。Python 的设计理念是……”。对于非对称任务，在语料库中翻找通常没有意义。</p><p>选择适合任务类型的模型非常重要。</p><p>适合对称语义搜索的模型：<a href="https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models">Pre-Trained Sentence Embedding Models</a></p><p>适合非对称语义搜索的模型：<a href="https://www.sbert.net/docs/pretrained-models/msmarco-v3.html">Pre-Trained MS MARCO Models</a></p><h3 id="Python">Python</h3><p>在数据量不大的语料库中（条目数量最多大约100万），我们有能力计算出搜索词与语料库内每一个条目之间的余弦相似度。</p><p>在接下来的示例中，我们创建了一个包括几个样本句子的小型语料库，并为这个语料库以及我们的搜索词分别计算了它们的嵌入向量。</p><p>接着，我们运用 <code>sentence_transformers.util.cos_sim()</code> 函数来测量搜索词与语料库中所有条目之间的余弦相似性。</p><p>面对庞大的语料库，对所有评分逐一排序实在是效率太低。所以，我们采用了 <code>torch.topk</code> 函数来直接提取得分最高的前 k 个条目。</p><p>下面是一个简单的示例；参见 <a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search.py">semantic_search.py</a>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">This is a simple application for sentence embeddings: semantic search</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">We have a corpus with various sentences. Then, for a given query sentence,</span></span><br><span class="line"><span class="string">we want to find the most similar sentence in this corpus.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This script outputs for various queries the top 5 most similar sentences in the corpus.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer, util</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">embedder = SentenceTransformer(<span class="string">&#x27;all-MiniLM-L6-v2&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Corpus with example sentences</span></span><br><span class="line">corpus = [</span><br><span class="line">    <span class="string">&#x27;A man is eating food.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;A man is eating a piece of bread.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;The girl is carrying a baby.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;A man is riding a horse.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;A woman is playing violin.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Two men pushed carts through the woods.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;A man is riding a white horse on an enclosed ground.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;A monkey is playing drums.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;A cheetah is running behind its prey.&#x27;</span></span><br><span class="line">]</span><br><span class="line">corpus_embeddings = embedder.encode(corpus, convert_to_tensor=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Query sentences:</span></span><br><span class="line">queries = [<span class="string">&#x27;A man is eating pasta.&#x27;</span>, <span class="string">&#x27;Someone in a gorilla costume is playing a set of drums.&#x27;</span>, <span class="string">&#x27;A cheetah chases prey on across a field.&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity</span></span><br><span class="line">top_k = <span class="built_in">min</span>(<span class="number">5</span>, <span class="built_in">len</span>(corpus))</span><br><span class="line"><span class="keyword">for</span> query <span class="keyword">in</span> queries:</span><br><span class="line">    query_embedding = embedder.encode(query, convert_to_tensor=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We use cosine-similarity and torch.topk to find the highest 5 scores</span></span><br><span class="line">    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[<span class="number">0</span>]</span><br><span class="line">    top_results = torch.topk(cos_scores, k=top_k)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n\n======================\n\n&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Query:&quot;</span>, query)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nTop 5 most similar sentences in corpus:&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> score, idx <span class="keyword">in</span> <span class="built_in">zip</span>(top_results[<span class="number">0</span>], top_results[<span class="number">1</span>]):</span><br><span class="line">        <span class="built_in">print</span>(corpus[idx], <span class="string">&quot;(Score: &#123;:.4f&#125;)&quot;</span>.<span class="built_in">format</span>(score))</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk</span></span><br><span class="line"><span class="string">    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)</span></span><br><span class="line"><span class="string">    hits = hits[0]      #Get the hits for the first query</span></span><br><span class="line"><span class="string">    for hit in hits:</span></span><br><span class="line"><span class="string">        print(corpus[hit[&#x27;corpus_id&#x27;]], &quot;(Score: &#123;:.4f&#125;)&quot;.format(hit[&#x27;score&#x27;]))</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="速度优化">速度优化</h3><p>要想让 <code>sentence_transformers.util.cos_sim()</code> 方法运行得更快，最好的做法是将 <code>query_embeddings</code> 和 <code>corpus_embeddings</code> 存在同一块 GPU 设备上。这样做可以明显提升处理性能。</p><p>另外，我们还可以对语料库嵌入进行标准化处理，使每个语料库嵌入的长度都为 1。这样，我们就可以通过点积运算来计算得分了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus_embeddings = corpus_embeddings.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">corpus_embeddings = util.normalize_embeddings(corpus_embeddings)</span><br><span class="line"></span><br><span class="line">query_embeddings = query_embeddings.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">query_embeddings = util.normalize_embeddings(query_embeddings)</span><br><span class="line">hits = util.semantic_search(query_embeddings, corpus_embeddings, score_function=util.dot_score)</span><br></pre></td></tr></table></figure><h3 id="ElasticSearch">ElasticSearch</h3><p>从 7.3 版本开始，<a href="https://www.elastic.co/elasticsearch/">ElasticSearch</a> 推出了一个新功能，即能够索引密集向量 (dense vectors)，并将其用于对文档进行评分。所以，我们可以利用 ElasticSearch 对文档以及嵌入向量（embeddings）进行索引，以此在搜索时使用对应的嵌入向量寻找相关的文档信息。</p><p>ElasticSearch的一个优点是，它便于向索引中添加新的文档，而且能够和我们的向量一起存储其他数据。但缺点是它的性能较慢，这是因为它需要将搜索的嵌入内容和每一个已经存储的嵌入内容进行比较。这种操作的时间成本是线性的，对于大规模（超过 100k）的数据集来说，可能会慢得无法接受。</p><p>更多详细信息，请参见 <a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_elasticsearch.py">semantic_search_quora_elasticsearch.py</a>。</p><h3 id="近似最近邻点">近似最近邻点</h3><p>如果使用精确的最近邻搜索方法（如 <code>sentence_transformers.util.semantic_search</code> 所采用的方式），在一个巨大的语料库中进行查找，特别是这个语料库中包含数百万个嵌入，可能会花费大量的时间。</p><p>在这种情况下，近似最近邻（Approximate Nearest Neighor，ANN）可能会很有帮助。这里，数据被划分为相似的嵌入小部分。利用索引可以有效地进行搜索，甚至在有数百万的向量时也能在毫秒内检索到最高相似性的嵌入（即最近的邻居）。</p><p>不过，结果未必都是精确的。可能有些具有高度相似性的向量被遗漏了。这就是我们称它为“近似最近邻居”的原因。</p><p>所有的人工神经网络（ANN）方法都通常需要调整一到多个参数，以达到召回率与搜索速度的权衡。如果你追求极高的搜索速度，可能会错过一些重要的搜索结果。反之，如果你期望得到高召回率，搜索的速度就可能会变慢。</p><p>近似最近邻搜索库中，<a href="https://github.com/spotify/annoy" title="Annoy">Annoy</a>、<a href="https://github.com/facebookresearch/faiss" title="FAISS">FAISS</a> 和 <a href="https://github.com/nmslib/hnswlib/" title="hnswlib">hnswlib</a> 都很热门。但是，我个人更偏向 <code>hnswlib</code>，因为它不仅使用起来十分简单，性能卓越，而且包含了许多在实际应用中至关重要的特色功能。</p><p>示例：</p><ul><li><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_hnswlib.py" title="semantic_search_quora_hnswlib.py">semantic_search_quora_hnswlib.py</a></li><li><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_annoy.py" title="semantic_search_quora_annoy.py">semantic_search_quora_annoy.py</a></li><li><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_faiss.py" title="semantic_search_quora_faiss.py">semantic_search_quora_faiss.py</a></li></ul><h3 id="召回和重排">召回和重排</h3><p>对于复杂的语义搜索场景，建议使用「召回和重排」流程：</p><p><img src="//s3.mindex.xyz/blog/Courses/3d66117e5374e1f95d858d7d422fc22e.png" alt=""></p><p>当我们有一个搜索请求时，我们会首先使用一个检索系统，这个系统能够找出大约 100 个可能的结果，这些结果可能与我们的搜索请求相关。在进行检索时，我们可以选择使用词汇搜索，比如说使用 ElasticSearch 这样的工具，或者我们也可以选择使用双向编码器进行深度检索。</p><p>但是，这个检索系统可能会找到一些与搜索请求并不太相关的文档。因此，在第二步，我们会使用一个基于交叉编码器的重新排序系统，这个系统会评估所有候选结果与搜索请求的相关性。</p><p>最终，我们将得到一个排名的结果列表，这个列表可以直接呈现给用户。</p><h4 id="召回-Bi-Encoder">召回: Bi-Encoder</h4><p>在寻找候选结果集的过程中，我们可以选择使用词汇搜索（比如 ElasticSearch），或者我们也可以选择使用在这个代码库中实现的双向编码器。</p><p>词汇搜索是在你的文档库中寻找与查询词完全匹配的内容，它无法识别同义词、缩写或拼写的不同形式。而语义搜索（也被称为密集检索）则是将搜索的关键词转化为向量空间的形式，然后找出在这个向量空间中与其最接近的文档。</p><p>语义搜索弥补了词汇搜索的不足，能够识别同义词和缩写词。</p><h4 id="重排：Cross-Encoder">重排：Cross-Encoder</h4><p>检索器需要能够高效处理包含数百万条目的大型文档库。但是，有时候它可能会找出一些与查询无关的结果。</p><p>利用 Cross-Encoder 的重新排序技术，我们可以大幅提升搜索结果的质量。在这个过程中，我们会把搜索请求和可能的文档同时输入到 Transformer 网络中，然后网络会输出一个介于 0 到 1 之间的分数，这个分数代表了文档与搜索请求的匹配程度。</p><p><img src="s3.mindex.xyz/blog/Courses/73b1bebb8fcf7b548f1828585c696898.png" alt=""></p><p>Cross-Encoders的优势在于它们能提供更出色的性能，这是因为它们能在处理查询和文档时运用注意力机制。</p><p>如果我们要对大量的（查询，文档）对进行评分，那将会非常耗时。所以，我们采用的策略是使用检索器先生成一组可能的候选者，比如 100 个，然后再通过 Cross-Encoder 对这些候选者进行重新排序。</p><h3 id="完整示例">完整示例</h3><h4 id="相似问题检索">相似问题检索</h4><p><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_pytorch.py">semantic_search_quora_pytorch.py</a> [<a href="https://colab.research.google.com/drive/11GunvCqJuebfeTlgbJWkIMT0xJH6PWF1?usp=sharing">Colab Version</a>] 是一个基于Quora重复问题数据集的应用案例。通过它，用户可以输入任何问题，然后代码会运用 <code>sentence_transformers.util.semantic_search</code> 方法从数据集中找出与输入问题最相近的问题。模型是 distilbert-multilingual-nli-stsb-quora-ranking，它的主要任务是去识别类似的问题，并且它支持超过50种语言。所以，无论用户用这50多种语言中的何种来提问，都可以得到有效的答案。这是一个对称的搜索任务，因为搜索查询的长度和内容与语料库中的问题相同。</p><h4 id="相似出版物检索">相似出版物检索</h4><p><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_publications.py">semantic_search_publications.py</a> [<a href="https://colab.research.google.com/drive/12hfBveGHRsxhPIUMmJYrll2lFU4fOX06?usp=sharing">Colab Version</a>] 这个示例演示了如何找到与某篇科学论文相似的其他论文。我们的语料库由在 EMNLP 2016 - 2018 会议上发表的所有论文组成。在搜索过程中，我们会输入最近发表的论文的标题和摘要，然后在我们的语料库中寻找相关的论文。我们使用的是 <a href="https://arxiv.org/abs/2004.07180">SPECTER</a> 模型。这个搜索任务是对称的，因为我们的语料库中的论文和我们搜索的内容都是由标题和摘要组成。</p><h4 id="问答检索">问答检索</h4><p><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_wikipedia_qa.py">semantic_search_wikipedia_qa.py</a> [<a href="https://colab.research.google.com/drive/11GunvCqJuebfeTlgbJWkIMT0xJH6PWF1?usp=sharing">Colab Version</a>]：这个例子展示了一个在 <a href="https://ai.google.com/research/NaturalQuestions/">Natural Questions dataset</a> 数据集上进行训练的模型。这个数据集包含了大约十万条真实的 Google 搜索请求，以及从维基百科获取并附带注解的段落，这些段落提供了问题的答案。这是一个非对称搜索任务的典型例子。在这个例子中，我们使用了体积较小的 <a href="https://simple.wikipedia.org/wiki/Main_Page">Simple English Wikipedia</a> 作为语料库，这样它就可以轻松地加载到内存中。</p><p><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.py">retrieve_rerank_simple_wikipedia.py</a> [<a href="https://colab.research.google.com/github/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb">Colab Version </a>]：这个脚本采用了 <strong>召回和重排</strong> 的策略，是一个非对称搜索任务的典型例子。我们把所有维基百科的文章切分成各个段落，并用双向编码器进行编码处理。当有新的查询或问题输入时，我们也用同样的双向编码器进行编码，然后找出与之余弦相似度最高的段落。然后，我们用一个交叉编码器对找到的候选段落进行重新排序，最终将得分最高的5个段落展示给用户。我们使用的模型是在 <a href="https://github.com/microsoft/MSMARCO-Passage-Ranking/">MS Marco Passage Reranking datase</a> 数据集上进行训练的，这个数据集包含了大约 500k 来自 Bing 搜索的真实查询。</p><h3 id="Reference">Reference</h3><p><a href="https://www.sbert.net/examples/applications/semantic-search/README.html">Semantic Search</a></p>]]></content>
    
    <summary type="html">
    
      语义搜索通过理解搜索查询的内容来提高搜索准确性。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>《我就是你啊》</title>
    <link href="https://neo1989.net/Notes/NOTE-Sept-graines-de-lumiere-dans-le-coeur-des-guerriers/"/>
    <id>https://neo1989.net/Notes/NOTE-Sept-graines-de-lumiere-dans-le-coeur-des-guerriers/</id>
    <published>2024-01-06T12:20:47.000Z</published>
    <updated>2024-02-18T13:50:18.460Z</updated>
    
    <content type="html"><![CDATA[<p>必要前提：记住“武力对抗”会适得其反。在整个余人交流的过程中，都要避免陷入<strong>解释</strong>、<strong>威胁</strong>和<strong>人身攻击</strong>的怪圈当中。</p><h3 id="第一步：平复自己的情绪">第一步：平复自己的情绪</h3><p>当我们感觉内心出现了想要攻击对方的冲动时，我们需要尽力控制住它。比如，我们可以通过以下几种方法做到这一点：</p><ul><li>纠正对事实的误判；</li><li>通过深呼吸来分散注意力；</li><li>或只需收住自己想要伸出的拳头；</li><li>或其他任何一种可能奏效的方法。</li></ul><p>在与人交流的过程中，每当你感到内心再次燃起了这种冲动，就需要在脑海里回顾这一步。完成这一步只需要几秒钟的时间，这大概是最难完成，也是最重要的一步。</p><h3 id="第二步：平复他人的情绪">第二步：平复他人的情绪</h3><p>如果与你对话的人能够保持冷静，那就好说了，你就可以直接跳过这一步。但是如果对方不冷静呢，你又该怎么做？答案是 “什么也不要做”，或者说 “几乎什么也不要做”。尤其不要和对方说“你冷静点儿！”或者“你生气是没有用的！”。此时你应该遵循“<strong>不唱反调，不做评判</strong>”的原则。具体如何做到这一点呢？你可以用 “同意” “好的” “是的” “没错” 等来回复对方。这些字眼可以向对方传达一种信息：“你这样说以及你选择以这样的方式说，自然有你的道理。我愿意与你探讨这个问题。”这样做你会收获到惊人的效果：对方的情绪起初会有些波动，但随后便会逐渐稳定，直到最后慢慢平复。实现这一步，只需要你说出 “<strong>我同意</strong>” 的一刹那就够了。 如果你成功控制住了自己的情绪，又成功平复了他人的情绪，那就可以进入下一步了。</p><h3 id="第三步：试着理解他人而非让他人被理解">第三步：试着理解他人而非让他人被理解</h3><p>如何做到这一步呢？最简单的方法就是 “<strong>向对方提问</strong>”。最有效的问题就是：“你为什么步同意我的观点呢？”之后便要倾听他的回答，并试着站在他们的角度看问题，甚至是设身处地地为他人的利益着想。努力去理解和接受别人的观点，而非将自己的观点强加于人。</p><p>你要学会从他们在做解释时所说的话语中寻找双方的“共识”，并欣然接受对方的观点。这是双方达成“共识”的先决条件。这时候你多一些对别人的“私心”：让自己多为对方的利益考虑！。</p><p>一旦你理解了他人的想法，解决方法便会自己现身，分歧也就迎刃而解了。但通常来说，做到这一点还不够，你需要继续完成下一步…</p><h3 id="第四步：通过复述别人的话来让对方明白“你以及理解了对方的观点”">第四步：通过复述别人的话来让对方明白“你以及理解了对方的观点”</h3><p>如果你想让别人倾听你的观点，你需要先让对方发言。然后再用自己的话将你所理解的对方的观点讲一遍。之后问对方 “我说得对吗？”。这样你会收获意想不到的神奇效果——对方会眉头上扬，露出满意的笑容，大赞一声“对啊！”。而后他便会闭上嘴，听你说话。复述有两个好处：第一，你可以检验一下自己是否真的理解了对方的观点；第二，让你的对话者知道自己被人理解了，进而打消继续争论的念头。</p><p>但实现这一步有一个前提条件——不要因为用错了一个词，让你之前的努力都付之东流。这个词就是接下来这一步的关键词。这个词的使用也是一门艺术。</p><h3 id="第五步：使用表并列的词汇提出自己的观点，而非将双方观点对立">第五步：使用表并列的词汇提出自己的观点，而非将双方观点对立</h3><p>如何做到这一点呢？你可以运用以下这些字眼，比如：就我而言、对我来说、与此同时、从我的角度出发…而不要用极其生硬的 “没错，但是…”。然后等到双方都明确了对方的观点之后，问问你自己 “如何才能让对方的诉求得到满足，同时又能达到自己的目的呢？”。如此一来，你就可以发挥两个人的聪明材质，来寻求解决方案。接下来，我们开始进入第六步。</p><h3 id="第六步：提出解决方案">第六步：提出解决方案</h3><p>采用可能的双赢方案。如果我们自己找不到可行的方案，试着问问别人有没有好主意吧。让拿回我们可以一起针对其进行讨论。</p><p>如果找不到任何双赢的方法，我们可以采用妥协折中的方式解决问题。人们往往会接受折中的方案，因为这样至少恩纳锅够建立友好写作的关系。</p><p>那么，如果连折中的方案也不存在呢？这种情况非常罕见，但并非绝不可能发生。此时，可以与对方协商，再花点时间一起探讨可能的出路。尽管最终不一定能找到解决方案，但至少维持了良好的关系。而这种良性关系对于未来协作的达成至关重要。</p><p><img src="https://s3.mindex.xyz/blog/Notes/bf65eb5d1adba66e1f624e828eaf5b7f.png" alt=""></p><h3 id="Reference">Reference</h3><p>[1] <a href="https://book.douban.com/subject/35445960/">我就是你啊 : 走进他人内心的7项修炼</a></p>]]></content>
    
    <summary type="html">
    
      化解一场纷争需要经历哪些重要的步骤？
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="SoftSkills" scheme="https://neo1989.net/tags/SoftSkills/"/>
    
      <category term="Wisdom" scheme="https://neo1989.net/tags/Wisdom/"/>
    
  </entry>
  
  <entry>
    <title>日落收集 (二)</title>
    <link href="https://neo1989.net/SeizeTheDay/COLLECTION-sunsets-2/"/>
    <id>https://neo1989.net/SeizeTheDay/COLLECTION-sunsets-2/</id>
    <published>2023-12-31T14:34:15.000Z</published>
    <updated>2024-02-18T05:53:03.288Z</updated>
    
    <content type="html"><![CDATA[<h3 id="May-20-2023">May 20, 2023</h3><p><img src="//s3.mindex.xyz/blog/Notes/a8dcfd31636d678b6bc786345a7342a9.png" alt="西湖·太子湾公园 | 浙江"></p><h3 id="May-13-2023">May 13, 2023</h3><p><img src="//s3.mindex.xyz/blog/Notes/02be63a08b4d3f50fe9f022c13f21291.png" alt="清水路·环湖大道 | 苏州"></p><h3 id="Apr-15-2023">Apr 15, 2023</h3><p><img src="//s3.mindex.xyz/blog/Notes/97765437415756031a4b064691e3fc5e.png" alt="清水路·环湖大道 | 苏州"></p><h3 id="Mar-11-2023">Mar 11, 2023</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/d1c296dc199be5df7087034673e4267a.png" alt="On the Rock @ NamPhrae | Thailand"></p><h3 id="Mar-10-2023">Mar 10, 2023</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/3454d0d46192236de9293e1261e6a65c.png" alt="Route 107 | Thailand"></p><h3 id="Mar-4-2023">Mar 4, 2023</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/7d8d79718363def5e1f35fc64c131589.png" alt="米堆山 | 苏州"></p><h3 id="Jan-30-2023">Jan 30, 2023</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/cb9d279f773c498fb81893d84e3801a1.png" alt="滨江中路 | 上海"></p>]]></content>
    
    <summary type="html">
    
      日落尤其温柔，人间皆是浪漫。
    
    </summary>
    
    
      <category term="SeizeTheDay" scheme="https://neo1989.net/categories/SeizeTheDay/"/>
    
    
  </entry>
  
  <entry>
    <title>高效做事的底层逻辑</title>
    <link href="https://neo1989.net/Notes/NOTE-work-efficiently/"/>
    <id>https://neo1989.net/Notes/NOTE-work-efficiently/</id>
    <published>2023-12-15T09:26:25.000Z</published>
    <updated>2023-12-17T06:03:20.899Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第一步：WOOP模型，让你对目标动力十足">第一步：WOOP模型，让你对目标动力十足</h2><p><strong>W</strong>ish: 明确愿望和目标是什么？<br><strong>O</strong>utcome: 实现愿望后有什么收获？<br><strong>O</strong>bstacle: 追求目标过程中有哪些障碍？<br><strong>P</strong>lan: 制定什么样的计划来克服那些障碍？</p><h2 id="第二步：SMART法则，设定更科学的目标">第二步：SMART法则，设定更科学的目标</h2><p><strong>S</strong>pecific: 目标设定必须明确具体，很多目标之所以不能实现，就是因为设定之初模棱两可。<br><strong>M</strong>easureable: 所谓可衡量，就是要有一组明确的数据。用数据作为衡量目标是否实现的标准。<br><strong>A</strong>ttainable: 设定的目标要在自己的能力范围之内。<br><strong>R</strong>elevant: 所设立ide目标要与其他目标相关联。<br><strong>T</strong>ime-bound: 目标是要有时间限制的。设定一个目标完成的期限，促进目标的达成。</p><h2 id="第三步：用GRAI定期复盘">第三步：用GRAI定期复盘</h2><p><strong>G</strong>oal: 当初立了哪些flag。期望的结果是什么？<br><strong>R</strong>esult: 对照目标，完成的怎么样了？<br><strong>A</strong>nalysis: 成功或失败的关键原因是什么？<br><strong>I</strong>nsight: 得失的体会是什么？是否有规律性的东西值得思考并指导下一次行动计划？</p><h2 id="第四步：用PDCA不断优化">第四步：用PDCA不断优化</h2><p><strong>P</strong>lan:<br>- 分析现状，找出问题<br>- 分析产生问题的影响因素<br>- 找出主要因素<br>- 设定目标，指定计划</p><p><strong>D</strong>o: 按照预订计划，努力实现预期目标</p><p><strong>C</strong>heck: 评估结果，检查效果，分析原因</p><p><strong>A</strong>ction:<br>- 将成功经验进行标准化及流程制定，作为下次行动的参考<br>- 总结经验和问题，为开展新一轮PDCA提供依据</p>]]></content>
    
    <summary type="html">
    
      4个步骤让你效率暴涨。
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="SoftSkills" scheme="https://neo1989.net/tags/SoftSkills/"/>
    
  </entry>
  
  <entry>
    <title>查理·芒格 语录</title>
    <link href="https://neo1989.net/Notes/NOTE-Charles-Thomas-Munger/"/>
    <id>https://neo1989.net/Notes/NOTE-Charles-Thomas-Munger/</id>
    <published>2023-11-29T07:36:36.000Z</published>
    <updated>2023-12-17T09:21:12.799Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于认知">关于认知</h2><ul><li>变化总在发生，你不去迎接进步的变化，就会等到退步的变化。</li><li>我们能取得今时今日的成就，不是因为我们的能力比别人高出多少，而是我们比别人更清楚自己能力的大小。</li><li>决定结果的主要有两个因素：一个是形势，一个是人。形势太强，任凭你有多大能力，都无济于事。</li><li>一个人，手里拿着锤子，看什么都像钉子。</li><li>只要做好准备，在人生中抓住几个机会。迅速地采取适当的行动，去做简单而合乎逻辑的事情，这辈子的财富就会得到极大的增长。</li><li>我们很清楚自己的不足，很清楚有很多试我们做不到，所以我们谨小慎微地留在我们的“能力圈”中。“能力圈”是沃伦提出的概念。沃伦和我都认为，我们的“能力圈”是一个非常小的圆圈。</li><li>在显示世界中，学会看透本质，我们能活得更从容。探究本质并非朝夕之功，必须有板凳要坐十年冷的精神。</li><li>如果你想要说服别人，要诉诸利益，而非诉诸理性。</li><li>卓越的人很少，有机会追随他们，和他们走到一起，或许值得付出溢价，将来可能获得丰厚的回报。</li><li>保持理性是一种道德律令。在不该犯傻的时候千万别犯傻。</li><li>和被人比是比不过来的，无论做什么，都是一山更比一山高，强中自有强中手。</li><li>所有人都看好的机会，最容易发生踩踏，造成的损失最惨烈。</li><li>每一天都追求比醒来的那一刻多增长一分智慧；每一天都追求有能力承担更大的责任；每一天都追求尽善尽美地完成所有工作。日复一日，年复一年，你终将出人头地。</li><li>按照我的经验，解决问题的最佳方法是不让问题出现。</li><li>所有人的潜意识里都有这样的偏见：给别人提建议时，以为是在为别人考虑，其实是从自己的利益出发。</li><li>我们能成功，不是因为我们善于解决难题，而是因为我们善于远离难题。我们只是找简单的事做而已。</li><li>反过来想，总是反过来想。</li><li>如何才能成功？严格自律、遵守道德，找到志同道合的人，抓住难得的大机会，说出来都是些很简单的道理。</li><li>我不会因为人性而感到意外，也不会花太多时间感受背叛，我总是低下头调整自己去适应这类事情，我不喜欢任何成为受害者的感受。我不是受害者，我是幸存者。</li><li>人类都试图变得精明，而我只想证明自己并不是在做傻事，但这比许多人想想的要困难的多。</li></ul><h2 id="关于成长">关于成长</h2><ul><li>想要得到一件东西，最稳妥的方法就是让自己配得上它。</li><li>找出你最擅长的事情，然后持之以恒，乐此不疲地去把它做好。</li><li>承认无知是智慧的开始。</li><li>一个不能从别人的经验汇总学习的人，一辈子注定一只摔跟头。</li><li>把问题彻底想明白，问题就解决了一半。</li><li>我不会质疑过去，而是从过去中学习，为未来做决定。</li><li>如果你真想成功，真想取得别人无法取得的成就，就甘坐冷板凳，日复一日地阅读。如果你想拥有良好的认知能力，如果你想比别人更具智慧，能在艰难时刻有更好的表现，除了拿出大量时间思考，别无他法。</li><li>我这辈子遇到的聪明人没有不每天阅读的——一个都没有。</li><li>独学而无友，则孤陋寡闻。所有人都需要找到志同道合的人，相互切磋、共同进步。</li><li>我们成功源自我的长期专注。</li><li>在过去的任何一年，如果你一次都没有推翻过自己最中意的想法，那么这一年就算浪费了。</li><li>我喜欢能够坦然承认自己很愚蠢的人。我知道，如果正面承认自己的错误，我会表现得更好。这是一个非常棒的学习窍门。</li><li>我见识过很多取得很大成就的人。虽然他们既不是最聪明的人，甚至也不是最勤奋的人，但他们都是很善于学习的人。</li><li>极度专业化才是成功之道。比起理解整个世界来说，大多数人更加擅长专攻一个方面。</li><li>我做过很多傻事，我一直在和自己的成见做斗争。消除错误的想法是一件好事，我把消除错误的想法作为自己的一种追求。</li><li>进步不总是肉眼可见，而是往往出现在不经意间，但进步总是源于长期坚持，源于每一天的努力。</li><li>如果不终身学习，你们将不会取得很高的成就。光靠已有的知识，你们在生活中走不了多远。</li><li>只要能达到正确的终点，路途再颠簸，我都受得了。</li><li>脚踏实地，一步一个脚印，坚持不懈地长期努力，这是我的成功之道。</li></ul><h2 id="关于人生">关于人生</h2><ul><li>生活的铁律就是，只有20%的人能够取得比其他80%的人优秀的成绩。</li><li>不要同一头猪摔跤，因为这样你会把全身弄脏，而对方却乐此不疲。</li><li>你不必非常出色，只要在很长、很长的时间内保持比其他人聪明一点点就够了。</li><li>如果你的生活方式是正确的，那么到了晚年只会比年轻时更加幸福。</li><li>我会尽我所能逆流而上，而不是去预测潮汐何时到来。</li><li>如果你专注的时间周期足够长，你不断地为解决难题而努力，你就会跌跌撞撞地得到一个答案。这是人生的半个秘方。</li><li>任何人都会有错过机会的时候，这是命中注定的。我一直认为，改变不了的事情，不要太纠结。<strong>牢骚和抱怨是人生中的大忌。</strong></li><li>在生活中，很多人抱残守缺，他们满脑子的旧思想，新思想根本进不去。有句德国谚语说得好：“我们总是老的太快，聪明的太迟”。所有人都有这个问题。</li><li>身处逆境的时候，你要有一股咬紧牙关、埋头苦干的拼劲。怨天尤人、牢骚不断，只能越来越苦、越来越难。</li><li>想要什么，就立刻要得到，这样的人不但一事无成，还可能坠入深渊。</li><li>抵抗衰老的最佳措施就是在老之前好好生活。</li><li>生活总是以某种防护死伤害某人，又以某种方式帮助他人。面对生活中的打击，每个人都应该积极应对。</li><li>人生的困难一个接一个，每个困难都是对我们的一次考验，都是我们表现自己的机会。</li><li>很多人总是一味地逃避，不愿承受短期的痛苦。自找苦吃、主动吃眼前的苦，这才是正确的处世态度。</li><li>当逆境不期而至时，我们应该敢于迎难而上，这才是一种积极向上的人生态度。整天哼哼唧唧地怨天尤人，谁都救不了你。</li><li>坚持做有意义的事；坚持做有价值的人；坚持追求理智、正直、诚信，总有一天，一定能获得成功。</li><li>要想幸福，第一条，降低自己的预期。这一点是你自己能掌控的。</li><li>我觉得犯嫉妒这种罪的人最蠢，得不到一丁点快乐，整个人都被痛苦包围着，何必遭这份罪呢？</li><li>托马斯·卡莱尔有一句名言：“与其为朦胧的未来而烦恼忧虑，不如脚踏实地地、做好眼前的事。” 这句话说的很对。大多数时候，我们应该把眼前的事做好，尽人事，听天命。</li></ul><h2 id="关于商业管理">关于商业管理</h2><ul><li>最理想的公司，每年创造的现金高于净利润，能为所有者提供大量可自由支配的现金。</li><li>充分认清客观条件的限制，充分认识自身能力的限制，谨小慎微地在限制范围内活动，这是赚钱的诀窍。这个诀窍，与其说是“谦卑”，不如说是“有克制的贪婪”。</li><li>一家公司建立好了文化之后，就能走上良性循环的轨道。</li><li>为了防范风险，我们制定的规矩，恰恰是不赚最后一个铜板。</li><li>任何一家高杠杆的金融机构，无论管理者多么尽职尽责，都可能遭遇意外的损失。关键是遭遇意外之后，能否第一时间解决问题。在问题暴露出来以后，很多公司首先想到的是如何隐瞒，如何用会计手段蒙混过去。我们认为，应该不遮不掩、立刻解决。</li><li>即使拥有诚实守信的优良传统，时间久了，制度漏洞还是会毁掉优良传统。</li><li>裁员成本是一项巨大的隐形负债。很多公司因为裁员而支付的成本高达几亿、几十亿美金。公司明明要为缩小规模而付出成本，但这项成本并没有在资产负债表上体现出来。</li><li>我们从不签署允许我们懒惰的合同，以免我们走向堕落。</li><li>纵观商业史，很多公司辉煌过，赚过大钱，但是当它们被新的科技浪潮淘汰后，它们的家底很快就会耗光，最终走向消亡。</li><li>在服务业，只有全力以赴，为客户消除所有痛点，才能超越竞争对手。</li><li>经营一家公司，你懂的延迟满足，能把公司经营的越来越好。在人生中懂得延迟满足，你死的时候能很风光。</li><li>在沃伦眼中，优秀的管理者是这样的：你把他从火车上扔下去，扔到一个偏僻的小镇，不给他钱，他在这个小镇上诚实本分地经营，用不了多长时间，又发家致富了。</li><li>在与别人合作的过程中，沃伦和我都是首先以高标准要求自己。因为有优秀的人与我们一道努力，我们才能取得今天的成绩。</li><li>要找到优秀的伴侣，只有一个办法，就是自己得配得上。同样的道理，要找到优秀的人共事，你自己首先要是一个优秀的人。</li><li>我们不懂具体的软件业务，那我们怎么领导每日期刊公司呢？我们主要靠知人善任。</li><li>在做管理工作的过程中，最容易犯的错误是，已经发现该换人了，但迟迟下不了决心，拖了很长时间，才把不合适的人换掉。即使是有着多年管理经验的人，也很容易犯这个错误。</li><li>公司越大，越难建立起正确的文化。大公司特别容易患上官僚主义这个通病。</li><li>人们钻空子，肯定是因为激励制度有漏洞。</li><li>职业生涯的三条规则：不要销售你自己都不愿意买的东西；不要为你不尊重和不欣赏的人工作；只和你喜欢的人一起工作。</li><li>我们很少换人，不是因为我们软弱或愚蠢，而是因为我们一开始就把人选对了。</li><li>我愿意和优秀的人共事，不愿意和平庸的人为伍。</li><li>信任是你自己赢得的。你自己做事总是很靠谱，时间久了，别人自然会信任你。</li><li>我觉得在面对难题的时候，列一张清单非常有用。在单子上一列，所有问题一目了然，能把问题考虑得更周全，不会有什么遗漏。</li><li>凡是往简单处想，往认真处行。</li></ul><h2 id="关于投资理念">关于投资理念</h2><ul><li>真正做收购是好事多磨，要熬过辛苦的等待，经历反复的波折。</li><li>大多数时候，我们什么都不做。我们出手的时候很少。即使是出手的时候，我们也是如履薄冰，对可能承担的风险感到不安。</li><li>有些人收集邮票，而我收集疯狂和荒谬，然后避开它们。</li><li>钱多机会少，总比钱少机会多强。</li><li>我们只在很少的时候，能看透重大的机会。</li><li>我们始终把眼前所有的投资机会进行比较，力求找到当下最合理的投资逻辑，这才是重中之重。找到了最合理的同欧字逻辑之后，无论周期波动如何剧烈，是顺境还是逆境，我们都泰然自若。这就是我们的投资之道。</li><li>投资要选容错率高的好生意。有点管理问题，有点困难，有点错误，好生意照样还是好生意。</li><li>按我们这种方式投资，必须准确判断一家公司的前景。也就是说，你不但要能看出来，一家公司现在的生意是好生意，而且要能看出来，它在将来的很长时间里仍然是好生意。</li><li>买入好生意长期持有才是正道。</li><li>真正的好公司，现在的价格，大家可能觉得很贵，其实不贵。</li><li>格雷厄姆提出了安全边际的原则，这个概念永不过时。“市场是我们的仆人，不是我们的老师”。</li><li>做投资，一个是必须等大眼睛等待机会出现，另一个是机会出现时，必须果断出手。</li><li>钓鱼的第一条原则是，在有鱼的地方钓。钓鱼的第二条规则是，记住第一条规则。投资是同样的道理。</li><li>归根结底，投资只有价值投资一种。为什么这么说？因为我们每做一笔投资，把钱投进去，都是为了将来能获得更多的价值。</li><li>首先，要找自己能看懂的机会，不做自己看不懂的投资。然后，要踏踏实实地去做大量实际的工作。</li><li>成功的投资即需要进取心又需要耐心，而且还需要准备好在机会出现时抓住它，因为在这个世界上，机会不会持续很久。</li><li>我们能够成功，不是因为我们善于解决难题，而是因为我们善于远离难题。我们只是找到了容易做的事情。</li></ul>]]></content>
    
    <summary type="html">
    
      卓越的人很少，有机会追随他们，和他们走到一起...
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Wisdom" scheme="https://neo1989.net/tags/Wisdom/"/>
    
  </entry>
  
  <entry>
    <title>SQ3R阅读法</title>
    <link href="https://neo1989.net/Notes/NOTE-SQ3R/"/>
    <id>https://neo1989.net/Notes/NOTE-SQ3R/</id>
    <published>2023-07-11T11:33:19.000Z</published>
    <updated>2023-07-11T11:39:47.879Z</updated>
    
    <content type="html"><![CDATA[<p><img src="//s3.mindex.xyz/tmp/fc643b514ad76d94a6f205de37e747f8.png" alt=""></p><h4 id="Survey">Survey</h4><p>快速扫描章节小标题，识别出来几个关键点，如果有章节小结的，重点阅读。</p><h4 id="Question">Question</h4><p>把章节的标题换成一个问题，阅读这章节的目的就是为了回答这个问题。</p><h4 id="Read">Read</h4><p>带着问题去阅读，阅读过程中始终记得为这个问题寻找答案</p><h4 id="Recite">Recite</h4><p>阅读完之后，用自己的话尝试解答这个问题，如果回答不出来，就重复以上四个步骤，直到回答出来位置。</p><h4 id="Review">Review</h4><p>最后再次回顾，并用自己的话来复述整本书的主要观点。</p>]]></content>
    
    <summary type="html">
    
      方法
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="SoftSkills" scheme="https://neo1989.net/tags/SoftSkills/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · Embeddings （下）</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-Embeddings-2/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-Embeddings-2/</id>
    <published>2023-07-06T05:02:57.000Z</published>
    <updated>2023-07-10T14:17:47.993Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>上篇文章简单介绍了Embeddings，以及Glove。本篇将简单介绍加入Embedding层的CNN。</p><p>注意，所有的前置工作与<a href="https://neo1989.net/Way2AI/Way2AI-CNN/" title="卷积神经网络">《Way2AI · 卷积神经网络》</a>这篇文章里的介绍没有太大区别，最大的区别在于建模的时候加入了Embeddings层。</p><h2 id="Set-up">Set up</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip install numpy==1.21.2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_seeds</span>(<span class="params">seed=<span class="number">1024</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Set seeds for reproducibility.&quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)  <span class="comment"># multi-GPU</span></span><br><span class="line"></span><br><span class="line">set_seeds(seed=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">cuda = <span class="literal">True</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span>(torch.cuda.is_available() <span class="keyword">and</span> cuda) <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">torch.set_default_tensor_type(&#123;<span class="string">&quot;cuda&quot;</span>: <span class="string">&quot;torch.cuda.FloatTensor&quot;</span>, <span class="string">&quot;cpu&quot;</span>: <span class="string">&quot;torch.FloatTensor&quot;</span>&#125;.get(<span class="built_in">str</span>(device)))</span><br></pre></td></tr></table></figure><h2 id="Load-data">Load data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">&quot;https://s3.mindex.xyz/datasets/news.csv&quot;</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>)</span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df[<span class="string">&quot;title&quot;</span>][:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 0     Israel announces West Bank housing plan; barri...</span></span><br><span class="line"><span class="comment"># 1     Red Sox #39;s Feat: As far back as I can remember</span></span><br><span class="line"><span class="comment"># 2     J.P. Morgan Cancels IBM Outsourcing Deal (Reut...</span></span><br><span class="line"><span class="comment"># 3                          Intel Names Otellini New CEO</span></span><br><span class="line"><span class="comment"># 4     Branson Launches Virgin Atlantic Flights to Au...</span></span><br><span class="line"><span class="comment">#                             ...</span></span><br><span class="line"><span class="comment"># 95    Yahoo Profit Surges on Sales of Ads, Google Stock</span></span><br><span class="line"><span class="comment"># 96                                     DirecT Touchdown</span></span><br><span class="line"><span class="comment"># 97         Struggling Bucs Best Dismal Bears, 19-7 (AP)</span></span><br><span class="line"><span class="comment"># 98    Romania PM, Bucharest Mayor Battle for Preside...</span></span><br><span class="line"><span class="comment"># 99                      Glazer Quest for United Falters</span></span><br><span class="line"><span class="comment"># Name: title, Length: 100, dtype: object</span></span><br></pre></td></tr></table></figure><h2 id="Processing">Processing</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> PorterStemmer</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">nltk.download(<span class="string">&quot;stopwords&quot;</span>)</span><br><span class="line">STOPWORDS = stopwords.words(<span class="string">&quot;english&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (STOPWORDS[:<span class="number">5</span>])</span><br><span class="line">porter = PorterStemmer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">text, stopwords=STOPWORDS</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Conditional preprocessing on our text unique to our task.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Lower</span></span><br><span class="line">    text = text.lower()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove stopwords</span></span><br><span class="line">    pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;\b(&quot;</span> + <span class="string">r&quot;|&quot;</span>.join(stopwords) + <span class="string">r&quot;)\b\s*&quot;</span>)</span><br><span class="line">    text = pattern.sub(<span class="string">&quot;&quot;</span>, text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove words in parenthesis</span></span><br><span class="line">    text = re.sub(<span class="string">r&quot;\([^)]*\)&quot;</span>, <span class="string">&quot;&quot;</span>, text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Spacing and filters</span></span><br><span class="line">    text = re.sub(<span class="string">r&quot;([-;;.,!?&lt;=&gt;])&quot;</span>, <span class="string">r&quot; \1 &quot;</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">&quot;[^A-Za-z0-9]+&quot;</span>, <span class="string">&quot; &quot;</span>, text) <span class="comment"># remove non alphanumeric chars</span></span><br><span class="line">    text = re.sub(<span class="string">&quot; +&quot;</span>, <span class="string">&quot; &quot;</span>, text)  <span class="comment"># remove multiple spaces</span></span><br><span class="line">    text = text.strip()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply to dataframe</span></span><br><span class="line">preprocessed_df = df.copy()</span><br><span class="line">preprocessed_df.title = preprocessed_df.title.apply(preprocess)</span><br></pre></td></tr></table></figure><h2 id="Split-data">Split data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_val_test_split</span>(<span class="params">X, y, train_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Split dataset into data splits.&quot;&quot;&quot;</span></span><br><span class="line">    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)</span><br><span class="line">    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Data</span></span><br><span class="line">X = preprocessed_df[<span class="string">&quot;title&quot;</span>].values</span><br><span class="line">y = preprocessed_df[<span class="string">&quot;category&quot;</span>].values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (84000,), y_train: (84000,)</span></span><br><span class="line"><span class="comment"># X_val: (18000,), y_val: (18000,)</span></span><br><span class="line"><span class="comment"># X_test: (18000,), y_test: (18000,)</span></span><br><span class="line"><span class="comment"># Sample point: ibm wins time talks pension case → Sci/Tech</span></span><br></pre></td></tr></table></figure><h2 id="Label-encoding">Label encoding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LabelEncoder</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Label encoder for tag labels.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, class_to_index=&#123;&#125;</span>):</span><br><span class="line">        self.class_to_index = class_to_index <span class="keyword">or</span> &#123;&#125;  <span class="comment"># mutable defaults ;)</span></span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = <span class="built_in">list</span>(self.class_to_index.keys())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.class_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&lt;LabelEncoder(num_classes=<span class="subst">&#123;<span class="built_in">len</span>(self)&#125;</span>)&gt;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, y</span>):</span><br><span class="line">        classes = np.unique(y)</span><br><span class="line">        <span class="keyword">for</span> i, class_ <span class="keyword">in</span> <span class="built_in">enumerate</span>(classes):</span><br><span class="line">            self.class_to_index[class_] = i</span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = <span class="built_in">list</span>(self.class_to_index.keys())</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, y</span>):</span><br><span class="line">        encoded = np.zeros((<span class="built_in">len</span>(y)), dtype=<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(y):</span><br><span class="line">            encoded[i] = self.class_to_index[item]</span><br><span class="line">        <span class="keyword">return</span> encoded</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, y</span>):</span><br><span class="line">        classes = []</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(y):</span><br><span class="line">            classes.append(self.index_to_class[item])</span><br><span class="line">        <span class="keyword">return</span> classes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, fp</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fp, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;<span class="string">&#x27;class_to_index&#x27;</span>: self.class_to_index&#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">cls, fp</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fp, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Encode</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">label_encoder.fit(y_train)</span><br><span class="line">NUM_CLASSES = <span class="built_in">len</span>(label_encoder)</span><br><span class="line"><span class="built_in">print</span>(label_encoder.class_to_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;&#x27;Business&#x27;: 0, &#x27;Sci/Tech&#x27;: 1, &#x27;Sports&#x27;: 2, &#x27;World&#x27;: 3&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert labels to tokens</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line">y_train = label_encoder.encode(y_train)</span><br><span class="line">y_val = label_encoder.encode(y_val)</span><br><span class="line">y_test = label_encoder.encode(y_test)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: Sci/Tech</span></span><br><span class="line"><span class="comment"># y_train[0]: 1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> <span class="built_in">enumerate</span>(counts)&#125;</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># counts: [21000 21000 21000 21000]</span></span><br><span class="line"><span class="comment"># weights: &#123;0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Tokenizer">Tokenizer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> more_itertools <span class="keyword">import</span> take</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tokenizer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, char_level, num_tokens=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 pad_token=<span class="string">&quot;&lt;PAD&gt;&quot;</span>, oov_token=<span class="string">&quot;&lt;UNK&gt;&quot;</span>,</span></span><br><span class="line"><span class="params">                 token_to_index=<span class="literal">None</span></span>):</span><br><span class="line">        self.char_level = char_level</span><br><span class="line">        self.separator = <span class="string">&quot;&quot;</span> <span class="keyword">if</span> self.char_level <span class="keyword">else</span> <span class="string">&quot; &quot;</span></span><br><span class="line">        <span class="keyword">if</span> num_tokens: num_tokens -= <span class="number">2</span> <span class="comment"># pad + unk tokens</span></span><br><span class="line">        self.num_tokens = num_tokens</span><br><span class="line">        self.pad_token = pad_token</span><br><span class="line">        self.oov_token = oov_token</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> token_to_index:</span><br><span class="line">            token_to_index = &#123;pad_token: <span class="number">0</span>, oov_token: <span class="number">1</span>&#125;</span><br><span class="line">        self.token_to_index = token_to_index</span><br><span class="line">        self.index_to_token = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.token_to_index.items()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.token_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&lt;Tokenizer(num_tokens=<span class="subst">&#123;<span class="built_in">len</span>(self)&#125;</span>)&gt;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit_on_texts</span>(<span class="params">self, texts</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.char_level:</span><br><span class="line">            texts = [text.split(<span class="string">&quot; &quot;</span>) <span class="keyword">for</span> text <span class="keyword">in</span> texts]</span><br><span class="line">        all_tokens = [token <span class="keyword">for</span> text <span class="keyword">in</span> texts <span class="keyword">for</span> token <span class="keyword">in</span> text]</span><br><span class="line">        counts = Counter(all_tokens).most_common(self.num_tokens)</span><br><span class="line">        self.min_token_freq = counts[-<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> token, count <span class="keyword">in</span> counts:</span><br><span class="line">            index = <span class="built_in">len</span>(self)</span><br><span class="line">            self.token_to_index[token] = index</span><br><span class="line">            self.index_to_token[index] = token</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">texts_to_sequences</span>(<span class="params">self, texts</span>):</span><br><span class="line">        sequences = []</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> texts:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.char_level:</span><br><span class="line">                text = text.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">            sequence = []</span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> text:</span><br><span class="line">                sequence.append(self.token_to_index.get(</span><br><span class="line">                    token, self.token_to_index[self.oov_token]))</span><br><span class="line">            sequences.append(np.asarray(sequence))</span><br><span class="line">        <span class="keyword">return</span> sequences</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sequences_to_texts</span>(<span class="params">self, sequences</span>):</span><br><span class="line">        texts = []</span><br><span class="line">        <span class="keyword">for</span> sequence <span class="keyword">in</span> sequences:</span><br><span class="line">            text = []</span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> sequence:</span><br><span class="line">                text.append(self.index_to_token.get(index, self.oov_token))</span><br><span class="line">            texts.append(self.separator.join([token <span class="keyword">for</span> token <span class="keyword">in</span> text]))</span><br><span class="line">        <span class="keyword">return</span> texts</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, fp</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fp, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;</span><br><span class="line">                <span class="string">&quot;char_level&quot;</span>: self.char_level,</span><br><span class="line">                <span class="string">&quot;oov_token&quot;</span>: self.oov_token,</span><br><span class="line">                <span class="string">&quot;token_to_index&quot;</span>: self.token_to_index</span><br><span class="line">            &#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">cls, fp</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fp, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tokenize</span></span><br><span class="line">tokenizer = Tokenizer(char_level=<span class="literal">False</span>, num_tokens=<span class="number">5000</span>)</span><br><span class="line">tokenizer.fit_on_texts(texts=X_train)</span><br><span class="line">VOCAB_SIZE = <span class="built_in">len</span>(tokenizer)</span><br><span class="line"><span class="built_in">print</span> (tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output </span></span><br><span class="line"><span class="comment"># &lt;Tokenizer(num_tokens=5000)&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample of tokens</span></span><br><span class="line"><span class="built_in">print</span> (take(<span class="number">5</span>, tokenizer.token_to_index.items()))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;least freq token&#x27;s freq: <span class="subst">&#123;tokenizer.min_token_freq&#125;</span>&quot;</span>) <span class="comment"># use this to adjust num_tokens</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [(&#x27;&lt;PAD&gt;&#x27;, 0), (&#x27;&lt;UNK&gt;&#x27;, 1), (&#x27;39&#x27;, 2), (&#x27;b&#x27;, 3), (&#x27;gt&#x27;, 4)]</span></span><br><span class="line"><span class="comment"># least freq token&#x27;s freq: 14</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert texts to sequences of indices</span></span><br><span class="line">X_train = tokenizer.texts_to_sequences(X_train)</span><br><span class="line">X_val = tokenizer.texts_to_sequences(X_val)</span><br><span class="line">X_test = tokenizer.texts_to_sequences(X_test)</span><br><span class="line">preprocessed_text = tokenizer.sequences_to_texts([X_train[<span class="number">0</span>]])[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Text to indices:\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  (preprocessed) → <span class="subst">&#123;preprocessed_text&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  (tokenized) → <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Text to indices:</span></span><br><span class="line"><span class="comment">#   (preprocessed) → ibm wins time talks pension case</span></span><br><span class="line"><span class="comment">#   (tokenized) → [ 31  32  69  26 715 100]</span></span><br></pre></td></tr></table></figure><h2 id="Padding">Padding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pad_sequences</span>(<span class="params">sequences, max_seq_len=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Pad sequences to max length in sequence.&quot;&quot;&quot;</span></span><br><span class="line">    max_seq_len = <span class="built_in">max</span>(max_seq_len, <span class="built_in">max</span>(<span class="built_in">len</span>(sequence) <span class="keyword">for</span> sequence <span class="keyword">in</span> sequences))</span><br><span class="line">    padded_sequences = np.zeros((<span class="built_in">len</span>(sequences), max_seq_len))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> <span class="built_in">enumerate</span>(sequences):</span><br><span class="line">        padded_sequences[i][:<span class="built_in">len</span>(sequence)] = sequence</span><br><span class="line">    <span class="keyword">return</span> padded_sequences</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2D sequences</span></span><br><span class="line">padded = pad_sequences(X_train[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span> (padded.shape)</span><br><span class="line"><span class="built_in">print</span> (padded)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># (3, 8)</span></span><br><span class="line"><span class="comment"># [[3.100e+01 3.200e+01 6.900e+01 2.600e+01 7.150e+02 1.000e+02 0.000e+00</span></span><br><span class="line"><span class="comment">#   0.000e+00]</span></span><br><span class="line"><span class="comment">#  [3.568e+03 9.000e+00 4.520e+03 2.000e+00 1.000e+00 2.396e+03 7.760e+02</span></span><br><span class="line"><span class="comment">#   1.500e+01]</span></span><br><span class="line"><span class="comment">#  [1.000e+01 1.094e+03 7.600e+01 5.960e+02 5.740e+02 8.000e+02 0.000e+00</span></span><br><span class="line"><span class="comment">#   0.000e+00]]</span></span><br></pre></td></tr></table></figure><h2 id="Dataset">Dataset</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">FILTER_SIZES = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">2</span>, <span class="number">5</span>)) <span class="comment"># bi, tri and 4 grams</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, X, y, max_filter_size</span>):</span><br><span class="line">        self.X = X</span><br><span class="line">        self.y = y</span><br><span class="line">        self.max_filter_size = max_filter_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&lt;Dataset(N=<span class="subst">&#123;<span class="built_in">len</span>(self)&#125;</span>)&gt;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        X = self.X[index]</span><br><span class="line">        y = self.y[index]</span><br><span class="line">        <span class="keyword">return</span> [X, y]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">self, batch</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Processing on a batch.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Get inputs</span></span><br><span class="line">        batch = np.array(batch)</span><br><span class="line">        X = batch[:, <span class="number">0</span>]</span><br><span class="line">        y = batch[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pad sequences</span></span><br><span class="line">        X = pad_sequences(X)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cast</span></span><br><span class="line">        X = torch.LongTensor(X.astype(np.int32))</span><br><span class="line">        y = torch.LongTensor(y.astype(np.int32))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_dataloader</span>(<span class="params">self, batch_size, shuffle=<span class="literal">False</span>, drop_last=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">return</span> torch.utils.data.DataLoader(</span><br><span class="line">            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,</span><br><span class="line">            shuffle=shuffle, drop_last=drop_last, pin_memory=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create datasets</span></span><br><span class="line">max_filter_size = <span class="built_in">max</span>(FILTER_SIZES)</span><br><span class="line">train_dataset = Dataset(X=X_train, y=y_train, max_filter_size=max_filter_size)</span><br><span class="line">val_dataset = Dataset(X=X_val, y=y_val, max_filter_size=max_filter_size)</span><br><span class="line">test_dataset = Dataset(X=X_test, y=y_test, max_filter_size=max_filter_size)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Datasets:\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  Train dataset:<span class="subst">&#123;train_dataset.__str__()&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  Val dataset: <span class="subst">&#123;val_dataset.__str__()&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  Test dataset: <span class="subst">&#123;test_dataset.__str__()&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Sample point:\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  X: <span class="subst">&#123;train_dataset[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  y: <span class="subst">&#123;train_dataset[<span class="number">0</span>][<span class="number">1</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Datasets:</span></span><br><span class="line"><span class="comment">#   Train dataset:&lt;Dataset(N=84000)&gt;</span></span><br><span class="line"><span class="comment">#   Val dataset: &lt;Dataset(N=18000)&gt;</span></span><br><span class="line"><span class="comment">#   Test dataset: &lt;Dataset(N=18000)&gt;</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: [ 31  32  69  26 715 100]</span></span><br><span class="line"><span class="comment">#   y: 1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create dataloaders</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_dataloader = train_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">val_dataloader = val_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">test_dataloader = test_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">batch_X, batch_y = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Sample batch:\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  X: <span class="subst">&#123;<span class="built_in">list</span>(batch_X.size())&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  y: <span class="subst">&#123;<span class="built_in">list</span>(batch_y.size())&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">&quot;Sample point:\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  X: <span class="subst">&#123;batch_X[<span class="number">0</span>]&#125;</span>\n&quot;</span></span><br><span class="line">    <span class="string">f&quot;  y: <span class="subst">&#123;batch_y[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Sample batch:</span></span><br><span class="line"><span class="comment">#   X: [64, 10]</span></span><br><span class="line"><span class="comment">#   y: [64]</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: tensor([ 31,  32,  69,  26, 715, 100,   0,   0,   0,   0])</span></span><br><span class="line"><span class="comment">#   y: 1</span></span><br></pre></td></tr></table></figure><h2 id="Model">Model</h2><p>可视化一下模型的前向传播.</p><ul><li>首先对输入tokenizer化 (batch_size, max_seq_len)</li><li>然后我们对tokenizered输入进行embed (batch_size, max_seq_len, embedding_dim)</li><li>接下来，使用filters（filter_size, vocab_size, num_filter)进行卷积，然后批归一化。我们讲使用三个不同size的filter（2, 3 和 4）分别充当bi-gram, tri-gram 和 4-gram 特征提取器。</li><li>紧跟着，应用一维max polling，从特征图中提取最相关信息以做出决策</li><li>再接一个含dropout的全连接层</li><li>最后再使用一个softmax全连接层以输出最终的类别概率</li></ul><p><img src="//s3.mindex.xyz/tmp/89231298b192a7831de7c18f7c52f6ad.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">HIDDEN_DIM = <span class="number">100</span></span><br><span class="line">DROPOUT_P = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dim, vocab_size, num_filters,</span></span><br><span class="line"><span class="params">                 filter_sizes, hidden_dim, dropout_p, num_classes,</span></span><br><span class="line"><span class="params">                 pretrained_embeddings=<span class="literal">None</span>, freeze_embeddings=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 padding_idx=<span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Filter sizes</span></span><br><span class="line">        self.filter_sizes = filter_sizes</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize embeddings</span></span><br><span class="line">        <span class="keyword">if</span> pretrained_embeddings <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.embeddings = nn.Embedding(</span><br><span class="line">                embedding_dim=embedding_dim, num_embeddings=vocab_size,</span><br><span class="line">                padding_idx=padding_idx)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).<span class="built_in">float</span>()</span><br><span class="line">            self.embeddings = nn.Embedding(</span><br><span class="line">                embedding_dim=embedding_dim, num_embeddings=vocab_size,</span><br><span class="line">                padding_idx=padding_idx, _weight=pretrained_embeddings)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Freeze embeddings or not</span></span><br><span class="line">        <span class="keyword">if</span> freeze_embeddings:</span><br><span class="line">            self.embeddings.weight.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Conv weights</span></span><br><span class="line">        self.conv = nn.ModuleList(</span><br><span class="line">            [nn.Conv1d(in_channels=embedding_dim,</span><br><span class="line">                       out_channels=num_filters,</span><br><span class="line">                       kernel_size=f) <span class="keyword">for</span> f <span class="keyword">in</span> filter_sizes])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FC weights</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout_p)</span><br><span class="line">        self.fc1 = nn.Linear(num_filters*<span class="built_in">len</span>(filter_sizes), hidden_dim)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, channel_first=<span class="literal">False</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Embed</span></span><br><span class="line">        x_in, = inputs</span><br><span class="line">        x_in = self.embeddings(x_in)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Rearrange input so num_channels is in dim 1 (N, C, L)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> channel_first:</span><br><span class="line">            x_in = x_in.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Conv outputs</span></span><br><span class="line">        z = []</span><br><span class="line">        max_seq_len = x_in.shape[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">for</span> i, f <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.filter_sizes):</span><br><span class="line">            <span class="comment"># `SAME` padding</span></span><br><span class="line">            padding_left = <span class="built_in">int</span>((self.conv[i].stride[<span class="number">0</span>]*(max_seq_len-<span class="number">1</span>) - max_seq_len + self.filter_sizes[i])/<span class="number">2</span>)</span><br><span class="line">            padding_right = <span class="built_in">int</span>(math.ceil((self.conv[i].stride[<span class="number">0</span>]*(max_seq_len-<span class="number">1</span>) - max_seq_len + self.filter_sizes[i])/<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Conv + pool</span></span><br><span class="line">            _z = self.conv[i](F.pad(x_in, (padding_left, padding_right)))</span><br><span class="line">            _z = F.max_pool1d(_z, _z.size(<span class="number">2</span>)).squeeze(<span class="number">2</span>)</span><br><span class="line">            z.append(_z)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Concat conv outputs</span></span><br><span class="line">        z = torch.cat(z, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FC layers</span></span><br><span class="line">        z = self.fc1(z)</span><br><span class="line">        z = self.dropout(z)</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Using-GloVe">Using GloVe</h2><p>先实现一些方便能够将预训练的GloVe加载到我们的模型中的公共方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_glove_embeddings</span>(<span class="params">embeddings_file</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Load embeddings from a file.&quot;&quot;&quot;</span></span><br><span class="line">    embeddings = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(embeddings_file, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        <span class="keyword">for</span> index, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(fp):</span><br><span class="line">            values = line.split()</span><br><span class="line">            word = values[<span class="number">0</span>]</span><br><span class="line">            embedding = np.asarray(values[<span class="number">1</span>:], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            embeddings[word] = embedding</span><br><span class="line">    <span class="keyword">return</span> embeddings</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_embeddings_matrix</span>(<span class="params">embeddings, word_index, embedding_dim</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Create embeddings matrix to use in Embedding layer.&quot;&quot;&quot;</span></span><br><span class="line">    embedding_matrix = np.zeros((<span class="built_in">len</span>(word_index), embedding_dim))</span><br><span class="line">    <span class="keyword">for</span> word, i <span class="keyword">in</span> word_index.items():</span><br><span class="line">        embedding_vector = embeddings.get(word)</span><br><span class="line">        <span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            embedding_matrix[i] = embedding_vector</span><br><span class="line">    <span class="keyword">return</span> embedding_matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create embeddings</span></span><br><span class="line">embeddings_file = <span class="string">&#x27;glove.6B.&#123;0&#125;d.txt&#x27;</span>.<span class="built_in">format</span>(EMBEDDING_DIM)</span><br><span class="line">glove_embeddings = load_glove_embeddings(embeddings_file=embeddings_file)</span><br><span class="line">embedding_matrix = make_embeddings_matrix(</span><br><span class="line">    embeddings=glove_embeddings, word_index=tokenizer.token_to_index,</span><br><span class="line">    embedding_dim=EMBEDDING_DIM)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;&lt;Embeddings(words=<span class="subst">&#123;embedding_matrix.shape[<span class="number">0</span>]&#125;</span>, dim=<span class="subst">&#123;embedding_matrix.shape[<span class="number">1</span>]&#125;</span>)&gt;&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;Embeddings(words=5000, dim=100)&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Experiments">Experiments</h2><p>接下来，我们将进行三个实验：</p><ul><li>随机初始化的embeddings (fine-tuned)</li><li>GloVe embeddings (frozen)</li><li>GloVe embeddings (fine-tuned)</li></ul><p>先定义我们的Trainer</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">NUM_FILTERS = <span class="number">50</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-3</span></span><br><span class="line">PATIENCE = <span class="number">5</span></span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Trainer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, device, loss_fn=<span class="literal">None</span>, optimizer=<span class="literal">None</span>, scheduler=<span class="literal">None</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set params</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.device = device</span><br><span class="line">        self.loss_fn = loss_fn</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self.scheduler = scheduler</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">self, dataloader</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Train step.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Set model to train mode</span></span><br><span class="line">        self.model.train()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over train batches</span></span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Step</span></span><br><span class="line">            batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">            inputs, targets = batch[:-<span class="number">1</span>], batch[-<span class="number">1</span>]</span><br><span class="line">            self.optimizer.zero_grad()  <span class="comment"># Reset gradients</span></span><br><span class="line">            z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">            J = self.loss_fn(z, targets)  <span class="comment"># Define loss</span></span><br><span class="line">            J.backward()  <span class="comment"># Backward pass</span></span><br><span class="line">            self.optimizer.step()  <span class="comment"># Update weights</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Cumulative Metrics</span></span><br><span class="line">            loss += (J.detach().item() - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">eval_step</span>(<span class="params">self, dataloader</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Validation or test step.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line">        y_trues, y_probs = [], []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Step</span></span><br><span class="line">                batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">                inputs, y_true = batch[:-<span class="number">1</span>], batch[-<span class="number">1</span>]</span><br><span class="line">                z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">                J = self.loss_fn(z, y_true).item()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Cumulative Metrics</span></span><br><span class="line">                loss += (J - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line">                y_trues.extend(y_true.cpu().numpy())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, np.vstack(y_trues), np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict_step</span>(<span class="params">self, dataloader</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Prediction step.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">        y_probs = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Forward pass w/ inputs</span></span><br><span class="line">                inputs, targets = batch[:-<span class="number">1</span>], batch[-<span class="number">1</span>]</span><br><span class="line">                z = self.model(inputs)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, num_epochs, patience, train_dataloader, val_dataloader</span>):</span><br><span class="line">        best_val_loss = np.inf</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">            <span class="comment"># Steps</span></span><br><span class="line">            train_loss = self.train_step(dataloader=train_dataloader)</span><br><span class="line">            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)</span><br><span class="line">            self.scheduler.step(val_loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Early stopping</span></span><br><span class="line">            <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">                best_val_loss = val_loss</span><br><span class="line">                best_model = self.model</span><br><span class="line">                _patience = patience  <span class="comment"># reset _patience</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                _patience -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> _patience:  <span class="comment"># 0</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Stopping early!&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Logging</span></span><br><span class="line">            <span class="built_in">print</span>(</span><br><span class="line">                <span class="string">f&quot;Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> | &quot;</span></span><br><span class="line">                <span class="string">f&quot;train_loss: <span class="subst">&#123;train_loss:<span class="number">.5</span>f&#125;</span>, &quot;</span></span><br><span class="line">                <span class="string">f&quot;val_loss: <span class="subst">&#123;val_loss:<span class="number">.5</span>f&#125;</span>, &quot;</span></span><br><span class="line">                <span class="string">f&quot;lr: <span class="subst">&#123;self.optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]:<span class="number">.2</span>E&#125;</span>, &quot;</span></span><br><span class="line">                <span class="string">f&quot;_patience: <span class="subst">&#123;_patience&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> best_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_metrics</span>(<span class="params">y_true, y_pred, classes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Per-class performance metrics.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">&quot;overall&quot;</span>: &#123;&#125;, <span class="string">&quot;class&quot;</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">&quot;weighted&quot;</span>)</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;precision&quot;</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;recall&quot;</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;f1&quot;</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;num_samples&quot;</span>] = np.float64(<span class="built_in">len</span>(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(classes)):</span><br><span class="line">        performance[<span class="string">&quot;class&quot;</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">&quot;precision&quot;</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">&quot;recall&quot;</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">&quot;f1&quot;</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">&quot;num_samples&quot;</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br></pre></td></tr></table></figure><h3 id="Random-initialization">Random initialization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">PRETRAINED_EMBEDDINGS = <span class="literal">None</span></span><br><span class="line">FREEZE_EMBEDDINGS = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = CNN(</span><br><span class="line">    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,</span><br><span class="line">    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,</span><br><span class="line">    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,</span><br><span class="line">    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)</span><br><span class="line">model = model.to(device) <span class="comment"># set device</span></span><br><span class="line"><span class="built_in">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of CNN(</span></span><br><span class="line"><span class="comment">#   (embeddings): Embedding(5000, 100, padding_idx=0)</span></span><br><span class="line"><span class="comment">#   (conv): ModuleList(</span></span><br><span class="line"><span class="comment">#     (0): Conv1d(100, 50, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (1): Conv1d(100, 50, kernel_size=(3,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (2): Conv1d(100, 50, kernel_size=(4,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   )</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=150, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(<span class="built_in">list</span>(class_weights.values())).to(device)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer &amp; scheduler</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">    optimizer, mode=<span class="string">&quot;min&quot;</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trainer module</span></span><br><span class="line">trainer = Trainer(model=model, device=device, loss_fn=loss_fn,</span><br><span class="line">                  optimizer=optimizer, scheduler=scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">best_model = trainer.train(NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 1 | train_loss: 0.78800, val_loss: 0.64168, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 2 | train_loss: 0.49324, val_loss: 0.60757, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 3 | train_loss: 0.38917, val_loss: 0.63572, lr: 1.00E-03, _patience: 4</span></span><br><span class="line"><span class="comment"># Epoch: 4 | train_loss: 0.31891, val_loss: 0.70638, lr: 1.00E-03, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 5 | train_loss: 0.26606, val_loss: 0.76403, lr: 1.00E-03, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 6 | train_loss: 0.22631, val_loss: 0.79747, lr: 1.00E-04, _patience: 1</span></span><br><span class="line"><span class="comment"># Stopping early!</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions</span></span><br><span class="line">test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Determine performance</span></span><br><span class="line">performance = get_metrics(</span><br><span class="line">    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)</span><br><span class="line"><span class="built_in">print</span> (json.dumps(performance[<span class="string">&quot;overall&quot;</span>], indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &quot;precision&quot;: 0.8065551302331581,</span></span><br><span class="line"><span class="comment">#   &quot;recall&quot;: 0.8066666666666666,</span></span><br><span class="line"><span class="comment">#   &quot;f1&quot;: 0.8062901077799052,</span></span><br><span class="line"><span class="comment">#   &quot;num_samples&quot;: 18000.0</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><h3 id="Glove-frozen">Glove (frozen)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">PRETRAINED_EMBEDDINGS = embedding_matrix</span><br><span class="line">FREEZE_EMBEDDINGS = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = CNN(</span><br><span class="line">    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,</span><br><span class="line">    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,</span><br><span class="line">    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,</span><br><span class="line">    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)</span><br><span class="line">model = model.to(device) <span class="comment"># set device</span></span><br><span class="line"><span class="built_in">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of CNN(</span></span><br><span class="line"><span class="comment">#   (embeddings): Embedding(5000, 100, padding_idx=0)</span></span><br><span class="line"><span class="comment">#   (conv): ModuleList(</span></span><br><span class="line"><span class="comment">#     (0): Conv1d(100, 50, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (1): Conv1d(100, 50, kernel_size=(3,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (2): Conv1d(100, 50, kernel_size=(4,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   )</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=150, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(<span class="built_in">list</span>(class_weights.values())).to(device)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer &amp; scheduler</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">    optimizer, mode=<span class="string">&quot;min&quot;</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trainer module</span></span><br><span class="line">trainer = Trainer(model=model, device=device, loss_fn=loss_fn,</span><br><span class="line">                  optimizer=optimizer, scheduler=scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">best_model = trainer.train(NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 1 | train_loss: 0.51462, val_loss: 0.49800, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 2 | train_loss: 0.43604, val_loss: 0.49792, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 3 | train_loss: 0.39698, val_loss: 0.50526, lr: 1.00E-03, _patience: 4</span></span><br><span class="line"><span class="comment"># Epoch: 4 | train_loss: 0.36507, val_loss: 0.51659, lr: 1.00E-03, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 5 | train_loss: 0.33745, val_loss: 0.53612, lr: 1.00E-03, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 6 | train_loss: 0.31418, val_loss: 0.56722, lr: 1.00E-04, _patience: 1</span></span><br><span class="line"><span class="comment"># Stopping early!</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions</span></span><br><span class="line">test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Determine performance</span></span><br><span class="line">performance = get_metrics(</span><br><span class="line">    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)</span><br><span class="line"><span class="built_in">print</span> (json.dumps(performance[<span class="string">&quot;overall&quot;</span>], indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &quot;precision&quot;: 0.8264024010717701,</span></span><br><span class="line"><span class="comment">#   &quot;recall&quot;: 0.8269444444444445,</span></span><br><span class="line"><span class="comment">#   &quot;f1&quot;: 0.8263287754212785,</span></span><br><span class="line"><span class="comment">#   &quot;num_samples&quot;: 18000.0</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><h3 id="GloVe-fine-tuned">GloVe (fine-tuned)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">PRETRAINED_EMBEDDINGS = embedding_matrix</span><br><span class="line">FREEZE_EMBEDDINGS = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = CNN(</span><br><span class="line">    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,</span><br><span class="line">    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,</span><br><span class="line">    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,</span><br><span class="line">    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)</span><br><span class="line">model = model.to(device) <span class="comment"># set device</span></span><br><span class="line"><span class="built_in">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of CNN(</span></span><br><span class="line"><span class="comment">#   (embeddings): Embedding(5000, 100, padding_idx=0)</span></span><br><span class="line"><span class="comment">#   (conv): ModuleList(</span></span><br><span class="line"><span class="comment">#     (0): Conv1d(100, 50, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (1): Conv1d(100, 50, kernel_size=(3,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (2): Conv1d(100, 50, kernel_size=(4,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   )</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=150, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Lossclass_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer &amp; scheduler</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">    optimizer, mode=<span class="string">&quot;min&quot;</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trainer module</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, device=device, loss_fn=loss_fn,</span><br><span class="line">    optimizer=optimizer, scheduler=scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">best_model = trainer.train(</span><br><span class="line">    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 1 | train_loss: 0.48751, val_loss: 0.45729, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 2 | train_loss: 0.38391, val_loss: 0.45669, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 3 | train_loss: 0.33045, val_loss: 0.47826, lr: 1.00E-03, _patience: 4</span></span><br><span class="line"><span class="comment"># Epoch: 4 | train_loss: 0.27825, val_loss: 0.52608, lr: 1.00E-03, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 5 | train_loss: 0.22646, val_loss: 0.60470, lr: 1.00E-03, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 6 | train_loss: 0.18130, val_loss: 0.70291, lr: 1.00E-04, _patience: 1</span></span><br><span class="line"><span class="comment"># Stopping early!</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions</span></span><br><span class="line">test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Determine performance</span></span><br><span class="line">performance = get_metrics(</span><br><span class="line">    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)</span><br><span class="line"><span class="built_in">print</span> (json.dumps(performance[<span class="string">&quot;overall&quot;</span>], indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &quot;precision&quot;: 0.8246875013006352,</span></span><br><span class="line"><span class="comment">#   &quot;recall&quot;: 0.8251666666666667,</span></span><br><span class="line"><span class="comment">#   &quot;f1&quot;: 0.8248028697657125,</span></span><br><span class="line"><span class="comment">#   &quot;num_samples&quot;: 18000.0</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><p>Ok, 保存一些必要的模型数据，以供后续能够完整的加载和使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save artifacts</span></span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="built_in">dir</span> = Path(<span class="string">&quot;cnn&quot;</span>)</span><br><span class="line"><span class="built_in">dir</span>.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">label_encoder.save(fp=Path(<span class="built_in">dir</span>, <span class="string">&quot;label_encoder.json&quot;</span>))</span><br><span class="line">tokenizer.save(fp=Path(<span class="built_in">dir</span>, <span class="string">&quot;tokenizer.json&quot;</span>))</span><br><span class="line">torch.save(best_model.state_dict(), Path(<span class="built_in">dir</span>, <span class="string">&quot;model.pt&quot;</span>))</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(Path(<span class="built_in">dir</span>, <span class="string">&quot;performance.json&quot;</span>), <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    json.dump(performance, indent=<span class="number">2</span>, sort_keys=<span class="literal">False</span>, fp=fp)</span><br></pre></td></tr></table></figure><h2 id="Inference">Inference</h2><p>接下来看看如何利用模型进行推理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_probability_distribution</span>(<span class="params">y_prob, classes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Create a dict of class probabilities from an array.&quot;&quot;&quot;</span></span><br><span class="line">    results = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, class_ <span class="keyword">in</span> <span class="built_in">enumerate</span>(classes):</span><br><span class="line">        results[class_] = np.float64(y_prob[i])</span><br><span class="line">    sorted_results = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">sorted</span>(</span><br><span class="line">        results.items(), key=<span class="keyword">lambda</span> item: item[<span class="number">1</span>], reverse=<span class="literal">True</span>)&#125;</span><br><span class="line">    <span class="keyword">return</span> sorted_results</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load artifacts</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">label_encoder = LabelEncoder.load(fp=Path(<span class="built_in">dir</span>, <span class="string">&quot;label_encoder.json&quot;</span>))</span><br><span class="line">tokenizer = Tokenizer.load(fp=Path(<span class="built_in">dir</span>, <span class="string">&quot;tokenizer.json&quot;</span>))</span><br><span class="line">model = CNN(</span><br><span class="line">    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,</span><br><span class="line">    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,</span><br><span class="line">    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,</span><br><span class="line">    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)</span><br><span class="line">model.load_state_dict(torch.load(Path(<span class="built_in">dir</span>, <span class="string">&quot;model.pt&quot;</span>), map_location=device))</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># CNN(</span></span><br><span class="line"><span class="comment">#   (embeddings): Embedding(5000, 100, padding_idx=0)</span></span><br><span class="line"><span class="comment">#   (conv): ModuleList(</span></span><br><span class="line"><span class="comment">#     (0): Conv1d(100, 50, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (1): Conv1d(100, 50, kernel_size=(3,), stride=(1,))</span></span><br><span class="line"><span class="comment">#     (2): Conv1d(100, 50, kernel_size=(4,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   )</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=150, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize trainer</span></span><br><span class="line">trainer = Trainer(model=model, device=device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dataloader</span></span><br><span class="line">text = <span class="string">&quot;The final tennis tournament starts next week.&quot;</span></span><br><span class="line">X = tokenizer.texts_to_sequences([preprocess(text)])</span><br><span class="line"><span class="built_in">print</span> (tokenizer.sequences_to_texts(X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [&#x27;final tennis tournament starts next week&#x27;]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y_filler = label_encoder.encode([label_encoder.classes[<span class="number">0</span>]]*<span class="built_in">len</span>(X))</span><br><span class="line">dataset = Dataset(X=X, y=y_filler, max_filter_size=max_filter_size)</span><br><span class="line">dataloader = dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line">y_prob = trainer.predict_step(dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line">label_encoder.decode(y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Class distributions</span></span><br><span class="line">prob_dist = get_probability_distribution(y_prob=y_prob[<span class="number">0</span>], classes=label_encoder.classes)</span><br><span class="line"><span class="built_in">print</span> (json.dumps(prob_dist, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &quot;Sports&quot;: 1.0,</span></span><br><span class="line"><span class="comment">#   &quot;World&quot;: 7.881690092248483e-12,</span></span><br><span class="line"><span class="comment">#   &quot;Sci/Tech&quot;: 1.270132816196673e-13,</span></span><br><span class="line"><span class="comment">#   &quot;Business&quot;: 2.3282168800871726e-18</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><p>推理结果是 “The final tennis tournament starts next week.” 这篇文章属于 “Sports” 这个分类。</p><p>我们可以看看不同的n-gram提取器，在最大池化层里提取都是什么。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">sample_index = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;Original text:\n<span class="subst">&#123;text&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;\nPreprocessed text:\n<span class="subst">&#123;tokenizer.sequences_to_texts(X)[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;\nMost important n-grams:&quot;</span>)</span><br><span class="line"><span class="comment"># Process conv outputs for each unique filter size</span></span><br><span class="line"><span class="keyword">for</span> i, filter_size <span class="keyword">in</span> <span class="built_in">enumerate</span>(FILTER_SIZES):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Identify most important n-gram (excluding last token)</span></span><br><span class="line">    popular_indices = collections.Counter([np.argmax(conv_output) \</span><br><span class="line">            <span class="keyword">for</span> conv_output <span class="keyword">in</span> conv_outputs[i]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get corresponding text</span></span><br><span class="line">    start = popular_indices.most_common(<span class="number">1</span>)[-<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">    n_gram = <span class="string">&quot; &quot;</span>.join([token <span class="keyword">for</span> token <span class="keyword">in</span> tokens[start:start+filter_size]])</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">f&quot;[<span class="subst">&#123;filter_size&#125;</span>-gram]: <span class="subst">&#123;n_gram&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Original text:</span></span><br><span class="line"><span class="comment"># The final tennis tournament starts next week.</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Preprocessed text:</span></span><br><span class="line"><span class="comment"># final tennis tournament starts next week</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Most important n-grams:</span></span><br><span class="line"><span class="comment"># [2-gram]: tennis tournament</span></span><br><span class="line"><span class="comment"># [3-gram]: final tennis tournament</span></span><br><span class="line"><span class="comment"># [4-gram]: final tennis tournament starts</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>如你所见，加入Embedding层的卷积神经网络模型的表现，相较于只有one-hot编码的模型，性能上有了很大的提升。</p>]]></content>
    
    <summary type="html">
    
      Explore and motivate the need for representation via embeddings.
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
</feed>
