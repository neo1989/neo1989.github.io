<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>愚苏记</title>
  
  <subtitle>To no avail but try.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://neo1989.net/"/>
  <updated>2023-06-30T15:33:15.125Z</updated>
  <id>https://neo1989.net/</id>
  
  <author>
    <name>Neo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Way2AI · 卷积神经网络</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-CNN/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-CNN/</id>
    <published>2023-06-27T09:30:43.000Z</published>
    <updated>2023-06-30T15:33:15.125Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>本文简单示范了如何利用CNN处理NLP任务。</p><p>CNNs的核心就是利用卷积（滑动）操作来提取数据特征的卷积核（aka kernels, filters,weights, etc.)。它们随机初始化但通过参数共享来提取特征。</p><p><img src="//s3.mindex.xyz/tmp/1a58a0c1e58cd3f543995ecee0eb71d4.gif" alt=""></p><h2 id="Set-up">Set up</h2><p>复用<a href="http://neo1989.net/Way2AI/Way2AI-utilities/" title="PyTorch实现神经网络的基本套路">《PyTorch实现神经网络的基本套路》</a> 里介绍的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_seeds</span><span class="params">(seed=<span class="number">1024</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Set seeds for reproducibility."""</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    touch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)  <span class="comment"># multi-GPU</span></span><br><span class="line"></span><br><span class="line">set_seeds(seed=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">cuda = <span class="literal">True</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span>(torch.cuda.is_available() <span class="keyword">and</span> cuda) <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">torch.set_default_tensor_type(&#123;<span class="string">"cuda"</span>: <span class="string">"torch.cuda.FloatTensor"</span>, <span class="string">"cpu"</span>: <span class="string">"torch.FloatTensor"</span>&#125;.get(str(device)))</span><br></pre></td></tr></table></figure><h3 id="Load-data">Load data</h3><p>我们将在<a href="https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset" target="_blank" rel="noopener" title="AG News Classification Dataset">AGNews dataset</a> 这个数据集上完成本次学习任务。这是一份来自4个不同新闻分类120k条新闻标题样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load data</span></span><br><span class="line">url = <span class="string">"https://s3.mindex.xyz/datasets/news.csv"</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>)</span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/tmp/5820e01cf3a5f9f93ce85ba8d488647e.png" alt=""></p><h3 id="Preprocessing">Preprocessing</h3><p>首先要做的，是对这些数据进行预处理，手段包括删除停用词、字母小写（英文）、词形还原词干提取、中文分词、正则处理等。</p><p>由于我们的任务是纯英文数据，这里使用英文的通用处理方法。中文任务以后再表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> PorterStemmer</span><br><span class="line"></span><br><span class="line">nltk.download(<span class="string">"stopwords"</span>)</span><br><span class="line">STOPWORDS = stopwords.words(<span class="string">"english"</span>)</span><br><span class="line"><span class="keyword">print</span> (STOPWORDS[:<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># ['i', 'me', 'my', 'myself', 'we']</span></span><br><span class="line"></span><br><span class="line">porter = PorterStemmer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(text, stopwords=STOPWORDS)</span>:</span></span><br><span class="line">    <span class="string">"""Conditional preprocessing on our text unique to our task."""</span></span><br><span class="line">    <span class="comment"># Lower</span></span><br><span class="line">    text = text.lower()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove stopwords</span></span><br><span class="line">    pattern = re.compile(<span class="string">r"\b("</span> + <span class="string">r"|"</span>.join(stopwords) + <span class="string">r")\b\s*"</span>)</span><br><span class="line">    text = pattern.sub(<span class="string">""</span>, text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove words in parenthesis</span></span><br><span class="line">    text = re.sub(<span class="string">r"\([^)]*\)"</span>, <span class="string">""</span>, text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Spacing and filters</span></span><br><span class="line">    text = re.sub(<span class="string">r"([-;;.,!?&lt;=&gt;])"</span>, <span class="string">r" \1 "</span>, text)  <span class="comment"># separate punctuation tied to words</span></span><br><span class="line">    text = re.sub(<span class="string">"[^A-Za-z0-9]+"</span>, <span class="string">" "</span>, text)  <span class="comment"># remove non alphanumeric chars</span></span><br><span class="line">    text = re.sub(<span class="string">" +"</span>, <span class="string">" "</span>, text)  <span class="comment"># remove multiple spaces</span></span><br><span class="line">    text = text.strip()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply to dataframe</span></span><br><span class="line">preprocessed_df = df.copy()</span><br><span class="line">preprocessed_df.title = preprocessed_df.title.apply(preprocess)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"<span class="subst">&#123;df.title.values[<span class="number">-1</span>]&#125;</span>\n\n<span class="subst">&#123;preprocessed_df.title.values[<span class="number">-1</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Oil Slips Under \$55 a Barrel</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># oil slips 55 barrel</span></span><br></pre></td></tr></table></figure><h3 id="Split-data">Split data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_test_split</span><span class="params">(X, y, train_size)</span>:</span></span><br><span class="line">    X_train, X_, y_train,y_ = train_test_split(X, y, train_size=train_size, stratify=y)</span><br><span class="line">    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Data</span></span><br><span class="line">X = preprocessed_df[<span class="string">"title"</span>].values</span><br><span class="line">y = preprocessed_df[<span class="string">"category"</span>].values</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (84000,), y_train: (84000,)</span></span><br><span class="line"><span class="comment"># X_val: (18000,), y_val: (18000,)</span></span><br><span class="line"><span class="comment"># X_test: (18000,), y_test: (18000,)</span></span><br><span class="line"><span class="comment"># Sample point: wenger plans buy new keeper → Sports</span></span><br></pre></td></tr></table></figure><h2 id="Label-encoding">Label encoding</h2><p>复用<a href="http://neo1989.net/Way2AI/Way2AI-utilities/" title="PyTorch实现神经网络的基本套路">《PyTorch实现神经网络的基本套路》</a> 里介绍的 LabelEncoder</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Encode</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">label_encoder.fit(y_train)</span><br><span class="line">NUM_CLASSES = len(label_encoder)</span><br><span class="line"><span class="keyword">print</span> (label_encoder.class_to_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert labels to tokens</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">y_train = label_encoder.encode(y_train)</span><br><span class="line">y_val = label_encoder.encode(y_val)</span><br><span class="line">y_test = label_encoder.encode(y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: Sports</span></span><br><span class="line"><span class="comment"># y_train[0]: 2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> enumerate(counts)&#125;</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># counts: [21000 21000 21000 21000]</span></span><br><span class="line"><span class="comment"># weights: &#123;0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Tokenizer">Tokenizer</h2><p>由于任务要处理的是文本，无法直接送给模型。因此我们定义一个Tokenizer来处理文本数据，目的是将文本序列转化成离散的标记（tokens)，以便后续的处理和分析。这意味着每个token可以映射到一个唯一的索引，这样我们就可以用一个索引数组（向量）来表示文本序列。而一个token可以是一个字符、一个单词、一个词组等等。</p><p>下面是一个示例实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> more_itertools <span class="keyword">import</span> take</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, char_level, num_tokens=None, pad_token=<span class="string">"&lt;PAD&gt;"</span>, oov_token=<span class="string">"&lt;UNK&gt;"</span>, token_to_index=None)</span>:</span></span><br><span class="line">        self.char_level = char_level</span><br><span class="line">        self.separator = <span class="string">""</span> <span class="keyword">if</span> self.char_level <span class="keyword">else</span> <span class="string">" "</span></span><br><span class="line">        <span class="keyword">if</span> num_tokens:</span><br><span class="line">            num_tokens -= <span class="number">2</span> <span class="comment"># pad + unk tokens</span></span><br><span class="line">        self.num_tokens = num_tokens</span><br><span class="line">        self.pad_token = pad_token</span><br><span class="line">        self.oov_token = oov_token</span><br><span class="line">        self.token_to_index = token_to_index <span class="keyword">if</span> token_to_index <span class="keyword">else</span> &#123;pad_token: <span class="number">0</span>, oov_token: <span class="number">1</span>&#125;</span><br><span class="line">        self.index_to_token = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.token_to_index.items()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.token_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"&lt;Tokenizer(num_tokens=<span class="subst">&#123;len(self)&#125;</span>)&gt;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_on_texts</span><span class="params">(self, texts)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.char_level:</span><br><span class="line">            texts = [text.split(<span class="string">" "</span>) <span class="keyword">for</span> text <span class="keyword">in</span> texts]</span><br><span class="line">        all_tokens = [token <span class="keyword">for</span> text <span class="keyword">in</span> texts <span class="keyword">for</span> token <span class="keyword">in</span> text]</span><br><span class="line">        counts = Counter(all_tokens).most_common(self.num_tokens)</span><br><span class="line">        self.min_token_freq = counts[<span class="number">-1</span>][<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> token, count <span class="keyword">in</span> counts:</span><br><span class="line">            index = len(self)</span><br><span class="line">            self.token_to_index[token] = index</span><br><span class="line">            self.index_to_token[index] = token</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">texts_to_sequences</span><span class="params">(self, texts)</span>:</span></span><br><span class="line">        sequences = []</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> texts:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.char_level:</span><br><span class="line">                text = text.split(<span class="string">" "</span>)</span><br><span class="line">            sequence = []</span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> text:</span><br><span class="line">                sequence.append(self.token_to_index.get(</span><br><span class="line">                    token, self.token_to_index[self.oov_token]))</span><br><span class="line">            sequences.append(np.asarray(sequence))</span><br><span class="line">        <span class="keyword">return</span> sequences</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sequences_to_texts</span><span class="params">(self, sequences)</span>:</span></span><br><span class="line">        texts = []</span><br><span class="line">        <span class="keyword">for</span> sequence <span class="keyword">in</span> sequences:</span><br><span class="line">            text = []</span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> sequence:</span><br><span class="line">                text.append(self.index_to_token.get(index, self.oov_token))</span><br><span class="line">            texts.append(self.separator.join([token <span class="keyword">for</span> token <span class="keyword">in</span> text]))</span><br><span class="line">        <span class="keyword">return</span> texts</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;</span><br><span class="line">                <span class="string">"char_level"</span>: self.char_level,</span><br><span class="line">                <span class="string">"oov_token"</span>: self.oov_token,</span><br><span class="line">                <span class="string">"token_to_index"</span>: self.token_to_index</span><br><span class="line">            &#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(cls, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"r"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br></pre></td></tr></table></figure><p>本次实验我们限制tokens的数量为500个(停用词已删除)，其中包括两个占位的。如果您的计算资源足够，可以使用更大的tokens数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tokenize</span></span><br><span class="line">tokenizer = Tokenizer(char_level=<span class="literal">False</span>, num_tokens=<span class="number">500</span>)</span><br><span class="line">tokenizer.fit_on_texts(texts=X_train)</span><br><span class="line">VOCAB_SIZE = len(tokenizer)</span><br><span class="line"><span class="keyword">print</span> (tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;Tokenizer(num_tokens=500)&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample of tokens</span></span><br><span class="line"><span class="keyword">print</span> (take(<span class="number">10</span>, tokenizer.token_to_index.items()))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"least freq token's freq: <span class="subst">&#123;tokenizer.min_token_freq&#125;</span>"</span>) <span class="comment"># use this to adjust num_tokens</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [('&lt;PAD&gt;', 0), ('&lt;UNK&gt;', 1), ('39', 2), ('b', 3), ('gt', 4), ('lt', 5), ('us', 6), ('new', 7), ('oil', 8), ('microsoft', 9)]</span></span><br><span class="line"><span class="comment"># least freq token's freq: 166</span></span><br></pre></td></tr></table></figure><p>Ok，接下来将我们文本数据全部token化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert texts to sequences of indices</span></span><br><span class="line">X_train = tokenizer.texts_to_sequences(X_train)</span><br><span class="line">X_val = tokenizer.texts_to_sequences(X_val)</span><br><span class="line">X_test = tokenizer.texts_to_sequences(X_test)</span><br><span class="line">preprocessed_text = tokenizer.sequences_to_texts([X_train[<span class="number">0</span>]])[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Text to indices:\n"</span></span><br><span class="line">    <span class="string">f"  (preprocessed) → <span class="subst">&#123;preprocessed_text&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  (tokenized) → <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Text to indices:</span></span><br><span class="line"><span class="comment">#   (preprocessed) → ibm wins time talks &lt;UNK&gt; case</span></span><br><span class="line"><span class="comment">#   (tokenized) → [ 31  32  69  26   1 100]</span></span><br></pre></td></tr></table></figure><h2 id="One-hot-encoding">One-hot encoding</h2><p>One-hot编码是一种将离散变量表示为二进制向量的技术。它允许我们以一种模型可以理解的方式来表示数据，并且不受token的实际值的影响。</p><p>举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设我们有个只含5个字符的词表：</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"a"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"e"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"i"</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">"o"</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">"u"</span>: <span class="number">4</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 那么文本 aou 就会表示成一个二维矩阵：</span></span><br><span class="line"><span class="comment"># 列对应着词表，而每一行表示单个token的二进制向量（只在词表对应位置置为1，其他位置为0）</span></span><br><span class="line">[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]]</span><br></pre></td></tr></table></figure><p>我们手动实现一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_categorical</span><span class="params">(seq, num_classes)</span>:</span></span><br><span class="line">    <span class="string">"""One-hot encode a sequence of tokens."""</span></span><br><span class="line">    one_hot = np.zeros((len(seq), num_classes))</span><br><span class="line">    <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(seq):</span><br><span class="line">        one_hot[i, item] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> one_hot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># One-hot encoding</span></span><br><span class="line"><span class="keyword">print</span> (X_train[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">print</span> (len(X_train[<span class="number">0</span>]))</span><br><span class="line">cat = to_categorical(seq=X_train[<span class="number">0</span>], num_classes=len(tokenizer))</span><br><span class="line"><span class="keyword">print</span> (cat)</span><br><span class="line"><span class="keyword">print</span> (cat.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [ 31  32  69  26   1 100]</span></span><br><span class="line"><span class="comment"># 6</span></span><br><span class="line"><span class="comment"># [[0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 0.]]</span></span><br><span class="line"><span class="comment"># (6, 500)</span></span><br></pre></td></tr></table></figure><p>接下来需要将我们的数据进行one-hot编码处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert tokens to one-hot</span></span><br><span class="line">vocab_size = len(tokenizer)</span><br><span class="line">X_train = [to_categorical(seq, num_classes=vocab_size) <span class="keyword">for</span> seq <span class="keyword">in</span> X_train]</span><br><span class="line">X_val = [to_categorical(seq, num_classes=vocab_size) <span class="keyword">for</span> seq <span class="keyword">in</span> X_val]</span><br><span class="line">X_test = [to_categorical(seq, num_classes=vocab_size) <span class="keyword">for</span> seq <span class="keyword">in</span> X_test]</span><br></pre></td></tr></table></figure><h2 id="Padding">Padding</h2><p>由于我们的数据是不定长的新闻标题，而模型能够处理的是相同形状的数据，所以引入padding来预处理数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_sequences</span><span class="params">(sequences, max_seq_len=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Pad sequences to max length in sequence."""</span></span><br><span class="line">    max_seq_len = max(max_seq_len, max(len(sequence) <span class="keyword">for</span> sequence <span class="keyword">in</span> sequences))</span><br><span class="line">    num_classes = sequences[<span class="number">0</span>].shape[<span class="number">-1</span>]</span><br><span class="line">    padded_sequences = np.zeros((len(sequences), max_seq_len, num_classes))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        padded_sequences[i][:len(sequence)] = sequence</span><br><span class="line">    <span class="keyword">return</span> padded_sequences</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3D sequences</span></span><br><span class="line"><span class="keyword">print</span> (X_train[<span class="number">0</span>].shape, X_train[<span class="number">1</span>].shape, X_train[<span class="number">2</span>].shape)</span><br><span class="line">padded = pad_sequences(X_train[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> (padded.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># (6, 500) (8, 500) (6, 500)</span></span><br><span class="line"><span class="comment"># (3, 8, 500)</span></span><br></pre></td></tr></table></figure><h2 id="Dataset">Dataset</h2><p>一如上篇文章里介绍的，我们需要把数据放在 Dataset 中，并使用 DataLoader 来有效地创建用于训练和验证的批次。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">FILTER_SIZE = <span class="number">1</span> <span class="comment"># unigram</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, X, y, max_filter_size)</span>:</span></span><br><span class="line">        self.X = X</span><br><span class="line">        self.y = y</span><br><span class="line">        self.max_filter_size = max_filter_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"&lt;Dataset(N=<span class="subst">&#123;len(self)&#125;</span>)&gt;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        X = self.X[index]</span><br><span class="line">        y = self.y[index]</span><br><span class="line">        <span class="keyword">return</span> [X, y]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(self, batch)</span>:</span></span><br><span class="line">        <span class="string">"""Processing on a batch."""</span></span><br><span class="line">        <span class="comment"># Get inputs</span></span><br><span class="line">        batch = np.array(batch)</span><br><span class="line">        X = batch[:, <span class="number">0</span>]</span><br><span class="line">        y = batch[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pad sequences</span></span><br><span class="line">        X = pad_sequences(X, max_seq_len=self.max_filter_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cast</span></span><br><span class="line">        X = torch.FloatTensor(X.astype(np.int32))</span><br><span class="line">        y = torch.LongTensor(y.astype(np.int32))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_dataloader</span><span class="params">(self, batch_size, shuffle=False, drop_last=False)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.utils.data.DataLoader(</span><br><span class="line">            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,</span><br><span class="line">            shuffle=shuffle, drop_last=drop_last, pin_memory=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create datasets for embedding</span></span><br><span class="line">train_dataset = Dataset(X=X_train, y=y_train, max_filter_size=FILTER_SIZE)</span><br><span class="line">val_dataset = Dataset(X=X_val, y=y_val, max_filter_size=FILTER_SIZE)</span><br><span class="line">test_dataset = Dataset(X=X_test, y=y_test, max_filter_size=FILTER_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Datasets:\n"</span></span><br><span class="line">    <span class="string">f"  Train dataset:<span class="subst">&#123;train_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  Val dataset: <span class="subst">&#123;val_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  Test dataset: <span class="subst">&#123;test_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">    <span class="string">"Sample point:\n"</span></span><br><span class="line">    <span class="string">f"  X: <span class="subst">&#123;test_dataset[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  y: <span class="subst">&#123;test_dataset[<span class="number">0</span>][<span class="number">1</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Datasets:</span></span><br><span class="line"><span class="comment">#   Train dataset:&lt;Dataset(N=84000)&gt;</span></span><br><span class="line"><span class="comment">#   Val dataset: &lt;Dataset(N=18000)&gt;</span></span><br><span class="line"><span class="comment">#   Test dataset: &lt;Dataset(N=18000)&gt;</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: [[0. 1. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 0.]]</span></span><br><span class="line"><span class="comment">#   y: 1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create dataloaders</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_dataloader = train_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">val_dataloader = val_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">test_dataloader = test_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">batch_X, batch_y = next(iter(test_dataloader))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Sample batch:\n"</span></span><br><span class="line">    <span class="string">f"  X: <span class="subst">&#123;list(batch_X.size())&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  y: <span class="subst">&#123;list(batch_y.size())&#125;</span>\n"</span></span><br><span class="line">    <span class="string">"Sample point:\n"</span></span><br><span class="line">    <span class="string">f"  X: <span class="subst">&#123;batch_X[<span class="number">0</span>]&#125;</span>\n"</span></span><br><span class="line">    <span class="string">f"  y: <span class="subst">&#123;batch_y[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Sample batch:</span></span><br><span class="line"><span class="comment">#   X: [64, 15, 500]</span></span><br><span class="line"><span class="comment">#   y: [64]</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: tensor([[0., 1., 0.,  ..., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 1., 0.,  ..., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.,  ..., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         ...,</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.,  ..., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.,  ..., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.,  ..., 0., 0., 0.]])</span></span><br><span class="line"><span class="comment">#   y: 1</span></span><br><span class="line"><span class="comment">#</span></span><br></pre></td></tr></table></figure><h2 id="CNN">CNN</h2><p>接下来呢要进入本篇的重点，CNN了。</p><h3 id="Inputs">Inputs</h3><p>下面这个简单的示例里，我们随机给出了N个样本，每个样本有8个token，而我们的词表大小是10个。</p><p>也就意味着，我们inputs的形状是 (N, 8, 10)</p><p>但需要注意的是，当我使用PyTorch处理CNN时，通道数需要在第二个维度，也就意味着，在这个例子里，我们的inputs的形状得是 (N, 10, 8)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume all our inputs are padded to have the same num of tokens.</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">max_seq_len = <span class="number">8</span>  <span class="comment"># tokens per input</span></span><br><span class="line">vocab_size = <span class="number">10</span>  <span class="comment"># one-hot size</span></span><br><span class="line">x = torch.randn(batch_size, max_seq_len, vocab_size)</span><br><span class="line">print(<span class="string">f"X: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">x = x.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">print(<span class="string">f"X: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X: torch.Size([64, 8, 10])</span></span><br><span class="line"><span class="comment"># X: torch.Size([64, 10, 8])</span></span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/tmp/fea49de2fb514aafde1b3545c5163058.png" alt=""></p><h3 id="Filters">Filters</h3><p>在下面的动画中，我们将卷积核和输入简化成2D，以便于可视化，而且实际上值并不总是是0或1，而是任意的浮点数。</p><p><img src="//s3.mindex.xyz/tmp/1a58a0c1e58cd3f543995ecee0eb71d4.gif" alt=""></p><p>现在回到我们的示例数据，单个样本的形状是(8, 10) [max_seq_len, vocab_size]，然后我们考虑用50个形状是(1, 3)的一维卷积来提取数据的特征，由于我们的数据的通道数是10 （num_channels = vocab_size = one_hot_size = 10）, 这边意味着这个卷积核的形状便是 (3, 10, 50) [kernel_size, vocab_size, num_filters]</p><p><img src="//s3.mindex.xyz/tmp/18c83d441855deaf7671fea9f9f26bdc.png" alt=""></p><p>这里有两个关键的概念，步长(stride) 和 填充(padding). 详见下图</p><p><img src="//s3.mindex.xyz/tmp/49dc51e7b89f5215577c01e74a17ce73.png" alt=""></p><p>这里采用一维卷积<a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d" target="_blank" rel="noopener" title="Conv1d">Conv1D</a>来处理示例数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convolutional filters (VALID padding)</span></span><br><span class="line">num_filters = <span class="number">50</span> <span class="comment"># num filters</span></span><br><span class="line">filter_size = <span class="number">3</span></span><br><span class="line">stride = <span class="number">1</span></span><br><span class="line">padding = <span class="number">0</span>  <span class="comment"># valid padding (no padding)</span></span><br><span class="line">conv1 = nn.Conv1d(in_channels=vocab_size, out_channels=num_filters,</span><br><span class="line">                  kernel_size=filter_size, stride=stride,</span><br><span class="line">                  padding=padding, padding_mode=<span class="string">"zeros"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"conv: <span class="subst">&#123;conv1.weight.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># conv: torch.Size([50, 10, 3])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Forward pass</span></span><br><span class="line">z = conv1(x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"z: <span class="subst">&#123;z.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># z: torch.Size([64, 50, 6])</span></span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/tmp/0fcb44386fcdfdd5c7e56e79be18515a.png" alt=""></p><p>如你所见，我们输入数据max_seq_len=8，而经过卷积后的output的长度却是6。如果需要保证长度一致，那么就需要引入padding了。<br>$$<br>\begin{split}<br>W = \frac{W - F + 2P}{S} + 1 \\<br>P = \frac{S(W - 1) - W + F}{2}<br>\end{split}<br>$$</p><p>如果P不是一个整数，考虑向上取整(math.ceil)在右侧填充。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convolutional filters (SAME padding)</span></span><br><span class="line">num_filters = <span class="number">50</span> <span class="comment"># num filters</span></span><br><span class="line">filter_size = <span class="number">3</span></span><br><span class="line">stride = <span class="number">1</span></span><br><span class="line">padding = <span class="number">0</span>  <span class="comment"># valid padding (no padding)</span></span><br><span class="line">conv = nn.Conv1d(in_channels=vocab_size, out_channels=num_filters,</span><br><span class="line">                 kernel_size=filter_size, stride=stride,</span><br><span class="line">                 padding=padding, padding_mode=<span class="string">"zeros"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"conv: <span class="subst">&#123;conv.weight.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># conv: torch.Size([50, 10, 3])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># `SAME` padding</span></span><br><span class="line">padding_left = int((conv.stride[<span class="number">0</span>]*(max_seq_len<span class="number">-1</span>) - max_seq_len + filter_size) / <span class="number">2</span>)</span><br><span class="line">padding_right =int(math.ceil((conv.stride[<span class="number">0</span>]*(max_seq_len<span class="number">-1</span>) - max_seq_len + filter_size) / <span class="number">2</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"padding: <span class="subst">&#123;(padding_left, padding_right)&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># padding: (1, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Forward pass</span></span><br><span class="line">z = conv(F.pad(x, (padding_left, padding_right)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"z: <span class="subst">&#123;z.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># z: torch.Size([64, 50, 8])</span></span><br></pre></td></tr></table></figure><p>未来我们会探索更高维度的卷积层。包括使用Conv2D来处理3D数据（图像、字符级别文本等），使用Conv3D来处理4D数据（视频、时间序列数据等）</p><h3 id="Pooling">Pooling</h3><p>池化是一种用于简化下游计算的方法，通过将高维特征图总结为较低维特征图来减少冗余信息。在卷积滤波器对输入进行处理后产生的特征映射中，由于卷积和重叠的性质，会存在大量的冗余信息。池化操作可以采用最大值或平均值等方式。下面是一个池化的示例：假设来自卷积层的输出是4x4的特征图，我们使用2x2的最大池化过滤器进行处理。</p><p><img src="//s3.mindex.xyz/tmp/16910ad2e084e2b567c1cfadffcabab4.png" alt=""></p><p>在这个例子里，我们使用<a href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d" target="_blank" rel="noopener" title="MaxPool1d">MaxPool1D</a>取一个max值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Max pooling</span></span><br><span class="line">pool_output = F.max_pool1d(z, z.size(<span class="number">2</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Size: <span class="subst">&#123;pool_output.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([64, 50, 1])</span></span><br></pre></td></tr></table></figure><h3 id="Batch-normalization">Batch normalization</h3><p>在构建模型前，需要讨论的最后一个主题便是<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener" title="batch normalization">batch normalization</a>.  它是一种对来自前一层激活的标准化操作，使其均值为0，标准差为1。</p><p>在以前的笔记本中，我们对输入进行标准化，以便模型能够更快地进行优化，并提高学习率。这里采用相同的概念，但我们在重复的前向传递过程中保持标准化的值，以进一步帮助优化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Batch normalization</span></span><br><span class="line">batch_norm = nn.BatchNorm1d(num_features=num_filters)</span><br><span class="line">z = batch_norm(conv(x)) <span class="comment"># applied to activations (after conv layer &amp; before pooling)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"z: <span class="subst">&#123;z.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># z: torch.Size([64, 50, 6])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Mean and std before batchnorm</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;torch.mean(conv(x)):<span class="number">.2</span>f&#125;</span>, std: <span class="subst">&#123;torch.std(conv(x)):<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># mean: -0.00, std: 0.59</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Mean and std after batchnorm</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;torch.mean(z):<span class="number">.2</span>f&#125;</span>, std: <span class="subst">&#123;torch.std(z):<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># mean: -0.00, std: 1.00</span></span><br></pre></td></tr></table></figure><h2 id="Modling">Modling</h2><h3 id="Model">Model</h3><p>可视化一下模型的前向传播.</p><ul><li>首先对输入tokenizer化 (batch_size, max_seq_len)</li><li>然后，one-hot编码 (batch_size, max_seq_len, vocab_size)</li><li>接下来，使用filters（filter_size, vocab_size, num_filter)进行卷积，然后批归一化。这里我们的filters相当于一个n-gram检测器。</li><li>紧跟着，应用max polling，从特征图中提取最相关信息</li><li>再接一个含dropout的全连接层</li><li>最后再一个softmax全连接层以输出最终的类别概率</li></ul><p><img src="//s3.mindex.xyz/tmp/3984d8e8ac31581c8b9525fd221e275c.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">NUM_FILTERS = <span class="number">50</span></span><br><span class="line">HIDDEN_DIM = <span class="number">100</span></span><br><span class="line">DROPOUT_P = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, num_filters, filter_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_dim, dropout_p, num_classes)</span>:</span></span><br><span class="line">        super(CNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># COnvolutional filters</span></span><br><span class="line">        self.filter_size = filter_size</span><br><span class="line">        self.conv = nn.Conv1d(</span><br><span class="line">            in_channels=vocab_size, out_channels=num_filters,</span><br><span class="line">            kernel_size=filter_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, padding_mode=<span class="string">'zeros'</span>)</span><br><span class="line">        self.batch_norm = nn.BatchNorm1d(num_features=num_filters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FC layers</span></span><br><span class="line">        self.fc1 = nn.Linear(num_filters, hidden_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, channel_first=False,)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Rearrange input so num_channels is in dim 1 (N, C, L)</span></span><br><span class="line">        x_in, = inputs</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> channel_first:</span><br><span class="line">            x_in = x_in.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Padding for `SAME` padding</span></span><br><span class="line">        max_seq_len = x_in.shape[<span class="number">2</span>]</span><br><span class="line">        padding_left = int((self.conv.stride[<span class="number">0</span>]*(max_seq_len<span class="number">-1</span>) - max_seq_len + self.filter_size)/<span class="number">2</span>)</span><br><span class="line">        padding_right = int(math.ceil((self.conv.stride[<span class="number">0</span>]*(max_seq_len<span class="number">-1</span>) - max_seq_len + self.filter_size)/<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Conv outputs</span></span><br><span class="line">        z = self.conv(F.pad(x_in, (padding_left, padding_right)))</span><br><span class="line">        z = F.max_pool1d(z, z.size(<span class="number">2</span>)).squeeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FC layer</span></span><br><span class="line">        z = self.fc1(z)</span><br><span class="line">        z = self.dropout(z)</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = CNN(vocab_size=VOCAB_SIZE, num_filters=NUM_FILTERS, filter_size=FILTER_SIZE,</span><br><span class="line">            hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of CNN(</span></span><br><span class="line"><span class="comment">#   (conv): Conv1d(500, 50, kernel_size=(1,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=50, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h3 id="Training">Training</h3><p>接下来，利用到<a href="http://neo1989.net/Way2AI/Way2AI-utilities/" title="PyTorch实现神经网络的基本套路">《PyTorch实现神经网络的基本套路》</a> 里介绍到Trainer类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">LEARNING_RATE = <span class="number">1e-3</span></span><br><span class="line">PATIENCE = <span class="number">5</span></span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, device, loss_fn=None, optimizer=None, scheduler=None)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set params</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.device = device</span><br><span class="line">        self.loss_fn = loss_fn</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self.scheduler = scheduler</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Train step."""</span></span><br><span class="line">        <span class="comment"># Set model to train mode</span></span><br><span class="line">        self.model.train()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over train batches</span></span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Step</span></span><br><span class="line">            batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">            inputs, targets = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">            self.optimizer.zero_grad()  <span class="comment"># Reset gradients</span></span><br><span class="line">            z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">            J = self.loss_fn(z, targets)  <span class="comment"># Define loss</span></span><br><span class="line">            J.backward()  <span class="comment"># Backward pass</span></span><br><span class="line">            self.optimizer.step()  <span class="comment"># Update weights</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Cumulative Metrics</span></span><br><span class="line">            loss += (J.detach().item() - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Validation or test step."""</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.eval()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line">        y_trues, y_probs = [], []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Step</span></span><br><span class="line">                batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">                inputs, y_true = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">                z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">                J = self.loss_fn(z, y_true).item()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Cumulative Metrics</span></span><br><span class="line">                loss += (J - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line">                y_trues.extend(y_true.cpu().numpy())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, np.vstack(y_trues), np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Prediction step."""</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.eval()</span><br><span class="line">        y_probs = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Forward pass w/ inputs</span></span><br><span class="line">                inputs, targets = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">                z = self.model(inputs)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, num_epochs, patience, train_dataloader, val_dataloader)</span>:</span></span><br><span class="line">        best_val_loss = np.inf</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">            <span class="comment"># Steps</span></span><br><span class="line">            train_loss = self.train_step(dataloader=train_dataloader)</span><br><span class="line">            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)</span><br><span class="line">            self.scheduler.step(val_loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Early stopping</span></span><br><span class="line">            <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">                best_val_loss = val_loss</span><br><span class="line">                best_model = self.model</span><br><span class="line">                _patience = patience  <span class="comment"># reset _patience</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                _patience -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> _patience:  <span class="comment"># 0</span></span><br><span class="line">                print(<span class="string">"Stopping early!"</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Logging</span></span><br><span class="line">            print(</span><br><span class="line">                <span class="string">f"Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> | "</span></span><br><span class="line">                <span class="string">f"train_loss: <span class="subst">&#123;train_loss:<span class="number">.5</span>f&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"val_loss: <span class="subst">&#123;val_loss:<span class="number">.5</span>f&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"lr: <span class="subst">&#123;self.optimizer.param_groups[<span class="number">0</span>][<span class="string">'lr'</span>]:<span class="number">.2</span>E&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"_patience: <span class="subst">&#123;_patience&#125;</span>"</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> best_model</span><br></pre></td></tr></table></figure><p>定义必要的组件，然后开始训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Defince Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer &amp; scheduler</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">    optimizer, mode=<span class="string">"min"</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train module</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, device=device, loss_fn=loss_fn,</span><br><span class="line">    optimizer=optimizer, scheduler=scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">best_model = trainer.train(</span><br><span class="line">    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 1 | train_loss: 0.86713, val_loss: 0.79795, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 2 | train_loss: 0.77799, val_loss: 0.79238, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 3 | train_loss: 0.77053, val_loss: 0.78976, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 4 | train_loss: 0.76625, val_loss: 0.78882, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 5 | train_loss: 0.76305, val_loss: 0.78799, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 6 | train_loss: 0.76027, val_loss: 0.78786, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 7 | train_loss: 0.75813, val_loss: 0.78810, lr: 1.00E-03, _patience: 4</span></span><br><span class="line"><span class="comment"># Epoch: 8 | train_loss: 0.75588, val_loss: 0.78725, lr: 1.00E-03, _patience: 5</span></span><br><span class="line"><span class="comment"># Epoch: 9 | train_loss: 0.75429, val_loss: 0.78740, lr: 1.00E-03, _patience: 4</span></span><br><span class="line"><span class="comment"># Epoch: 10 | train_loss: 0.75270, val_loss: 0.78747, lr: 1.00E-03, _patience: 3</span></span><br></pre></td></tr></table></figure><h3 id="Evaluaton">Evaluaton</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(y_true, y_pred, classes)</span>:</span></span><br><span class="line">    <span class="string">"""Per-class performance metrics."""</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">"overall"</span>: &#123;&#125;, <span class="string">"class"</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">"weighted"</span>)</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"precision"</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"recall"</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"f1"</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"num_samples"</span>] = np.float64(len(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">        performance[<span class="string">"class"</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">"precision"</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">"recall"</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">"f1"</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">"num_samples"</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions</span></span><br><span class="line">test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Determine performance</span></span><br><span class="line">performance = get_metrics(</span><br><span class="line">    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance[<span class="string">"overall"</span>], indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "precision": 0.7074944756886696,</span></span><br><span class="line"><span class="comment">#   "recall": 0.6868333333333333,</span></span><br><span class="line"><span class="comment">#   "f1": 0.6866617275444412,</span></span><br><span class="line"><span class="comment">#   "num_samples": 18000.0</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><p>保存一些必要的模型数据，以供后续能够完整的加载和使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save artifacts</span></span><br><span class="line">dir = Path(<span class="string">"cnn"</span>)</span><br><span class="line">dir.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">label_encoder.save(fp=Path(dir, <span class="string">"label_encoder.json"</span>))</span><br><span class="line">tokenizer.save(fp=Path(dir, <span class="string">'tokenizer.json'</span>))</span><br><span class="line">torch.save(best_model.state_dict(), Path(dir, <span class="string">"model.pt"</span>))</span><br><span class="line"><span class="keyword">with</span> open(Path(dir, <span class="string">'performance.json'</span>), <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    json.dump(performance, indent=<span class="number">2</span>, sort_keys=<span class="literal">False</span>, fp=fp)</span><br></pre></td></tr></table></figure><h3 id="Inference">Inference</h3><p>接下来看看如何利用模型进行新的推理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load artifacts</span></span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line">label_encoder = LabelEncoder.load(fp=Path(dir, <span class="string">"label_encoder.json"</span>))</span><br><span class="line">tokenizer = Tokenizer.load(fp=Path(dir, <span class="string">'tokenizer.json'</span>))</span><br><span class="line">model = CNN(</span><br><span class="line">    vocab_size=VOCAB_SIZE, num_filters=NUM_FILTERS, filter_size=FILTER_SIZE,</span><br><span class="line">    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line">model.load_state_dict(torch.load(Path(dir, <span class="string">"model.pt"</span>), map_location=device))</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># CNN(</span></span><br><span class="line"><span class="comment">#   (conv): Conv1d(500, 50, kernel_size=(1,), stride=(1,))</span></span><br><span class="line"><span class="comment">#   (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=50, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=4, bias=True)</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize trainer</span></span><br><span class="line">trainer = Trainer(model=model, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dataloader</span></span><br><span class="line">text = <span class="string">"China’s economic recovery fades as services, factory activity show weakness"</span></span><br><span class="line">sequences = tokenizer.texts_to_sequences([preprocess(text)])</span><br><span class="line"><span class="keyword">print</span> (tokenizer.sequences_to_texts(sequences))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># ['china economic &lt;UNK&gt; &lt;UNK&gt; services &lt;UNK&gt; &lt;UNK&gt; show &lt;UNK&gt;']</span></span><br><span class="line"></span><br><span class="line">X = [to_categorical(seq, num_classes=len(tokenizer)) <span class="keyword">for</span> seq <span class="keyword">in</span> sequences]</span><br><span class="line">y_filler = label_encoder.encode([label_encoder.classes[<span class="number">0</span>]]*len(X))</span><br><span class="line">dataset = Dataset(X=X, y=y_filler, max_filter_size=FILTER_SIZE)</span><br><span class="line">dataloader = dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line">y_prob = trainer.predict_step(dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">print</span> (label_encoder.decode(y_pred))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># ['Business']</span></span><br></pre></td></tr></table></figure><p>推理结果是 “China’s economic recovery fades as services, factory activity show weakness” 这篇文章属于 “Business” 这个分类，符合预期。</p><p>我们来看一下这个case的具体概率分布</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_probability_distribution</span><span class="params">(y_prob, classes)</span>:</span></span><br><span class="line">    <span class="string">"""Create a dict of class probabilities from an array."""</span></span><br><span class="line">    results = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, class_ <span class="keyword">in</span> enumerate(classes):</span><br><span class="line">        results[class_] = np.float64(y_prob[i])</span><br><span class="line">    sorted_results = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> sorted(</span><br><span class="line">        results.items(), key=<span class="keyword">lambda</span> item: item[<span class="number">1</span>], reverse=<span class="literal">True</span>)&#125;</span><br><span class="line">    <span class="keyword">return</span> sorted_results</span><br><span class="line"></span><br><span class="line"><span class="comment"># Class distributions</span></span><br><span class="line">prob_dist = get_probability_distribution(y_prob=y_prob[<span class="number">0</span>], classes=label_encoder.classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(prob_dist, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "Business": 0.7551461458206177,</span></span><br><span class="line"><span class="comment">#   "World": 0.23087970912456512,</span></span><br><span class="line"><span class="comment">#   "Sci/Tech": 0.01362547930330038,</span></span><br><span class="line"><span class="comment">#   "Sports": 0.0003486045461613685</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>本篇给出了一个使用CNN对文本进行分类的完整示例，有很多细节需要深入学习。</p><p>但无论如何，先跑起来再说，在战斗中学习战斗。</p>]]></content>
    
    <summary type="html">
    
      先上手再说。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT开放函数调用能力 · 好用到震惊！</title>
    <link href="https://neo1989.net/Notes/NOTE-openai-function-calling/"/>
    <id>https://neo1989.net/Notes/NOTE-openai-function-calling/</id>
    <published>2023-06-16T04:06:39.000Z</published>
    <updated>2023-06-16T05:41:43.132Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>ChatGPT已经自带函数调用能力了，本文给了一个简单的示例。</p><h2 id="回顾">回顾</h2><p>笔者曾经在<a href="http://neo1989.net/Notes/NOTE-langchain-3/" title="LangChain | 快速释放LLMs的能力">LangChain</a>系列文章里交代利用LangChain赋予ChatGPT上网的能力。</p><p>然而OpenAI官方在<a href="https://openai.com/blog/function-calling-and-other-api-updates" target="_blank" rel="noopener" title="function-calling-and-other-api-updates">June 13, 2023</a>的更新里提出了function calling的能力，可以说在这个方向上直接灭掉了LangChain。</p><p>先看一下官方有哪些更新。<br><img src="//s3.mindex.xyz/blog/Courses/68e512444d547a4c6727d27d049050f4.png" alt=""></p><ul><li>在Chat Completions API中提供了新的函数调用能力</li><li><code>gpt-4</code> 和 <code>gpt-3.5-turbo</code> 模型的小版本迭代</li><li><code>gpt-3.5-turbo</code> 扩展到了4倍（16k）的上下文的能力</li><li>SOTA embeddings 模型降价 75%</li><li><code>gpt-3.5-turbo</code> 降价25%</li><li><code>gpt-3.5-turbo-0301</code> 和 <code>gpt-4-0314</code> 的退役时间</li></ul><p>而最令人激动的，实属 <code>function calling</code></p><h2 id="一个示范">一个示范</h2><p>如下图，依然是用人话要股票信息，能够直接给出df数据（完成函数调用）。<br><img src="//s3.mindex.xyz/blog/Courses/ec1319f72a0c54c58a3d8080a9231041.png" alt=""></p><h2 id="如何实现">如何实现</h2><p>简单到令人发指。</p><p>首先，只需定义functions manifest，如下示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">functions = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"get_stock_a"</span>,</span><br><span class="line">        <span class="string">"description"</span>: <span class="string">"获取指定A股股票一段时间内的量价信息"</span>,</span><br><span class="line">        <span class="string">"parameters"</span>: &#123;</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">            <span class="string">"properties"</span>: &#123;</span><br><span class="line">                <span class="string">"stock_name"</span>: &#123;</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"具体的股票名称或代号，如贵州茅台、中国移动"</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">"start_date"</span>: &#123;</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"开始日期，格式为2023-01-01"</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                 <span class="string">"end_date"</span>: &#123;</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"结束日期，格式为2023-01-01"</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">"required"</span>: [<span class="string">"stock_name"</span>, <span class="string">"start_date"</span>, <span class="string">"end_date"</span>],</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>当然这个是不能乱来的，需要遵循一定的规则，具体需要参考官方<a href="https://swagger.io/specification/" target="_blank" rel="noopener" title="OpenAPI Specification">specification</a></p><p>然后，实现你的自定义方法，这个示例就是实现 <code>get_stock_a</code> 方法，以获取指定A股股票的量价数据。这里我不给出具体实现，有兴趣私聊。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stock_a</span><span class="params">(stock_name, start_date, end_date)</span>:</span></span><br><span class="line">    <span class="string">""" 获取指定A股股票的量价数据 """</span></span><br><span class="line">    print(<span class="string">f"stock_name: <span class="subst">&#123;stock_name&#125;</span>, start_date: <span class="subst">&#123;start_date&#125;</span>, end_date: <span class="subst">&#123;end_date&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><p>接着，只需在调用Chat Completions API时候，把functions带上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">requests.post(<span class="string">f"<span class="subst">&#123;openai.api_base&#125;</span>/chat/completions"</span>, headers=headers,</span><br><span class="line">    json=&#123;</span><br><span class="line">        <span class="string">"model"</span>: GPT_MODEL,  <span class="comment"># 'gpt-3.5-turbo-0613' or 'gpt-4-0613'</span></span><br><span class="line">        <span class="string">"messages"</span>: messages,</span><br><span class="line">        <span class="string">"functions"</span>: functions</span><br><span class="line">    &#125;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>最后，你会看到类似下面这样的返回，完成一点解析和调用的动作，这个事就成了。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"role"</span>: <span class="string">"assistant"</span>,</span><br><span class="line">    <span class="attr">"content"</span>: <span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"function_call"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"get_stock_a"</span>,</span><br><span class="line">        <span class="attr">"arguments"</span>: <span class="string">"&#123;\n  \"stock_name\": \"招商银行\",\n  \"start_date\": \"2023-05-01\",\n  \"end_date\": \"2023-06-01\"\n&#125;"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="完整示例">完整示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">GPT_MODEL = <span class="string">"gpt-3.5-turbo-0613"</span></span><br><span class="line"></span><br><span class="line">openai.api_key = <span class="string">""</span>  <span class="comment"># 你的密钥</span></span><br><span class="line">openai.api_base = <span class="string">"https://api.openai.com/v1"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(messages, functions)</span>:</span></span><br><span class="line">    headers = &#123;<span class="string">"Content-Type"</span>: <span class="string">"application/json"</span>, <span class="string">"Authorization"</span>: <span class="string">f"Bearer <span class="subst">&#123;openai.api_key&#125;</span>"</span>&#125;</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.post(<span class="string">f"<span class="subst">&#123;openai.api_base&#125;</span>/chat/completions"</span>, headers=headers,</span><br><span class="line">            json=&#123;</span><br><span class="line">                <span class="string">"model"</span>: GPT_MODEL,</span><br><span class="line">                <span class="string">"messages"</span>: messages,</span><br><span class="line">                <span class="string">"functions"</span>: functions</span><br><span class="line">            &#125;,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> response.json()[<span class="string">"choices"</span>][<span class="number">0</span>][<span class="string">"message"</span>]</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">"Unable to generate ChatCompletion response"</span>)</span><br><span class="line">        print(<span class="string">f"Exception: <span class="subst">&#123;e&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">functions = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"get_stock_a"</span>,</span><br><span class="line">        <span class="string">"description"</span>: <span class="string">"获取指定A股股票一段时间内的量价信息"</span>,</span><br><span class="line">        <span class="string">"parameters"</span>: &#123;</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">            <span class="string">"properties"</span>: &#123;</span><br><span class="line">                <span class="string">"stock_name"</span>: &#123;</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"具体的股票名称或代号，如贵州茅台、中国移动"</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">"start_date"</span>: &#123;</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"开始日期，格式为2023-01-01"</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                 <span class="string">"end_date"</span>: &#123;</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"结束日期，格式为2023-01-01"</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">"required"</span>: [<span class="string">"stock_name"</span>, <span class="string">"start_date"</span>, <span class="string">"end_date"</span>],</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stock_a</span><span class="params">(stock_name, start_date, end_date)</span>:</span></span><br><span class="line">    print(<span class="string">f"stock_name: <span class="subst">&#123;stock_name&#125;</span>, start_date: <span class="subst">&#123;start_date&#125;</span>, end_date: <span class="subst">&#123;end_date&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chat_call</span><span class="params">(message)</span>:</span></span><br><span class="line">    messages = [&#123;<span class="string">"role"</span>: <span class="string">"system"</span>, <span class="string">"content"</span>: <span class="string">"不要对函数中应该填入的数值作出自作主张的假设。如果用户的要求不够明确，要求澄清。"</span>&#125;]</span><br><span class="line">    messages.append(&#123;<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: message&#125;)</span><br><span class="line">    r = call(messages, functions)</span><br><span class="line"></span><br><span class="line">    fcall = r[<span class="string">"function_call"</span>]</span><br><span class="line">    <span class="keyword">return</span> eval(fcall[<span class="string">"name"</span>])(**json.loads(fcall[<span class="string">"arguments"</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">chat_call(<span class="string">"2023年5月1日到6月1日，招商银行的A股量价给我一份"</span>)</span><br></pre></td></tr></table></figure><p>如果你足够幸运，你将看到这样一串文本: “stock_name: 招商银行, start_date: 2023-05-01, end_date: 2023-06-01”</p><h2 id="Ending">Ending</h2><p>不多说了，黄老板已经说过了，“跑起来掠食，或是努力奔跑免得成为掠食者的食物”。</p>]]></content>
    
    <summary type="html">
    
      Function calling capability in the Chat Completions API.
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · PyTorch实现神经网络的基本套路</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-utilities/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-utilities/</id>
    <published>2023-06-15T05:53:07.000Z</published>
    <updated>2023-06-27T11:03:55.252Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>本文简单交代了神经网络的基本套路以及部分实用组件，以简化开发过程。</p><h2 id="Set-up">Set up</h2><p>通常我们需要为重复实验设置很多seed，所以我们可以将其打包到一个函数里。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_seeds</span><span class="params">(seed=<span class="number">1024</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Set seeds for reproducibility."""</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    touch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)  <span class="comment"># multi-GPU</span></span><br><span class="line"></span><br><span class="line">set_seeds(seed=<span class="number">1024</span>)</span><br></pre></td></tr></table></figure><h2 id="Device">Device</h2><p>当我们有大型数据集和更大的模型要训练时，我们可以通过在 GPU 上并行化张量操作来加速。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cuda = <span class="literal">True</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span>(torch.cuda.is_available() <span class="keyword">and</span> cuda) <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">torch.set_default_tensor_type(&#123;<span class="string">"cuda"</span>: <span class="string">"torch.cuda.FloatTensor"</span>, <span class="string">"cpu"</span>: <span class="string">"torch.FloatTensor"</span>&#125;.get(str(device)))</span><br></pre></td></tr></table></figure><h2 id="Load-data">Load data</h2><p>这里依然使用前文引入的螺旋数据作为演示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">url = <span class="string">"http://s3.mindex.xyz/datasets/9378f64fc8dd2817e4c92be0a3bae8e7.csv"</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>) <span class="comment"># load</span></span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>) <span class="comment"># shuffle</span></span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Data shapes</span></span><br><span class="line">X = df[[<span class="string">"X1"</span>, <span class="string">"X2"</span>]].values</span><br><span class="line">y = df[<span class="string">"color"</span>].values</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X: "</span>, np.shape(X))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: "</span>, np.shape(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X:  (1500, 2)</span></span><br><span class="line"><span class="comment"># y:  (1500,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize data</span></span><br><span class="line">plt.title(<span class="string">"Generated non-linear data"</span>)</span><br><span class="line">colors = &#123;<span class="string">"c1"</span>: <span class="string">"red"</span>, <span class="string">"c2"</span>: <span class="string">"yellow"</span>, <span class="string">"c3"</span>: <span class="string">"blue"</span>&#125;</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=[colors[_y] <span class="keyword">for</span> _y <span class="keyword">in</span> y], edgecolors=<span class="string">"k"</span>, s=<span class="number">25</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/147e784e6ecae3fd226abce4f3905550.png" alt=""></p><h2 id="Split-data">Split data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_test_split</span><span class="params">(X, y, train_size)</span>:</span></span><br><span class="line">    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)</span><br><span class="line">    X_test, X_val, y_test, y_val = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (1050, 2), y_train: (1050,)</span></span><br><span class="line"><span class="comment"># X_val: (225, 2), y_val: (225,)</span></span><br><span class="line"><span class="comment"># X_test: (225, 2), y_test: (225,)</span></span><br><span class="line"><span class="comment"># Sample point: [0.17003003 0.63079261] → c3</span></span><br></pre></td></tr></table></figure><h2 id="Label-encoding">Label encoding</h2><p>接下来定义一个 LabelEncoder 来将文本标签编码成唯一的索引。</p><p>这里不再使用 scikit-learn 的 LabelEncoder，因为我们希望能够以我们想要的方式保存和加载我们的实例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelEncoder</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Label encoder for tag labels."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, class_to_index=None)</span>:</span></span><br><span class="line">        self.class_to_index = class_to_index <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = list(self.class_to_index.keys())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.class_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"&lt;LabelEncoder(num_classes=<span class="subst">&#123;len(self)&#125;</span>&gt;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        classes = np.unique(y)</span><br><span class="line">        <span class="keyword">for</span> i, class_ <span class="keyword">in</span> enumerate(classes):</span><br><span class="line">            self.class_to_index[class_] = i</span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = list(self.class_to_index.keys())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        encoded = np.zeros((len(y)), dtype=int)</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            encoded[i] = self.class_to_index[item]</span><br><span class="line">        <span class="keyword">return</span> encoded</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        classes = []</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            classes.append(self.index_to_class[item])</span><br><span class="line">        <span class="keyword">return</span> classes</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;<span class="string">'class_to_index'</span>: self.class_to_index&#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(cls, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"r"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Encode</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">label_encoder.fit(y_train)</span><br><span class="line">label_encoder.class_to_index</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert labels to tokens</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">y_train = label_encoder.encode(y_train)</span><br><span class="line">y_val = label_encoder.encode(y_val)</span><br><span class="line">y_test = label_encoder.encode(y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> enumerate(counts)&#125;</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: c3</span></span><br><span class="line"><span class="comment"># y_train[0]: 2</span></span><br><span class="line"><span class="comment"># counts: [350 350 350]</span></span><br><span class="line"><span class="comment"># weights: &#123;0: 0.002857142857142857, 1: 0.002857142857142857, 2: 0.002857142857142857&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Standardize-data">Standardize data</h2><p>我们需要标准化我们的数据（零均值和单位方差），这样特定特征的大小就不会影响模型学习其权重的方式。</p><p>我们只对输入X进行标准化，因为我们的输出y是类值。</p><p>我们将编写自己的 StandardScaler 类，以便在推理过程中轻松保存和加载它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StandardScaler</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mean=None, std=None)</span>:</span></span><br><span class="line">        self.mean = np.array(mean)</span><br><span class="line">        self.std = np.array(std)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        self.mean = np.mean(X_train, axis=<span class="number">0</span>)</span><br><span class="line">        self.std = np.std(X_train, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scale</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (X - self.mean) / self.std</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">unscale</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (X * self.std) + self.mean</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;<span class="string">"mean"</span>: self.mean.tolist(), <span class="string">"std"</span>: self.std.tolist()&#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(cls, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"r"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the data (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler()</span><br><span class="line">X_scaler.fit(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[0]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[1]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_test[0]: mean: 0.0, std: 1.0</span></span><br><span class="line"><span class="comment"># X_test[1]: mean: -0.0, std: 1.0</span></span><br></pre></td></tr></table></figure><h2 id="DataLoader">DataLoader</h2><p>我们将把数据放在 Dataset 中，并使用 DataLoader 来有效地创建用于训练和验证的批次。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        self.X = X</span><br><span class="line">        self.y = y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"&lt;Dataset(N=<span class="subst">&#123;len(self)&#125;</span>)&gt;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        X = self.X[index]</span><br><span class="line">        y = self.y[index]</span><br><span class="line">        <span class="keyword">return</span> [X, y]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(self, batch)</span>:</span></span><br><span class="line">        <span class="string">"""Processing on a batch."""</span></span><br><span class="line">        <span class="comment"># Get inputs</span></span><br><span class="line">        batch = np.array(batch)</span><br><span class="line">        X = np.stack(batch[:, <span class="number">0</span>], axis=<span class="number">0</span>)</span><br><span class="line">        y = batch[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cast</span></span><br><span class="line">        X = torch.FloatTensor(X.astype(np.float32))</span><br><span class="line">        y = torch.LongTensor(y.astype(np.int32))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_dataloader</span><span class="params">(self, batch_size, shuffle=False, drop_last=False)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> torch.utils.data.DataLoader(</span><br><span class="line">            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,</span><br><span class="line">            shuffle=shuffle, drop_last=drop_last, pin_memory=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>事实上我们并不需要 collate_fn ，但我们可以让它透明（无副作用），因为当我想要对批处理做一些处理的时候，需要用到这个方法。(如：数据padding）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create datasets</span></span><br><span class="line">train_dataset = Dataset(X=X_train, y=y_train)</span><br><span class="line">val_dataset = Dataset(X=X_val, y=y_val)</span><br><span class="line">test_dataset = Dataset(X=X_test, y=y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Datasets:\n"</span></span><br><span class="line">       <span class="string">f"  Train dataset:<span class="subst">&#123;train_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">       <span class="string">f"  Val dataset: <span class="subst">&#123;val_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">       <span class="string">f"  Test dataset: <span class="subst">&#123;test_dataset.__str__()&#125;</span>\n"</span></span><br><span class="line">       <span class="string">"Sample point:\n"</span></span><br><span class="line">       <span class="string">f"  X: <span class="subst">&#123;train_dataset[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>\n"</span></span><br><span class="line">       <span class="string">f"  y: <span class="subst">&#123;train_dataset[<span class="number">0</span>][<span class="number">1</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Datasets:</span></span><br><span class="line"><span class="comment">#   Train dataset: &lt;Dataset(N=1050)&gt;</span></span><br><span class="line"><span class="comment">#   Val dataset: &lt;Dataset(N=225)&gt;</span></span><br><span class="line"><span class="comment">#   Test dataset: &lt;Dataset(N=225)&gt;</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: [-1.47355106 -1.67417243]</span></span><br><span class="line"><span class="comment">#   y: 0</span></span><br></pre></td></tr></table></figure><p>之前的文章中都是利用全部的数据进行梯度计算，然而更标准的做法是 <strong>mini-batch</strong> 随机梯度下降，也就是将样本分成多个只有 n(BATCH_SIZE) 个样本的 mini-batch。这就是 Dataloader 派上用场的地方。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create dataloaders</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_dataloader = train_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">val_dataloader = val_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">test_dataloader = test_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">batch_X, batch_y = next(iter(train_dataloader))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Sample batch:\n"</span></span><br><span class="line">       <span class="string">f"  X: <span class="subst">&#123;list(batch_X.size())&#125;</span>\n"</span></span><br><span class="line">       <span class="string">f"  y: <span class="subst">&#123;list(batch_y.size())&#125;</span>\n"</span></span><br><span class="line">       <span class="string">"Sample point:\n"</span></span><br><span class="line">       <span class="string">f"  X: <span class="subst">&#123;batch_X[<span class="number">0</span>]&#125;</span>\n"</span></span><br><span class="line">       <span class="string">f"  y: <span class="subst">&#123;batch_y[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Sample batch:</span></span><br><span class="line"><span class="comment">#   X: [64, 2]</span></span><br><span class="line"><span class="comment">#   y: [64]</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: tensor([ 0.4535, -0.3570], dtype=torch.float64)</span></span><br><span class="line"><span class="comment">#   y: 0</span></span><br></pre></td></tr></table></figure><h2 id="Model">Model</h2><p>我们需要定义一个模型，以便继续给出训练阶段的实用组件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>]  <span class="comment"># 2D</span></span><br><span class="line">HIDDEN_DIM = <span class="number">100</span></span><br><span class="line">DROPOUT_P = <span class="number">.01</span></span><br><span class="line">NUM_CLASSES = len(label_encoder.classes)</span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, dropout_p, num_classes)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = F.relu(self.fc1(x_in))</span><br><span class="line">        z = self.dropout(z)</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span class="line">            dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of MLP(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (droput): Dropout(p=0.01, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=3, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Trainer">Trainer</h2><p>之前的文章，我们一直在编写只使用循环来训练分割后的训练数据，然后在测试集上评估。</p><p>但实际工作中，我们会遵循下面这个过程：</p><ul><li>使用mini-batches进行训练</li><li>在验证集上评估损失，并更新超参</li><li>训练结束后，在测试集上评估模型</li></ul><p>所以我们需要创建 Trainer 类来组织这些过程。</p><p>首先，train_step 用来执行小批量数据训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">    self.model.train()</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">        batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]</span><br><span class="line">        inputs, targets = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">        self.optimizer.zero_grad()  <span class="comment"># reset gradients</span></span><br><span class="line">        z = self.model(inputs)  <span class="comment"># forward pass</span></span><br><span class="line">        J = self.loss_fn(z, targets)</span><br><span class="line">        J.backward()  <span class="comment"># backward pass</span></span><br><span class="line">        self.optimizer.step()  <span class="comment"># Update weights</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cumulative Metrics</span></span><br><span class="line">        loss += (j.detach().item() - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>然后 eval_step，用于验证</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">    self.model.eval()</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    y_trues, x_probs = [], []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.inference_model():</span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">            batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]</span><br><span class="line">            inputs, y_trye = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">            z = self.model(inputs)</span><br><span class="line">            J = self.loss_fn(z, y_true).item()</span><br><span class="line"></span><br><span class="line">            loss += (J - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Store outputs</span></span><br><span class="line">            y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">            y_probs.extend(y_prob)</span><br><span class="line">            y_trues.extend(y_true.cpu().numpy())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, np.vstack(y_trues), np.vstack(y_probs)</span><br></pre></td></tr></table></figure><p>最后 predict_step, 只是用来对数据进行预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">    self.model.eval()</span><br><span class="line">    y_prods = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.inference_model():</span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">            inputs, y_trye = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">            z = self.model(inputs)</span><br><span class="line">            y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">            y_probs.extend(y_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.vstack(y_probs)</span><br></pre></td></tr></table></figure><h2 id="LR-scheduler">LR scheduler</h2><p>我们将向优化器添加一个学习率调度器，以在训练期间调整我们的学习率。</p><p>有许多<a href="%22https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate%22" title="How to adjust learning rate">调度器</a>可供选择，但最受欢迎的是 ReduceLROnPlateau ，它在指标（例如：验证损失）停止改进的时候，减少学习率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the LR scheduler</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="string">"min"</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS * <span class="number">10</span>):</span><br><span class="line">    ...</span><br><span class="line">    train_loss = trainer.train_step(dataloader=train_dataloader)</span><br><span class="line">    val_loss, _, _ = trainer.eval_step(dataloader=val_dataloader)</span><br><span class="line">    scheduler.step(val_loss)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><h2 id="Early-stopping">Early stopping</h2><p>我们不应该拍脑袋训练足够多的epoch，而是应该有个明确的停止标准。</p><p>常见的停止标准，是模型达到一个期望的性能时，即停止训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Early stopping</span></span><br><span class="line"><span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">    best_val_loss = val_loss</span><br><span class="line">    best_model = trainer.model</span><br><span class="line">    _patience = patience  <span class="comment"># reset _patience</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    _patience -= <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> _patience:  <span class="comment"># 0</span></span><br><span class="line">    print(<span class="string">"Stopping early!"</span>)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><p>现在把上面这些放到一起</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">LEARNING_RATE = <span class="number">1e-2</span></span><br><span class="line">NUM_EPOCHS = <span class="number">100</span></span><br><span class="line">PATIENCE = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer &amp; scheduler</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">    optimizer, mode=<span class="string">"min"</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, device, loss_fn=None, optimizer=None, scheduler=None)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set params</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.device = device</span><br><span class="line">        self.loss_fn = loss_fn</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self.scheduler = scheduler</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Train step."""</span></span><br><span class="line">        <span class="comment"># Set model to train mode</span></span><br><span class="line">        self.model.train()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over train batches</span></span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Step</span></span><br><span class="line">            batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">            inputs, targets = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">            self.optimizer.zero_grad()  <span class="comment"># Reset gradients</span></span><br><span class="line">            z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">            J = self.loss_fn(z, targets)  <span class="comment"># Define loss</span></span><br><span class="line">            J.backward()  <span class="comment"># Backward pass</span></span><br><span class="line">            self.optimizer.step()  <span class="comment"># Update weights</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Cumulative Metrics</span></span><br><span class="line">            loss += (J.detach().item() - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Validation or test step."""</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.eval()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line">        y_trues, y_probs = [], []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Step</span></span><br><span class="line">                batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">                inputs, y_true = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">                z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">                J = self.loss_fn(z, y_true).item()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Cumulative Metrics</span></span><br><span class="line">                loss += (J - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line">                y_trues.extend(y_true.cpu().numpy())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, np.vstack(y_trues), np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_step</span><span class="params">(self, dataloader)</span>:</span></span><br><span class="line">        <span class="string">"""Prediction step."""</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.eval()</span><br><span class="line">        y_probs = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Forward pass w/ inputs</span></span><br><span class="line">                inputs, targets = batch[:<span class="number">-1</span>], batch[<span class="number">-1</span>]</span><br><span class="line">                z = self.model(inputs)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, num_epochs, patience, train_dataloader, val_dataloader)</span>:</span></span><br><span class="line">        best_val_loss = np.inf</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">            <span class="comment"># Steps</span></span><br><span class="line">            train_loss = self.train_step(dataloader=train_dataloader)</span><br><span class="line">            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)</span><br><span class="line">            self.scheduler.step(val_loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Early stopping</span></span><br><span class="line">            <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">                best_val_loss = val_loss</span><br><span class="line">                best_model = self.model</span><br><span class="line">                _patience = patience  <span class="comment"># reset _patience</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                _patience -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> _patience:  <span class="comment"># 0</span></span><br><span class="line">                print(<span class="string">"Stopping early!"</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Logging</span></span><br><span class="line">            print(</span><br><span class="line">                <span class="string">f"Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> | "</span></span><br><span class="line">                <span class="string">f"train_loss: <span class="subst">&#123;train_loss:<span class="number">.5</span>f&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"val_loss: <span class="subst">&#123;val_loss:<span class="number">.5</span>f&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"lr: <span class="subst">&#123;self.optimizer.param_groups[<span class="number">0</span>][<span class="string">'lr'</span>]:<span class="number">.2</span>E&#125;</span>, "</span></span><br><span class="line">                <span class="string">f"_patience: <span class="subst">&#123;_patience&#125;</span>"</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> best_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trainer module</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, device=device, loss_fn=loss_fn,</span><br><span class="line">    optimizer=optimizer, scheduler=scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">best_model = trainer.train(</span><br><span class="line">    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 1 | train_loss: 0.87488, val_loss: 0.66353, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 2 | train_loss: 0.66368, val_loss: 0.55748, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># Epoch: 67 | train_loss: 0.03002, val_loss: 0.02305, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 68 | train_loss: 0.03011, val_loss: 0.02309, lr: 1.00E-02, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 69 | train_loss: 0.02544, val_loss: 0.02227, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 70 | train_loss: 0.02680, val_loss: 0.02154, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 71 | train_loss: 0.02897, val_loss: 0.02162, lr: 1.00E-02, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 72 | train_loss: 0.02737, val_loss: 0.02190, lr: 1.00E-02, _patience: 1</span></span><br><span class="line"><span class="comment"># Stopping early!</span></span><br></pre></td></tr></table></figure><h2 id="Evaluation">Evaluation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(y_true, y_pred, classes)</span>:</span></span><br><span class="line">    <span class="string">"""Per-class performance metrics."""</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">"overall"</span>: &#123;&#125;, <span class="string">"class"</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">"weighted"</span>)</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"precision"</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"recall"</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"f1"</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"num_samples"</span>] = np.float64(len(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">        performance[<span class="string">"class"</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">"precision"</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">"recall"</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">"f1"</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">"num_samples"</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions</span></span><br><span class="line">test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Determine performance</span></span><br><span class="line">performance = get_metrics(</span><br><span class="line">    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance[<span class="string">"overall"</span>], indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "precision": 0.9956140350877192,</span></span><br><span class="line"><span class="comment">#   "recall": 0.9955555555555555,</span></span><br><span class="line"><span class="comment">#   "f1": 0.9955553580159118,</span></span><br><span class="line"><span class="comment">#   "num_samples": 225.0</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><h2 id="Saving-loading">Saving &amp; loading</h2><p>我们需要保存一些必要的模型数据，以供后续能够完整的加载和使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Save artifacts</span></span><br><span class="line">dir = Path(<span class="string">"mlp"</span>)</span><br><span class="line">dir.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">label_encoder.save(fp=Path(dir, <span class="string">"label_encoder.json"</span>))</span><br><span class="line">X_scaler.save(fp=Path(dir, <span class="string">"X_scaler.json"</span>))</span><br><span class="line">torch.save(best_model.state_dict(), Path(dir, <span class="string">"model.pt"</span>))</span><br><span class="line"><span class="keyword">with</span> open(Path(dir, <span class="string">'performance.json'</span>), <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    json.dump(performance, indent=<span class="number">2</span>, sort_keys=<span class="literal">False</span>, fp=fp)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load artifacts</span></span><br><span class="line">device = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line">label_encoder = LabelEncoder.load(fp=Path(dir, <span class="string">"label_encoder.json"</span>))</span><br><span class="line">X_scaler = StandardScaler.load(fp=Path(dir, <span class="string">"X_scaler.json"</span>))</span><br><span class="line">model = MLP(</span><br><span class="line">    input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span class="line">    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line">model.load_state_dict(torch.load(Path(dir, <span class="string">"model.pt"</span>), map_location=device))</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize trainer</span></span><br><span class="line">trainer = Trainer(model=model, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dataloader</span></span><br><span class="line">sample = [[<span class="number">0.106737</span>, <span class="number">0.114197</span>]] <span class="comment"># c1</span></span><br><span class="line">X = X_scaler.scale(sample)</span><br><span class="line">y_filler = label_encoder.encode([label_encoder.classes[<span class="number">0</span>]]*len(X))</span><br><span class="line">dataset = Dataset(X=X, y=y_filler)</span><br><span class="line">dataloader = dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line">y_prob = trainer.predict_step(dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line">label_encoder.decode(y_pred)</span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>本文给出了一个机器学习项目的基本组件， 事实上，还有一些其他的重要组成没有覆盖到。比如：</p><ul><li>文本序列化的Tokenizers</li><li>表征数据的Encoders</li><li>数据padding</li><li>实验跟踪及可视化结果</li><li>超惨优化</li><li>等等</li></ul><p>后续我们会继续学习，至少到这里，我们有了入门深度学习的基础了。</p>]]></content>
    
    <summary type="html">
    
      先上手再说。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 神经网络 (二)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-neural-networks-2/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-neural-networks-2/</id>
    <published>2023-06-12T14:56:13.000Z</published>
    <updated>2023-06-14T05:21:05.688Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>接上篇，本文使用PyTorch实现一个相同的神经网络模型。</p><h2 id="Model">Model</h2><p>我们将使用两个线性连接层，并在前向传播中添加ReLU激活函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, num_classes)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = F.relu(self.fc1(x_in))</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initalize model</span></span><br><span class="line">model = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)</span><br><span class="line">print(model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of MLP(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=3, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><p>训练模型的代码跟之前学到的逻辑回归几乎没有区别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">LEARNING_RATE = <span class="number">1e-2</span></span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values()))</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_fn</span><span class="params">(y_pred, y_true)</span>:</span></span><br><span class="line">    n_correct = torch.eq(y_pred, y_true).sum().item()</span><br><span class="line">    accuarcy = (n_correct / len(y_pred)) * <span class="number">100</span></span><br><span class="line">    <span class="keyword">return</span> accuarcy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.LongTensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS * <span class="number">10</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        predictions = y_pred.max(dim=<span class="number">1</span>)[<span class="number">1</span>]  <span class="comment"># class</span></span><br><span class="line">        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 0.09, accuracy: 98.6</span></span><br><span class="line"><span class="comment"># Epoch: 10 | loss: 0.06, accuracy: 99.0</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.05, accuracy: 99.2</span></span><br><span class="line"><span class="comment"># Epoch: 30 | loss: 0.04, accuracy: 99.6</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.03, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 50 | loss: 0.03, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 60 | loss: 0.03, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 70 | loss: 0.02, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 80 | loss: 0.02, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 90 | loss: 0.02, accuracy: 99.7</span></span><br></pre></td></tr></table></figure><h2 id="Evaluation">Evaluation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predictiions</span></span><br><span class="line">y_prob = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line">y_pred = y_prob.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 1.0,</span></span><br><span class="line"><span class="comment">#     "recall": 1.0,</span></span><br><span class="line"><span class="comment">#     "f1": 1.0,</span></span><br><span class="line"><span class="comment">#     "num_samples": 225.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "c1": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 1.0,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c2": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 1.0,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c3": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 1.0,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/3ac0940bf9a117acde72a4d36241c2b8.png" alt=""></p><p>如你所见，PyTorch的直观和易用性能让我的学习曲线相对平缓。</p><p>需要我们编写的核心代码，只集中在定义模型、定义损失函数和优化器、定义训练循环、验证和测试这个四个部分。</p><p>当然，还有许多细节需要考虑，比如说数据预处理、模型的保存和加载、使用GPU等。</p><h2 id="Initializing-weights">Initializing weights</h2><p>到目前为止，我们都是使用了一个很小的随机值初始化权重，这其实不是让模型在训练阶段能够收敛的最佳方式。</p><p>我们的目标是初始化一个合适的权重，使得我们激活的输出不会消失或者爆炸，因为这两种情况都会阻碍模型收敛。事实上我们可以<a href="https://pytorch.org/docs/stable/nn.init.html" target="_blank" rel="noopener" title="nn.init">自定义权重初始化</a>方法。目前比较常用的是<a href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_normal_" target="_blank" rel="noopener" title="nn.init.xavier_normal_">Xavier初始化方法</a>和<a href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_" target="_blank" rel="noopener" title="nn.init.kaiming_normal_">He初始化方法</a>。</p><p>事实上PyTorch的Linear类默认使用了kaiming_uniform_初始化方法，相关源代码看<a href="https://github.com/pytorch/pytorch/blob/af7dc23124a6e3e7b8af0637e3b027f3a8b3fb76/torch/nn/modules/linear.py#L101" target="_blank" rel="noopener" title="Linear源码">这里</a>，后续我们会学习到更高级的优化收敛的策略如batch normalization。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, num_classes)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        init.xavier_normal_(self.fc1.weight, gain=init.calculate_gain(<span class="string">"relu"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = F.relu(self.fc1(x_in))</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure><h2 id="Dropout">Dropout</h2><p>能够让我们的模型表现的好的最好的技术是增加数据，但这并不总是一个可选项。幸运的是，还有有一些帮助模型更健壮的其他办法，如正则化、dropout等。</p><p>Dropout是在训练过程中允许我们将神经元的输出置0的技术。由于我们每批次都会丢弃一组不同的神经元，所以Dropout可以作为一种采样策略，防止过拟合。</p><p><img src="//s3.mindex.xyz/blog/Courses/2c301aaf51dcbdc7fb1556b1cf547228.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">DROPOUT_P = <span class="number">0.1</span> <span class="comment"># percentage of weights that are dropped each pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, dropout_p, num_classes)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p) <span class="comment"># dropout</span></span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        init.xavier_normal(self.fc1.weight, gain=init.calculate_gain(<span class="string">"relu"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = F.relu(self.fc1(x_in))</span><br><span class="line">        z = self.dropout(z) <span class="comment"># dropout</span></span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span class="line">            dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of MLP(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=3, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Overfitting">Overfitting</h2><p>虽然神经网络很擅长捕捉非线性关系，但它们非常容易对训练数据进行过度拟合，且无法对测试数据进行归纳。</p><p>看看下面的例子，我们使用完全随机的数据，并试图拟合含 $2 * N * C + D $ (其中N=样本数，C=标签，D表示输入纬度) 隐藏神经元的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line">NUM_EPOCHS = <span class="number">500</span></span><br><span class="line">NUM_SAMPLES_PER_CLASS = <span class="number">50</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line">HIDDEN_DIM = <span class="number">2</span> * NUM_SAMPLES_PER_CLASS * NUM_CLASSES + INPUT_DIM <span class="comment"># 2*N*C + D</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate random data</span></span><br><span class="line">X = np.random.rand(NUM_SAMPLES_PER_CLASS * NUM_CLASSES, INPUT_DIM)</span><br><span class="line">y = np.array([[i] * NUM_SAMPLES_PER_CLASS <span class="keyword">for</span> i <span class="keyword">in</span> range(NUM_CLASSES)]).reshape(<span class="number">-1</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X: "</span>, format(np.shape(X)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: "</span>, format(np.shape(y)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X:  (150, 2)</span></span><br><span class="line"><span class="comment"># y:  (150,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (105, 2), y_train: (105,)</span></span><br><span class="line"><span class="comment"># X_val: (23, 2), y_val: (23,)</span></span><br><span class="line"><span class="comment"># X_test: (22, 2), y_test: (22,)</span></span><br><span class="line"><span class="comment"># Sample point: [0.51102894 0.55377194] → 2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the inputs (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler().fit(X_train)</span><br><span class="line">X_train = X_scaler.transform(X_train)</span><br><span class="line">X_val = X_scaler.transform(X_val)</span><br><span class="line">X_test = X_scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.LongTensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span class="line">            dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of MLP(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=302, bias=True)</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=302, out_features=3, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">50</span>==<span class="number">0</span>:</span><br><span class="line">        predictions = y_pred.max(dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># class</span></span><br><span class="line">        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 1.07, accuracy: 43.8</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.94, accuracy: 52.4</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.89, accuracy: 55.2</span></span><br><span class="line"><span class="comment"># Epoch: 60 | loss: 0.87, accuracy: 49.5</span></span><br><span class="line"><span class="comment"># Epoch: 80 | loss: 0.82, accuracy: 63.8</span></span><br><span class="line"><span class="comment"># Epoch: 100 | loss: 0.84, accuracy: 62.9</span></span><br><span class="line"><span class="comment"># Epoch: 120 | loss: 0.75, accuracy: 63.8</span></span><br><span class="line"><span class="comment"># Epoch: 140 | loss: 0.77, accuracy: 60.0</span></span><br><span class="line"><span class="comment"># Epoch: 160 | loss: 0.75, accuracy: 60.0</span></span><br><span class="line"><span class="comment"># Epoch: 180 | loss: 0.75, accuracy: 66.7</span></span><br><span class="line"><span class="comment"># Epoch: 200 | loss: 0.75, accuracy: 67.6</span></span><br><span class="line"><span class="comment"># Epoch: 220 | loss: 0.69, accuracy: 68.6</span></span><br><span class="line"><span class="comment"># Epoch: 240 | loss: 0.75, accuracy: 65.7</span></span><br><span class="line"><span class="comment"># Epoch: 260 | loss: 0.73, accuracy: 71.4</span></span><br><span class="line"><span class="comment"># Epoch: 280 | loss: 0.73, accuracy: 69.5</span></span><br><span class="line"><span class="comment"># Epoch: 300 | loss: 0.71, accuracy: 62.9</span></span><br><span class="line"><span class="comment"># Epoch: 320 | loss: 0.68, accuracy: 69.5</span></span><br><span class="line"><span class="comment"># Epoch: 340 | loss: 0.74, accuracy: 65.7</span></span><br><span class="line"><span class="comment"># Epoch: 360 | loss: 0.68, accuracy: 71.4</span></span><br><span class="line"><span class="comment"># Epoch: 380 | loss: 0.78, accuracy: 63.8</span></span><br><span class="line"><span class="comment"># Epoch: 400 | loss: 0.69, accuracy: 66.7</span></span><br><span class="line"><span class="comment"># Epoch: 420 | loss: 0.75, accuracy: 67.6</span></span><br><span class="line"><span class="comment"># Epoch: 440 | loss: 0.76, accuracy: 69.5</span></span><br><span class="line"><span class="comment"># Epoch: 460 | loss: 0.71, accuracy: 67.6</span></span><br><span class="line"><span class="comment"># Epoch: 480 | loss: 0.66, accuracy: 66.7</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">y_prob = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line">y_pred = y_prob.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 0.45959595959595956,</span></span><br><span class="line"><span class="comment">#     "recall": 0.45454545454545453,</span></span><br><span class="line"><span class="comment">#     "f1": 0.4512987012987013,</span></span><br><span class="line"><span class="comment">#     "num_samples": 22.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "c1": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.5,</span></span><br><span class="line"><span class="comment">#       "recall": 0.375,</span></span><br><span class="line"><span class="comment">#       "f1": 0.42857142857142855,</span></span><br><span class="line"><span class="comment">#       "num_samples": 8.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c2": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.4444444444444444,</span></span><br><span class="line"><span class="comment">#       "recall": 0.5714285714285714,</span></span><br><span class="line"><span class="comment">#       "f1": 0.5,</span></span><br><span class="line"><span class="comment">#       "num_samples": 7.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c3": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.42857142857142855,</span></span><br><span class="line"><span class="comment">#       "recall": 0.42857142857142855,</span></span><br><span class="line"><span class="comment">#       "f1": 0.42857142857142855,</span></span><br><span class="line"><span class="comment">#       "num_samples": 7.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/8b43366da170668bdeaee171c279362d.png" alt=""></p><p>正如你所见，虽然模型在训练集上做到了接近70%的准确率，但模型在测试集上的表现并不能令人满意。</p><p>重要的是我们需要进行实验，从不合适（高偏差）的简单模型开始，并试图改进到良好的拟合，以及避免过拟合。</p><p><img src="//s3.mindex.xyz/blog/Courses/9a3b5a8d871020ccda41430ca7958bc1.png" alt=""></p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Neural networks - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      PyTorch实现一个神经网络。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 神经网络 (一)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-neural-networks-1/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-neural-networks-1/</id>
    <published>2023-06-01T08:16:24.000Z</published>
    <updated>2023-06-08T15:33:37.054Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>本章的目标依然是学习一种模型 $\hat{y}$，能准确对输入 $X$ 及对应的输出 $y$ 进行建模。</p><p>你会注意到神经网络只是我们迄今为止看到的广义线性方法的扩展，但具有非线性激活函数，因为我们的数据是高度非线性的。</p><p><img src="//s3.mindex.xyz/blog/Courses/908c174b73d8c8bb1c1ec3ba9e4cf885.png" alt=""></p><p>$$<br>z_1 = XW_1<br>$$</p><p>$$<br>a_1 = f(z_1)<br>$$</p><p>$$<br>z_2 = a_1W_2<br>$$</p><p>$$<br>\hat{y} = softmax(x)<br>$$</p><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">解释</th></tr></thead><tbody><tr><td style="text-align:center">$N$</td><td style="text-align:center">样本数</td></tr><tr><td style="text-align:center">$D$</td><td style="text-align:center">特征数</td></tr><tr><td style="text-align:center">$H$</td><td style="text-align:center">隐藏神经元</td></tr><tr><td style="text-align:center">$C$</td><td style="text-align:center">标签数</td></tr><tr><td style="text-align:center">$W_1$</td><td style="text-align:center">第一层的权重</td></tr><tr><td style="text-align:center">$z_1$</td><td style="text-align:center">第一层的输出</td></tr><tr><td style="text-align:center">$f$</td><td style="text-align:center">非线性激活函数</td></tr><tr><td style="text-align:center">$a_1$</td><td style="text-align:center">第一层的激活值</td></tr><tr><td style="text-align:center">$W_2$</td><td style="text-align:center">第二层的权重</td></tr><tr><td style="text-align:center">$z_2$</td><td style="text-align:center">第二层的输出</td></tr></tbody></table><h2 id="Set-up">Set up</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">np.random.seed(SEED)</span><br><span class="line">random.seed(SEED)</span><br></pre></td></tr></table></figure><h3 id="Load-data">Load data</h3><p>这里准备了一份非线性可分的螺旋数据来学习。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">url = <span class="string">"http://s3.mindex.xyz/datasets/9378f64fc8dd2817e4c92be0a3bae8e7.csv"</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>) <span class="comment"># load</span></span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>) <span class="comment"># shuffle</span></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/4022744de8e63b11599cdd95aab6ac62.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Data shapes</span></span><br><span class="line">X = df[[<span class="string">"X1"</span>, <span class="string">"X2"</span>]].values</span><br><span class="line">y = df[<span class="string">"color"</span>].values</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X: "</span>, np.shape(X))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: "</span>, np.shape(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X:  (1500, 2)</span></span><br><span class="line"><span class="comment"># y:  (1500,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize data</span></span><br><span class="line">plt.title(<span class="string">"Generated non-linear data"</span>)</span><br><span class="line">colors = &#123;<span class="string">"c1"</span>: <span class="string">"red"</span>, <span class="string">"c2"</span>: <span class="string">"yellow"</span>, <span class="string">"c3"</span>: <span class="string">"blue"</span>&#125;</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=[colors[_y] <span class="keyword">for</span> _y <span class="keyword">in</span> y], edgecolors=<span class="string">"k"</span>, s=<span class="number">25</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/147e784e6ecae3fd226abce4f3905550.png" alt=""></p><h3 id="Split-data">Split data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_test_split</span><span class="params">(X, y, train_size)</span>:</span></span><br><span class="line">    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)</span><br><span class="line">    X_test, X_val, y_test, y_val = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (1050, 2), y_train: (1050,)</span></span><br><span class="line"><span class="comment"># X_val: (225, 2), y_val: (225,)</span></span><br><span class="line"><span class="comment"># X_test: (225, 2), y_test: (225,)</span></span><br><span class="line"><span class="comment"># Sample point: [0.17003003 0.63079261] → c3</span></span><br></pre></td></tr></table></figure><h3 id="Label-encoding">Label encoding</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output vectorizer</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line"></span><br><span class="line"><span class="comment"># FIt on train date</span></span><br><span class="line">label_encoder = label_encoder.fit(y_train)</span><br><span class="line">classes = list(label_encoder.classes_)</span><br><span class="line">print(<span class="string">f"classes: <span class="subst">&#123;classes&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># classes: ['c1', 'c2', 'c3']</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert labels to tokens</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">y_train = label_encoder.transform(y_train)</span><br><span class="line">y_val = label_encoder.transform(y_val)</span><br><span class="line">y_test = label_encoder.transform(y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: c3</span></span><br><span class="line"><span class="comment"># y_train[0]: 2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> enumerate(counts)&#125;</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># counts: [350 350 350]</span></span><br><span class="line"><span class="comment"># weights: &#123;0: 0.002857142857142857, 1: 0.002857142857142857, 2: 0.002857142857142857&#125;</span></span><br></pre></td></tr></table></figure><h3 id="Standardize-data">Standardize data</h3><p>因为 $y$ 是类别值，所以我们只标准化 $X$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the data (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler().fit(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply scaler on training and test data (don't standardize outputs for classification)</span></span><br><span class="line">X_train = X_scaler.transform(X_train)</span><br><span class="line">X_val = X_scaler.transform(X_val)</span><br><span class="line">X_test = X_scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[0]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[1]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_test[0]: mean: 0.0, std: 1.0</span></span><br><span class="line"><span class="comment"># X_test[1]: mean: -0.0, std: 1.0</span></span><br></pre></td></tr></table></figure><h2 id="Linear-model">Linear model</h2><p>在尝试使用神经网络之前，为了解释激活函数，我们先用前面学到的逻辑回归模型来学习我们的数据。</p><p>你会发现一个用线性激活函数的线性模型对我们的数据来说并不是合适的。</p><h3 id="Model-Train">Model &amp; Train</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>] <span class="comment"># X is 2-dimensional</span></span><br><span class="line">HIDDEN_DIM = <span class="number">100</span></span><br><span class="line">NUM_CLASSES = len(classes) <span class="comment"># 3 classes</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, num_classes)</span>:</span></span><br><span class="line">        super(LinearModel, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = self.fc1(x_in) <span class="comment"># linear activation</span></span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = LinearModel(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)</span><br><span class="line"></span><br><span class="line">LEARNING_RATE = <span class="number">1e-2</span></span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values()))</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_fn</span><span class="params">(y_pred, y_true)</span>:</span></span><br><span class="line">    n_correct = torch.eq(y_pred, y_true).sum().item()</span><br><span class="line">    accuracy = (n_correct / len(y_pred)) * <span class="number">100</span></span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.LongTensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">1</span>==<span class="number">0</span>:</span><br><span class="line">        predictions = y_pred.max(dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># class</span></span><br><span class="line">        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 1.18, accuracy: 43.7</span></span><br><span class="line"><span class="comment"># Epoch: 1 | loss: 0.92, accuracy: 55.6</span></span><br><span class="line"><span class="comment"># Epoch: 2 | loss: 0.79, accuracy: 54.5</span></span><br><span class="line"><span class="comment"># Epoch: 3 | loss: 0.74, accuracy: 54.4</span></span><br><span class="line"><span class="comment"># Epoch: 4 | loss: 0.73, accuracy: 53.9</span></span><br><span class="line"><span class="comment"># Epoch: 5 | loss: 0.73, accuracy: 53.9</span></span><br><span class="line"><span class="comment"># Epoch: 6 | loss: 0.74, accuracy: 55.0</span></span><br><span class="line"><span class="comment"># Epoch: 7 | loss: 0.75, accuracy: 55.8</span></span><br><span class="line"><span class="comment"># Epoch: 8 | loss: 0.76, accuracy: 56.2</span></span><br><span class="line"><span class="comment"># Epoch: 9 | loss: 0.77, accuracy: 56.7</span></span><br></pre></td></tr></table></figure><h3 id="Evaluation">Evaluation</h3><p>我们来看一下这个线性模型在螺旋数据上的表现如何。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(y_true, y_pred, classes)</span>:</span></span><br><span class="line">    <span class="string">"""Per-class performance metrics."""</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">"overall"</span>: &#123;&#125;, <span class="string">"class"</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">"weighted"</span>)</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"precision"</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"recall"</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"f1"</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"num_samples"</span>] = np.float64(len(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">        performance[<span class="string">"class"</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">"precision"</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">"recall"</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">"f1"</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">"num_samples"</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">y_prob = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample probability: <span class="subst">&#123;y_prob[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">y_pred = y_prob.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample class: <span class="subst">&#123;y_pred[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># sample probability: tensor([0.3424, 0.0918, 0.5659], grad_fn=&lt;SelectBackward0&gt;)</span></span><br><span class="line"><span class="comment"># sample class: 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 0.5174825174825175,</span></span><br><span class="line"><span class="comment">#     "recall": 0.5155555555555555,</span></span><br><span class="line"><span class="comment">#     "f1": 0.5162093875662788,</span></span><br><span class="line"><span class="comment">#     "num_samples": 225.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "c1": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.5194805194805194,</span></span><br><span class="line"><span class="comment">#       "recall": 0.5333333333333333,</span></span><br><span class="line"><span class="comment">#       "f1": 0.5263157894736841,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c2": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.46153846153846156,</span></span><br><span class="line"><span class="comment">#       "recall": 0.48,</span></span><br><span class="line"><span class="comment">#       "f1": 0.47058823529411764,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c3": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.5714285714285714,</span></span><br><span class="line"><span class="comment">#       "recall": 0.5333333333333333,</span></span><br><span class="line"><span class="comment">#       "f1": 0.5517241379310344,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiclass_decision_boundary</span><span class="params">(model, X, y)</span>:</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].max() + <span class="number">0.1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].max() + <span class="number">0.1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.linspace(x_min, x_max, <span class="number">101</span>), np.linspace(y_min, y_max, <span class="number">101</span>))</span><br><span class="line">    cmap = plt.cm.Spectral</span><br><span class="line"></span><br><span class="line">    X_test = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()</span><br><span class="line">    y_pred = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line">    _, y_pred = y_pred.max(dim=<span class="number">1</span>)</span><br><span class="line">    y_pred = y_pred.reshape(xx.shape)</span><br><span class="line">    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.RdYlBu)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/e722233774a0fd9b91f4d77a3068287d.png" alt=""></p><h2 id="Activation-functions">Activation functions</h2><p>使用广义的线性方法产生了较差的结果，因为我们试图用线性激活函数去学习非线性数据。</p><p>所以我们需要一个可以能让模型学习到数据中的非线性的激活函数。有几种不同的选择，我们稍微探索一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fig size</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Data</span></span><br><span class="line">x = torch.arange(<span class="number">-5.</span>, <span class="number">5.</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sigmoid activation (constrain a value between 0 and 1.)</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Sigmoid activation"</span>)</span><br><span class="line">y = torch.sigmoid(x)</span><br><span class="line">plt.plot(x.numpy(), y.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tanh activation (constrain a value between -1 and 1.)</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">y = torch.tanh(x)</span><br><span class="line">plt.title(<span class="string">"Tanh activation"</span>)</span><br><span class="line">plt.plot(x.numpy(), y.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Relu (clip the negative values to 0)</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">y = F.relu(x)</span><br><span class="line">plt.title(<span class="string">"ReLU activation"</span>)</span><br><span class="line">plt.plot(x.numpy(), y.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show plots</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/499e55bee0461d4b7471247d91ec78b1.png" alt=""></p><p>ReLU激活函数$(max(0, z))$ 是目前为止用的最广泛的激活函数。但每个激活函数都有自己适用场景。比如：如果我们需要输出在0和1之间，那么sigmoid是合适的选择。</p><p>*（在某些情况下，ReLU函数也是不够的。例如，当神经元的输出大多为负时，激活函数的输出为0，这将导致神经元“死去”。为了减轻这种影响，我们可以降低学习率活着使用<a href="https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7" target="_blank" rel="noopener" title="ReLU">“ReLU变种”</a>。 如 Leaky ReLU 或 PRelu，它们会适当倾斜于神经元的负输出。）</p><h2 id="NumPy">NumPy</h2><p>现在，我们创建一个与逻辑回归模型完全相似的多层感知机，但包含一个学习数据中非线性的激活函数。</p><h3 id="Initialize-weights">Initialize weights</h3><p><strong>第一步</strong>: 随机初始化模型的权重$W$。（后面会介绍更有效的初始化策略）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize first layer's weights</span></span><br><span class="line">W1 = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, HIDDEN_DIM)</span><br><span class="line">b1 = np.zeros((<span class="number">1</span>, HIDDEN_DIM))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"W1: <span class="subst">&#123;W1.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"b1: <span class="subst">&#123;b1.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># W1: (2, 100)</span></span><br><span class="line"><span class="comment"># b1: (1, 100)</span></span><br></pre></td></tr></table></figure><h3 id="Model">Model</h3><p><strong>第二步</strong>: 讲输入 $X$ 送到模型中进行前向传播以得到网络的输出。</p><p>首先，我们将输入传给第一层。<br>$$<br>z_1 = XW_1<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># z1 = [NX2] · [2X100] + [1X100] = [NX100]</span></span><br><span class="line">z1 = np.dot(X_train, W1) + b1</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"z1: <span class="subst">&#123;z1.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># z1: (1050, 100)</span></span><br></pre></td></tr></table></figure><p>接下来，我们应用非线性激活函数Relu。<br>$$<br>a_1 = f(z_1)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply activation function</span></span><br><span class="line">a1 = np.maximum(<span class="number">0</span>, z1) <span class="comment"># ReLU</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"a_1: <span class="subst">&#123;a1.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># a_1: (1050, 100)</span></span><br></pre></td></tr></table></figure><p>然着我们将激活函数的输出传给第二层，以获得logit。<br>$$<br>z_2 = a_1W_2<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize second layer's weights</span></span><br><span class="line">W2 = <span class="number">0.01</span> * np.random.randn(HIDDEN_DIM, NUM_CLASSES)</span><br><span class="line">b2 = np.zeros((<span class="number">1</span>, NUM_CLASSES))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"W2: <span class="subst">&#123;W2.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"b2: <span class="subst">&#123;b2.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># W2: (100, 3)</span></span><br><span class="line"><span class="comment"># b2: (1, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># z2 = logits = [NX100] · [100X3] + [1X3] = [NX3]</span></span><br><span class="line">logits = np.dot(a1, W2) + b2</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"logits: <span class="subst">&#123;logits.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample: <span class="subst">&#123;logits[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># logits: (1050, 3)</span></span><br><span class="line"><span class="comment"># sample: [ 0.00017606 -0.0023457   0.00035913]</span></span><br></pre></td></tr></table></figure><p>之后，我们将应用softmax来获得网络的概率输出。</p><p>$$<br>\hat{y} = softmax(z_2)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normalization via softmax to obtain class probabilities</span></span><br><span class="line">exp_logits = np.exp(logits)</span><br><span class="line">y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_hat: <span class="subst">&#123;y_hat.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample: <span class="subst">&#123;y_hat[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_hat: (1050, 3)</span></span><br><span class="line"><span class="comment"># sample: [0.33359304 0.33275285 0.33365411]</span></span><br></pre></td></tr></table></figure><h3 id="Loss">Loss</h3><p><strong>第三步</strong>： 利用交叉熵计算我们分类任务的损失。<br>$$<br>J(\theta) = - \sum_i^K{log(\hat{y}_i)} = - \sum_i^K{log(\frac{e^{W_yX_i}}{\sum_j{e^{WX_i}}})}\\<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss</span></span><br><span class="line">correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])</span><br><span class="line">loss = np.sum(correct_class_logprobs) / len(y_train)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># loss: 1.10</span></span><br></pre></td></tr></table></figure><h3 id="Gradients">Gradients</h3><p><strong>第四步</strong> 计算损失函数 $J(\theta)$ 相对于权重的梯度。</p><p>对于$W_2$的梯度，与前篇逻辑回归的梯度相同，因为 $\hat{y} = softmax(z_2)$</p><p>$$<br>\begin{split}<br>\frac{\partial{J}}{\partial{W_{2j}}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{W_{2j}}}  \\<br>&amp;= - \frac{1}{\hat{y}} \frac{\partial{\hat{y}}}{\partial{W_{2j}}} \\<br>&amp;= - \frac{1}{\frac{e^{a_1 W_{2y}}}{\sum_j{e^{a_1 W}}}} \frac{\sum_j{e^{a_1 W}e^{a_1 W_{2y}} 0 - e^{a_1 W_{2y}} e^{a_1 W_{2j}} a_1 }}{(\sum_j{e^{a_1 W}})^2} \\<br>&amp;= \frac{a_1 e^{a_1 W_{2j}}}{\sum_j{e^{a_1 W}}} \\<br>&amp;= a_1 \hat{y}<br>\end{split}<br>$$</p><p>$$<br>\begin{split}<br>\frac{\partial{J}}{\partial{W_{2y}}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{W_{2y}}}  \\<br>&amp;= - \frac{1}{\hat{y}} \frac{\partial{\hat{y}}}{\partial{W_{2y}}} \\<br>&amp;= - \frac{1}{\frac{e^{a_1 W_{2y}}}{\sum_j{e^{a_1 W}}}} \frac{\sum_j{e^{a_1 W}e^{a_1 W_{2y}} a_1 - e^{W_{2y} a_1}e^{a_1 W_{2y}} a_1}}{(\sum_j{e^{a_1 W}})^2} = \frac{1}{\hat{y}} (a_1 \hat{y}^2 - a_1 \hat{y}) \\<br>&amp;= a_1 (\hat{y} - 1)<br>\end{split}<br>$$</p><p>对于 $W_1$ 的梯度计算有点棘手，因为我们必须要通过两组权重进行反向传播。</p><p>$$<br>\begin{split}<br>\frac{\partial{J}}{\partial{W_1}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{X}} \frac{\partial{X}}{\partial{z_1}}  \frac{\partial{z_1}}{\partial{W_1}} \\<br>&amp;= W_2 (\partial{\hat{y}})(\partial{ReLU})X<br>\end{split}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dJ/dW2</span></span><br><span class="line">dscores = y_hat</span><br><span class="line">dscores[range(len(y_hat)), y_train] -= <span class="number">1</span></span><br><span class="line">dscores /= len(y_train)</span><br><span class="line">dW2 = np.dot(a1.T, dscores)</span><br><span class="line">db2 = np.sum(dscores, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dJ/dW1</span></span><br><span class="line">dhidden = np.dot(dscores, W2.T)</span><br><span class="line">dhidden[a1 &lt;= <span class="number">0</span>] = <span class="number">0</span> <span class="comment"># ReLu backprop</span></span><br><span class="line">dW1 = np.dot(X_train.T, dhidden)</span><br><span class="line">db1 = np.sum(dhidden, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="Update-weights">Update weights</h3><p><strong>第五步</strong> 指定一个学习率来更新权重 $W$，惩罚错误的分类奖励正确的分类。<br>$$<br>W_i = W_i - \alpha \frac{\partial{J}}{\partial{W_i}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Update weights</span></span><br><span class="line">W1 += -LEARNING_RATE * dW1</span><br><span class="line">b1 += -LEARNING_RATE * db1</span><br><span class="line">W2 += -LEARNING_RATE * dW2</span><br><span class="line">b2 += -LEARNING_RATE * db2</span><br></pre></td></tr></table></figure><h3 id="Training">Training</h3><p><strong>第六步</strong>: 重复步骤 2 ~ 5，以最小化损失为目的来训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert tensors to NumPy arrays</span></span><br><span class="line">X_train = X_train.numpy()</span><br><span class="line">y_train = y_train.numpy()</span><br><span class="line">X_val = X_val.numpy()</span><br><span class="line">y_val = y_val.numpy()</span><br><span class="line">X_test = X_test.numpy()</span><br><span class="line">y_test = y_test.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random weights</span></span><br><span class="line">W1 = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, HIDDEN_DIM)</span><br><span class="line">b1 = np.zeros((<span class="number">1</span>, HIDDEN_DIM))</span><br><span class="line">W2 = <span class="number">0.01</span> * np.random.randn(HIDDEN_DIM, NUM_CLASSES)</span><br><span class="line">b2 = np.zeros((<span class="number">1</span>, NUM_CLASSES))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training loop</span></span><br><span class="line"><span class="keyword">for</span> epoch_num <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># First layer forward pass [NX2] · [2X100] = [NX100]</span></span><br><span class="line">    z1 = np.dot(X_train, W1) + b1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply activation function</span></span><br><span class="line">    a1 = np.maximum(<span class="number">0</span>, z1) <span class="comment"># ReLU</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># z2 = logits = [NX100] · [100X3] = [NX3]</span></span><br><span class="line">    logits = np.dot(a1, W2) + b2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Normalization via softmax to obtain class probabilities</span></span><br><span class="line">    exp_logits = np.exp(logits)</span><br><span class="line">    y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])</span><br><span class="line">    loss = np.sum(correct_class_logprobs) / len(y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># show progress</span></span><br><span class="line">    <span class="keyword">if</span> epoch_num%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Accuracy</span></span><br><span class="line">        y_pred = np.argmax(logits, axis=<span class="number">1</span>)</span><br><span class="line">        accuracy =  np.mean(np.equal(y_train, y_pred))</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch_num&#125;</span>, loss: <span class="subst">&#123;loss:<span class="number">.3</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.3</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dJ/dW2</span></span><br><span class="line">    dscores = y_hat</span><br><span class="line">    dscores[range(len(y_hat)), y_train] -= <span class="number">1</span></span><br><span class="line">    dscores /= len(y_train)</span><br><span class="line">    dW2 = np.dot(a1.T, dscores)</span><br><span class="line">    db2 = np.sum(dscores, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dJ/dW1</span></span><br><span class="line">    dhidden = np.dot(dscores, W2.T)</span><br><span class="line">    dhidden[a1 &lt;= <span class="number">0</span>] = <span class="number">0</span> <span class="comment"># ReLu backprop</span></span><br><span class="line">    dW1 = np.dot(X_train.T, dhidden)</span><br><span class="line">    db1 = np.sum(dhidden, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    W1 += <span class="number">-1e0</span> * dW1</span><br><span class="line">    b1 += <span class="number">-1e0</span> * db1</span><br><span class="line">    W2 += <span class="number">-1e0</span> * dW2</span><br><span class="line">    b2 += <span class="number">-1e0</span> * db2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0, loss: 1.098, accuracy: 0.519</span></span><br><span class="line"><span class="comment"># Epoch: 100, loss: 0.541, accuracy: 0.680</span></span><br><span class="line"><span class="comment"># Epoch: 200, loss: 0.305, accuracy: 0.893</span></span><br><span class="line"><span class="comment"># Epoch: 300, loss: 0.135, accuracy: 0.951</span></span><br><span class="line"><span class="comment"># Epoch: 400, loss: 0.091, accuracy: 0.976</span></span><br><span class="line"><span class="comment"># Epoch: 500, loss: 0.069, accuracy: 0.984</span></span><br><span class="line"><span class="comment"># Epoch: 600, loss: 0.056, accuracy: 0.989</span></span><br><span class="line"><span class="comment"># Epoch: 700, loss: 0.048, accuracy: 0.991</span></span><br><span class="line"><span class="comment"># Epoch: 800, loss: 0.043, accuracy: 0.994</span></span><br><span class="line"><span class="comment"># Epoch: 900, loss: 0.039, accuracy: 0.994</span></span><br></pre></td></tr></table></figure><h3 id="Evaluation-2">Evaluation</h3><p>在测试集上评估这个模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLPFromScratch</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        z1 = np.dot(x, W1) + b1</span><br><span class="line">        a1 = np.maximum(<span class="number">0</span>, z1)</span><br><span class="line">        logits = np.dot(a1, W2) + b2</span><br><span class="line">        exp_logits = np.exp(logits)</span><br><span class="line">        y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> y_hat</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluation</span></span><br><span class="line">model = MLPFromScratch()</span><br><span class="line">y_prob = model.predict(X_test)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 0.9826749826749827,</span></span><br><span class="line"><span class="comment">#     "recall": 0.9822222222222222,</span></span><br><span class="line"><span class="comment">#     "f1": 0.9822481383871041,</span></span><br><span class="line"><span class="comment">#     "num_samples": 225.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "c1": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 0.9733333333333334,</span></span><br><span class="line"><span class="comment">#       "f1": 0.9864864864864865,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c2": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.9615384615384616,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 0.9803921568627451,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c3": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.9864864864864865,</span></span><br><span class="line"><span class="comment">#       "recall": 0.9733333333333334,</span></span><br><span class="line"><span class="comment">#       "f1": 0.9798657718120806,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiclass_decision_boundary_numpy</span><span class="params">(model, X, y, savefig_fp=None)</span>:</span></span><br><span class="line">    <span class="string">"""Plot the multiclass decision boundary for a model that accepts 2D inputs.</span></span><br><span class="line"><span class="string">    Credit: https://cs231n.github.io/neural-networks-case-study/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        model &#123;function&#125; -- trained model with function model.predict(x_in).</span></span><br><span class="line"><span class="string">        X &#123;numpy.ndarray&#125; -- 2D inputs with shape (N, 2).</span></span><br><span class="line"><span class="string">        y &#123;numpy.ndarray&#125; -- 1D outputs with shape (N,).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Axis boundaries</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].max() + <span class="number">0.1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].max() + <span class="number">0.1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.linspace(x_min, x_max, <span class="number">101</span>),</span><br><span class="line">                         np.linspace(y_min, y_max, <span class="number">101</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create predictions</span></span><br><span class="line">    x_in = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    y_pred = model.predict(x_in)</span><br><span class="line">    y_pred = np.argmax(y_pred, axis=<span class="number">1</span>).reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot decision boundary</span></span><br><span class="line">    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.RdYlBu)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot</span></span><br><span class="line">    <span class="keyword">if</span> savefig_fp:</span><br><span class="line">        plt.savefig(savefig_fp, format=<span class="string">"png"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary_numpy(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary_numpy(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/0878670f7a00ef84e2037f39161b6fa5.png" alt=""></p><h2 id="Ending">Ending</h2><p>神经网络是机器学习和人工智能领域的基础，我们必须彻底掌握。</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Neural networks - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Numpy实现一个神经网络。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 机器学习之Logistic Regression (二)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-LogisticRegression-2/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-LogisticRegression-2/</id>
    <published>2023-05-29T01:51:29.000Z</published>
    <updated>2023-05-29T06:34:55.041Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>接上篇，本文使用PyTorch实现一个简单的逻辑回归。</p><h2 id="Get-ready">Get ready</h2><p>复用前篇的数据准备及预处理工作，这里直接建模。</p><h2 id="Model">Model</h2><p>我们使用PyTorch的<a href="https://pytorch.org/docs/stable/nn.html#linear-layers" target="_blank" rel="noopener" title="Linear layers">Linear layers</a> 来构建与前篇相同的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, num_classes)</span>:</span></span><br><span class="line">        super(LogisticRegression, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = self.fc1(x_in)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = LogisticRegression(input_dim=INPUT_DIM, num_classes=NUM_CLASSES)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of LogisticRegression(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=2, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Loss">Loss</h2><p>这里使用<a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss" target="_blank" rel="noopener" title="nn.CrossEntropyLoss">交叉熵损失</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">y_pred = torch.randn(<span class="number">3</span>, NUM_CLASSES, requires_grad=<span class="literal">False</span>)</span><br><span class="line">y_true = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(NUM_CLASSES)</span><br><span class="line"><span class="keyword">print</span> (y_true)</span><br><span class="line">loss = loss_fn(y_pred, y_true)</span><br><span class="line">print(<span class="string">f"Loss: <span class="subst">&#123;loss.numpy()&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor([0, 1, 1])</span></span><br><span class="line"><span class="comment"># Loss: 1.0754622220993042</span></span><br></pre></td></tr></table></figure><p>在这个任务中，我们将数据的类别权重纳入到损失函数中，以对抗样本的类别不平衡。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values()))</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br></pre></td></tr></table></figure><h2 id="Metrics">Metrics</h2><p>我们将在训练模型时引入准确度来衡量模型的性能，因为仅查看损失值并不是非常直观。</p><p>后面的章节会介绍相关指标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_fn</span><span class="params">(y_pred, y_true)</span>:</span></span><br><span class="line">    n_correct = torch.eq(y_pred, y_true).sum().item()</span><br><span class="line">    accuracy = (n_correct / len(y_pred)) * <span class="number">100</span></span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line">y_pred = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">y_true = torch.Tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">"Accuracy: &#123;accuracy_fn(y_pred, y_true):.1f&#125;"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Accuracy: &#123;accuracy_fn(y_pred, y_true):.1f&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Optimizer">Optimizer</h2><p>与之前介绍的线性回归一样，这里同样使用Adam优化器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.LongTensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line">NUM_EPOCHS = <span class="number">50</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">        predictions = y_pred.max(dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># class</span></span><br><span class="line">        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 0.71, accuracy: 49.6</span></span><br><span class="line"><span class="comment"># Epoch: 10 | loss: 0.23, accuracy: 93.1</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.14, accuracy: 97.4</span></span><br><span class="line"><span class="comment"># Epoch: 30 | loss: 0.11, accuracy: 98.3</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.09, accuracy: 98.0</span></span><br></pre></td></tr></table></figure><h2 id="Evaluation">Evaluation</h2><p>首先，我们看看一下测试集的准确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">pred_train = F.softmax(model(X_train), dim=<span class="number">1</span>)</span><br><span class="line">pred_test = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample probability: <span class="subst">&#123;pred_test[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">pred_train = pred_train.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">pred_test = pred_test.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample class: <span class="subst">&#123;pred_test[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># sample probability: tensor([0.9934, 0.0066], grad_fn=&lt;SelectBackward0&gt;)</span></span><br><span class="line"><span class="comment"># sample class: 0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy (could've also used our own accuracy function)</span></span><br><span class="line">train_acc = accuracy_score(y_train, pred_train)</span><br><span class="line">test_acc = accuracy_score(y_test, pred_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"train acc: <span class="subst">&#123;train_acc:<span class="number">.2</span>f&#125;</span>, test acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train acc: 0.98, test acc: 0.97</span></span><br></pre></td></tr></table></figure><p>我们还可以根据其他有意义的指标来评估我们的模型，如精确度和召回率。<br>$$<br>accuracy = \frac{TP + FN}{TP + TN + FP + FN}<br>$$<br>$$<br>recall = \frac{TP}{TP + FN}<br>$$<br>$$<br>precision = \frac{TP}{TP + FP}<br>$$<br>$$<br>F1 = 2 * \frac{precision * recall}{precision + recall}<br>$$</p><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">解释</th></tr></thead><tbody><tr><td style="text-align:center">TP</td><td style="text-align:center">truly predicted to be positive and were positive</td></tr><tr><td style="text-align:center">TN</td><td style="text-align:center">truly predicted to negative and where negative</td></tr><tr><td style="text-align:center">FP</td><td style="text-align:center">falsely predicted to be positive but where negative</td></tr><tr><td style="text-align:center">FN</td><td style="text-align:center">falsely predicted to be negative but where positive</td></tr></tbody></table><p>格式化指标，以供前端展示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(y_true, y_pred, classes)</span>:</span></span><br><span class="line">    <span class="string">"""Per-class performance metrics."""</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">"overall"</span>: &#123;&#125;, <span class="string">"class"</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">"weighted"</span>)</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"precision"</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"recall"</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"f1"</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">"overall"</span>][<span class="string">"num_samples"</span>] = np.float64(len(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">        performance[<span class="string">"class"</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">"precision"</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">"recall"</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">"f1"</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">"num_samples"</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br><span class="line"></span><br><span class="line"><span class="comment"># # Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=pred_test, classes=label_encoder.classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 0.9744444444444446,</span></span><br><span class="line"><span class="comment">#     "recall": 0.9733333333333334,</span></span><br><span class="line"><span class="comment">#     "f1": 0.9731408308004051,</span></span><br><span class="line"><span class="comment">#     "num_samples": 150.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "benign": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 0.9310344827586207,</span></span><br><span class="line"><span class="comment">#       "f1": 0.9642857142857143,</span></span><br><span class="line"><span class="comment">#       "num_samples": 58.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "malignant": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.9583333333333334,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 0.9787234042553191,</span></span><br><span class="line"><span class="comment">#       "num_samples": 92.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><p>同样的，利用PyTorch实现的逻辑回归模型建模了一个线性决策边界，我们可视化一下结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiclass_decision_boundary</span><span class="params">(model, X, y)</span>:</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].max() + <span class="number">0.1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].max() + <span class="number">0.1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.linspace(x_min, x_max, <span class="number">101</span>), np.linspace(y_min, y_max, <span class="number">101</span>))</span><br><span class="line">    cmap = plt.cm.Spectral</span><br><span class="line"></span><br><span class="line">    X_test = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()</span><br><span class="line">    y_pred = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line">    _, y_pred = y_pred.max(dim=<span class="number">1</span>)</span><br><span class="line">    y_pred = y_pred.reshape(xx.shape)</span><br><span class="line">    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.RdYlBu)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/8d309a3958c1769ef2403cc39a31dd17.png" alt=""></p><h2 id="Inference">Inference</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inputs for inference</span></span><br><span class="line">X_infer = pd.DataFrame([&#123;<span class="string">"leukocyte_count"</span>: <span class="number">13</span>, <span class="string">"blood_pressure"</span>: <span class="number">12</span>&#125;])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize</span></span><br><span class="line">X_infer = X_scaler.transform(X_infer)</span><br><span class="line"><span class="keyword">print</span> (X_infer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[-0.66859939 -3.09473005]]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">y_infer = F.softmax(model(torch.Tensor(X_infer)), dim=<span class="number">1</span>)</span><br><span class="line">prob, _class = y_infer.max(dim=<span class="number">1</span>)</span><br><span class="line">label = label_encoder.decode(_class.detach().numpy())[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"The probability that you have a <span class="subst">&#123;label&#125;</span> tumor is <span class="subst">&#123;prob.detach().numpy()[<span class="number">0</span>]*<span class="number">100.0</span>:<span class="number">.0</span>f&#125;</span>%"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># The probability that you have a benign tumor is 90%</span></span><br></pre></td></tr></table></figure><h2 id="Unscaled-weights">Unscaled weights</h2><p>同样的，我们亦可以逆标准化我们的权重和偏差。</p><p>注意到只有$X$被标准化过<br>$$<br>\hat{y}_{unscaled} = \sum_{j=1}^k W_{scaled(j)} x_{scaled(j)} + b_{scaled}<br>$$</p><p>已知<br>$$<br>\hat{x}_{scaled} = \frac{x_{j} - \overline{x}_{j}}{\sigma_{j}}<br>$$</p><p>于是<br>$$<br>\hat{y}_{unscaled} = (b_{scaled} - \sum_{j=1}^k W_{scaled(j)} \frac{\overline{x}_j}{\sigma_{j}}) + \sum_j{\frac{W_{scaled(j)}}{\sigma_j}}x_j<br>$$</p><p>对比公式</p><p>$$<br>\hat{y}_{unscaled} = W_{unscaled} x + b_{unscaled}<br>$$</p><p>便可得知<br>$$<br>W_{unscaled} = \frac{W_{scaled(j)}}{\sigma_j}<br>$$</p><p>$$<br>b_{unscaled} = b_{scaled} - \sum_{j=1}^k W_{unscaled(j)} \overline{x}_j<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unstandardize weights</span></span><br><span class="line">W = model.fc1.weight.data.numpy()</span><br><span class="line">b = model.fc1.bias.data.numpy()</span><br><span class="line">W_unscaled = W / X_scaler.scale_</span><br><span class="line">b_unscaled = b - np.sum((W_unscaled * X_scaler.mean_))</span><br><span class="line"><span class="keyword">print</span> (W_unscaled)</span><br><span class="line"><span class="keyword">print</span> (b_unscaled)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[ 0.80800055 -1.47212977]</span></span><br><span class="line"><span class="comment">#  [-0.88854214  0.77129243]]</span></span><br><span class="line"><span class="comment"># [11.4279   13.336911]</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>到这里，我们便完成了PyTorch的逻辑回归任务的介绍。</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Logistic regression - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      使用PyTorch实现逻辑回归。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 机器学习之Logistic Regression (一)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-LogisticRegression-1/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-LogisticRegression-1/</id>
    <published>2023-05-28T11:24:37.000Z</published>
    <updated>2023-06-08T09:42:41.627Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文的目标是使用NumPy实现一个多项式逻辑回归模型 $\hat{y}$ ，以实现对给定的特征$X$预测其目标类别的概率分布。</p><p>$$<br>\hat{y} = \frac{e^{XW_y}}{\sum_j{e^{XW}}}<br>$$</p><p>逻辑回归与前篇所述的线性回归，同属于广义线性回归。皆是建模一条直线(或一个平面)。但不同的是，逻辑回归是一种分类模型。</p><h2 id="Set-up">Set up</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">np.random.seed(SEED)</span><br><span class="line">random.seed(SEED)</span><br></pre></td></tr></table></figure><h2 id="Load-data">Load data</h2><p>我们的任务是基于白细胞计数和血压来确定肿瘤是否为良性（无害）或恶性（有害）。</p><p>请注意，这是一个合成数据集，没有临床意义。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pandas.plotting <span class="keyword">import</span> scatter_matrix</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read from CSV to Pandas DataFrame</span></span><br><span class="line">url = <span class="string">"http://s3.mindex.xyz/datasets/tumors.csv"</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>) <span class="comment"># load</span></span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>) <span class="comment"># shuffle</span></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/7fdb16f4e4e0b9cc36e4c1b765fbc718.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define X and y</span></span><br><span class="line">X = df[[<span class="string">"leukocyte_count"</span>, <span class="string">"blood_pressure"</span>]].values</span><br><span class="line">y = df[<span class="string">"tumor_class"</span>].values</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot data</span></span><br><span class="line">colors = &#123;<span class="string">"benign"</span>: <span class="string">"red"</span>, <span class="string">"malignant"</span>: <span class="string">"blue"</span>&#125;</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=[colors[_y] <span class="keyword">for</span> _y <span class="keyword">in</span> y], s=<span class="number">25</span>, edgecolors=<span class="string">"k"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"leukocyte count"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"blood pressure"</span>)</span><br><span class="line">plt.legend([<span class="string">"malignant"</span>, <span class="string">"benign"</span>], loc=<span class="string">"upper right"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/57d000f09d3f8c9d9f9971618d58ffd9.png" alt=""></p><h2 id="Split-data">Split data</h2><p>我们希望将数据集分成三份，使得每个子集中的类别分布相同，以便进行适当的训练和评估。</p><p><a href="https://scikit-learn.org/stable/index.html" target="_blank" rel="noopener" title="scikit-learn">scikit-learn</a> 提供的方法 <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener" title="train_test_split">train_test_split</a> 可以很容易的做到。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_test_split</span><span class="params">(X, y, train_size)</span>:</span></span><br><span class="line">    <span class="string">"""Split dataset into data splits."""</span></span><br><span class="line">    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)</span><br><span class="line">    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (700, 2), y_train: (700,)</span></span><br><span class="line"><span class="comment"># X_val: (150, 2), y_val: (150,)</span></span><br><span class="line"><span class="comment"># X_test: (150, 2), y_test: (150,)</span></span><br><span class="line"><span class="comment"># Sample point: [18.60187909 18.37050035] → malignant</span></span><br></pre></td></tr></table></figure><p>现在让我们看一下分割后的数据集中样本分布：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Overall class distribution</span></span><br><span class="line">class_counts = dict(collections.Counter(y))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Classes: <span class="subst">&#123;class_counts&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f'm:b = <span class="subst">&#123;class_counts[<span class="string">"malignant"</span>]/class_counts[<span class="string">"benign"</span>]:<span class="number">.2</span>f&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Classes: &#123;'malignant': 611, 'benign': 389&#125;</span></span><br><span class="line"><span class="comment"># m:b = 1.57</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Per data split class distribution</span></span><br><span class="line">train_class_counts = dict(collections.Counter(y_train))</span><br><span class="line">val_class_counts = dict(collections.Counter(y_val))</span><br><span class="line">test_class_counts = dict(collections.Counter(y_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f'train m:b = <span class="subst">&#123;train_class_counts[<span class="string">"malignant"</span>]/train_class_counts[<span class="string">"benign"</span>]:<span class="number">.2</span>f&#125;</span>'</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f'val m:b = <span class="subst">&#123;val_class_counts[<span class="string">"malignant"</span>]/val_class_counts[<span class="string">"benign"</span>]:<span class="number">.2</span>f&#125;</span>'</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f'test m:b = <span class="subst">&#123;test_class_counts[<span class="string">"malignant"</span>]/test_class_counts[<span class="string">"benign"</span>]:<span class="number">.2</span>f&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train m:b = 1.57</span></span><br><span class="line"><span class="comment"># val m:b = 1.59</span></span><br><span class="line"><span class="comment"># test m:b = 1.54</span></span><br></pre></td></tr></table></figure><p>可以看出，分割后的数据集里样本分布大体是接近的</p><h2 id="Label-encoding">Label encoding</h2><p>注意到我们的分类标签是文本。我们需要将其进行编码，以便在模型中使用。</p><p>通常我们使用scikit-learn的<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder" target="_blank" rel="noopener" title="LabelEncoder">LabelEncoder</a>快速编码。</p><p>不过这里我们将编写自己的编码器，以便了解其具体的实现机制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelEncoder</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Label encoder for tag labels."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, class_to_index=&#123;&#125;)</span>:</span></span><br><span class="line">        self.class_to_index = class_to_index <span class="keyword">or</span> &#123;&#125;  <span class="comment"># mutable defaults ;)</span></span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = list(self.class_to_index.keys())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.class_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"&lt;LabelEncoder(num_classes=<span class="subst">&#123;len(self)&#125;</span>)&gt;"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        classes = np.unique(y)</span><br><span class="line">        <span class="keyword">for</span> i, class_ <span class="keyword">in</span> enumerate(classes):</span><br><span class="line">            self.class_to_index[class_] = i</span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = list(self.class_to_index.keys())</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        encoded = np.zeros((len(y)), dtype=int)</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            encoded[i] = self.class_to_index[item]</span><br><span class="line">        <span class="keyword">return</span> encoded</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        classes = []</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            classes.append(self.index_to_class[item])</span><br><span class="line">        <span class="keyword">return</span> classes</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"w"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;<span class="string">'class_to_index'</span>: self.class_to_index&#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(cls, fp)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(fp, <span class="string">"r"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">label_encoder.fit(y_train)</span><br><span class="line"><span class="keyword">print</span> (label_encoder.class_to_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;'benign': 0, 'malignant': 1&#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Encoder</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line">y_train = label_encoder.encode(y_train)</span><br><span class="line">y_val = label_encoder.encode(y_val)</span><br><span class="line">y_test = label_encoder.encode(y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"decoded: <span class="subst">&#123;label_encoder.decode([y_train[<span class="number">0</span>]])&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: malignant</span></span><br><span class="line"><span class="comment"># y_train[0]: 1</span></span><br><span class="line"><span class="comment"># decoded: ['malignant']</span></span><br></pre></td></tr></table></figure><p>我们还想计算出类别的权重，这对于在训练期间加权损失函数非常有用。它会告诉模型需要关注样本量不足的类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> enumerate(counts)&#125;</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># counts: [272 428]</span></span><br><span class="line"><span class="comment"># weight: &#123;0: 0.003676470588235294, 1: 0.002336448598130841&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Standardize-data">Standardize data</h2><p>我们需要对数据进行标准化处理（零均值和单位方差），以便某个特定特征的大小不会影响模型学习其权重。</p><p>这里我们只需要标准化输入$X$，因为我们的输出$y$是离散标签，无须标准化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the data (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler().fit(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply scaler on training and test data (don't standardize outputs for classification)</span></span><br><span class="line">X_train = X_scaler.transform(X_train)</span><br><span class="line">X_val = X_scaler.transform(X_val)</span><br><span class="line">X_test = X_scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[0]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test[1]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_test[0]: mean: -0.0, std: 1.0</span></span><br><span class="line"><span class="comment"># X_test[1]: mean: 0.1, std: 1.0</span></span><br></pre></td></tr></table></figure><h2 id="Weights">Weights</h2><p>我们的目标是学习一个逻辑回归模型 $\hat{y}$，用来建模自变量 $X$ 与 因变量 $y$ 的关系。</p><p>$$<br>\hat{y} = \frac{e^{W_yX}}{\sum_j{e^{WX}}}<br>$$</p><p>尽管我们的示例任务只涉及两个类别，但我们仍将使用多项式逻辑回归，因为softmax分类器可以推广到任意数量的类别。</p><p><strong>第一步</strong>: 随机初始化模型的权重 $W$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>] <span class="comment"># X is 2-dimensional</span></span><br><span class="line">NUM_CLASSES = len(label_encoder.classes) <span class="comment"># y has two possibilities (benign or malignant)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random weights</span></span><br><span class="line">W = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, NUM_CLASSES)</span><br><span class="line">b = np.zeros((<span class="number">1</span>, NUM_CLASSES))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"W: <span class="subst">&#123;W.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"b: <span class="subst">&#123;b.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># W: (2, 2)</span></span><br><span class="line"><span class="comment"># b: (1, 2)</span></span><br></pre></td></tr></table></figure><h2 id="Model">Model</h2><p><strong>第二步</strong> 计算输入 $X$ 的对数值（$ z = WX $）。然后执行softmax操作得到预测类别的独热编码形式。</p><p>举个例子，如果有三个类别，那么一种可能的预测概率是[0.3, 0.3, 0.4]。<br>$$<br>\hat{y} = softmax(z) = softmax(WX) = \frac{e^{W_yX}}{\sum_j{e^{WX}}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Forward pass [NX2] · [2X2] + [1,2] = [NX2]</span></span><br><span class="line">logits = np.dot(X_train, W) + b</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"logits: <span class="subst">&#123;logits.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample: <span class="subst">&#123;logits[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># logits: (700, 2)</span></span><br><span class="line"><span class="comment"># sample: [0.0151033  0.03500428]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalization via softmax to obtain class probabilities</span></span><br><span class="line">exp_logits = np.exp(logits)</span><br><span class="line">y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_hat: <span class="subst">&#123;y_hat.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"sample: <span class="subst">&#123;y_hat[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_hat: (700, 2)</span></span><br><span class="line"><span class="comment"># sample: [0.49502492 0.50497508]</span></span><br></pre></td></tr></table></figure><h2 id="Loss">Loss</h2><p><strong>第三步</strong> 使用代价函数比较预测值 $\hat{y}$ （比如：[0.3, 0.3, 0.4]）和目标值 $y$ （比如：[0. 0. 1]）来确定损失 $J$。</p><p>逻辑回归的常见目标函数是交叉熵损失.</p><p>$$<br>J(\theta) = - \sum_i^K{log(\hat{y}_i)} = - \sum_i^K{log(\frac{e^{W_yX_i}}{\sum_j{e^{WX_i}}})}\\<br>$$</p><p>交叉熵损失函数的目标仍然是最小化预测与实际标签之间的差距，从而让模型能够更准确地进行分类.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss</span></span><br><span class="line">correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])</span><br><span class="line">loss = np.sum(correct_class_logprobs) / len(y_train)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 0.69</span></span><br></pre></td></tr></table></figure><h2 id="Gradients">Gradients</h2><p><strong>第四步</strong> 计算损失函数 $J(\theta)$ 相对于权重的梯度。这里假设我们的类别是互斥的。<br>$$<br>\begin{split}<br>\frac{\partial{J}}{\partial{W_j}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{W_j}} = - \frac{1}{\hat{y}} \frac{\partial{\hat{y}}}{\partial{W_j}} \\<br>&amp;= - \frac{1}{\frac{e^{W_yX}}{\sum_j{e^{WX}}}} \frac{\sum_j{e^{WX}e^{W_yX}0 - e^{W_yX}e^{W_jX}X}}{(\sum_j{e^{WX}})^2} = \frac{Xe^{W_jX}}{\sum_j{e^{WX}}} = X\hat{y}<br>\end{split}<br>$$</p><p>$$<br>\begin{split}<br>\frac{\partial{J}}{\partial{W_y}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{W_y}} = - \frac{1}{\hat{y}} \frac{\partial{\hat{y}}}{\partial{W_y}} \\<br>&amp;= - \frac{1}{\frac{e^{W_yX}}{\sum_j{e^{WX}}}} \frac{\sum_j{e^{WX}e^{W_yX}X - e^{W_yX}e^{W_yX}X}}{(\sum_j{e^{WX}})^2} = \frac{1}{\hat{y}} (X\hat{y}^2 - X\hat{y}) = X(\hat{y} - 1)<br>\end{split}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Backpropagation</span></span><br><span class="line">y_hat[range(len(y_hat)), y_train] -= <span class="number">1</span></span><br><span class="line">y_hat /= len(y_train)</span><br><span class="line">dW = np.dot(X_train.T, y_hat)</span><br><span class="line">db = np.sum(y_hat, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="Update-weights">Update weights</h2><p><strong>第五步</strong> 指定一个学习率来更新权重 $W$，惩罚错误的分类奖励正确的分类。<br>$$<br>W_j = W_j - \alpha \frac{\partial{J}}{\partial{W_j}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Update weights</span></span><br><span class="line">W == -LEARNING_RATE * dW</span><br><span class="line">b += -LEARNING_RATE * db</span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><p><strong>第六步</strong>: 重复步骤 2 ~ 5，以最小化损失为目的来训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Initialize random weights</span></span><br><span class="line">W = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, NUM_CLASSES)</span><br><span class="line">b = np.zeros((<span class="number">1</span>, NUM_CLASSES))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training loop</span></span><br><span class="line">NUM_EPOCHS = <span class="number">50</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line"><span class="keyword">for</span> epoch_num <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass [NX2] · [2X2] = [NX2]</span></span><br><span class="line">    logits = np.dot(X_train, W) + b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Normalization via softmax to obtain class probabilities</span></span><br><span class="line">    exp_logits = np.exp(logits)</span><br><span class="line">    y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])</span><br><span class="line">    loss = np.sum(correct_class_logprobs) / len(y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># show progress</span></span><br><span class="line">    <span class="keyword">if</span> epoch_num%<span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Accuracy</span></span><br><span class="line">        y_pred = np.argmax(logits, axis=<span class="number">1</span>)</span><br><span class="line">        accuracy =  np.mean(np.equal(y_train, y_pred))</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch_num&#125;</span>, loss: <span class="subst">&#123;loss:<span class="number">.3</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.3</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backpropagation</span></span><br><span class="line">    y_hat[range(len(y_hat)), y_train] -= <span class="number">1</span></span><br><span class="line">    y_hat /= len(y_train)</span><br><span class="line">    dW = np.dot(X_train.T, y_hat)</span><br><span class="line">    db = np.sum(y_hat, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    W += -LEARNING_RATE * dW</span><br><span class="line">    b += -LEARNING_RATE * db</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0, loss: 0.694, accuracy: 0.093</span></span><br><span class="line"><span class="comment"># Epoch: 10, loss: 0.451, accuracy: 0.973</span></span><br><span class="line"><span class="comment"># Epoch: 20, loss: 0.353, accuracy: 0.973</span></span><br><span class="line"><span class="comment"># Epoch: 30, loss: 0.299, accuracy: 0.973</span></span><br><span class="line"><span class="comment"># Epoch: 40, loss: 0.264, accuracy: 0.976</span></span><br></pre></td></tr></table></figure><h2 id="Evaluation">Evaluation</h2><p>在测试集上评估我们训练好的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegressionFromScratch</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        logits = np.dot(x, W) + b</span><br><span class="line">        exp_logits = np.exp(logits)</span><br><span class="line">        y_hat = exp_logits / np.sum(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> y_hat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluation</span></span><br><span class="line">model = LogisticRegressionFromScratch()</span><br><span class="line">logits_train = model.predict(X_train)</span><br><span class="line">pred_train = np.argmax(logits_train, axis=<span class="number">1</span>)</span><br><span class="line">logits_test = model.predict(X_test)</span><br><span class="line">pred_test = np.argmax(logits_test, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Training and test accuracy</span></span><br><span class="line">train_acc =  np.mean(np.equal(y_train, pred_train))</span><br><span class="line">test_acc = np.mean(np.equal(y_test, pred_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"train acc: <span class="subst">&#123;train_acc:<span class="number">.2</span>f&#125;</span>, test acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train acc: 0.98, test acc: 0.97</span></span><br></pre></td></tr></table></figure><p>可视化我们的结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiclass_decision_boundary</span><span class="params">(model, X, y, savefig_fp=None)</span>:</span></span><br><span class="line">    <span class="string">"""Plot the multiclass decision boundary for a model that accepts 2D inputs.</span></span><br><span class="line"><span class="string">    Credit: https://cs231n.github.io/neural-networks-case-study/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        model &#123;function&#125; -- trained model with function model.predict(x_in).</span></span><br><span class="line"><span class="string">        X &#123;numpy.ndarray&#125; -- 2D inputs with shape (N, 2).</span></span><br><span class="line"><span class="string">        y &#123;numpy.ndarray&#125; -- 1D outputs with shape (N,).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Axis boundaries</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].max() + <span class="number">0.1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].max() + <span class="number">0.1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.linspace(x_min, x_max, <span class="number">101</span>),</span><br><span class="line">                         np.linspace(y_min, y_max, <span class="number">101</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create predictions</span></span><br><span class="line">    x_in = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    y_pred = model.predict(x_in)</span><br><span class="line">    y_pred = np.argmax(y_pred, axis=<span class="number">1</span>).reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot decision boundary</span></span><br><span class="line">    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.RdYlBu)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot</span></span><br><span class="line">    <span class="keyword">if</span> savefig_fp:</span><br><span class="line">        plt.savefig(savefig_fp, format=<span class="string">"png"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/d6ba4a09a521c4639bf58e05962a6ebe.png" alt=""></p><h2 id="Ending">Ending</h2><p>可以看出，使用NumPy实现的代码相对复杂，下一篇我们即将看到PyTorch是如何便捷的实现逻辑回归。</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Logistic regression - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      使用NumPy实现逻辑回归。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · PyTorch的CrossEntropyLoss实现的不对？</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-CrossEntropyLoss/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-CrossEntropyLoss/</id>
    <published>2023-05-26T03:25:41.000Z</published>
    <updated>2023-05-27T13:56:31.413Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>作者在学习机器学习之逻辑回归任务时，遇到的交叉熵计算不符合预期，才发现了PyTorch的别有洞天。</p><p>于是，本文便是结合实验交代了PyTorch中交叉熵损失的真实计算过程。</p><h2 id="数据准备">数据准备</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.random.seed(seed=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">X = np.random.dirichlet(np.ones(<span class="number">3</span>), size=<span class="number">3</span>)</span><br><span class="line">Y = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (X)</span><br><span class="line"><span class="keyword">print</span> (Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[0.13807842 0.76510708 0.09681451]</span></span><br><span class="line"><span class="comment">#  [0.31698662 0.26993158 0.4130818 ]</span></span><br><span class="line"><span class="comment">#  [0.46864621 0.01320047 0.51815332]]</span></span><br><span class="line"><span class="comment"># [0 1 2]</span></span><br></pre></td></tr></table></figure><h2 id="正文">正文</h2><p>大部分博客给出的公式如下：</p><p>$$<br>H = - \sum_i{y_i log(\hat{y}_i)}<br>$$</p><p>其中 $\hat{y}_i$ 为预测值，$y_i$ 为真实值。</p><p>我们在低维空间复现此公式，注意到PyTorch可以采用<a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html" target="_blank" rel="noopener" title="CROSS_ENTROPY">class indices</a>直接取下标进行计算，这里采用同样的方式模拟。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    h = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Y)):</span><br><span class="line">        _h = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> X[i]:</span><br><span class="line">            _h += np.log(X[i][Y[i]])</span><br><span class="line">        h += - _h</span><br><span class="line">    <span class="keyword">return</span> np.around(h / len(Y), <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cross_entropy_(X, Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 3.947</span></span><br></pre></td></tr></table></figure><p>我们看一下PyTorch的计算结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">entroy = nn.CrossEntropyLoss()</span><br><span class="line">entroy(torch.from_numpy(X), torch.from_numpy(Y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor(1.1484, dtype=torch.float64)</span></span><br></pre></td></tr></table></figure><p>可以看到，结果并不相同。所以PyTorch应该是采用了另外的<a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" target="_blank" rel="noopener" title="CROSSENTROPYLOSS">实现方式</a>，而这也是大部分教程没有交代的。</p><p>$$<br>H(x, class) = - log{\frac{e^{x_{class}}}{\sum_i{e^{x_i}}}} = - x_{class} + log{\sum_i{e^{x_i}}}<br>$$</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    h = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Y)):</span><br><span class="line">        _h = sum([np.exp(j) <span class="keyword">for</span> j <span class="keyword">in</span> X[i]])</span><br><span class="line">        h += (- X[i][Y[i]] + np.log(_h))</span><br><span class="line">    <span class="keyword">return</span> np.around(h / len(Y), <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">cross_entropy(X, Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 1.1484</span></span><br></pre></td></tr></table></figure><p>如此，可以看到PyTorch的CrossEntropyLoss的真正计算过程。</p><h2 id="More">More</h2><p>事实上，我们还可以发现，nn.CrossEntropyLoss() 其实是 nn.logSoftmax() 和 nn.NLLLoss() 的整合版本。<br>$$<br>logSoftmax = log{\frac{e^x}{\sum_i{e^{x_i}}}}<br>$$</p><p>$$<br>NLLLoss(x, class) = -x[class]<br>$$</p><p>验证代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">X = np.random.dirichlet(np.ones(<span class="number">3</span>), size=<span class="number">3</span>)</span><br><span class="line">Y = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">X_ = torch.from_numpy(X)</span><br><span class="line">Y_ = torch.from_numpy(Y)</span><br><span class="line"></span><br><span class="line">entroy = nn.CrossEntropyLoss()</span><br><span class="line">print(entroy(X_, Y_))</span><br><span class="line"></span><br><span class="line">softmax = nn.LogSoftmax()</span><br><span class="line">loss = nn.NLLLoss()</span><br><span class="line">print(loss(softmax(X_), Y_))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor(1.0033, dtype=torch.float64)</span></span><br><span class="line"><span class="comment"># tensor(1.0033, dtype=torch.float64)</span></span><br></pre></td></tr></table></figure><h2 id="结论">结论</h2><ol><li><p>nn.CrossEntropyLoss() 的计算公式如下：<br>$$<br>H(x, class) = - log{\frac{e^{x_{class}}}{\sum_i{e^{x_i}}}} = - x_{class} + log{\sum_i{e^{x_i}}}<br>$$</p></li><li><p>nn.CrossEntropyLoss() 是 nn.logSoftmax() 和 nn.NLLLoss() 的整合。</p></li></ol>]]></content>
    
    <summary type="html">
    
      学习需要脚踏实地。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 机器学习之Linear Regression (二)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-LinearRegression-2/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-LinearRegression-2/</id>
    <published>2023-05-21T10:37:38.000Z</published>
    <updated>2023-05-29T06:08:40.370Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>接上篇，本文的目标是使用PyTorch实现一个线性回归模型。</p><h2 id="Generate-data">Generate data</h2><p>我们复用上篇生成的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line">NUM_SAMPLES = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate synthetic data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_data</span><span class="params">(num_samples)</span>:</span></span><br><span class="line">    <span class="string">"""Generate dummy data for linear regression."""</span></span><br><span class="line">    X = np.array(range(num_samples))</span><br><span class="line">    random_noise = np.random.uniform(<span class="number">-10</span>, <span class="number">20</span>, size=num_samples)</span><br><span class="line">    y = <span class="number">3.5</span>*X + random_noise <span class="comment"># add some noise</span></span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate random (linear) data</span></span><br><span class="line">X, y = generate_data(num_samples=NUM_SAMPLES)</span><br><span class="line">data = np.vstack([X, y]).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load into a Pandas DataFrame</span></span><br><span class="line">df = pd.DataFrame(data, columns=[<span class="string">"X"</span>, <span class="string">"y"</span>])</span><br><span class="line">X = df[[<span class="string">"X"</span>]].values</span><br><span class="line">y = df[[<span class="string">"y"</span>]].values</span><br><span class="line"></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/44b3f9578128ebd663285e3dd1ac4120.png" alt=""></p><p>我们将数据绘制成散点图，可以看到它们有很明显的线性趋势。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scatter plot</span></span><br><span class="line">plt.title(<span class="string">"Generated data"</span>)</span><br><span class="line">plt.scatter(x=df[<span class="string">"X"</span>], y=df[<span class="string">"y"</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/48c4cc75b277978a0b5e3eadf9e252e7.png" alt=""></p><h2 id="Split-data">Split data</h2><p>区别于上一篇中我们使用自定义摇骰子的方式分割数据，这里选择使用<a href="https://scikit-learn.org/stable/" target="_blank" rel="noopener" title="scikit-learn">scikit-learn</a>包里提供的<code>train_test_split</code>方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Split (train)</span></span><br><span class="line">X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"train: <span class="subst">&#123;len(X_train)&#125;</span> (<span class="subst">&#123;(len(X_train) / len(X)):<span class="number">.2</span>f&#125;</span>)\n"</span></span><br><span class="line">       <span class="string">f"remaining: <span class="subst">&#123;len(X_)&#125;</span> (<span class="subst">&#123;(len(X_) / len(X)):<span class="number">.2</span>f&#125;</span>)"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train: 35 (0.70)</span></span><br><span class="line"><span class="comment"># remaining: 15 (0.30)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Split (test)</span></span><br><span class="line">X_val, X_test, y_val, y_test = train_test_split(</span><br><span class="line">    X_, y_, train_size=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"train: <span class="subst">&#123;len(X_train)&#125;</span> (<span class="subst">&#123;len(X_train)/len(X):<span class="number">.2</span>f&#125;</span>)\n"</span></span><br><span class="line">  <span class="string">f"val: <span class="subst">&#123;len(X_val)&#125;</span> (<span class="subst">&#123;len(X_val)/len(X):<span class="number">.2</span>f&#125;</span>)\n"</span></span><br><span class="line">  <span class="string">f"test: <span class="subst">&#123;len(X_test)&#125;</span> (<span class="subst">&#123;len(X_test)/len(X):<span class="number">.2</span>f&#125;</span>)"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train: 35 (0.70)</span></span><br><span class="line"><span class="comment"># val: 7 (0.14)</span></span><br><span class="line"><span class="comment"># test: 8 (0.16)</span></span><br></pre></td></tr></table></figure><h2 id="Standardize-data">Standardize data</h2><p>同样的，我们需要对数据进行标准化处理。这里使用<code>scikit-learn</code>里提供的<code>StandardScaler</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the data (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler().fit(X_train)</span><br><span class="line">y_scaler = StandardScaler().fit(y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply scaler on training and test data</span></span><br><span class="line">X_train = X_scaler.transform(X_train)</span><br><span class="line">y_train = y_scaler.transform(y_train).ravel().reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">X_val = X_scaler.transform(X_val)</span><br><span class="line">y_val = y_scaler.transform(y_val).ravel().reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">X_test = X_scaler.transform(X_test)</span><br><span class="line">y_test = y_scaler.transform(y_test).ravel().reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;np.mean(X_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;np.mean(y_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(y_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># mean: 0.4, std: 1.0</span></span><br><span class="line"><span class="comment"># mean: 0.3, std: 0.9</span></span><br></pre></td></tr></table></figure><h2 id="Weights">Weights</h2><p>我们将使用PyTorch的<code>Linear layers</code>来实现一个没有隐含层的神经网络。<br>关于神经网络我们后面会具体学习。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">torch.manual_seed(SEED)</span><br><span class="line"></span><br><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>] <span class="comment"># X is 1-dimensional</span></span><br><span class="line">OUTPUT_DIM = y_train.shape[<span class="number">1</span>] <span class="comment"># y is 1-dimensional</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Inputs</span></span><br><span class="line">N = <span class="number">3</span> <span class="comment"># num samples</span></span><br><span class="line">x = torch.randn(N, INPUT_DIM)</span><br><span class="line"><span class="keyword">print</span> (x.shape)</span><br><span class="line"><span class="keyword">print</span> (x.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"><span class="comment"># [[-1.4836688 ]</span></span><br><span class="line"><span class="comment">#  [ 0.26714355]</span></span><br><span class="line"><span class="comment">#  [-1.8336787 ]]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Weights</span></span><br><span class="line">m = nn.Linear(INPUT_DIM, OUTPUT_DIM)</span><br><span class="line"><span class="keyword">print</span> (m)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"weights (<span class="subst">&#123;m.weight.shape&#125;</span>): <span class="subst">&#123;m.weight[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"bias (<span class="subst">&#123;m.bias.shape&#125;</span>): <span class="subst">&#123;m.bias[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Linear(in_features=1, out_features=1, bias=True)</span></span><br><span class="line"><span class="comment"># weights (torch.Size([1, 1])): -0.2795013189315796</span></span><br><span class="line"><span class="comment"># bias (torch.Size([1])): -0.7643394470214844</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Forward pass</span></span><br><span class="line">z = m(x)</span><br><span class="line"><span class="keyword">print</span> (z.shape)</span><br><span class="line"><span class="keyword">print</span> (z.detach().numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"><span class="comment"># [[-0.34965205]</span></span><br><span class="line"><span class="comment">#  [-0.8390064 ]</span></span><br><span class="line"><span class="comment">#  [-0.25182384]]</span></span><br></pre></td></tr></table></figure><h2 id="Model">Model</h2><p>$$<br>\hat{y} = WX + b<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, output_dim)</span>:</span></span><br><span class="line">        super(LinearRegression, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        y_pred = self.fc1(x_in)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = LinearRegression(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of LinearRegression(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=1, out_features=1, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Loss">Loss</h2><p>同样的，我们使用PyTorch自带的<a href="">Loss Functions</a>, 这里指定<code>MSELoss</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line">y_pred = torch.Tensor([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line">y_true =  torch.Tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>])</span><br><span class="line">loss = loss_fn(y_pred, y_true)</span><br><span class="line">print(<span class="string">"Loss: "</span>, loss.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Loss:  0.75</span></span><br></pre></td></tr></table></figure><h2 id="Optimizer">Optimizer</h2><p>上一篇中我们介绍了使用梯度下降的方法来更新我们的权重。PyTorch中有很多不同的权重更新方法，需要根据不同的场景来选择合适的。详见<a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener" title="TORCH.OPTIM">TORCH.OPTIM</a>。<br>这里我们采用适合大多数场景的<code>ADAM optimizer</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.Tensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.Tensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.Tensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line">NUM_EPOCHS = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    <span class="comment"># https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 1.11</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.10</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.04</span></span><br><span class="line"><span class="comment"># Epoch: 60 | loss: 0.03</span></span><br><span class="line"><span class="comment"># Epoch: 80 | loss: 0.03</span></span><br></pre></td></tr></table></figure><h2 id="Evaluation">Evaluation</h2><p>现在我们准备评估我们训练好的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">pred_train = model(X_train)</span><br><span class="line">pred_test = model(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">train_error = loss_fn(pred_train, y_train)</span><br><span class="line">test_error = loss_fn(pred_test, y_test)</span><br><span class="line">print(<span class="string">f"train_error: <span class="subst">&#123;train_error:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"test_error: <span class="subst">&#123;test_error:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train_error: 0.03</span></span><br><span class="line"><span class="comment"># test_error: 0.04</span></span><br></pre></td></tr></table></figure><p>由于我们只有一个特征，因此可以轻松地对模型进行可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Figure size</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot train data</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plt.scatter(X_train, y_train, label=<span class="string">"y_train"</span>)</span><br><span class="line">plt.plot(X_train, pred_train.detach().numpy(), color=<span class="string">"red"</span>, linewidth=<span class="number">1</span>, linestyle=<span class="string">"-"</span>, label=<span class="string">"model"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot test data</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plt.scatter(X_test, y_test, label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.plot(X_test, pred_test.detach().numpy(), color=<span class="string">"red"</span>, linewidth=<span class="number">1</span>, linestyle=<span class="string">"-"</span>, label=<span class="string">"model"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show plots</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/332563148a19dae3df2c54de08dafccd.png" alt=""></p><h2 id="Inference">Inference</h2><p>训练完模型后，我们可以使用它来对新数据进行预测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Feed in your own inputs</span></span><br><span class="line">sample_indices = [<span class="number">10</span>, <span class="number">15</span>, <span class="number">25</span>]</span><br><span class="line">X_infer = np.array(sample_indices, dtype=np.float32)</span><br><span class="line">X_infer = torch.Tensor(X_scaler.transform(X_infer.reshape(<span class="number">-1</span>, <span class="number">1</span>)))</span><br></pre></td></tr></table></figure><p>由于我们对数据都进行了标准化，所以对预测值需要进行逆操作。<br>$$<br>\hat{y}_{scaled} = \frac{\hat{y} - \mu_{\hat{y}}}{\sigma_{\hat{y}}}<br>$$</p><p>$$<br>\hat{y} = \hat{y}_{scaled} * \sigma_{\hat{y}} + \mu_{\hat{y}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Unstandardize predictions</span></span><br><span class="line">pred_infer = model(X_infer).detach().numpy() * np.sqrt(y_scaler.var_) + y_scaler.mean_</span><br><span class="line"><span class="keyword">for</span> i, index <span class="keyword">in</span> enumerate(sample_indices):</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">f"<span class="subst">&#123;df.iloc[index][<span class="string">'y'</span>]:<span class="number">.2</span>f&#125;</span> (actual) → <span class="subst">&#123;pred_infer[i][<span class="number">0</span>]:<span class="number">.2</span>f&#125;</span> (predicted)"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 28.10 (actual) → 40.99 (predicted)</span></span><br><span class="line"><span class="comment"># 56.45 (actual) → 58.62 (predicted)</span></span><br><span class="line"><span class="comment"># 100.83 (actual) → 93.88 (predicted)</span></span><br></pre></td></tr></table></figure><h2 id="Interpretability">Interpretability</h2><p>线性回归具有高度可解释性的巨大优势。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unstandardize coefficients</span></span><br><span class="line">W = model.fc1.weight.data.numpy()[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">b = model.fc1.bias.data.numpy()[<span class="number">0</span>]</span><br><span class="line">W_unscaled = W * (y_scaler.scale_/X_scaler.scale_)</span><br><span class="line">b_unscaled = b * y_scaler.scale_ + y_scaler.mean_ - np.sum(W_unscaled*X_scaler.mean_)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"[actual] y = 3.5X + noise"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"[model] y_hat = <span class="subst">&#123;W_unscaled[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>X + <span class="subst">&#123;b_unscaled[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [actual] y = 3.5X + noise</span></span><br><span class="line"><span class="comment"># [model] y_hat = 3.5X + 5.7</span></span><br></pre></td></tr></table></figure><h2 id="Regularization">Regularization</h2><p>正则化有助于减少过拟合。本例使用<code>L2正则化</code> (岭回归)。</p><p>通过L2正则化，我们对大的权重值进行惩罚，鼓励权重是较小值。 还有其他类型的正则化，比如L1（套索回归），它可以用于创建稀疏模型，其中一些特征系数被清零，或者结合了L1和L2惩罚的弹性正则化。</p><p>正则化不仅适用于线性回归，您可以使用它来处理任何模型的权重，包括我们将在未来学习到的模型。</p><p>$$<br>J(\theta) = \frac{1}{2} \sum_{i}(WX_i - y_i)^2 + \frac{\lambda}{2} \sum_i{W_i}^2<br>$$</p><p>$$<br>\frac{\partial(J)}{\partial(W)} = (\hat{y} - y)X + \lambda{W}<br>$$</p><p>$$<br>W = W - \alpha{\frac{\partial{J}}{\partial{W}}}<br>$$</p><p>$\lambda$: 正则化系数; $\alpha$: 学习率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">L2_LAMBDA = <span class="number">1e-2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = LinearRegression(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer (w/ L2 regularization)</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 0.67</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.06</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.03</span></span><br><span class="line"><span class="comment"># Epoch: 60 | loss: 0.03</span></span><br><span class="line"><span class="comment"># Epoch: 80 | loss: 0.03</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">pred_train = model(X_train)</span><br><span class="line">pred_test = model(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">train_error = loss_fn(pred_train, y_train)</span><br><span class="line">test_error = loss_fn(pred_test, y_test)</span><br><span class="line">print(<span class="string">f"train_error: <span class="subst">&#123;train_error:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"test_error: <span class="subst">&#123;test_error:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line">train_error: <span class="number">0.03</span></span><br><span class="line">test_error: <span class="number">0.03</span></span><br></pre></td></tr></table></figure><p>对于这个特定的例子，正则化并没有在性能上产生差异，因为我们的数据是从一个完美的线性方程生成的。但是对于大规模真实数据，正则化可以帮助我们的模型很好地泛化。</p><h2 id="Ending">Ending</h2><p>本篇基于上一篇的基础，简单介绍了如何用PyTorch实现线性回归。</p><p>Peace out.</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Linear regression - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      使用PyTorch实现线性回归。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · 机器学习之Linear Regression (一)</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-LinearRegression-1/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-LinearRegression-1/</id>
    <published>2023-05-18T10:31:52.000Z</published>
    <updated>2023-05-28T07:34:03.591Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文的目标是使用NumPy实现一个线性回归模型 $\hat{y}$ ，该模型通过最小化预测值和真实值之间的距离来拟合出一条最佳的直线(或一个平面)。<br>我们使用标记数据 $(X, y)$ 来训练模型，利用梯度下降的方法来学习权重 $W$ 和 偏差 $b$。</p><p>$$<br>\hat{y} = WX + b<br>$$</p><h2 id="Generate-data">Generate data</h2><p>我们会生成一些简单的数据，数据大致符合线性分布，但加上一点随机噪声以模拟现实情况( $y = 3.5 * X + 噪声$)，意味着这些数据并不完全在一条直线上。</p><p>我们的目标是使模型收敛到类似的线性方程上（由于随机噪声的加入，最终的模型结果可能会有差异）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line">NUM_SAMPLES = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate synthetic data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_data</span><span class="params">(num_samples)</span>:</span></span><br><span class="line">    <span class="string">"""Generate dummy data for linear regression."""</span></span><br><span class="line">    X = np.array(range(num_samples))</span><br><span class="line">    random_noise = np.random.uniform(<span class="number">-10</span>, <span class="number">20</span>, size=num_samples)</span><br><span class="line">    y = <span class="number">3.5</span>*X + random_noise <span class="comment"># add some noise</span></span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate random (linear) data</span></span><br><span class="line">X, y = generate_data(num_samples=NUM_SAMPLES)</span><br><span class="line">data = np.vstack([X, y]).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load into a Pandas DataFrame</span></span><br><span class="line">df = pd.DataFrame(data, columns=[<span class="string">"X"</span>, <span class="string">"y"</span>])</span><br><span class="line">X = df[[<span class="string">"X"</span>]].values</span><br><span class="line">y = df[[<span class="string">"y"</span>]].values</span><br><span class="line"></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/44b3f9578128ebd663285e3dd1ac4120.png" alt=""></p><p>我们将数据绘制成散点图，可以看到它们有很明显的线性趋势。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scatter plot</span></span><br><span class="line">plt.title(<span class="string">"Generated data"</span>)</span><br><span class="line">plt.scatter(x=df[<span class="string">"X"</span>], y=df[<span class="string">"y"</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/48c4cc75b277978a0b5e3eadf9e252e7.png" alt=""></p><h2 id="Split-data">Split data</h2><p>现在我们已经准备好了数据，接下来需要随机将数据集分成三个部分：训练集、验证集和测试集。</p><ul><li>训练集：用来训练我们的模型</li><li>验证集：在训练过程中用来检验我们模型的性能</li><li>测试集：用来检测我们最终训练好的模型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Shuffle data</span></span><br><span class="line"><span class="comment"># 不要错误的将 X 和 y 分开shuffle，务必保持自变量和因变量始终对齐</span></span><br><span class="line">indices = list(range(NUM_SAMPLES))</span><br><span class="line">np.random.shuffle(indices)</span><br><span class="line">X = X[indices]</span><br><span class="line">y = y[indices]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split indices</span></span><br><span class="line">train_start = <span class="number">0</span></span><br><span class="line">train_end = int(<span class="number">0.7</span>*NUM_SAMPLES)</span><br><span class="line">val_start = train_end</span><br><span class="line">val_end = int((TRAIN_SIZE+VAL_SIZE)*NUM_SAMPLES)</span><br><span class="line">test_start = val_end</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data</span></span><br><span class="line">X_train = X[train_start:train_end]</span><br><span class="line">y_train = y[train_start:train_end]</span><br><span class="line">X_val = X[val_start:val_end]</span><br><span class="line">y_val = y[val_start:val_end]</span><br><span class="line">X_test = X[test_start:]</span><br><span class="line">y_test = y[test_start:]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_test: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (35, 1), y_train: (35, 1)</span></span><br><span class="line"><span class="comment"># X_val: (7, 1), y_test: (7, 1)</span></span><br><span class="line"><span class="comment"># X_test: (8, 1), y_test: (8, 1)</span></span><br></pre></td></tr></table></figure><h2 id="Standardize-data">Standardize data</h2><p>我们需要对数据进行标准化处理（零均值和单位方差），这样做的目的是使不同特征之间的值具有可比性，并且能够减少不同特征之间的差异性。</p><p>$$<br>z = \frac{x_i - \mu}{\sigma}<br>$$</p><p>$z$: 标准化后的值;  $x_i$: 第i个输入;  $\mu$: 平均值;  $\sigma$: 标准差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standardize_data</span><span class="params">(data, mean, std)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (data - mean) / std</span><br></pre></td></tr></table></figure><p>需要注意的是，我们将验证集和测试集视为隐藏数据集。因此，我们只使用训练集来确定均值和标准差，以避免偏向我们的训练过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Determine means and stds</span></span><br><span class="line">X_mean = np.mean(X_train)</span><br><span class="line">X_std = np.std(X_train)</span><br><span class="line">y_mean = np.mean(y_train)</span><br><span class="line">y_std = np.std(y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize</span></span><br><span class="line">X_train = standardize_data(X_train, X_mean, X_std)</span><br><span class="line">y_train = standardize_data(y_train, y_mean, y_std)</span><br><span class="line">X_val = standardize_data(X_val, X_mean, X_std)</span><br><span class="line">y_val = standardize_data(y_val, y_mean, y_std)</span><br><span class="line">X_test = standardize_data(X_test, X_mean, X_std)</span><br><span class="line">y_test = standardize_data(y_test, y_mean, y_std)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;np.mean(X_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;np.mean(y_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(y_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># mean: 0.3, std: 1.0</span></span><br><span class="line"><span class="comment"># mean: 0.3, std: 1.0</span></span><br></pre></td></tr></table></figure><h2 id="Weights">Weights</h2><p>我们的目标是学习一个模型 $\hat{y}$ ，用权重向量 $W\in\mathbb{R}^{d}$ 和截距 $b\in\mathbb{R}$ 来表达自变量 $X\in\mathbb{R}^{d}$ 与因变量 $y\in\mathbb{R}$ 之间的线性关系:<br>$$<br>\hat{y} = XW + b\<br>$$</p><p><strong>第一步</strong>: 随机初始化模型的权重 $W$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>] <span class="comment"># X is 1-dimensional</span></span><br><span class="line">OUTPUT_DIM = y_train.shape[<span class="number">1</span>] <span class="comment"># y is 1-dimensional</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random weights</span></span><br><span class="line">W = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, OUTPUT_DIM)</span><br><span class="line">b = np.zeros((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"W: <span class="subst">&#123;W.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"b: <span class="subst">&#123;b.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># W: (1, 1)</span></span><br><span class="line"><span class="comment"># b: (1, 1)</span></span><br></pre></td></tr></table></figure><h2 id="Model">Model</h2><p><strong>第二步</strong>: 给模型喂 $X$ 以获得预测结果 $\hat{y}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Forward pass [NX1] · [1X1] = [NX1]</span></span><br><span class="line">y_pred = np.dot(X_train, W) + b</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"y_pred: <span class="subst">&#123;y_pred.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_pred: (35, 1)</span></span><br></pre></td></tr></table></figure><h2 id="Loss">Loss</h2><p><strong>第三步</strong>: 通过比较预测值和实际目标值来确定损失 $J$ 。线性回归常见的损失函数是<code>均方误差(MSE)</code>。<br>$$<br>J(\theta) =  \frac{1}{N} \sum_i(y_i - \hat{y}_i)^2<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss</span></span><br><span class="line">N = len(y_train)</span><br><span class="line">loss = (<span class="number">1</span>/N) * np.sum((y_train - y_pred)**<span class="number">2</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># loss: 0.99</span></span><br></pre></td></tr></table></figure><h2 id="Gradients">Gradients</h2><p><strong>第四步</strong>: 计算损失函数 $J(\theta)$ 的梯度，并更新模型的权重<br>$$<br>\rightarrow \frac{\partial(J)}{\partial(W)} = -\frac{2}{N}\sum_i(y_i - \hat{y}_i)^2 * X_i<br>$$</p><p>$$<br>\rightarrow \frac{\partial(J)}{\partial(b)} = -\frac{2}{N}\sum_i(y_i - \hat{y}_i)^2 * 1<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Backpropagation</span></span><br><span class="line">dW = -(<span class="number">2</span>/N) * np.sum((y_train - y_pred) * X_train)</span><br><span class="line">db = -(<span class="number">2</span>/N) * np.sum((y_train - y_pred) * <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>梯度是函数的导数或变化率。它是一个向量，指向函数增长最快的方向。<br>在我们这个例子里面，损失函数 $J$ 在 $W$ 的梯度告诉我们如何改变 $W$ 以最大化 $J$ 。然而我们希望最小化损失，因此我们需要从 $W$ 中减去梯度。</p><h2 id="Update-weights">Update weights</h2><p><strong>第五步</strong>: 利用一个很小的学习率 $\alpha$ 更新权重<br>$$<br>W = W - \alpha \frac{\partial(J)}{\partial(W)}<br>$$</p><p>$$<br>b = b - \alpha \frac{\partial(J)}{\partial(b)}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Update weights</span></span><br><span class="line">W += -LEARNING_RATE * dW</span><br><span class="line">b += -LEARNING_RATE * db</span><br></pre></td></tr></table></figure><h2 id="Training">Training</h2><p><strong>第六步</strong>: 重复步骤 2 ~ 5，以最小化损失为目的来训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">NUM_EPOCHS = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random weights</span></span><br><span class="line">W = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, OUTPUT_DIM)</span><br><span class="line">b = np.zeros((<span class="number">1</span>, ))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training loop</span></span><br><span class="line"><span class="keyword">for</span> epoch_num <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass [NX1] · [1X1] = [NX1]</span></span><br><span class="line">    y_pred = np.dot(X_train, W) + b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = (<span class="number">1</span>/len(y_train)) * np.sum((y_train - y_pred)**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Show progress</span></span><br><span class="line">    <span class="keyword">if</span> epoch_num%<span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch_num&#125;</span>, loss: <span class="subst">&#123;loss:<span class="number">.3</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backpropagation</span></span><br><span class="line">    dW = -(<span class="number">2</span>/N) * np.sum((y_train - y_pred) * X_train)</span><br><span class="line">    db = -(<span class="number">2</span>/N) * np.sum((y_train - y_pred) * <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    W += -LEARNING_RATE * dW</span><br><span class="line">    b += -LEARNING_RATE * db</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0, loss: 0.992</span></span><br><span class="line"><span class="comment"># Epoch: 10, loss: 0.043</span></span><br><span class="line"><span class="comment"># Epoch: 20, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 30, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 40, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 50, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 60, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 70, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 80, loss: 0.032</span></span><br><span class="line"><span class="comment"># Epoch: 90, loss: 0.032</span></span><br></pre></td></tr></table></figure><p>为了保持简洁，我们在这里没有计算和显示每个Epoch后的验证集损失。在后续的学习会体现出，在验证集上的表现对训练进程有着至关重要的影响（学习率是否合理、什么时候停止训练等等）</p><h2 id="Evaluation">Evaluation</h2><p>接下来看一下我们训练好的模型在测试数据集上的表现如何。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">pred_train = W*X_train + b</span><br><span class="line">pred_test = W*X_test + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train and test MSE</span></span><br><span class="line">train_mse = np.mean((y_train - pred_train) ** <span class="number">2</span>)</span><br><span class="line">test_mse = np.mean((y_test - pred_test) ** <span class="number">2</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"train_MSE: <span class="subst">&#123;train_mse:<span class="number">.2</span>f&#125;</span>, test_MSE: <span class="subst">&#123;test_mse:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train_MSE: 0.03, test_MSE: 0.04</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Figure size</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot train data</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plt.scatter(X_train, y_train, label=<span class="string">"y_train"</span>)</span><br><span class="line">plt.plot(X_train, pred_train, color=<span class="string">"red"</span>, linewidth=<span class="number">1</span>, linestyle=<span class="string">"-"</span>, label=<span class="string">"model"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot test data</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plt.scatter(X_test, y_test, label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.plot(X_test, pred_test, color=<span class="string">"red"</span>, linewidth=<span class="number">1</span>, linestyle=<span class="string">"-"</span>, label=<span class="string">"model"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show plots</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/be75da05ba1c2d284e29c1e95319c6f9.png" alt=""></p><h2 id="Interpretability">Interpretability</h2><p>由于我们对输入和输出进行了标准化，因此我们的权重适配了这些标准化值。因此，我们需要将权重还原为非标准化状态，以便与真实权重（3.5）进行比较。<br>注意到 $X$ 和 $\hat{y}$ 都已经标准化过。</p><p>$$<br>\hat{y}_{scaled} = \sum_{j=1}^k W_{scaled(j)} x_{scaled(j)} + b_{scaled}<br>$$</p><p>已知<br>$$<br>\hat{y}_{scaled} = \frac{\hat{y}_{unscaled} - \overline{y}}{\sigma}<br>$$</p><p>$$<br>\hat{x}_{scaled} = \frac{x_{j} - \overline{x}_{j}}{\sigma_{j}}<br>$$</p><p>于是<br>$$<br>\frac{\hat{y}_{unscaled} - \overline{y}}{\sigma} = \sum_{j=1}^k W_{scaled(j)} (\frac{x_{j} - \overline{x}_{j}}{\sigma_{j}}) + b_{scaled}<br>$$</p><p>进一步可以得到</p><p>$$<br>\hat{y}_{unscaled} = \sum_{j=1}^k W_{scaled(j)} (\frac{\sigma_{y}}{\sigma_{j}}) x_{j} - \sum_{j=1}^k W_{scaled(j)} (\frac{\sigma_{y}}{\sigma_{j}}) \overline{x}_{j} + b_{scaled} \sigma_y + \overline{y}<br>$$</p><p>对比公式</p><p>$$<br>\hat{y}_{unscaled} = W_{unscaled} x + b_{unscaled}<br>$$</p><p>便可得知<br>$$<br>W_{unscaled} = W_{scaled} (\frac{\sigma_{y}}{\sigma_{j}})<br>$$</p><p>$$<br>b_{unscaled} = - \sum_{j=1}^k W_{unscaled(j)} \overline{x}_{j} + b_{scaled} \sigma_y + \overline{y}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unscaled weights</span></span><br><span class="line">W_unscaled = W * (y_std / X_std)</span><br><span class="line">b_unscaled = - np.sum(W_unscaled * X_mean) + b * y_std + y_mean</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"[actual] y = 3.5X + noise"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"[model] y_hat = <span class="subst">&#123;W_unscaled[<span class="number">0</span>][<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>X + <span class="subst">&#123;b_unscaled[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [actual] y = 3.5X + noise</span></span><br><span class="line"><span class="comment"># [model] y_hat = 3.4X + 7.9</span></span><br></pre></td></tr></table></figure><p>从上面的结果可以看出，我们成功的拟合出了这个线性方程。</p><h2 id="Ending">Ending</h2><p>该模型的优势：计算简单，高度可解释, 可以处理连续的和可分类的特征.  而缺点也很明显：只有当数据是线性可分的时候，该模型才能表现良好。</p><p>除了回归任务，你也可以将线性回归用于二元分类任务，其中如果预测的连续值高于阈值，则属于某个类别。 不过未来，我们会介绍更好的分类技术。</p><p>下一篇，我们将介绍如何用PyTorch实现本篇的内容。</p><p>Peace out.</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Linear regression - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      使用NumPy实现线性回归。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · PyTorch</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-PyTorch/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-PyTorch/</id>
    <published>2023-05-17T10:11:12.000Z</published>
    <updated>2023-05-27T13:56:17.775Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文简单介绍了PyTorch这个机器学习框架的必备知识。</p><h2 id="Set-up">Set up</h2><p>首先，我们将导入NumPy和PyTorch库，并设置随机种子以实现可重复性。<br>请注意，PyTorch 也需要一个种子，因为我们将生成随机张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">np.random.seed(seed=SEED)</span><br><span class="line">torch.manual_seed(SEED)</span><br></pre></td></tr></table></figure><h2 id="Basic">Basic</h2><p>下面一些 PyTorch 的基础知识，例如如何创建张量以及将常见的数据结构（列表、数组等）转换为张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating a random tensor</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># normal distribution (rand(2,3) -&gt; uniform distribution)</span></span><br><span class="line">print(<span class="string">f"Type: <span class="subst">&#123;x.type()&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Type: torch.FloatTensor</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-1.4837,  0.2671, -1.8337],</span></span><br><span class="line"><span class="comment">#         [-0.1047,  0.6002, -0.5496]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Zero and Ones tensor</span></span><br><span class="line">x = torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.]])</span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># List → Tensor</span></span><br><span class="line">x = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[1., 2., 3.],</span></span><br><span class="line"><span class="comment">#         [4., 5., 6.]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># NumPy array → Tensor</span></span><br><span class="line">x = torch.Tensor(np.random.rand(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[0.4445, 0.3168, 0.9231],</span></span><br><span class="line"><span class="comment">#         [0.4659, 0.7984, 0.1992]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Changing tensor type</span></span><br><span class="line">x = torch.Tensor(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(<span class="string">f"Type: <span class="subst">&#123;x.type()&#125;</span>"</span>)</span><br><span class="line">x = x.long()</span><br><span class="line">print(<span class="string">f"Type: <span class="subst">&#123;x.type()&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Type: torch.FloatTensor</span></span><br><span class="line"><span class="comment"># Type: torch.LongTensor</span></span><br></pre></td></tr></table></figure><h2 id="Operations">Operations</h2><p>下面探索一些张量的基本操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Addition</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">z = x + y</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;z.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;z&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-0.4446,  0.4933, -1.4847],</span></span><br><span class="line"><span class="comment">#         [ 0.8493,  0.6911, -0.3357]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dot product</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">z = torch.mm(x, y)</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;z.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;z&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 2])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[ 0.2733, -4.0392],</span></span><br><span class="line"><span class="comment">#         [ 1.6385, -4.7220]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Transpose</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line">y = torch.t(x)</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;y.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;y&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[ 0.5920, -0.6301, -0.8856],</span></span><br><span class="line"><span class="comment">#         [ 1.2261, -0.4671, -1.0279]])</span></span><br><span class="line"><span class="comment"># Size: torch.Size([3, 2])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[ 0.5920,  1.2261],</span></span><br><span class="line"><span class="comment">#         [-0.6301, -0.4671],</span></span><br><span class="line"><span class="comment">#         [-0.8856, -1.0279]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reshape</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">z = x.view(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;z.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;z&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([3, 2])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-1.0387,  0.1039],</span></span><br><span class="line"><span class="comment">#         [ 0.5989, -1.4801],</span></span><br><span class="line"><span class="comment">#         [-0.8618, -0.9181]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dangers of reshaping (unintended consequences)</span></span><br><span class="line">x = torch.tensor([</span><br><span class="line">    [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]],</span><br><span class="line">    [[<span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>], [<span class="number">20</span>, <span class="number">20</span>, <span class="number">20</span>, <span class="number">20</span>], [<span class="number">30</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">30</span>]]</span><br><span class="line">])</span><br><span class="line">print(<span class="string">f"Size: <span class="subst">&#123;x.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"x: \n<span class="subst">&#123;x&#125;</span>\n"</span>)</span><br><span class="line"></span><br><span class="line">a = x.view(x.size(<span class="number">1</span>), <span class="number">-1</span>)</span><br><span class="line">print(<span class="string">f"\nSize: <span class="subst">&#123;a.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"a: \n<span class="subst">&#123;a&#125;</span>\n"</span>)</span><br><span class="line"></span><br><span class="line">b = x.transpose(<span class="number">0</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">print(<span class="string">f"\nSize: <span class="subst">&#123;b.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"b: \n<span class="subst">&#123;b&#125;</span>\n"</span>)</span><br><span class="line"></span><br><span class="line">c = b.view(b.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">print(<span class="string">f"\nSize: <span class="subst">&#123;c.shape&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"c: \n<span class="subst">&#123;c&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Size: torch.Size([2, 3, 4])</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment"># tensor([[[ 1,  1,  1,  1],</span></span><br><span class="line"><span class="comment">#          [ 2,  2,  2,  2],</span></span><br><span class="line"><span class="comment">#          [ 3,  3,  3,  3]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[10, 10, 10, 10],</span></span><br><span class="line"><span class="comment">#          [20, 20, 20, 20],</span></span><br><span class="line"><span class="comment">#          [30, 30, 30, 30]]])</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Size: torch.Size([3, 8])</span></span><br><span class="line"><span class="comment"># a:</span></span><br><span class="line"><span class="comment"># tensor([[ 1,  1,  1,  1,  2,  2,  2,  2],</span></span><br><span class="line"><span class="comment">#         [ 3,  3,  3,  3, 10, 10, 10, 10],</span></span><br><span class="line"><span class="comment">#         [20, 20, 20, 20, 30, 30, 30, 30]])</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Size: torch.Size([3, 2, 4])</span></span><br><span class="line"><span class="comment"># b:</span></span><br><span class="line"><span class="comment"># tensor([[[ 1,  1,  1,  1],</span></span><br><span class="line"><span class="comment">#          [10, 10, 10, 10]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[ 2,  2,  2,  2],</span></span><br><span class="line"><span class="comment">#          [20, 20, 20, 20]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[ 3,  3,  3,  3],</span></span><br><span class="line"><span class="comment">#          [30, 30, 30, 30]]])</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Size: torch.Size([3, 8])</span></span><br><span class="line"><span class="comment"># c:</span></span><br><span class="line"><span class="comment"># tensor([[ 1,  1,  1,  1, 10, 10, 10, 10],</span></span><br><span class="line"><span class="comment">#         [ 2,  2,  2,  2, 20, 20, 20, 20],</span></span><br><span class="line"><span class="comment">#         [ 3,  3,  3,  3, 30, 30, 30, 30]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dimensional operations</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line">y = torch.sum(x, dim=<span class="number">0</span>) <span class="comment"># add each row's value for every column</span></span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;y&#125;</span>"</span>)</span><br><span class="line">z = torch.sum(x, dim=<span class="number">1</span>) <span class="comment"># add each columns's value for every row</span></span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;z&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-0.0355,  0.4145,  0.6798],</span></span><br><span class="line"><span class="comment">#         [-0.2936,  0.1872, -0.2724]])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([-0.3292,  0.6017,  0.4074])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([ 1.0588, -0.3788])</span></span><br></pre></td></tr></table></figure><h2 id="Indexing">Indexing</h2><p>可以使用索引从张量中提取指定的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"x: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"x[:1]: \n<span class="subst">&#123;x[:<span class="number">1</span>]&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"x[:1, 1:3]: \n<span class="subst">&#123;x[:<span class="number">1</span>, <span class="number">1</span>:<span class="number">3</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line">x:</span><br><span class="line"><span class="comment"># tensor([[-0.5524, -0.8358, -2.8240,  0.2564],</span></span><br><span class="line"><span class="comment">#         [ 0.5045, -1.1290,  0.7631,  1.0155],</span></span><br><span class="line"><span class="comment">#         [-1.2475, -0.0335,  0.5442,  0.4280]])</span></span><br><span class="line"><span class="comment"># x[:1]:</span></span><br><span class="line"><span class="comment"># tensor([[-0.5524, -0.8358, -2.8240,  0.2564]])</span></span><br><span class="line"><span class="comment"># x[:1, 1:3]:</span></span><br><span class="line"><span class="comment"># tensor([[-0.8358, -2.8240]])</span></span><br></pre></td></tr></table></figure><h2 id="Slicing">Slicing</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Select with dimensional indices</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">col_indices = torch.LongTensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">chosen = torch.index_select(x, dim=<span class="number">1</span>, index=col_indices) <span class="comment"># values from column 0 &amp; 2</span></span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;chosen&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">row_indices = torch.LongTensor([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">col_indices = torch.LongTensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">chosen = x[row_indices, col_indices] <span class="comment"># values from (0, 0) &amp; (1, 2)</span></span><br><span class="line">print(<span class="string">f"Values: \n<span class="subst">&#123;chosen&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-1.6357,  0.7964,  0.9450],</span></span><br><span class="line"><span class="comment">#         [-1.6535,  1.8129,  0.9162]])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([[-1.6357,  0.9450],</span></span><br><span class="line"><span class="comment">#         [-1.6535,  0.9162]])</span></span><br><span class="line"><span class="comment"># Values:</span></span><br><span class="line"><span class="comment"># tensor([-1.6357,  0.9162])</span></span><br></pre></td></tr></table></figure><h2 id="Joining">Joining</h2><p>我们还可以使用concatenate和stack来合并张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor([[-0.7004,  0.8429,  0.8971],</span></span><br><span class="line"><span class="comment">#         [-0.0272,  0.4722,  1.1621]])</span></span><br><span class="line"><span class="comment"># torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Concatenation</span></span><br><span class="line">y = torch.cat([x, x], dim=<span class="number">0</span>) <span class="comment"># concat on a specified dimension</span></span><br><span class="line"><span class="keyword">print</span> (y)</span><br><span class="line"><span class="keyword">print</span> (y.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor([[-0.7004,  0.8429,  0.8971],</span></span><br><span class="line"><span class="comment">#         [-0.0272,  0.4722,  1.1621],</span></span><br><span class="line"><span class="comment">#         [-0.7004,  0.8429,  0.8971],</span></span><br><span class="line"><span class="comment">#         [-0.0272,  0.4722,  1.1621]])</span></span><br><span class="line"><span class="comment"># torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Stacking</span></span><br><span class="line">z = torch.stack([x, x], dim=<span class="number">0</span>) <span class="comment"># stack on new dimension</span></span><br><span class="line"><span class="keyword">print</span> (z)</span><br><span class="line"><span class="keyword">print</span> (z.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor([[[-0.7004,  0.8429,  0.8971],</span></span><br><span class="line"><span class="comment">#          [-0.0272,  0.4722,  1.1621]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[-0.7004,  0.8429,  0.8971],</span></span><br><span class="line"><span class="comment">#          [-0.0272,  0.4722,  1.1621]]])</span></span><br><span class="line"><span class="comment"># torch.Size([2, 2, 3])</span></span><br></pre></td></tr></table></figure><h2 id="Gradients-梯度">Gradients 梯度</h2><p>我们可以使用梯度追踪(gradient bookkeeping) 来计算张量相对于其组成部分的梯度（变化率）。</p><p>梯度是机器学习和深度学习中最重要的概念，没有之一。后续会进一步介绍，这里先简单示例PyTorch如何计算某个函数在某点处的梯度:</p><p>$$<br>y = 3x + 2<br>$$</p><p>$$<br>z = \sum(y/N)<br>$$</p><p>$$<br>\frac{\partial(z)}{\partial(x)} = \frac{\partial(z)}{\partial(y)} \cdot \frac{\partial(y)}{\partial(x)} = \frac{1}{N} \cdot 3 = \frac{1}{3 \cdot 4} \cdot 3 = 0.25<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tensors with gradient bookkeeping</span></span><br><span class="line">x = torch.rand(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="literal">True</span>)  <span class="comment"># requires_grad=True 表示此处需要计算梯度</span></span><br><span class="line">y = <span class="number">3</span>*x + <span class="number">2</span></span><br><span class="line">z = y.mean()</span><br><span class="line">z.backward()  <span class="comment"># 此时开始计算梯度</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"x: \n<span class="subst">&#123;x&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"x.grad: \n<span class="subst">&#123;x.grad&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment"># tensor([[0.1154, 0.1101, 0.4831, 0.1580],</span></span><br><span class="line"><span class="comment">#         [0.4459, 0.2242, 0.9525, 0.8113],</span></span><br><span class="line"><span class="comment">#         [0.0387, 0.1512, 0.9678, 0.7512]], requires_grad=True)</span></span><br><span class="line"><span class="comment"># x.grad:</span></span><br><span class="line"><span class="comment"># tensor([[0.2500, 0.2500, 0.2500, 0.2500],</span></span><br><span class="line"><span class="comment">#         [0.2500, 0.2500, 0.2500, 0.2500],</span></span><br><span class="line"><span class="comment">#         [0.2500, 0.2500, 0.2500, 0.2500]])</span></span><br></pre></td></tr></table></figure><h2 id="CUDA">CUDA</h2><p>我们可以使用CUDA（Nvidia的并行计算平台和API）将张量加载到GPU上进行并行计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set device</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">x = torch.rand(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> (x.is_cuda)</span><br><span class="line">x = torch.rand(<span class="number">2</span>,<span class="number">3</span>).to(device)</span><br><span class="line"><span class="keyword">print</span> (x.is_cuda)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># False</span></span><br><span class="line"><span class="comment"># True</span></span><br></pre></td></tr></table></figure><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; PyTorch - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>到这里，便拥有了Way2AI路上需要的PyTorch的必备知识。我们可以发现，PyTorch的基础操作，与NumPy其实没什么太大的差别。</p><p>事实上NumPy和PyTorch可以相互转换，PyTorch提供了与NumPy兼容的接口，可以方便地将数据从NumPy数组转换为PyTorch张量，并在它们之间进行转换。这使得在使用PyTorch进行深度学习时，可以利用NumPy的强大功能进行数据预处理和后处理。</p><p>一般地，当我们需要进行常规的数值计算、数组操作和数学函数应用时，可以使用NumPy。当我们需要构建、训练和部署神经网络模型时，可以使用PyTorch。</p><p><a href="https://pytorch.org/" target="_blank" rel="noopener" title="PyTorch">PyTorch官网</a> 上有关于PyTorch的全部知识。</p>]]></content>
    
    <summary type="html">
    
      学习如何使用PyTorch这个机器学习框架。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="PyTorch" scheme="https://neo1989.net/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · Pandas</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-Pandas/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-Pandas/</id>
    <published>2023-05-17T08:15:17.000Z</published>
    <updated>2023-05-27T13:56:59.688Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文简单介绍了Pandas这个数据分析处理库的必备知识。</p><h2 id="Set-up">Set up</h2><p>首先，我们将导入NumPy和Pandas库，并设置随机种子以实现可重复性。<br>我们还要下载一个数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">np.random.seed(seed=<span class="number">1024</span>)</span><br></pre></td></tr></table></figure><h2 id="Load-Data">Load Data</h2><p>我们将在 <a href="https://www.kaggle.com/c/titanic/data" target="_blank" rel="noopener" title="Titanic dataset">Titanic</a> 这个数据集上完成学习，这是一个非常常见且丰富的数据集，包含了1912年登上泰坦尼克号的人员相关信息以及他们在远航中幸存与否，非常适合使用Pandas进行探索性数据分析。</p><p>让我们将CSV文件中的数据加载到Pandas dataframe中。header=0表示第一行（索引为0）是一个标题行，其中包含了我们数据集中每个列的名称。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Read from CSV to Pandas DataFrame</span></span><br><span class="line">url = <span class="string">"http://s3.mindex.xyz/datasets/titanic.csv"</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># First few items</span></span><br><span class="line">df.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>输出如下图所示：<br><img src="//s3.mindex.xyz/blog/Courses/11a15080ea7aff309c574598cf6d1f03.png" alt=""></p><p>解释一下数据的特征列：</p><ul><li>PassengerId: ID</li><li>Survived: 存活指标（0 - died, 1 - survived）</li><li>Pclass: 票的等级</li><li>Name: 旅客的全名</li><li>Sex: 性别</li><li>Age: 年龄</li><li>SibSp: 兄弟姐妹 / 配偶</li><li>Parch: 父母 / 子女</li><li>Ticket: 票号</li><li>Fare: 票价</li><li>Cabin: 房间号</li><li>Embarked: 出发的港口</li></ul><h2 id="探索性数据分析-EDA">探索性数据分析 (EDA)</h2><p>现在我们已经加载了数据，准备开始探索以找到有用的信息。<br>我们可以使用 .describe() 方法提取数值特征的一些标准细节。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Describe features</span></span><br><span class="line">df.describe()</span><br></pre></td></tr></table></figure><p>输出如下图所示：<br><img src="//s3.mindex.xyz/blog/Courses/094c0b85aee702ed965db3005be4b6e9.png" alt=""></p><p>导入matplotlib以提供更直观的数据可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Correlation matrix</span></span><br><span class="line">plt.matshow(df.corr())</span><br><span class="line">continuous_features = df.describe().columns</span><br><span class="line">plt.xticks(range(len(continuous_features)), continuous_features, rotation=<span class="string">"45"</span>)</span><br><span class="line">plt.yticks(range(len(continuous_features)), continuous_features, rotation=<span class="string">"45"</span>)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/6429df82847c28e9a8325d674bef1b41.png" alt=""></p><p>我们还可以使用.hist()函数来查看每个特征的值的直方图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Histograms</span></span><br><span class="line">df[<span class="string">"Age"</span>].hist()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/5482837c04020c25d4148ac1c83b5530.png" alt=""></p><p>使用.unique()函数查看特征值的所有类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unique values</span></span><br><span class="line">df[<span class="string">"Embarked"</span>].unique()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># array(['S', 'C', 'Q', nan], dtype=object)</span></span><br></pre></td></tr></table></figure><h2 id="Filtering">Filtering</h2><p>我们可以按照特征甚至是特定特征中的具体值（或值范围）来过滤数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Selecting data by feature</span></span><br><span class="line">df[<span class="string">"Name"</span>].head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 0                              Braund, Mr. Owen Harris</span></span><br><span class="line"><span class="comment"># 1    Cumings, Mrs. John Bradley (Florence Briggs Th...</span></span><br><span class="line"><span class="comment"># 2                               Heikkinen, Miss. Laina</span></span><br><span class="line"><span class="comment"># 3         Futrelle, Mrs. Jacques Heath (Lily May Peel)</span></span><br><span class="line"><span class="comment"># 4                             Allen, Mr. William Henry</span></span><br><span class="line"><span class="comment"># Name: Name, dtype: object</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Filtering</span></span><br><span class="line">df[df[<span class="string">"Sex"</span>]==<span class="string">"female"</span>].head() <span class="comment"># only the female data appear</span></span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/8c221ac048730e95fec59bd917ea6248.png" alt="filtering by sex"></p><h2 id="Sorting">Sorting</h2><p>我们还可以按升序或降序对功能进行排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sorting</span></span><br><span class="line">df.sort_values(<span class="string">"Age"</span>, ascending=<span class="literal">False</span>).head()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/4d34974359deaf3f3b1a27264a4b7ee6.png" alt=""></p><h2 id="Grouping">Grouping</h2><p>我们还可以针对特定分组获取特征的统计数据。在这里，我们想根据乘客是否幸存来查看连续特征的平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Grouping</span></span><br><span class="line">survived_group = df.groupby(<span class="string">"Survived"</span>)</span><br><span class="line">survived_group.mean()</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/68441798b47d6714e9ff255212a68128.png" alt=""></p><h2 id="Indexing">Indexing</h2><p>我们可以使用iloc在数据框中获取特定位置的行或列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Selecting row 0</span></span><br><span class="line">df.iloc[<span class="number">0</span>, :]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># PassengerId                          1</span></span><br><span class="line"><span class="comment"># Survived                             0</span></span><br><span class="line"><span class="comment"># Pclass                               3</span></span><br><span class="line"><span class="comment"># Name           Braund, Mr. Owen Harris</span></span><br><span class="line"><span class="comment"># Sex                               male</span></span><br><span class="line"><span class="comment"># Age                               22.0</span></span><br><span class="line"><span class="comment"># SibSp                                1</span></span><br><span class="line"><span class="comment"># Parch                                0</span></span><br><span class="line"><span class="comment"># Ticket                       A/5 21171</span></span><br><span class="line"><span class="comment"># Fare                              7.25</span></span><br><span class="line"><span class="comment"># Cabin                              NaN</span></span><br><span class="line"><span class="comment"># Embarked                             S</span></span><br><span class="line"><span class="comment"># Name: 0, dtype: object</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Selecting a specific value</span></span><br><span class="line">df.iloc[<span class="number">0</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 'Braund, Mr. Owen Harris'</span></span><br></pre></td></tr></table></figure><h2 id="Preprocessing">Preprocessing</h2><p>在探索完数据后，我们可以对数据集进行清洗和预处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Rows with at least one NaN value</span></span><br><span class="line">df[pd.isnull(df).any(axis=<span class="number">1</span>)]</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/9e922e0e75b89fa3d3104e990a5bd4b6.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Drop rows with Nan values</span></span><br><span class="line">df = df.dropna() <span class="comment"># removes rows with any NaN values</span></span><br><span class="line">df = df.reset_index() <span class="comment"># reset's row indexes in case any rows were dropped</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dropping multiple columns</span></span><br><span class="line">df = df.drop([<span class="string">"Name"</span>, <span class="string">"Cabin"</span>, <span class="string">"Ticket"</span>], axis=<span class="number">1</span>) <span class="comment"># we won't use text features for our initial basic models</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Map feature values</span></span><br><span class="line">df[<span class="string">"Sex"</span>] = df[<span class="string">"Sex"</span>].map( &#123;<span class="string">"female"</span>: <span class="number">0</span>, <span class="string">"male"</span>: <span class="number">1</span>&#125; ).astype(int)</span><br><span class="line">df[<span class="string">"Embarked"</span>] = df[<span class="string">"Embarked"</span>].dropna().map( &#123;<span class="string">"S"</span>:<span class="number">0</span>, <span class="string">"C"</span>:<span class="number">1</span>, <span class="string">"Q"</span>:<span class="number">2</span>&#125; ).astype(int)</span><br><span class="line"></span><br><span class="line">df</span><br></pre></td></tr></table></figure><p>结果如下：<br><img src="//s3.mindex.xyz/blog/Courses/ab30159a0aa58a9ca9fa1a2a3826a41a.png" alt=""></p><h2 id="Feature-Engineering-特征工程">Feature Engineering 特征工程</h2><p>我们现在要使用特征工程来创建一个名为FamilySize的列。我们将首先定义一个名为get_family_size的函数，该函数将使用父母和兄弟姐妹数量来确定家庭大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Lambda expressions to create new features</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_family_size</span><span class="params">(sibsp, parch)</span>:</span></span><br><span class="line">    family_size = sibsp + parch</span><br><span class="line">    <span class="keyword">return</span> family_size</span><br></pre></td></tr></table></figure><p>我们就可以使用lambda将该函数应用于每一行（使用每行中兄弟姐妹和父母的数量来确定每行的家庭规模）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">"FamilySize"</span>] = df[[<span class="string">"SibSp"</span>, <span class="string">"Parch"</span>]].apply(<span class="keyword">lambda</span> x: get_family_size(x[<span class="string">"SibSp"</span>], x[<span class="string">"Parch"</span>]), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reorganize headers</span></span><br><span class="line">df = df[[<span class="string">"Pclass"</span>, <span class="string">"Sex"</span>, <span class="string">"Age"</span>, <span class="string">"SibSp"</span>, <span class="string">"Parch"</span>, <span class="string">"FamilySize"</span>, <span class="string">"Fare"</span>, <span class="string">"Embarked"</span>, <span class="string">"Survived"</span>]]</span><br><span class="line"></span><br><span class="line">df</span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/917c83ce4d6135c15dd0214fc82aab46.png" alt=""></p><p>特征工程可以与领域专家合作进行，他们可以指导我们在工程和使用哪些特征。</p><h2 id="Save-Data">Save Data</h2><p>最后，让我们将预处理后的数据保存到一个新的CSV文件中以备后用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Saving dataframe to CSV</span></span><br><span class="line">df.to_csv(<span class="string">"processed_titanic.csv"</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="Scaling">Scaling</h2><p>当处理非常大的数据集时，我们的Pandas DataFrames可能会变得非常庞大，对它们进行操作可能会变得非常缓慢或不可行。这就是分布式工作负载或在更高效硬件上运行的软件包派上用场的地方。</p><ul><li><a href="https://dask.org/" target="_blank" rel="noopener" title="Dask">Dask</a>: 使用并行计算来扩展Numpy、Pandas和scikit-learn等软件包在单个/多台机器上的应用。</li><li><a href="https://github.com/rapidsai/cudf" target="_blank" rel="noopener" title="cuDF">cuDF</a>: 在GPU上高效加载和计算dataframe。</li></ul><p>当然，我们可以将它们（<a href="https://github.com/rapidsai/cudf/tree/main/python/dask_cudf" target="_blank" rel="noopener" title="Dask-cuDF">Dask-cuDF</a>）结合在一起，在GPU上对dataframe分块进行操作。</p><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Pandas - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>到这里，便拥有了Way2AI路上需要的Pandas的必备知识。</p><p>但我们不应该止步于此。<a href="https://pandas.pydata.org/" target="_blank" rel="noopener" title="Pandas">Pandas官网</a> 上有关于Pandas的全部知识。</p>]]></content>
    
    <summary type="html">
    
      使用Pandas库进行数据操作。
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · NumPy</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-NumPy/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-NumPy/</id>
    <published>2023-05-17T01:47:59.000Z</published>
    <updated>2023-05-27T13:56:52.237Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文简单介绍了NumPy这个科学计算扩展包的必备知识。</p><h2 id="Set-up">Set up</h2><p>首先我们需要导入NumPy包，做实验的时候可以设置随机种子以实现可重复性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">np.random.seed(seed=<span class="number">1024</span>)</span><br></pre></td></tr></table></figure><p>接下来分别示例 0D（标量）、1D（向量）、2D（矩阵）、3D（3维张量）。</p><p><img src="//s3.mindex.xyz/blog/Courses/49497eb3ffb5cdd0dc88c6e8e2d08270.png" alt="tensors"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scalar</span></span><br><span class="line">x = np.array(<span class="number">6</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x: "</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x ndim: "</span>, x.ndim) <span class="comment"># number of dimensions</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x shape:"</span>, x.shape) <span class="comment"># dimensions</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x size: "</span>, x.size) <span class="comment"># size of elements</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x dtype: "</span>, x.dtype) <span class="comment"># data type</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:  6</span></span><br><span class="line"><span class="comment"># x ndim:  0</span></span><br><span class="line"><span class="comment"># x shape: ()</span></span><br><span class="line"><span class="comment"># x size:  1</span></span><br><span class="line"><span class="comment"># x dtype:  int64 </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Vector</span></span><br><span class="line">x = np.array([<span class="number">1.3</span> , <span class="number">2.2</span> , <span class="number">1.7</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x: "</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x ndim: "</span>, x.ndim)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x shape:"</span>, x.shape)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x size: "</span>, x.size)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x dtype: "</span>, x.dtype) <span class="comment"># notice the float datatype</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:  [1.3 2.2 1.7]</span></span><br><span class="line"><span class="comment"># x ndim:  1</span></span><br><span class="line"><span class="comment"># x shape: (3,)</span></span><br><span class="line"><span class="comment"># x size:  3</span></span><br><span class="line"><span class="comment"># x dtype:  float64</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Matrix</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x ndim: "</span>, x.ndim)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x shape:"</span>, x.shape)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x size: "</span>, x.size)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x dtype: "</span>, x.dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[1 2]</span></span><br><span class="line"><span class="comment">#  [3 4]]</span></span><br><span class="line"><span class="comment"># x ndim:  2</span></span><br><span class="line"><span class="comment"># x shape: (2, 2)</span></span><br><span class="line"><span class="comment"># x size:  4</span></span><br><span class="line"><span class="comment"># x dtype:  int64</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3D Tensor</span></span><br><span class="line">x = np.array([[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], [[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x ndim: "</span>, x.ndim)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x shape:"</span>, x.shape)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x size: "</span>, x.size)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x dtype: "</span>, x.dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[[1 2]</span></span><br><span class="line"><span class="comment">#   [3 4]]</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#  [[5 6]</span></span><br><span class="line"><span class="comment">#   [7 8]]]</span></span><br><span class="line"><span class="comment"># x ndim:  3</span></span><br><span class="line"><span class="comment"># x shape: (2, 2, 2)</span></span><br><span class="line"><span class="comment"># x size:  8</span></span><br><span class="line"><span class="comment"># x dtype:  int64</span></span><br></pre></td></tr></table></figure><p>NumPy当然也提供了几个函数，可以快速创建张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Functions</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"np.zeros((2, 2)):\n"</span>, np.zeros((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"np.ones((2, 2)):\n"</span>, np.ones((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"np.eye((2)):\n"</span>, np.eye((<span class="number">2</span>))) <span class="comment"># identity matrix</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"np.random.random((2, 2)):\n"</span>, np.random.random((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># np.zeros((2, 2)):</span></span><br><span class="line"><span class="comment">#  [[0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0.]]</span></span><br><span class="line"><span class="comment"># np.ones((2, 2)):</span></span><br><span class="line"><span class="comment">#  [[1. 1.]</span></span><br><span class="line"><span class="comment">#  [1. 1.]]</span></span><br><span class="line"><span class="comment"># np.eye((2)):</span></span><br><span class="line"><span class="comment">#  [[1. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1.]]</span></span><br><span class="line"><span class="comment"># np.random.random((2, 2)):</span></span><br><span class="line"><span class="comment">#  [[0.64769123 0.99691358]</span></span><br><span class="line"><span class="comment">#  [0.51880326 0.65811273]]</span></span><br></pre></td></tr></table></figure><h2 id="Indexing">Indexing</h2><p>我们可以使用索引从张量中提取指定的值。<br>请记住，索引从0开始。与使用列表进行索引一样，我们也可以使用负数索引（其中-1是最后一个项目）。</p><p><img src="//s3.mindex.xyz/blog/Courses/f6ba5bef483ad3756ddb81f1d9a05576.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Indexing</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x: "</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[0]: "</span>, x[<span class="number">0</span>])</span><br><span class="line">x[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x: "</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:  [1 2 3]</span></span><br><span class="line"><span class="comment"># x[0]:  1</span></span><br><span class="line"><span class="comment"># x:  [0 2 3]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Slicing</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x column 1: "</span>, x[:, <span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x row 0: "</span>, x[<span class="number">0</span>, :])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x rows 0,1 &amp; cols 1,2: \n"</span>, x[<span class="number">0</span>:<span class="number">2</span>, <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[ 1  2  3  4]</span></span><br><span class="line"><span class="comment">#  [ 5  6  7  8]</span></span><br><span class="line"><span class="comment">#  [ 9 10 11 12]]</span></span><br><span class="line"><span class="comment"># x column 1:  [ 2  6 10]</span></span><br><span class="line"><span class="comment"># x row 0:  [1 2 3 4]</span></span><br><span class="line"><span class="comment"># x rows 0,1 &amp; cols 1,2: </span></span><br><span class="line"><span class="comment">#  [[2 3]</span></span><br><span class="line"><span class="comment">#  [6 7]]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Integer array indexing</span></span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line">rows_to_get = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"rows_to_get: "</span>, rows_to_get)</span><br><span class="line">cols_to_get = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"cols_to_get: "</span>, cols_to_get)</span><br><span class="line"><span class="comment"># Combine sequences above to get values to get</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"indexed values: "</span>, x[rows_to_get, cols_to_get]) <span class="comment"># (0, 0), (1, 2), (2, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[ 1  2  3  4]</span></span><br><span class="line"><span class="comment">#  [ 5  6  7  8]</span></span><br><span class="line"><span class="comment">#  [ 9 10 11 12]]</span></span><br><span class="line"><span class="comment"># rows_to_get:  [0 1 2]</span></span><br><span class="line"><span class="comment"># cols_to_get:  [0 2 1]</span></span><br><span class="line"><span class="comment"># indexed values:  [ 1  7 10]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Boolean array indexing</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x &gt; 2:\n"</span>, x &gt; <span class="number">2</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[x &gt; 2]:\n"</span>, x[x &gt; <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[1 2]</span></span><br><span class="line"><span class="comment">#  [3 4]</span></span><br><span class="line"><span class="comment">#  [5 6]]</span></span><br><span class="line"><span class="comment"># x &gt; 2:</span></span><br><span class="line"><span class="comment">#  [[False False]</span></span><br><span class="line"><span class="comment">#  [ True  True]</span></span><br><span class="line"><span class="comment">#  [ True  True]]</span></span><br><span class="line"><span class="comment"># x[x &gt; 2]:</span></span><br><span class="line"><span class="comment">#  [3 4 5 6]</span></span><br></pre></td></tr></table></figure><h2 id="Arithmetic-运算">Arithmetic 运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Basic math</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x + y:\n"</span>, np.add(x, y)) <span class="comment"># or x + y</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x - y:\n"</span>, np.subtract(x, y)) <span class="comment"># or x - y</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x * y:\n"</span>, np.multiply(x, y)) <span class="comment"># or x * y</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x + y:</span></span><br><span class="line"><span class="comment">#  [[2. 4.]</span></span><br><span class="line"><span class="comment">#  [6. 8.]]</span></span><br><span class="line"><span class="comment"># x - y:</span></span><br><span class="line"><span class="comment">#  [[0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 0.]]</span></span><br><span class="line"><span class="comment"># x * y:</span></span><br><span class="line"><span class="comment">#  [[ 1.  4.]</span></span><br><span class="line"><span class="comment">#  [ 9. 16.]]</span></span><br></pre></td></tr></table></figure><h2 id="Dot-product-点积">Dot product 点积</h2><p>在机器学习中，我们最常使用的NumPy操作之一是使用点积进行矩阵乘法。<br>假设我们需要取一个2x3的矩阵a和一个3x2的矩阵b的点积，我们将得到矩阵a的行及矩阵b的列作为点积的输出，也就是得到一个2x2的矩阵。点积能够正确运行需要满足的条件便是内部维度匹配，即示例中，矩阵a有3列，矩阵b有3行。</p><p><img src="//s3.mindex.xyz/blog/Courses/e11a898f34e386460b7dbb42d1cff842.gif" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Dot product</span></span><br><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=np.float64) <span class="comment"># we can specify dtype</span></span><br><span class="line">b = np.array([[<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>], [<span class="number">11</span>, <span class="number">12</span>]], dtype=np.float64)</span><br><span class="line">c = a.dot(b)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"<span class="subst">&#123;a.shape&#125;</span> · <span class="subst">&#123;b.shape&#125;</span> = <span class="subst">&#123;c.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># (2, 3) · (3, 2) = (2, 2)</span></span><br><span class="line"><span class="comment"># [[ 58.  64.]</span></span><br><span class="line"><span class="comment">#  [139. 154.]]</span></span><br></pre></td></tr></table></figure><h2 id="Axis-operations">Axis operations</h2><p>我们还可以沿着特定的轴进行操作。</p><p><img src="//s3.mindex.xyz/blog/Courses/a80be7714f670936b98bb5769037e313.gif" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sum across a dimension</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sum all: "</span>, np.sum(x)) <span class="comment"># adds all elements</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sum axis=0: "</span>, np.sum(x, axis=<span class="number">0</span>)) <span class="comment"># sum across rows</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sum axis=1: "</span>, np.sum(x, axis=<span class="number">1</span>)) <span class="comment"># sum across columns</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[1 2]</span></span><br><span class="line"><span class="comment">#  [3 4]]</span></span><br><span class="line"><span class="comment"># sum all:  10</span></span><br><span class="line"><span class="comment"># sum axis=0:  [4 6]</span></span><br><span class="line"><span class="comment"># sum axis=1:  [3 7]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Min/Max</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"min: "</span>, x.min())</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"max: "</span>, x.max())</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"min axis=0: "</span>, x.min(axis=<span class="number">0</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"min axis=1: "</span>, x.min(axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># min:  1</span></span><br><span class="line"><span class="comment"># max:  6</span></span><br><span class="line"><span class="comment"># min axis=0:  [1 2 3]</span></span><br><span class="line"><span class="comment"># min axis=1:  [1 4]</span></span><br></pre></td></tr></table></figure><h2 id="Broadcast">Broadcast</h2><p>当我们尝试使用看似不兼容的张量形状进行操作时会发生什么？<br>它们的维度不兼容，但是NumPy为何仍然给出了结果？这就是广播的作用。</p><p><img src="//s3.mindex.xyz/blog/Courses/db0288a489a3c03ebdea1be427f1d963.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Broadcasting</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>]) <span class="comment"># vector</span></span><br><span class="line">y = np.array(<span class="number">3</span>) <span class="comment"># scalar</span></span><br><span class="line">z = x + y</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"z:\n"</span>, z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># z:</span></span><br><span class="line"><span class="comment">#  [4 5]</span></span><br></pre></td></tr></table></figure><h2 id="Gotchas">Gotchas</h2><p>在下面的情况中，c的值是多少，它的形状是什么？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = np.array((<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">b = np.expand_dims(a, axis=<span class="number">1</span>)</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line">a.shape <span class="comment"># (3,)</span></span><br><span class="line">b.shape <span class="comment"># (3, 1)</span></span><br><span class="line">c.shape <span class="comment"># (3, 3)</span></span><br><span class="line"><span class="keyword">print</span> (c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># array([[ 6,  7,  8],</span></span><br><span class="line"><span class="comment">#         [ 7,  8,  9],</span></span><br><span class="line"><span class="comment">#         [ 8,  9, 10]])</span></span><br></pre></td></tr></table></figure><p>如果我们不想出现意外的广播行为，就需要小心确保 矩阵a 和 矩阵b 的形状相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = a.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">a.shape <span class="comment"># (3, 1)</span></span><br><span class="line">c = a + b</span><br><span class="line">c.shape <span class="comment"># (3, 1)</span></span><br><span class="line"><span class="keyword">print</span> (c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[ 6]</span></span><br><span class="line"><span class="comment">#  [ 8]</span></span><br><span class="line"><span class="comment">#  [10]]</span></span><br></pre></td></tr></table></figure><h2 id="Transpose-转置">Transpose 转置</h2><p>我们经常需要改变张量的维度，以进行诸如点积之类的操作。如果我们需要交换两个维度，可以对张量进行转置。</p><p><img src="//s3.mindex.xyz/blog/Courses/9fe94457fbf3d53aca519eb20ebf487f.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Transposing</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x.shape: "</span>, x.shape)</span><br><span class="line">y = np.transpose(x, (<span class="number">1</span>, <span class="number">0</span>)) <span class="comment"># flip dimensions at index 0 and 1</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y:\n"</span>, y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y.shape: "</span>, y.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]]</span></span><br><span class="line"><span class="comment"># x.shape:  (2, 3)</span></span><br><span class="line"><span class="comment"># y:</span></span><br><span class="line"><span class="comment">#  [[1 4]</span></span><br><span class="line"><span class="comment">#  [2 5]</span></span><br><span class="line"><span class="comment">#  [3 6]]</span></span><br><span class="line"><span class="comment"># y.shape:  (3, 2)</span></span><br></pre></td></tr></table></figure><h2 id="Reshape">Reshape</h2><p>reshape是另一种改变张量形状的办法。<br>如下面所示，我们reshape后的张量与原始张量具有相同数量的值。我们还可以在一个维度上使用<code>-1</code>，NumPy会根据输入张量自动推断该维度的大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reshaping</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x.shape: "</span>, x.shape)</span><br><span class="line">y = np.reshape(x, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: \n"</span>, y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y.shape: "</span>, y.shape)</span><br><span class="line">z = np.reshape(x, (<span class="number">2</span>, <span class="number">-1</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"z: \n"</span>, z)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"z.shape: "</span>, z.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[1 2 3 4 5 6]]</span></span><br><span class="line"><span class="comment"># x.shape:  (1, 6)</span></span><br><span class="line"><span class="comment"># y:</span></span><br><span class="line"><span class="comment">#  [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]]</span></span><br><span class="line"><span class="comment"># y.shape:  (2, 3)</span></span><br><span class="line"><span class="comment"># z:</span></span><br><span class="line"><span class="comment">#  [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]]</span></span><br><span class="line"><span class="comment"># z.shape:  (2, 3)</span></span><br></pre></td></tr></table></figure><p>reshape函数的工作原理是查看新张量的每个维度，并将原始张量分成相应数量的单元。因此，在这里，新张量<code>index 0</code>处的维度为2，因此我们将原始张量分成2个单元，每个单元都有3个值。</p><p><img src="//s3.mindex.xyz/blog/Courses/1aaa08d5c71d13009b4e969815e1e8a8.png" alt=""></p><h2 id="Joining">Joining</h2><p>我们还可以使用concatenate和stack来合并张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.random((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[0.14950888 0.698439   0.59335256]</span></span><br><span class="line"><span class="comment">#  [0.89991535 0.44445739 0.316785  ]]</span></span><br><span class="line"><span class="comment"># (2, 3)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Concatenation</span></span><br><span class="line">y = np.concatenate([x, x], axis=<span class="number">0</span>) <span class="comment"># concat on a specified axis</span></span><br><span class="line"><span class="keyword">print</span> (y)</span><br><span class="line"><span class="keyword">print</span> (y.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[0.14950888 0.698439   0.59335256]</span></span><br><span class="line"><span class="comment">#  [0.89991535 0.44445739 0.316785  ]</span></span><br><span class="line"><span class="comment">#  [0.14950888 0.698439   0.59335256]</span></span><br><span class="line"><span class="comment">#  [0.89991535 0.44445739 0.316785  ]]</span></span><br><span class="line"><span class="comment"># (4, 3)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Stacking</span></span><br><span class="line">z = np.stack([x, x], axis=<span class="number">0</span>) <span class="comment"># stack on new axis</span></span><br><span class="line"><span class="keyword">print</span> (z)</span><br><span class="line"><span class="keyword">print</span> (z.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[[0.14950888 0.698439   0.59335256]</span></span><br><span class="line"><span class="comment">#   [0.89991535 0.44445739 0.316785  ]]</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#  [[0.14950888 0.698439   0.59335256]</span></span><br><span class="line"><span class="comment">#   [0.89991535 0.44445739 0.316785  ]]]</span></span><br><span class="line"><span class="comment"># (2, 2, 3)</span></span><br></pre></td></tr></table></figure><h2 id="Expanding-Reducing">Expanding / Reducing</h2><p>我们还可以轻松地向张量中添加和删除维度，这样做是为了使张量能够兼容某些操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Adding dimensions</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x.shape: "</span>, x.shape)</span><br><span class="line">y = np.expand_dims(x, axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: \n"</span>, y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y.shape: "</span>, y.shape)   <span class="comment"># notice extra set of brackets are added</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]]</span></span><br><span class="line"><span class="comment"># x.shape:  (2, 3)</span></span><br><span class="line"><span class="comment"># y:</span></span><br><span class="line"><span class="comment">#  [[[1 2 3]]</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#  [[4 5 6]]]</span></span><br><span class="line"><span class="comment"># y.shape:  (2, 1, 3)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Removing dimensions</span></span><br><span class="line">x = np.array([[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]], [[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x:\n"</span>, x)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x.shape: "</span>, x.shape)</span><br><span class="line">y = np.squeeze(x, axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: \n"</span>, y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y.shape: "</span>, y.shape)  <span class="comment"># notice extra set of brackets are gone</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x:</span></span><br><span class="line"><span class="comment">#  [[[1 2 3]]</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#  [[4 5 6]]]</span></span><br><span class="line"><span class="comment"># x.shape:  (2, 1, 3)</span></span><br><span class="line"><span class="comment"># y:</span></span><br><span class="line"><span class="comment">#  [[1 2 3]</span></span><br><span class="line"><span class="comment">#  [4 5 6]]</span></span><br><span class="line"><span class="comment"># y.shape:  (2, 3)</span></span><br></pre></td></tr></table></figure><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; NumPy - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>到这里，便拥有了Way2AI路上需要的NumPy的必备知识。</p><p>但我们不应该止步于此。<a href="https://numpy.org/" target="_blank" rel="noopener" title="NumPy">NumPy官网</a> 上有关于NumPy的全部知识。</p>]]></content>
    
    <summary type="html">
    
      Numerical analysis with the NumPy computing package.
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Way2AI · Python</title>
    <link href="https://neo1989.net/Way2AI/Way2AI-python/"/>
    <id>https://neo1989.net/Way2AI/Way2AI-python/</id>
    <published>2023-05-16T10:36:00.000Z</published>
    <updated>2023-05-27T13:57:05.958Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p><p>本文简单介绍了Python这门语言的必备知识。</p><h2 id="Variables">Variables</h2><p>变量是用于存储数据的容器，它们由名称和值定义。<br>在我们的示例中，我们使用变量名x，但是当您处理特定任务时，请确保在创建变量（函数、类等）时指定<strong>明确含义的名称</strong>（例如first_name），以确保程序的可读性。</p><p><img src="//s3.mindex.xyz/blog/Courses/c249bf303d806ffea1f144af762cd99c.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># integer variable</span></span><br><span class="line">x = <span class="number">5</span></span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"><span class="keyword">print</span> (type(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 5 </span></span><br><span class="line"><span class="comment"># &lt;class 'int'&gt;</span></span><br></pre></td></tr></table></figure><p>我们可以通过将新值赋给变量来改变它的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sring variable</span></span><br><span class="line">x = <span class="string">"hello"</span></span><br><span class="line"><span class="keyword">print</span> (x, type(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># hello &lt;class 'str'&gt;</span></span><br></pre></td></tr></table></figure><p>python有许多不同类型的变量：integers, floats, strings, boolean 等等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># int variable</span></span><br><span class="line">x = <span class="number">5</span></span><br><span class="line"><span class="keyword">print</span> (x, type(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 5 &lt;class 'int'&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># float variable</span></span><br><span class="line">x = <span class="number">5.0</span></span><br><span class="line"><span class="keyword">print</span> (x, type(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 5.0 &lt;class 'float'&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># string variable</span></span><br><span class="line">x = <span class="string">"5"</span></span><br><span class="line"><span class="keyword">print</span> (x, type(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 5 &lt;class 'str'&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># boolean variable</span></span><br><span class="line">x = <span class="literal">True</span></span><br><span class="line"><span class="keyword">print</span> (x, type(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># True &lt;class 'bool'&gt;</span></span><br></pre></td></tr></table></figure><p>我们也可以使用变量进行运算操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Variables can be used with each other</span></span><br><span class="line">a = <span class="number">1</span></span><br><span class="line">b = <span class="number">2</span></span><br><span class="line">c = a + b</span><br><span class="line"><span class="keyword">print</span> (c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 3</span></span><br></pre></td></tr></table></figure><p>我们应该始终知道我们正在处理哪种类型的变量，以便可以正确地对它们进行操作。</p><h2 id="Lists">Lists</h2><p>列表是一个有序的、可变的值集合，由方括号包围并用逗号分隔。一个列表可以包含许多不同类型的变量。下面是一个包含整数、字符串和浮点数的列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating a list</span></span><br><span class="line">x = [<span class="number">3</span>, <span class="string">"hello"</span>, <span class="number">1.2</span>]</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [3, 'hello', 1.2]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Length of a list</span></span><br><span class="line">len(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 3</span></span><br></pre></td></tr></table></figure><p>使用append方法向列表里添加元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Adding to a list</span></span><br><span class="line">x.append(<span class="number">7</span>)</span><br><span class="line"><span class="keyword">print</span> (x, len(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [3, 'hello', 1.2, 7] 4</span></span><br></pre></td></tr></table></figure><p>通过indexing修改已存在的元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Replacing items in a list</span></span><br><span class="line">x[<span class="number">1</span>] = <span class="string">"bye"</span></span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [3, 'bye', 1.2, 7]</span></span><br></pre></td></tr></table></figure><p>列表也可以进行操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Operations</span></span><br><span class="line">y = [<span class="number">2.4</span>, <span class="string">"world"</span>]</span><br><span class="line">z = x + y</span><br><span class="line"><span class="keyword">print</span> (z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [3, 'bye', 1.2, 7, 2.4, 'world']</span></span><br></pre></td></tr></table></figure><h2 id="Tuples">Tuples</h2><p>元组是有序且不可变（无法更改）的集合。我们将使用元组来存储永远不会更改的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating a tuple</span></span><br><span class="line">x = (<span class="number">3.0</span>, <span class="string">"hello"</span>) <span class="comment"># tuples start and end with ()</span></span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># (3.0, 'hello')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Adding values to a tuple</span></span><br><span class="line">x = x + (<span class="number">5.6</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">print</span> (x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># (3.0, 'hello', 5.6, 4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Try to change (it won't work and we get an error)</span></span><br><span class="line">x[<span class="number">0</span>] = <span class="number">1.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># TypeError: 'tuple' object does not support item assignment</span></span><br></pre></td></tr></table></figure><h2 id="Sets">Sets</h2><p>集合是无序且可变的。而且，集合中的每个元素必须是唯一的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sets</span></span><br><span class="line">text = <span class="string">"Learn ML with Made With ML"</span></span><br><span class="line"><span class="keyword">print</span> (set(text))</span><br><span class="line"><span class="keyword">print</span> (set(text.split(<span class="string">" "</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;'e', 'M', ' ', "r", "w", 'd', 'a', 'h', 't', 'i', 'L', 'n', "w"&#125;</span></span><br><span class="line"><span class="comment"># &#123;'with', 'Learn', 'ML', 'Made', 'With'&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Indexing">Indexing</h2><p>从列表中进行索引和切片可以让我们检索到特定的值。请注意，索引可以是正数（从0开始）或负数（-1及以下，其中-1是列表中的最后一项）。</p><p><img src="//s3.mindex.xyz/blog/Courses/72e91f603cb74286d4147011b61e07f4.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Indexing</span></span><br><span class="line">x = [<span class="number">3</span>, <span class="string">"hello"</span>, <span class="number">1.2</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[0]: "</span>, x[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[1]: "</span>, x[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[-1]: "</span>, x[<span class="number">-1</span>]) <span class="comment"># the last item</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[-2]: "</span>, x[<span class="number">-2</span>]) <span class="comment"># the second to last item</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x[0]:  3</span></span><br><span class="line"><span class="comment"># x[1]:  hello</span></span><br><span class="line"><span class="comment"># x[-1]:  1.2</span></span><br><span class="line"><span class="comment"># x[-2]:  hello</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Slicing</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[:]: "</span>, x[:]) <span class="comment"># all indices</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[1:]: "</span>, x[<span class="number">1</span>:]) <span class="comment"># index 1 to the end of the list</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[1:2]: "</span>, x[<span class="number">1</span>:<span class="number">2</span>]) <span class="comment"># index 1 to index 2 (not including index 2)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[:-1]: "</span>, x[:<span class="number">-1</span>]) <span class="comment"># index 0 to last index (not including last index)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x[:]:  [3, 'hello', 1.2</span></span><br><span class="line"><span class="comment"># x[1:]:  ['hello', 1.2]</span></span><br><span class="line"><span class="comment"># x[1:2]:  ['hello']</span></span><br><span class="line"><span class="comment"># x[:-1]:  [3, 'hello']</span></span><br></pre></td></tr></table></figure><h2 id="Dictionaries">Dictionaries</h2><p>字典是一种无序的可变键值对集合。您可以根据键检索值，而且一个字典不能有两个相同的键。</p><p><img src="//s3.mindex.xyz/blog/Courses/fb3ab2534970cdff498d3914ed172716.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating a dictionary</span></span><br><span class="line">person = &#123;<span class="string">"name"</span>: <span class="string">"Goku"</span>,</span><br><span class="line">          <span class="string">"eye_color"</span>: <span class="string">"brown"</span>&#125;</span><br><span class="line"><span class="keyword">print</span> (person)</span><br><span class="line"><span class="keyword">print</span> (person[<span class="string">"name"</span>])</span><br><span class="line"><span class="keyword">print</span> (person[<span class="string">"eye_color"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;"name": "Goku", "eye_color": "brown"&#125;</span></span><br><span class="line"><span class="comment"># Goku</span></span><br><span class="line"><span class="comment"># brown</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Adding new key-value pairs</span></span><br><span class="line">person[<span class="string">"age"</span>] = <span class="number">24</span></span><br><span class="line"><span class="keyword">print</span> (person)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;"name": "Goku", "eye_color": "green", "age": 24&#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Length of a dictionary</span></span><br><span class="line"><span class="keyword">print</span> (len(person))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 3</span></span><br></pre></td></tr></table></figure><p>当然，使用Python几乎可以实现任何事情。例如，尽管原生字典是无序的，但我们可以利用OrderedDict数据结构来改变它（如果我们想按特定顺序迭代键等，则非常有用）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line"><span class="comment"># Native dict</span></span><br><span class="line">d = &#123;&#125;</span><br><span class="line">d[<span class="string">"a"</span>] = <span class="number">2</span></span><br><span class="line">d[<span class="string">"c"</span>] = <span class="number">3</span></span><br><span class="line">d[<span class="string">"b"</span>] = <span class="number">1</span></span><br><span class="line"><span class="keyword">print</span> (d)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;'a': 2, 'c': 3, 'b': 1&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Python3.7以后，原生的字典是是按插入的顺序排序的。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dictionary items</span></span><br><span class="line"><span class="keyword">print</span> (d.items())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># dict_items([('a', 2), ('c', 3), ('b', 1)])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Order by keys</span></span><br><span class="line"><span class="keyword">print</span> (OrderedDict(sorted(d.items())))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># OrderedDict([('a', 2), ('b', 1), ('c', 3)])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Order by values</span></span><br><span class="line"><span class="keyword">print</span> (OrderedDict(sorted(d.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># OrderedDict([('b', 1), ('a', 2), ('c', 3)])</span></span><br></pre></td></tr></table></figure><h2 id="If-statements">If statements</h2><p>我们可以使用if语句来有条件地执行某些操作。条件由if、elif（代表else if）和else这几个关键字定义。我们可以拥有任意多个elif语句。每个条件下面缩进的代码是在该条件为True时将要执行的代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If statement</span></span><br><span class="line">x = <span class="number">4</span></span><br><span class="line"><span class="keyword">if</span> x &lt; <span class="number">1</span>:</span><br><span class="line">    score = <span class="string">"low"</span></span><br><span class="line"><span class="keyword">elif</span> x &lt;= <span class="number">4</span>: <span class="comment"># elif = else if</span></span><br><span class="line">    score = <span class="string">"medium"</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    score = <span class="string">"high"</span></span><br><span class="line"><span class="keyword">print</span> (score)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># medium</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># If statement with a boolean</span></span><br><span class="line">x = <span class="literal">True</span></span><br><span class="line"><span class="keyword">if</span> x:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"it worked"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># it worked</span></span><br></pre></td></tr></table></figure><h2 id="Loops">Loops</h2><h3 id="For-loops">For loops</h3><p>for循环可以迭代值的集合（列表、元组、字典等）。缩进的代码对于集合中的每个项目都会执行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For loop</span></span><br><span class="line">veggies = [<span class="string">"carrots"</span>, <span class="string">"broccoli"</span>, <span class="string">"beans"</span>]</span><br><span class="line"><span class="keyword">for</span> veggie <span class="keyword">in</span> veggies:</span><br><span class="line">    <span class="keyword">print</span> (veggie)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># carrots</span></span><br><span class="line"><span class="comment"># broccoli</span></span><br><span class="line"><span class="comment"># beans</span></span><br></pre></td></tr></table></figure><p>当循环遇到break命令时，循环将立即终止。如果列表中还有更多的项，则它们不会被处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># `break` from a for loop</span></span><br><span class="line">veggies = [<span class="string">"carrots"</span>, <span class="string">"broccoli"</span>, <span class="string">"beans"</span>]</span><br><span class="line"><span class="keyword">for</span> veggie <span class="keyword">in</span> veggies:</span><br><span class="line">    <span class="keyword">if</span> veggie == <span class="string">"broccoli"</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">print</span> (veggie)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># carrots</span></span><br></pre></td></tr></table></figure><p>当循环遇到 continue 命令时，该循环将仅跳过列表中当前项的所有其他操作。如果列表中还有更多项，则循环将正常继续。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># `continue` to the next iteration</span></span><br><span class="line">veggies = [<span class="string">"carrots"</span>, <span class="string">"broccoli"</span>, <span class="string">"beans"</span>]</span><br><span class="line"><span class="keyword">for</span> veggie <span class="keyword">in</span> veggies:</span><br><span class="line">    <span class="keyword">if</span> veggie == <span class="string">"broccoli"</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">print</span> (veggie)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># carrots</span></span><br><span class="line"><span class="comment"># beans</span></span><br></pre></td></tr></table></figure><h3 id="While-Loops">While Loops</h3><p>当条件为真时，while循环可以重复执行。我们也可以在while循环中使用continue和break命令。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># While loop</span></span><br><span class="line">x = <span class="number">3</span></span><br><span class="line"><span class="keyword">while</span> x &gt; <span class="number">0</span>:</span><br><span class="line">    x -= <span class="number">1</span> <span class="comment"># same as x = x - 1</span></span><br><span class="line">    <span class="keyword">print</span> (x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 0</span></span><br></pre></td></tr></table></figure><h2 id="List-comprehension">List comprehension</h2><p>我们可以结合列表和for循环的知识，利用<code>列表推导式</code>来创建简洁的代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For loop</span></span><br><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">y = []</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> x:</span><br><span class="line">    <span class="keyword">if</span> item &gt; <span class="number">2</span>:</span><br><span class="line">        y.append(item)</span><br><span class="line"><span class="keyword">print</span> (y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [3, 4, 5]</span></span><br></pre></td></tr></table></figure><p><img src="//s3.mindex.xyz/blog/Courses/b4995feadf0e5ccf8642f301ede28c44.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># List comprehension</span></span><br><span class="line">y = [item <span class="keyword">for</span> item <span class="keyword">in</span> x <span class="keyword">if</span> item &gt; <span class="number">2</span>]</span><br><span class="line"><span class="keyword">print</span> (y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [3, 4, 5]</span></span><br></pre></td></tr></table></figure><p>下面示例一个复杂的列表推导式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Nested for loops</span></span><br><span class="line">words = [[<span class="string">"Am"</span>, <span class="string">"ate"</span>, <span class="string">"ATOM"</span>, <span class="string">"apple"</span>], [<span class="string">"bE"</span>, <span class="string">"boy"</span>, <span class="string">"ball"</span>, <span class="string">"bloom"</span>]]</span><br><span class="line">small_words = []</span><br><span class="line"><span class="keyword">for</span> letter_list <span class="keyword">in</span> words:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> letter_list:</span><br><span class="line">        <span class="keyword">if</span> len(word) &lt; <span class="number">3</span>:</span><br><span class="line">            small_words.append(word.lower())</span><br><span class="line"><span class="keyword">print</span> (small_words)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># ['am', 'be']</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Nested list comprehension</span></span><br><span class="line">small_words = [word.lower() <span class="keyword">for</span> letter_list <span class="keyword">in</span> words <span class="keyword">for</span> word <span class="keyword">in</span> letter_list <span class="keyword">if</span> len(word) &lt; <span class="number">3</span>]</span><br><span class="line"><span class="keyword">print</span> (small_words)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># ['am', 'be']</span></span><br></pre></td></tr></table></figure><h2 id="Functions">Functions</h2><p>函数是一种将可重用代码模块化的方式。它们由关键字def定义，代表definition，可以包含以下组件。<br><img src="//s3.mindex.xyz/blog/Courses/3898be6c32f04d864a2586e873e4926c.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_two</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Increase x by 2."""</span></span><br><span class="line">    x += <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>以下是使用该函数时可能需要的组件。我们需要确保函数名称和输入参数与上面定义的函数匹配。<br><img src="//s3.mindex.xyz/blog/Courses/9ca41d3f8448ba4c422a4ae26b76deee.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use the function</span></span><br><span class="line">score = <span class="number">0</span></span><br><span class="line">new_score = add_two(x=score)</span><br><span class="line"><span class="keyword">print</span> (new_score)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 2</span></span><br></pre></td></tr></table></figure><p>一个函数可以有任意多个输入参数和输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Function with multiple inputs</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join_name</span><span class="params">(first_name, last_name)</span>:</span></span><br><span class="line">    <span class="string">"""Combine first name and last name."""</span></span><br><span class="line">    joined_name = first_name + <span class="string">" "</span> + last_name</span><br><span class="line">    <span class="keyword">return</span> joined_name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the function</span></span><br><span class="line">first_name = <span class="string">"Goku"</span></span><br><span class="line">last_name = <span class="string">"Mohandas"</span></span><br><span class="line">joined_name = join_name(</span><br><span class="line">    first_name=first_name, last_name=last_name)</span><br><span class="line"><span class="keyword">print</span> (joined_name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Goku Mohandas</span></span><br></pre></td></tr></table></figure><p>在使用函数时，始终使用关键字参数是一个好习惯，这样非常清楚每个输入变量属于哪个函数输入参数。相关的是，您经常会看到术语*args和**kwargs，它们代表位置参数和关键字参数。当它们传递到函数中时，可以提取它们。星号的重要性在于任何数量的位置参数和关键字参数都可以传递到函数中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">    x = args[<span class="number">0</span>]</span><br><span class="line">    y = kwargs.get(<span class="string">"y"</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">f"x: <span class="subst">&#123;x&#125;</span>, y: <span class="subst">&#123;y&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">f(<span class="number">5</span>, y=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># x: 5, y: 2</span></span><br></pre></td></tr></table></figure><h2 id="Classes">Classes</h2><p>类是对象构造函数，也是Python面向对象编程的基本组成部分。它们由一组定义类及其操作的函数组成。</p><h3 id="Magic-methods">Magic methods</h3><p>可以使用像<code>__init__</code>和<code>__str__</code>这样的魔术方法来自定义类，以实现强大的操作。这些也被称为dunder方法，表示有两个前置和后置下划线。</p><p><code>__init__</code>方法用于初始化类的实例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating the class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pet</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Class object for a pet."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, species, name)</span>:</span></span><br><span class="line">        <span class="string">"""Initialize a Pet."""</span></span><br><span class="line">        self.species = species</span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Creating an instance of a class</span></span><br><span class="line">my_dog = Pet(species=<span class="string">"dog"</span>,</span><br><span class="line">             name=<span class="string">"Scooby"</span>)</span><br><span class="line"><span class="keyword">print</span> (my_dog)</span><br><span class="line"><span class="keyword">print</span> (my_dog.name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;__main__.Pet object at 0x7fe487e9c358&gt;</span></span><br><span class="line"><span class="comment"># Scooby</span></span><br></pre></td></tr></table></figure><p><code>print(my_dog)</code>命令打印了一些与我们不太相关的内容。让我们使用__str__函数来修复它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating the class</span></span><br><span class="line"><span class="comment"># Creating the class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pet</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Class object for a pet."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, species, name)</span>:</span></span><br><span class="line">        <span class="string">"""Initialize a Pet."""</span></span><br><span class="line">        self.species = species</span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Output when printing an instance of a Pet."""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"<span class="subst">&#123;self.species&#125;</span> named <span class="subst">&#123;self.name&#125;</span>"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Creating an instance of a class</span></span><br><span class="line">my_dog = Pet(species=<span class="string">"dog"</span>,</span><br><span class="line">             name=<span class="string">"Scooby"</span>)</span><br><span class="line"><span class="keyword">print</span> (my_dog)</span><br><span class="line"><span class="keyword">print</span> (my_dog.name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># dog named Scooby</span></span><br><span class="line"><span class="comment"># Scooby</span></span><br></pre></td></tr></table></figure><h3 id="Object-functions">Object functions</h3><p>除了这些魔术方法，类还可以拥有对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating the class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pet</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Class object for a pet."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, species, name)</span>:</span></span><br><span class="line">        <span class="string">"""Initialize a Pet."""</span></span><br><span class="line">        self.species = species</span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Output when printing an instance of a Pet."""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"<span class="subst">&#123;self.species&#125;</span> named <span class="subst">&#123;self.name&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">change_name</span><span class="params">(self, new_name)</span>:</span></span><br><span class="line">        <span class="string">"""Change the name of your Pet."""</span></span><br><span class="line">        self.name = new_name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Creating an instance of a class</span></span><br><span class="line">my_dog = Pet(species=<span class="string">"dog"</span>, name=<span class="string">"Scooby"</span>)</span><br><span class="line"><span class="keyword">print</span> (my_dog)</span><br><span class="line"><span class="keyword">print</span> (my_dog.name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># dog named Scooby</span></span><br><span class="line"><span class="comment"># Scooby</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Using a class's function</span></span><br><span class="line">my_dog.change_name(new_name=<span class="string">"Scrappy"</span>)</span><br><span class="line"><span class="keyword">print</span> (my_dog)</span><br><span class="line"><span class="keyword">print</span> (my_dog.name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># dog named Scrappy</span></span><br><span class="line"><span class="comment"># Scrappy</span></span><br></pre></td></tr></table></figure><h3 id="Inheritance">Inheritance</h3><p>我们还可以使用继承来构建类的层次结构，这样我们就能够从另一个类（父类）中继承所有属性和方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span><span class="params">(Pet)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, breed)</span>:</span></span><br><span class="line">        super().__init__(species=<span class="string">"dog"</span>, name=name)</span><br><span class="line">        self.breed = breed</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"A <span class="subst">&#123;self.breed&#125;</span> doggo named <span class="subst">&#123;self.name&#125;</span>"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scooby = Dog(species=<span class="string">"dog"</span>, breed=<span class="string">"Great Dane"</span>, name=<span class="string">"Scooby"</span>)</span><br><span class="line"><span class="keyword">print</span> (scooby)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># A Great Dane doggo named Scooby</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scooby.change_name(<span class="string">"Scooby Doo"</span>)</span><br><span class="line"><span class="keyword">print</span> (scooby)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># A Great Dane doggo named Scooby Doo</span></span><br></pre></td></tr></table></figure><p>注意我们从父类Pet继承了初始化变量，如species和name。我们还继承了像change_name()这样的函数。</p><h3 id="Methods">Methods</h3><p>在类方面，有两个重要的装饰器方法需要了解：@classmethod 和 @staticmethod。我们将在下一节中学习装饰器，但这些特定的方法与类相关，因此我们将在此处介绍它们。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span><span class="params">(Pet)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, breed)</span>:</span></span><br><span class="line">        super().__init__(species=<span class="string">"dog"</span>, name=name)</span><br><span class="line">        self.breed = breed</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"<span class="subst">&#123;self.breed&#125;</span> named <span class="subst">&#123;self.name&#125;</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_dict</span><span class="params">(cls, d)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(name=d[<span class="string">"name"</span>], breed=d[<span class="string">"breed"</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_cute</span><span class="params">(breed)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span>  <span class="comment"># all animals are cute!</span></span><br></pre></td></tr></table></figure><p><code>@classmethod</code>允许我们通过传递未实例化的类本身（cls）来创建类实例。这是从对象（例如字典）创建（或加载）类的好方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create instance</span></span><br><span class="line">d = &#123;<span class="string">"name"</span>: <span class="string">"Cassie"</span>, <span class="string">"breed"</span>: <span class="string">"Border Collie"</span>&#125;</span><br><span class="line">cassie = Dog.from_dict(d=d)</span><br><span class="line">print(cassie)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Border Collie named Cassie</span></span><br></pre></td></tr></table></figure><p>一个 <code>@staticmethod</code> 可以从未实例化的类对象中调用，因此我们可以这样做：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Static method</span></span><br><span class="line">res = Dog.is_cute(breed=<span class="string">"Border Collie"</span>)</span><br><span class="line"><span class="keyword">print</span> (res)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># True</span></span><br></pre></td></tr></table></figure><h2 id="Decorators">Decorators</h2><p>回想一下，函数允许我们将代码模块化并重复使用。然而，我们经常希望在主要函数执行之前或之后添加一些功能，并且可能希望对许多不同的函数进行此操作。我们可以使用装饰器来实现这个目的，而不是向原始函数添加更多代码！</p><p>装饰器：在函数之前/之后增加处理以增强该函数。装饰器包裹在主函数周围，允许我们对输入和/输出进行操作。</p><p>假设我们有一个名为operations的函数，它将输入值x增加1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">operations</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Basic operations."""</span></span><br><span class="line">    x += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">operations(x=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 2</span></span><br></pre></td></tr></table></figure><p>现在假设我们想要在操作函数执行前后将输入 x 增加 1，为了说明这个例子，让我们假设增量必须是单独的步骤。</p><p>下面是我们通过更改原始代码来完成的方案：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">operations</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Basic operations."""</span></span><br><span class="line">    x += <span class="number">1</span></span><br><span class="line">    x += <span class="number">1</span></span><br><span class="line">    x += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">operations(x=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 4</span></span><br></pre></td></tr></table></figure><p>我们已经实现了想要的功能，但是我们现在增加了操作函数的规模，如果我们想对任何其他函数进行相同的递增操作，那么我们也必须将相同的代码添加到所有这些函数中… 这不太高效。为了解决这个问题，让我们创建一个名为<code>add</code>的装饰器，在主函数f执行之前和之后将x增加1。</p><h3 id="Creating-a-decorator">Creating a decorator</h3><p>装饰器函数接受一个函数f作为参数，这个函数是我们想要包装的函数，在本例中，它是operations()。装饰器的输出是其包装器函数，该函数接收传递给函数f的参数和关键字参数。</p><p>在包装函数中，我们可以：</p><ul><li><ol><li>提取传递给函数f的输入参数</li></ol></li><li><ol start="2"><li>对函数输入进行任何更改</li></ol></li><li><ol start="3"><li>执行函数f</li></ol></li><li><ol start="4"><li>对函数输出进行任何更改</li></ol></li><li><ol start="5"><li>wrapper函数返回一些值，这就是装饰器返回的值，因为它也返回wrapper。</li></ol></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(f)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        <span class="string">"""Wrapper function for @add."""</span></span><br><span class="line">        x = kwargs.pop(<span class="string">"x"</span>) <span class="comment"># .get() if not altering x</span></span><br><span class="line">        x += <span class="number">1</span> <span class="comment"># executes before function f</span></span><br><span class="line">        x = f(*args, **kwargs, x=x)</span><br><span class="line">        x += <span class="number">1</span> <span class="comment"># executes after function f</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure><p>我们可以通过在主函数顶部添加 <code>@</code> 符号来简单地使用这个装饰器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@add</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">operations</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Basic operations."""</span></span><br><span class="line">    x += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">operations(x=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 4</span></span><br></pre></td></tr></table></figure><p>假设我们想要调试并查看实际执行operations()函数的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">operations.__name__, operations.__doc__</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># ('wrapper', 'Wrapper function for @add.')</span></span><br></pre></td></tr></table></figure><p>函数名和文档字符串并不是我们要寻找的内容，但它们出现在这里是因为执行的是包装器函数。为了解决这个问题，Python 提供了 functools.wraps 函数来保留主函数的元数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(f)</span>:</span></span><br><span class="line"><span class="meta">    @wraps(f)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrap</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        <span class="string">"""Wrapper function for @add."""</span></span><br><span class="line">        x = kwargs.pop(<span class="string">"x"</span>)</span><br><span class="line">        x += <span class="number">1</span></span><br><span class="line">        x = f(*args, **kwargs, x=x)</span><br><span class="line">        x += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">return</span> wrap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@add</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">operations</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Basic operations."""</span></span><br><span class="line">    x += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">operations.__name__, operations.__doc__</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># ('operations', 'Basic operations.')</span></span><br></pre></td></tr></table></figure><p>太棒了！我们成功地使用装饰器来实现自定义，而不必修改函数本身。我们可以将这个装饰器用于其他需要相同自定义的函数中！</p><h3 id="Callbacks">Callbacks</h3><p>装饰器允许在主函数执行前后进行自定义操作，但是中间呢？假设我们想有条件地/有场景地执行一些操作。我们可以使用回调函数，而不是编写大量的if语句并使我们的函数臃肿。</p><p>回调：在函数内的有条件/有场景地执行任务。</p><p>我们定义一个有特殊函数的类，这些函数将在主函数执行期间的各个时期执行。 函数名称由我们决定，但我们需要在主要功能中调用相同的回调函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Callback</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XTracker</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.history = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">at_start</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.history.append(x)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">at_end</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.history.append(x)</span><br></pre></td></tr></table></figure><p>我们可以传入任意数量的回调函数，因为它们具有特定命名的函数，所以它们将在适当的时间被调用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">operations</span><span class="params">(x, callbacks=[])</span>:</span></span><br><span class="line">    <span class="string">"""Basic operations."""</span></span><br><span class="line">    <span class="keyword">for</span> callback <span class="keyword">in</span> callbacks:</span><br><span class="line">        callback.at_start(x)</span><br><span class="line">    x += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> callback <span class="keyword">in</span> callbacks:</span><br><span class="line">        callback.at_end(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = <span class="number">1</span></span><br><span class="line">tracker = XTracker(x=x)</span><br><span class="line">operations(x=x, callbacks=[tracker])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tracker.history</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [1, 2]</span></span><br></pre></td></tr></table></figure><h3 id="Putting-it-all-together">Putting it all together</h3><p>decorators + callbacks = 不增加复杂度的情况下在函数执行的前、中或后执行超强的自定义任务。 我们将在未来的中使用这个组合来创建强大的机器学习/深度学习训练脚本，这些脚本具有高度可定制性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(f)</span>:</span></span><br><span class="line"><span class="meta">    @wraps(f)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrap</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        <span class="string">"""Wrapper function for @add."""</span></span><br><span class="line">        x = kwargs.pop(<span class="string">"x"</span>) <span class="comment"># .get() if not altering x</span></span><br><span class="line">        x += <span class="number">1</span> <span class="comment"># executes before function f</span></span><br><span class="line">        x = f(*args, **kwargs, x=x)</span><br><span class="line">        <span class="comment"># can do things post function f as well</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">return</span> wrap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Callback</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XTracker</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.history = [x]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">at_start</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.history.append(x)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">at_end</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.history.append(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Main function</span></span><br><span class="line"><span class="meta">@add</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">operations</span><span class="params">(x, callbacks=[])</span>:</span></span><br><span class="line">    <span class="string">"""Basic operations."""</span></span><br><span class="line">    <span class="keyword">for</span> callback <span class="keyword">in</span> callbacks:</span><br><span class="line">        callback.at_start(x)</span><br><span class="line">    x += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> callback <span class="keyword">in</span> callbacks:</span><br><span class="line">        callback.at_end(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = <span class="number">1</span></span><br><span class="line">tracker = XTracker(x=x)</span><br><span class="line">operations(x=x, callbacks=[tracker])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tracker.history</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [1, 2, 3]</span></span><br></pre></td></tr></table></figure><h2 id="Citation">Citation</h2><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Python - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Ending">Ending</h2><p>到这里，便拥有了Way2AI路上需要的Python的必备知识。</p><p>但我们不应该止步于此，应该学习更多的Python知识以面对更大的挑战。<br><a href="https://www.python.org/" target="_blank" rel="noopener" title="python">Python官网</a> 上有关于Python这门语言全部知识。</p>]]></content>
    
    <summary type="html">
    
      The fundamentals of Python programming for AI.
    
    </summary>
    
    
      <category term="Way2AI" scheme="https://neo1989.net/categories/Way2AI/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>NLP的神经网络简史</title>
    <link href="https://neo1989.net/Notes/NOTE-neural-history-of-nlp/"/>
    <id>https://neo1989.net/Notes/NOTE-neural-history-of-nlp/</id>
    <published>2023-05-13T15:08:58.000Z</published>
    <updated>2023-05-15T15:22:42.770Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>本文主要内容源于NLP神经网络发展史及<a href="https://aylien.com/blog/a-review-of-the-recent-history-of-natural-language-processing#2001neurallanguagemodels" target="_blank" rel="noopener" title="A Review of the Neural History of NLP">《A Review of the Neural History of Natural Language Processing》</a>，仅供学习探讨，如有错误均是本人的，与他人无关。</p><h2 id="2000-Neural-Language-Models">2000 - Neural Language Models</h2><p>语言建模的任务是在给定前面的词的情况下预测文本中的下一个词。这可能是最简单的语言处理任务，并且有具体的实际应用，比如智能输入法和电子邮件回复建议。语言建模有着丰富的历史，经典的方法是基于n-gram的，并采用平滑处理来解决未见n-grams的问题。</p><p>第一个神经网络语言模型 <strong>前馈神经网络</strong> 首次于2000年在文章《A Neural Probabilistic Language Model》中被提出，并收录于<a href="https://proceedings.neurips.cc/paper_files/paper/2000" target="_blank" rel="noopener" title="NIPS 2000">NIPS 2000</a>。</p><p><img src="//s3.mindex.xyz/blog/Notes/7bf9f44058ca39125ca0926b43dc0bb8.png" alt="前馈神经网络"></p><p>该神经网络语言模型的主要构成如下：</p><ul><li>输入层：输入通过查询表C得到的前n个词的向量表示。也就是如今大家熟知的词嵌入。</li><li>隐藏层：将n个词嵌入连接起来，作为隐藏层的输入。以进一步学习词之间的表示。</li><li>输出层：隐藏层的输出被提供给softmax以输出概率。</li></ul><p>该模型的主要novelty在于引入了词嵌入，这比简单的one-hot编码能提供更丰富的词义信息。这些词嵌入可以体现出词与词之间的关系，从而捕捉到更复杂的上下文关系。<br>模型通过最大似然估计来训练整个网络，找到最优的词嵌入和网络参数，使得模型可以更好地预测下一个词。<br>关于该模型的更多内容，参考<a href="https://www.ruder.io/word-embeddings-1/" target="_blank" rel="noopener" title="Word Embeddings">这篇博文</a>。</p><h2 id="2008-Multi-task-learning">2008 - Multi-task learning</h2><p>多任务学习，是一种通过在多个任务上训练模型来共享参数的一般方法。在神经网络中，多任务学习可以通过共享不同层的权重来简单实现。<br>多任务学习鼓励模型学习对许多任务都有用的表征。这对于学习通用的底层表征，聚焦模型的注意力或在训练数据有限的设置中特别有用。<br>多任务学习于2008年在文章<a href="http://machinelearning.org/archive/icml2008/papers/391.pdf" target="_blank" rel="noopener" title="A Unifield Architecture for NLP">《A Unified Architecture for Natural Language Processing》</a>中首次被应用于NLP神经网络。在他们的模型中，使用共享的lookup tables（词嵌入矩阵）实现多任务学习。如下图所示</p><p><img src="//s3.mindex.xyz/blog/Notes/b9dfb3dddfad8124c4c5314841284859.png" alt="共享词嵌入矩阵"></p><p>共享词嵌入使得模型能够协作并分享词嵌入矩阵中的底层信息，这通常构成了模型中最大数量的参数。该论文不仅使用了多任务学习，而且还推动了预训练词嵌入和使用卷积神经网络（CNNs）处理文本等思想，这些思想后来来才被广泛采用。而且这篇论文获得了<a href="https://research.facebook.com/blog/2018/07/facebook-researchers-win-test-of-time-award-at-icml-2018/" target="_blank" rel="noopener" title="the test-of-time award at ICML 2018">the test-of-time award at ICML 2018</a>。</p><h2 id="2013-Word-embeddings">2013 - Word embeddings</h2><p>文本的向量表示，在自然语言处理中有着悠久的历史。早在2001年，我们就已经看到了单词的密集向量表示或词嵌入。Mikolov等人于2013年提出的主要创新是通过去除隐藏层并近似目标来使这些单词嵌入的训练更加高效。虽然这些变化在性质上很简单，但它们与高效的word2vec实现一起，使得大规模训练单词嵌入成为可能。</p><p>Word2vec有两种不同的模型，如下图所示：连续词袋模型（CBOW）和Skip-gram模型。它们的目标不同：一个是基于周围单词预测中心单词，而另一个则相反。</p><p><img src="//s3.mindex.xyz/blog/Notes/52a8eded3320bc69bed21ce7393b708a.png" alt="Continuous bag-of-words and skip-gram architectures (Mikolov et al., 2013a; 2013b)"></p><p>虽然这些嵌入在概念上与使用前馈神经网络学习的嵌入没有区别，但是在大型语料库上进行训练使它们能够捕捉到单词之间的某些关系，例如性别、动词时态和国家-首都关系，如下图所示。</p><p><img src="//s3.mindex.xyz/blog/Notes/f091661e00c40308e7af75b2b0c81fcc.png" alt="Relations captured by word2vec (Mikolov et al., 2013a; 2013b)"></p><p>这些关系及其背后的含义引发了对词嵌入的初步兴趣，许多研究探讨了这些线性关系的起源。虽然word2vec捕获的关系具有直观和近乎神奇的特质，但后来的研究表明，word2vec本身并没有什么特别之处：词嵌入也可以通过矩阵分解进行学习，并且经过适当调整，像SVD和LSA这样的经典矩阵分解方法可以实现类似的结果。</p><h2 id="2013-Neural-networks-for-NLP">2013 - Neural networks for NLP</h2><p>2013年和2014年是神经网络模型开始在自然语言处理中得到应用的时间。三种主要类型的神经网络变得最为广泛使用：循环神经网络、卷积神经网络和递归神经网络。</p><p>循环神经网络（RNNs）是处理自然语言处理中普遍存在的动态输入序列的明显选择。但传统的<a href="https://www.dlsi.ua.es/~mlf/nnafmc/papers/elman90finding.pdf" target="_blank" rel="noopener" title="Finding Structure in Time">Vanilla RNNs（Elman, 1990）</a> 很快被经典的<a href="https://ieeexplore.ieee.org/abstract/document/6795963" target="_blank" rel="noopener" title="Long Short-Term Memory">长短期记忆网络(LSTM)（Hochreiter＆Schmidhuber, 1997）</a>所取代，后者对于梯度消失和爆炸问题更具鲁棒性。</p><p>LSTM单元如下图所示，而一个<a href="https://ieeexplore.ieee.org/abstract/document/6707742" target="_blank" rel="noopener" title="Hybrid speech recognition with Deep Bidirectional LSTM">bidirectional LSTM(Graves et al., 2013)</a>通常用于处理左右两侧的上下文。</p><p><img src="//s3.mindex.xyz/blog/Notes/134bf9c41fc64fb60f29effbc4adcca4.png" alt="An LSTM network"></p><p>随着卷积神经网络（CNNs）在计算机视觉中的广泛应用，它们也开始被应用于语言领域（<a href="https://arxiv.org/abs/1404.2188" target="_blank" rel="noopener" title="A Convolutional Neural Network for Modelling Sentences">Kalchbrenner et al., 2014</a>）。文本卷积神经网络仅在两个维度上运行，并且滤波器只需要沿时间维度移动。下图显示了NLP中使用的典型CNN。</p><p><img src="//s3.mindex.xyz/blog/Notes/32da185b5fde2ca9f37171f50682e7a2.png" alt="A convolutional neural network for text (Kim, 2014)"></p><p>卷积神经网络的一个优点是它们比循环神经网络更易于并行化，因为每个时间步的状态仅取决于局部上下文（通过卷积操作），而不像RNNs中那样依赖所有过去状态。可以使用扩张卷积来扩展具有更广泛感受野的CNNs以捕获更广泛的上下文（<a href="https://arxiv.org/abs/1610.10099" target="_blank" rel="noopener" title="Neural Machine Translation in Linear Time">Kalchbrenner et al., 2016</a>）。CNNs和LSTMs也可以组合和堆叠，并且可以使用卷积来加速LSTM。</p><p>RNNs和CNNs都将语言视为序列。然而，从语言学的角度来看，语言本质上是分层的：单词组成更高级别的短语和从句，这些短语和从句可以根据一组生成规则进行递归组合。在语言学启发下，将句子视为树而不是序列的想法引出了递归神经网络，如下图所示。</p><p><img src="//s3.mindex.xyz/blog/Notes/38990eb262a258a52ca3d53b52362337.png" alt="A recursive neural network (Socher et al., 2013)"></p><p>递归神经网络从底部向上构建序列的表示，与RNNs相比，后者是从左到右或从右到左处理句子。在树的每个节点处，通过组合子节点的表示来计算新的表示。由于树也可以被视为对RNNs施加不同的处理顺序，因此LSTMs自然而然地扩展到了树结构。</p><p>于是，不只是循环神经网络和长短时记忆网络可以扩展以处理分层结构，词嵌入亦可以基于局部上下文或语法上下文进行学习（<a href="https://aclanthology.org/P14-2050.pdf" target="_blank" rel="noopener" title="Dependency-Based Word Embeddings">Levy＆Goldberg, 2014</a>）， 语言模型可以根据句法堆栈生成单词（<a href="https://www.sciencedirect.com/science/article/abs/pii/S0028393220301500" target="_blank" rel="noopener" title="Localizing syntactic predictions using recurrent neural network grammars">Dyer et al., 2016</a>）， 而且图卷积神经网络可以在树上操作（<a href="https://arxiv.org/abs/1704.04675" target="_blank" rel="noopener" title="Graph Convolutional Encoders for Syntax-aware Neural Machine Translation">Bastings et al., 2017</a>）。</p><h2 id="2014-Seq2Seq-models">2014 - Seq2Seq models</h2><p>2014年，Sutskever等人提出了seq2seq，这是一种使用神经网络将一个序列映射到另一个序列的通用框架。在这个框架中，编码器神经网络逐个符号地处理一个句子，并将其压缩成向量表示；解码器神经网络根据编码器状态逐个预测输出符号，在每一步都以先前预测的符号作为输入，如下图所示。</p><p><img src="//s3.mindex.xyz/blog/Notes/259c165e05db3259338615a284cb7bc4.png" alt="A sequence-to-sequence model (Sutskever et al., 2014)"></p><p>机器翻译成为了该框架的杀手级应用。2016年，谷歌宣布开始用神经机器翻译模型（<a href="https://arxiv.org/abs/1609.08144" target="_blank" rel="noopener" title="Google's Neural Machine Translation System">Wu et al.，2016</a>）替换其基于短语的机器翻译模型。根据Jeff Dean的说法，这意味着用一个500行的神经网络模型代替了50万行的基于短语的机器翻译代码。</p><p>由于其灵活性，这个框架现在已成为自然语言生成任务的首选框架，不同的模型扮演编码器和解码器的角色。重要的是，解码器模型不仅可以基于序列进行条件化，还可以基于任意表示进行条件化。例如，在图像上生成标题（<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Vinyals_Show_and_Tell_2015_CVPR_paper.html" target="_blank" rel="noopener" title="A Neural Image Caption Generator">Vinyals et al., 2015</a>）（如下图所示），根据表格生成文本（<a href="https://arxiv.org/abs/1603.07771" target="_blank" rel="noopener" title="Neural Text Generation from Structured Data with Application to the Biography Domain">Lebret et al., 2016</a>），以及根据源代码更改生成描述（<a href="https://arxiv.org/abs/1704.04856" target="_blank" rel="noopener" title="A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes">Loyola et al., 2017</a>）等许多其他应用。</p><p><img src="//s3.mindex.xyz/blog/Notes/83398534fc191b613e45801767e23402.png" alt="Generating a caption based on an image (Vinyals et al., 2015)"></p><p>序列到序列学习甚至可以应用于NLP中常见的结构化预测任务，其中输出具有特定的结构。如下图简单的成分句法分析示例，可以看出输出被线性化了。神经网络已经证明了在给定足够数量的成分句法解析（<a href="https://arxiv.org/abs/1412.7449" target="_blank" rel="noopener" title="Grammar as a Foreign Language">Vinyals et al., 2015</a>）和命名实体识别（<a href="https://arxiv.org/abs/1512.00103" target="_blank" rel="noopener" title="Multilingual Language Processing From Bytes">Gillick et al., 2016</a>）训练数据的情况下直接学习生成这种线性化输出的能力。</p><p><img src="//s3.mindex.xyz/blog/Notes/1ec99e613705027ba8bb0ff275f0a45b.png" alt="Linearizing a constituency parse tree (Vinyals et al., 2015)"></p><h2 id="2015-Attention">2015 - Attention</h2><p>注意力机制（<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener" title="Neural Machine Translation by Jointly Learning to Align and Translate">Bahdanau et al., 2015</a>）是神经机器翻译（NMT）中的核心创新之一，也是使NMT模型胜过传统基于短语的机器翻译系统的关键思想。序列到序列学习的主要瓶颈在于需要将源序列的整个内容压缩成固定大小的向量。注意力通过允许解码器回顾源序列隐藏状态来缓解这个问题，然后将其作为加权平均值提供给解码器作为附加输入，如下图所示。</p><p><img src="//s3.mindex.xyz/blog/Notes/f45ebf52e4a5d70c6847755553774912.png" alt="Attention (Bahdanau et al., 2015)"></p><p>注意力机制广泛适用于任何需要基于输入的某些部分做出决策的任务，并具有潜在的实用价值。它已被应用于成分句法分析（<a href="https://arxiv.org/abs/1412.7449" target="_blank" rel="noopener" title="Grammar as a Foreign Language">Vinyals et al., 2015</a>）、阅读理解（<a href="https://arxiv.org/abs/1506.03340" target="_blank" rel="noopener" title="Teaching Machines to Read and Comprehend">Hermann et al., 2015</a>）和一次性学习（<a href="https://arxiv.org/abs/1606.04080" target="_blank" rel="noopener" title="Matching Networks for One Shot Learning">Vinyals et al., 2016</a>）等许多领域。输入甚至不需要是一个序列，而可以由其他表示形式组成，如下图所示的图像字幕生成（<a href="https://proceedings.mlr.press/v37/xuc15.html" target="_blank" rel="noopener" title="Show, Attend and Tell: Neural Image Caption Generation with Visual Attention">Xu et al., 2015</a>）。注意力机制的一个有用的副作用是通过检查注意权重来确定哪些输入部分与特定输出相关联，从而提供了对模型内部工作方式的一瞥。</p><p><img src="//s3.mindex.xyz/blog/Notes/2fad1cf2fd8986f0b2689aad1807eb75.png" alt="Visual attention in an image captioning model indicating what the model is attending to when generating the word &quot;frisbee&quot;. (Xu et al., 2015)"></p><p>注意力不仅限于查看输入序列；<strong>自注意力</strong>可以用来查看句子或文档中周围的单词，以获得更具上下文敏感性的单词表示。多层自注意是Transformer架构（<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener" title="Attention Is All You Need">Vaswani et al.,2017</a>）的核心，这是当前最先进的NMT模型。</p><h2 id="2015-Memory-based-networks">2015 - Memory-based networks</h2><p>注意力可以被看作是一种模糊记忆形式，其中记忆由模型的过去隐藏状态组成，而模型选择从记忆中检索什么。关于注意力及其与记忆的联系的更详细概述，请查看<a href="https://dennybritz.com/posts/wildml/attention-and-memory-in-deep-learning-and-nlp/" target="_blank" rel="noopener" title="Attention and Memory in Deep Learning and NLP">此文章</a>。已经提出了许多具有更明确内存的模型。它们有不同的变体，例如神经图灵机（<a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener" title="Neural Turing Machines">Graves et al., 2014</a>），记忆网络（<a href="https://arxiv.org/abs/1410.3916" target="_blank" rel="noopener" title="Memory Networks">Weston et al., 2015</a>）和端到端内存网络（<a href="https://arxiv.org/abs/1503.08895" target="_blank" rel="noopener" title="End-To-End Memory Networks">Sukhbaatar et al., 2015</a>），动态内存网络（<a href="https://arxiv.org/abs/1506.07285" target="_blank" rel="noopener" title="Ask Me Anythins: Dynamic Memory Networks for Natural Language Processing">Kumar et al., 2015</a>），神经可微分计算机（<a href="https://www.nature.com/articles/nature20101" target="_blank" rel="noopener" title="Hybrid computing using a neural network with dynamic external memory">Graves et al., 2016</a>）以及循环实体网络（<a href="https://arxiv.org/abs/1612.03969" target="_blank" rel="noopener" title="Tracking the World State with Recurrent Entity Networks">Henaff et al., 2017</a>）。</p><p>记忆通常是基于与当前状态的相似性进行访问，类似于注意力，并且通常可以进行写入和读取。模型在实现和利用内存方面存在差异。例如，端到端记忆网络多次处理输入并更新内存以实现多步推理。神经图灵机还具有基于位置的寻址功能，使它们能够学习简单的计算机程序，如排序。基于记忆的模型通常应用于需要保留信息更长时间跨度可能会有帮助的任务中，例如语言建模和阅读理解。记忆概念非常灵活：知识库或表格可以作为一种内存功能，而内存也可以根据整个输入或特定部分来填充。</p><h2 id="2018-Pretrained-language-models">2018 - Pretrained language models</h2><p>预训练的词嵌入是与上下文无关的，仅用于初始化我们模型中的第一层。后来，一系列监督任务已被用来预训练神经网络（<a href="https://arxiv.org/abs/1705.02364" target="_blank" rel="noopener" title="Supervised Learning of Universal Sentence Representations from Natural Language Inference Data">Conneau et al., 2017</a>; <a href="https://proceedings.neurips.cc/paper_files/paper/2017/hash/20c86a628232a67e7bd46f76fba7ce12-Abstract.html" target="_blank" rel="noopener" title="Learned in Translation: Contextualized Word Vectors">McCann et al., 2017</a>; <a href="https://arxiv.org/abs/1804.00079" target="_blank" rel="noopener" title="Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning">Subramanian et al., 2018</a>）。相比之下，语言模型只需要未标记的文本；因此可以扩展到数十亿个标记、新领域和新语言。<br>预先训练的语言模型最初是在2015年提出的（<a href="https://proceedings.neurips.cc/paper_files/paper/2015/hash/7137debd45ae4d0ab9aa953017286b20-Abstract.html" target="_blank" rel="noopener" title="Semi-supervised Sequence Learning">Dai＆Le, 2015</a>）；直到18年才显示它们对各种任务都有益处。<br>语言模型嵌入可以用作目标模型（<a href="https://arxiv.org/abs/1809.09795" target="_blank" rel="noopener" title="Deep contextualized word representations">Peters et al., 2018</a>）中的特征或者可以在目标任务数据上微调语言模型（<a href="https://arxiv.org/abs/1611.02683" target="_blank" rel="noopener" title="Unsupervised Pretraining for Sequence to Sequence Learning">Ramachandran et al., 2017</a>; <a href="https://arxiv.org/abs/1801.06146" target="_blank" rel="noopener" title="Universal Language Model Fine-tuning for Text Classification">Howard＆Ruder, 2018</a>）。<br>如下图所示，在许多不同任务中添加语言模型嵌入可大幅改善现有技术水平。</p><p><img src="//s3.mindex.xyz/blog/Notes/94b544d5bf32bd5298c52e477b3bcdf2.png" alt="Improvements with language model embeddings over the state-of-the-art (Peters et al., 2018)"></p><p>预训练语言模型被证明可以在使用更少数据的情况下进行学习。由于语言模型只需要无标签数据，因此它们对于标注数据稀缺的低资源语言尤其有益。</p><h2 id="2018-Large-language-models">2018 - Large language models</h2><p>如你所知，整个NLP领域发展到了LLMs时代。参考阅读<a href="https://mp.weixin.qq.com/s/_pDxwIy7Z8punWMKoJiJKQ" target="_blank" rel="noopener" title="ChatGPT核心技术的进化之路">《ChatGPT核心技术的进化之路》</a></p><h2 id="结尾">结尾</h2><p>GPT的提出标志着NLP技术的飞速发展，但对新手来说也增加了学习难度。要从头学习NLP，Claude给出了以下建议：</p><ul><li>了解NLP基础知识。包括语言建模，词嵌入，神经网络，LSTM，Transformer等基础模型和技术。这些知识可以通过阅读《自然语言处理综述》等资料得到。</li><li>实现基本NLP任务的模型。如语言建模，文本分类，问答系统等。可以从简单的RNN和LSTM模型实现，了解基本思路和步骤。</li><li>理解Transformer和GPT等新模型。阅读论文《Attention is All You Need》和《Improving Language Understanding by Generative Pre-Training》，理解其模型结构，训练方法和创新点。</li><li>学习PyTorch和TensorFlow等深度学习框架。这些框架可以让你更容易实现复杂的NLP模型，对学习NLP模型很有帮助。</li><li>了解BERT，XLNet，RoBERTa等预训练语言模型。这些模型的使用和微调技巧需要理解，可以带来很大便利。</li><li>跟进NLP最新进展。阅读ACL，EMNLP，NAACL等顶会论文，了解最新的模型，技术和数据集。这可以让你了解NLP最新研究热点和前沿方向。</li><li>多练习和实现。上手实践是学习NLP最重要的方式之一。可以实现论文中的模型，优化和改进这些模型，并在各种数据集上进行试验。这可以加深理解和提高实践能力。</li><li>多阅读优质学习资料。像《神经网络与深度学习》《动手学深度学习》等书籍，全面且深入地介绍了深度学习和NLP相关知识。这些资料对学习NLP大有帮助。</li></ul><p>总之，要系统和全面地学习NLP，需要理论与实践相结合。理解基础知识和最新研究进展，跟进理论前沿；并且要大量动手实践，通过实现和优化相关模型来加深理解。同时，也要利用优质学习资料，能够更全面和深入地学习这个领域的知识。持之以恒，NLP的学习之路才能走得更加通透。</p>]]></content>
    
    <summary type="html">
    
      了解历史背景可以帮助我们更好地理解其概念，同时也有助于掌握其未来发展方向和趋势。
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="NLP" scheme="https://neo1989.net/tags/NLP/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>LangChain | 快速释放LLMs的能力 (三)</title>
    <link href="https://neo1989.net/Notes/NOTE-langchain-3/"/>
    <id>https://neo1989.net/Notes/NOTE-langchain-3/</id>
    <published>2023-05-12T04:49:09.000Z</published>
    <updated>2023-05-27T13:58:33.795Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>本文介绍了一种方法，让LLM拥有了上网的能力。</p><h2 id="结果展示">结果展示</h2><p>如下图，问了它两个股票的价格现价，都能够正确的给出答案。</p><p><img src="//s3.mindex.xyz/blog/Notes/3c6908b694ec7ccaf268899e3e33c208.png" alt=""></p><h2 id="如何实现">如何实现</h2><p>在没有大模型之前，这个问题如何实现？</p><p>其实也简单，一点点NLP技能就可以解决。</p><p>NLP伪代码:</p><ul><li>抽取问题里的词槽，上述示例中词槽便是 stock_name: 科大讯飞</li><li>执行对应的action，也就是调用方法get_current_price(stockname=“科大讯飞”)</li><li>格式化输出</li></ul><p>那这里的问题是，在抽取阶段，你需要写很多的规则，或者自己训练一个信息抽取模型。</p><p>如今，交给LLMs就行了。</p><p>来看看LangChain是如何简化这件事情的。这里就需要用到开篇里介绍的<a href="https://python.langchain.com/en/latest/modules/agents.html" target="_blank" rel="noopener" title="LangChain Agents">Agents</a>组件了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sina <span class="keyword">import</span> Sina</span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel, Field</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain.chat_models <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> AgentType, initialize_agent</span><br><span class="line"><span class="keyword">from</span> langchain.tools <span class="keyword">import</span> BaseTool</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Type, Optional</span><br><span class="line"><span class="keyword">from</span> langchain.callbacks.manager <span class="keyword">import</span> CallbackManagerForToolRun, AsyncCallbackManagerForToolRun</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StockQueryInput</span><span class="params">(BaseModel)</span>:</span></span><br><span class="line">    stock_name: str = Field()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StockPriceQueryTool</span><span class="params">(BaseTool)</span>:</span></span><br><span class="line">    name = <span class="string">"StockPriceQuery"</span></span><br><span class="line">    description = <span class="string">"useful for when you need to answer questions about the price of a stock code"</span></span><br><span class="line">    args_schema: Type[BaseModel] = StockQueryInput</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run</span><span class="params">(self, stock_name: str, run_manager: Optional[CallbackManagerForToolRun] = None)</span> -&gt; str:</span></span><br><span class="line"></span><br><span class="line">        stock_code = StockCodeMapping.get(stock_name)</span><br><span class="line">        sina = Sina(stock_code)</span><br><span class="line">        res = sina.get_current_price()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f"当前时间<span class="subst">&#123;res[<span class="string">'date'</span>]&#125;</span> <span class="subst">&#123;res[<span class="string">'time'</span>]&#125;</span>，<span class="subst">&#123;res[<span class="string">'name'</span>]&#125;</span>的价格是<span class="subst">&#123;res[<span class="string">'new'</span>]&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">_arun</span><span class="params">(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None)</span> -&gt; str:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">"StockPriceQueryTool does not support async"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">tools = [StockPriceQueryTool(), ]</span><br><span class="line">agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=<span class="literal">True</span>)</span><br><span class="line">agent.run(<span class="string">"科大讯飞现在的股价是多少，什么时间的数据？"</span>)</span><br></pre></td></tr></table></figure><p>代码非常简单，核心就是继承BaseTool这个父类，以实现自定义。</p><p>打开调试模式，我们看看LangChain都干了些什么，见下图：</p><p><img src="//s3.mindex.xyz/blog/Notes/7455c3613be0d6748f24b30814cd3b7c.png" alt=""></p><p>LangChain大概的心理活动可能是：</p><ul><li>哦，任务来了，我需要提供科大讯飞的当前股价</li><li>我该找谁问呢？哦，股票的事情要问StockPriceQueryTool</li><li>StockPriceQueryTool.run(“科大讯飞”)</li><li>哦，答案来了，“当前时间2023-05-12 14:30:48，科大讯飞的价格是63.930”</li><li>这个答案完美，我组织一下语言</li></ul><p>是不是很酷？</p><h2 id="尾声">尾声</h2><p>其实笔者在LLM爆发前夜，实现了一个小型的ChatBot，利用的是NLP组合技，感兴趣的朋友看<a href="http://neo1989.net/HandMades/HANDMADE-build-a-cool-chatbot-step-by-step/" title="如何构建自己的ChatBot">这里</a></p><p>LangChain到这里就介绍完了。会不会不重要，重要的是它背后的思想。</p><p>猴子变成人，是因为学会了使用工具。既然新的工具出现了，要不要变一下？</p><p>Peace out。</p>]]></content>
    
    <summary type="html">
    
      学习使我快乐。
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>LangChain | 快速释放LLMs的能力 (二)</title>
    <link href="https://neo1989.net/Notes/NOTE-langchain-2/"/>
    <id>https://neo1989.net/Notes/NOTE-langchain-2/</id>
    <published>2023-05-09T15:18:31.000Z</published>
    <updated>2023-05-27T13:58:27.222Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR">TL;DR</h2><p>本文介绍了一种方法，让LLM拥有了学习新知识的能力。</p><h2 id="结果展示">结果展示</h2><p>LLM的训练是通过大量已有的数据进行学习，由于训练的难度和高额的成本，通常情况它是无法知道最新的时事的，于是它会胡编乱造，也就是是所谓的“幻觉”。如下图所示：<br><img src="//s3.mindex.xyz/blog/Notes/cd9244074883f119f2e21f77745dc193.png" alt="胡编乱造，但像模像样"></p><p>而通过提供资料让它学习，比如作者提供了以下两篇关于<code>星火大模型</code>的新闻稿给它学习:</p><ul><li><a href="https://new.qq.com/rain/a/20230508A01E8200" target="_blank" rel="noopener" title="腾讯网关于星火大模型的报道">腾讯网：星火认知大模型发布，科大讯飞入场科技巨头AI大战？</a></li><li><a href="https://finance.stockstar.com/IG2023050900014747.shtml" target="_blank" rel="noopener" title="证券之星关于星火大模型的报道">证券之星：科大讯飞星火认知大模型发布 AI 星火营生态计划同步开启</a></li></ul><p>于是它便能很好的回答相关的问题了。如下图所示：</p><p><img src="//s3.mindex.xyz/blog/Notes/fd3e43f90ecfa7ea380c69331c93bcfa.png" alt="它是真的在学习"></p><h2 id="如何实现">如何实现</h2><p>上一篇文章已经介绍了利用LangChain的基本操作实现了一个对任意文本进行总结的小应用。<br>本篇便是在前一篇的基础上引入了<code>embedding</code> 和 <code>vectorstore</code> 这两个核心能力来实现上述能力。</p><p>先说一下伪代码，流程如下图所示:</p><ul><li>加载文档</li><li>将文档分割成文本块</li><li>对文本块进行 <code>Embedding</code></li><li>将上述结果，也就是<code>Vectors</code>，存到向量数据库中</li><li>对用户的 <code>Query</code> 进行 <code>Embedding</code>, 生成 <code>Query Vector</code></li><li>在向量数据库中，利用向量similarity查询出 相关的文本块</li><li>将上述文本块，套上 <code>Prompt Template</code>，生成最终的<code>Prompt</code></li><li>丢给<code>LLM</code>，得到回答</li></ul><p><img src="//s3.mindex.xyz/blog/Notes/cc18b036c98679aafe101d7c47a529d5.png" alt=""></p><p>接下来看看LangChain是如何便捷的实现上述功能的。核心代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> SpacyTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings.openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> Chroma</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores.base <span class="keyword">import</span> VectorStoreRetriever</span><br><span class="line"></span><br><span class="line">doc = <span class="string">""</span></span><br><span class="line"></span><br><span class="line">text_splitter = SpacyTextSplitter(separator=<span class="string">"\n\n"</span>, pipeline=<span class="string">"zh_core_web_sm"</span>, chunk_size=<span class="number">200</span>)</span><br><span class="line">docs = text_splitter.create_documents([doc])</span><br><span class="line"></span><br><span class="line">embeddings = OpenAIEmbeddings(openai_api_version=<span class="string">'2020-11-07'</span>, model=<span class="string">"text-davinci-003"</span>)</span><br><span class="line">vectorstore = Chroma.from_documents(docs, embeddings, persist_directory=<span class="string">"vector_store"</span>)</span><br><span class="line"></span><br><span class="line">retriever = VectorStoreRetriever(vectorstore=vectorstore)</span><br><span class="line">qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=<span class="string">"refine"</span>, retriever=retriever)</span><br><span class="line"></span><br><span class="line">result = qa(&#123;<span class="string">"query"</span>: <span class="string">"星火认知大模型 是什么"</span>&#125;)</span><br><span class="line">result</span><br></pre></td></tr></table></figure><p>不超过10行的核心代码，是不是有手就会？</p><h2 id="什么是Embedding">什么是Embedding</h2><p>这里涉及到一点NLP的基础知识。</p><p>对于给定的一段自然语言文本，第一件事就是将其token化。所谓token就是分割出的一个字或一个词（中文场景）。</p><ul><li>给定：星火认知大模型是什么</li><li>按字：星/火/认/知/大/模/型/是/什/么</li><li>按词：星火/认知/大模型/是/什么</li></ul><p>当然还有其它的分割算法，这里按下不表。</p><p>Token化之后，如何表示呢，我们知道计算机只能处理数字，所以聪明的先驱们发明了利用一个包含很多小数的数组（也就是一组稠密向量）来表示文本。 Embedding 就是把文本变成向量的过程。</p><p>参考<a href="https://platform.openai.com/docs/guides/embeddings/what-are-embeddings" target="_blank" rel="noopener" title="OpenAI's text embeddings">OpenAI官方文档</a> ，Embeddings通常被用于以下场景：</p><ul><li>搜索（对结果集进行相关性排序）</li><li>聚类（对文本按照相关性进行分组）</li><li>分类（对文本按照标签相似度进行分类）</li><li>推荐（优先推荐相似度高的item）</li><li>异常检测（识别出相似度小的异常）</li><li>多样性衡量（对相似度的分布进行分析）</li></ul><p>那相似度是如何衡量的？就是计算两个向量之间的距离。距离近相关度高，距离远相关度低，就是这样。</p><h2 id="什么是VectorStore">什么是VectorStore</h2><p>顾名思义，向量数据库就是用来存储，检索，分析向量的数据库。</p><p>LangChain提供了非常多的选择，具体请<a href="https://python.langchain.com/en/latest/modules/indexes/vectorstores.html" target="_blank" rel="noopener" title="LangChain VectorStores">参考文档</a>。<br>本文示例中的<a href="https://www.trychroma.com/" target="_blank" rel="noopener" title="Chroma">Chroma</a>便是其中一种轻量级的，最近拿了$18M的种子轮。</p><h2 id="尾声">尾声</h2><p>另一个简单的示例，看起来已经可以拿去干点什么了。但LangChain的能力不止如此，我们下一篇再看。</p><p>Peace out。</p>]]></content>
    
    <summary type="html">
    
      学习使我快乐。
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>LangChain | 快速释放LLM的能力 (一)</title>
    <link href="https://neo1989.net/Notes/NOTE-langchain-1/"/>
    <id>https://neo1989.net/Notes/NOTE-langchain-1/</id>
    <published>2023-05-08T05:56:52.000Z</published>
    <updated>2023-05-27T13:58:21.131Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是LangChain">什么是LangChain</h2><p>其实就是用于开发基于LLM应用程序的开发框架。<br>一个强大的应用程序肯定不是只去调用LLM的API，于是LangChain便提供了两个核心的能力：</p><ul><li>有数据意识：将LLM与其他数据源连接起来</li><li>掌握主动权：允许LLM同运行环境交互</li></ul><p><img src="http://s3.mindex.xyz/blog/Notes/c151520a5f0042caff5ecb39519eed17.png" alt=""></p><h2 id="LangChain的组件">LangChain的组件</h2><p>LangChain提供了模块化抽象，用于处理与语言模型相关的组件。LangChain还拥有所有这些抽象的实现集合。这些组件被设计成易于使用，无论您是否使用LangChain框架的其余部分。</p><h3 id="Models">Models</h3><p>LangChain提供了各种不同类型的模型，方便集成和使用。如 LLMs、Chat Models、Text Embedding Models。</p><h3 id="Prompts">Prompts</h3><p>如你所知，跟模型打交道的方式便是prompt。通常prompt很少是硬编码的，而是通过多个组件搭建起来的。LangChain提供了多个类和方法，使的结合PromptTemplate构建prompts变得容易。</p><h3 id="Chains">Chains</h3><p>对于一些简单的应用来说，孤立地使用LLM是没有问题的，但许多更复杂的应用需要将LLM串联起来。LangChain提供了标准的接口，可以轻松的创建和管理链，以便更好的控制模型的输出。</p><h3 id="Agents">Agents</h3><p>有些应用不仅需要一个预先确定的调用LLM/其他工具的链，而且可能需要一个取决于用户输入的未知链。在这些类型的链中，有一个 “代理”，它可以访问一整套的工具。根据用户的输入，代理可以决定调用这些工具中的哪个（如果有的话）。</p><p><img src="http://s3.mindex.xyz/blog/Notes/bb20e5c953c69766556b713226d80125.png" alt=""></p><h3 id="Memory">Memory</h3><p>默认情况下，链和代理是无状态的，这意味着它们会独立处理每个传入的查询（正如底层的LLM和聊天模型）。在一些应用中（聊天机器人就是一个很好的例子），记住以前的互动是非常重要的，无论是短期的还是长期的。LangChain提供了方便的Memory组件和简单的方法以集成到应用程序中。</p><h3 id="Indexes">Indexes</h3><p>索引指的是结构化文件的方法，以便LLM能够与它们进行最好的交互。<br>使用索引通常是在检索这一步发生的。即返回与用户的输入最相关的文档。LangChain支持的主要索引和检索类型是围绕着向量数据库进行的。<br>这部分包含的模块有：</p><ul><li>Document Loaders： 文档加载器，可以从各种源头加载文档</li><li>Text Splitters: 文本分割</li><li>VectorStores：向量存储</li><li>Retrievers：检索</li></ul><h2 id="LangChain实战">LangChain实战</h2><p>基本的介绍如上，详细内容请参考<a href="https://python.langchain.com/en/latest/" target="_blank" rel="noopener" title="LangChain">官方文档</a>，LearnByDoing才是正经事。</p><p>一个很简单的需求，对任意文章进行总结并输出。核心代码及部分注释如下, 10行代码搞定长文本总结：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 相关依赖</span></span><br><span class="line"><span class="comment"># tiktoken==0.4.0</span></span><br><span class="line"><span class="comment"># openai==0.27.6</span></span><br><span class="line"><span class="comment"># langchain==0.0.161</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> SpacyTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.chains.summarize <span class="keyword">import</span> load_summarize_chain</span><br><span class="line"></span><br><span class="line">doc = <span class="string">""</span>  <span class="comment"># 文章内容</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定文本分割器，使用Spacy的中文模型</span></span><br><span class="line">text_splitter = SpacyTextSplitter(separator=<span class="string">"\n\n"</span>, pipeline=<span class="string">"zh_core_web_sm"</span>, chunk_size=<span class="number">200</span>)</span><br><span class="line">texts = text_splitter.create_documents([doc])</span><br><span class="line"></span><br><span class="line">llm = OpenAI(model_name=<span class="string">"text-davinci-003"</span>, max_tokens=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">tokens_num = llm.get_num_tokens(doc)  <span class="comment"># 计算文档需要多少token</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建总结链</span></span><br><span class="line">chain = load_summarize_chain(llm, chain_type=<span class="string">"map_reduce"</span>, verbose=<span class="literal">True</span>)</span><br><span class="line">output = chain.run(texts)</span><br><span class="line"></span><br><span class="line">output</span><br></pre></td></tr></table></figure><p>于是便可以很方便的集成进任意应用程序了。<br><img src="http://s3.mindex.xyz/blog/Notes/769492ddeaa20d8f26395bbe2ce9ffd3.png" alt="愚苏记示例"></p><h2 id="结尾">结尾</h2><p>一个简单的示例，但足以体现LLM的强大以及LangChain的便捷。接下来笔者将尝试在更复杂的应用场景下使用LangChain解决更复杂的问题，Peace out。</p>]]></content>
    
    <summary type="html">
    
      打不过就加入。
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT核心技术的进化之路</title>
    <link href="https://neo1989.net/Notes/NOTE-gpt-history/"/>
    <id>https://neo1989.net/Notes/NOTE-gpt-history/</id>
    <published>2023-04-23T08:25:31.000Z</published>
    <updated>2023-04-25T07:52:45.493Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>我们面临这样一个时代的机会。它既是机会，也是挑战。</p></blockquote><h2 id="TL-DR">TL;DR</h2><p>本文简单梳理了GPT系列的核心工作，从GPT-1到InstructGPT，帮助理解GPT的发展过程。</p><h2 id="GPT-1-2018">GPT-1 | 2018</h2><p><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener" title="GPT-1">GPT-1</a> 这篇论文提出了“Unsupervised pre-training + Supervised fine-tuning”核心架构，即首先使用大量未标记的文本数据来训练语言模型，然后使用标记数据来微调模型，以便更好地完成特定的判别性任务。</p><p>在 GPT 出现之前，NLP 的无监督预训练方法确实很长一段时间都是基于 WordVec 的模型，如 CBOW 和 SkipGram 等。这些模型主要的目的是通过在上下文中共现的词汇之间建立近似的距离关系，从而对自然语言进行表示和分析。</p><p>但词向量的模型的主要局限是一个词的向量表示没有考虑在不同的上下文环境里词的含义不同。 因此后续进化出了模型如 ELMo，使用双向 LSTM 对句子建模，得到前向和后向两种特征表示，然后将这两种特征表示拼接起来，作为最终的词向量表示，从而能够较好的处理多义词问题。</p><p>然而 ELMo 模型存在计算开销大，对上下文建模和长程依赖较弱，并且本身只能获得句子语义表示等一定的缺陷与局限性。 于是 Transformer 崛起，BERT 和 GPT 便是基于 Transformer 的杰出改进成果。</p><p>BERT 在训练过程中采用了 Masked Language Model(MLM) 和 Next Sentence Prediction(NSP) 任务使得两个方向的上下文信息保持一致，以更好的捕捉全局的语义信息。</p><p>区别于 BERT，GPT 的无监督预训练阶段，使用的是多层 Transformer Decoder，在对某个位置 i 的 token 进行预测时，利用 i 之前的所有 token 的信息输入 Transformer 进行编码，输出一个条件概率分布来预测 i 位置的 token.</p><p><img src="http://s3.mindex.xyz/blog/Notes/9cfe2357f08d51c4e2fe95f113ecefd0.png" alt="unsupervised pre-training | GPT"></p><p>在完成 GPT 的预训练，就进入有监督微调阶段。在特定的任务中使用标记数据微调模型，见下图右.</p><p><img src="http://s3.mindex.xyz/blog/Notes/cf4aa7093b4248108d916c90254f981f.png" alt="GPT summary"></p><p>概括下来，GPT-1 的核心是通过在大量未标记文本语料库上对语言模型进行生成式预训练(generative pre-training)，然后对每个特定任务进行判别性微调(discriminative fine-tuning)，来提高自然语言理解能力。</p><h2 id="GPT-2-2019">GPT-2 | 2019</h2><p><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener" title="GPT-2">GPT-2</a> 展现出的能力是在特定的数据集上训练时，能够在没有明确监督的情况下学习特定的任务，即 Zero-shot learning。GPT-2 表明在极限情况下，预训练技术能够学习直接执行任务，而不需要监督适应或者修改。</p><p>下图表明 GPT-2 在多个任务上进行了实验的结果，表明 GPT-2 只有在具有足够的容量时才能够在许多典型的任务上取得良好的零样本表现。<br><img src="http://s3.mindex.xyz/blog/Notes/45a1d879b6ffd17674d83c06e15556a7.png" alt="performance | GPT-2"></p><p>虽然 GPT-2 的零样本表现为很多任务建立了 baseline，但并不清楚 fine-tuning 的上限。于是沿着 GPT-2 的思路，诞生了 GPT-3.</p><h2 id="GPT-3-2020">GPT-3 | 2020</h2><p><a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener" title="GPT-3">GPT-3</a> 便沿着 GPT-2 的思路，思考如何让下游任务更好的适配预训练语言模型。<br>首先他们在无监督预训练阶段，加入了&quot;in-context learning&quot;。简单讲就是模型在不更新自身参数的情况下，通过在模型输入中代入新任务的描述和少量样本，就能让模型“学习”的新任务的特征。<br><img src="http://s3.mindex.xyz/blog/Notes/c74565286e6ebba3d5ebaed295769bb1.png" alt=""></p><p>GPT-3 没有使用 fine-tuning，而是在 in-context learning 阶段尝试了三种设定：Zero-sot，One-shot 以及 Few-shot。Prompt 便是在这个阶段加入的。<br><img src="http://s3.mindex.xyz/blog/Notes/25962a81939bed0c6b631ddefab0a633.png" alt=""></p><p>其次，他们使用了更多、更高质量的语料，训练出了更大的模型。<br><img src="http://s3.mindex.xyz/blog/Notes/f67772aa8935bf2630356cdcf701d30c.png" alt=""></p><p>GPT-3 一经发布，便引起了学术界产业界爱好者们前沿的广泛讨论, 列举几篇</p><ul><li><a href="https://www.huxiu.com/article/385064.html" target="_blank" rel="noopener" title="虎嗅的：与 GPT-3 对话：它的回答令人细思极恐">虎嗅的：与 GPT-3 对话：它的回答令人细思极恐</a></li><li><a href="https://zhuanlan.zhihu.com/p/334340996" target="_blank" rel="noopener" title="AI TIME：地表最强的 GPT-3，是在推理，还是胡言乱语？">AI TIME：地表最强的 GPT-3，是在推理，还是胡言乱语？</a></li><li><a href="https://mp.weixin.qq.com/s/Lp93p0sYJcw42JPG6ItEYg" target="_blank" rel="noopener" title="新智元：GPT-3 真是人工智能「核武器」吗？花 1200 万美元训练却没能通过图灵测试">新智元：GPT-3 真是人工智能「核武器」吗？花 1200 万美元训练却没能通过图灵测试</a></li></ul><p>研究者们也在论文中提出了一些关于 GPT-3 的担忧。</p><ul><li>语言模型的滥用</li><li>公平性、偏见上的挑战</li><li>资源消耗巨大</li></ul><p>当然，困难是不会打倒开拓者们的。</p><h2 id="InstructGPT-2022">InstructGPT | 2022</h2><p><a href="https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf" target="_blank" rel="noopener" title="InstructGPT">InstructGPT</a> 的核心是如何让 GPT 生成的回答更符合人类的需求。而该模型引入了一种名为 <a href="https://arxiv.org/abs/2204.05862" target="_blank" rel="noopener" title="RLHF">RLHF</a> 的方法来进行微调。结果表明，InstructGPT 模型的输出比 GPT-3 更受欢迎，而且更真实，更少的有毒输出。利用 RLHF 微调，是使语言模型与人类意图保持一致的一个有前途的方向。</p><p>概括起来就三个步骤</p><ul><li>预训练一个语言模型 (LM)，并在高质量 prompt 数据集上进行有监督微调</li><li>聚合对比数据并训练一个奖励模型 (Reward Model，RM)</li><li>用强化学习 (RL) 方式微调 LM</li></ul><p><img src="http://s3.mindex.xyz/blog/Notes/73e84a5630105ccd9d034ce8a24d943b.png" alt=""></p><p>于是，GPT 便朝着人类期望的方向不断的进化了。</p><h2 id="ChatGPT-2022-末">ChatGPT | 2022 末</h2><p>2022 年 11 月 30 日，OpenAI 发布了一个通过由 GPT-3.5 系列 LLM 微调而成的全新对话式 AI 工具 ChatGPT，掀起了人工智能的热潮。</p><h2 id="拐点已至-2023">拐点已至 | 2023</h2><ul><li><p>OpenAI 获得微软投资 100 亿美元</p></li><li><p><a href="https://www.microsoft.com/en-gb/bing" target="_blank" rel="noopener" title="New Bing 发布">Feb 7, New Bing 发布</a></p></li><li><p><a href="https://openai.com/research/gpt-4" target="_blank" rel="noopener" title="GPT-4 发布">Mar 14, GPT-4 发布</a></p></li><li><p><a href="https://www.anthropic.com/index/introducing-claude" target="_blank" rel="noopener" title="Claude 发布">Mar 14, Claude 发布</a></p></li><li><p><a href="https://github.com/features/preview/copilot-x" target="_blank" rel="noopener" title="Copilot-X 发布">Mar 22, Copilot-X 发布</a></p></li><li><p><a href="https://openai.com/blog/chatgpt-plugins" target="_blank" rel="noopener" title="ChatGPT-plugins 发布">Mar 23, ChatGPT-plugins 发布</a></p></li></ul><h2 id="One-More-Thing">One More Thing</h2><p>文章开头引用来自于陆奇最新演讲<a href="https://mp.weixin.qq.com/s/_ZvyxRpgIA4L4pqfcQtPTQ" target="_blank" rel="noopener" title="我的大模型世界观">《我的大模型世界观》</a> ，利用AI总结了以下几点take-away:</p><ul><li>大模型时代的发展速度非常快，甚至他自己都跟不上</li><li>OpenAI在大模型领域处于领先地位，并且未来有可能比Google更大</li><li>未来是一个模型无处不在的时代</li><li>大模型时代对每个人都将产生深远和系统性影响，每个人很快将有副驾驶员</li><li>陆奇认为，创业公司基本上有三类：数字化技术、满足人类需求和改变世界</li></ul><p>如果您还未能将使用ChatGPT等AI作为习惯，我感到非常遗憾。</p><p>如果您还没有使用 ChatGPT 的渠道，关注这个公众号👇并打招呼，可以获取使用入口。</p>]]></content>
    
    <summary type="html">
    
      打不过就加入。
    
    </summary>
    
    
      <category term="Notes" scheme="https://neo1989.net/categories/Notes/"/>
    
    
      <category term="Coder" scheme="https://neo1989.net/tags/Coder/"/>
    
      <category term="AI" scheme="https://neo1989.net/tags/AI/"/>
    
      <category term="ChatGPT" scheme="https://neo1989.net/tags/ChatGPT/"/>
    
  </entry>
  
  <entry>
    <title>日落收集</title>
    <link href="https://neo1989.net/SeizeTheDay/COLLECTION-sunsets/"/>
    <id>https://neo1989.net/SeizeTheDay/COLLECTION-sunsets/</id>
    <published>2023-03-04T14:34:15.000Z</published>
    <updated>2023-06-08T15:40:34.403Z</updated>
    
    <content type="html"><![CDATA[<h3 id="May-20-2023">May 20, 2023</h3><p><img src="//s3.mindex.xyz/blog/Notes/a8dcfd31636d678b6bc786345a7342a9.png" alt="西湖·太子湾公园 | 浙江"></p><h3 id="May-13-2023">May 13, 2023</h3><p><img src="//s3.mindex.xyz/blog/Notes/02be63a08b4d3f50fe9f022c13f21291.png" alt="清水路·环湖大道 | 苏州"></p><h3 id="Apr-15-2023">Apr 15, 2023</h3><p><img src="//s3.mindex.xyz/blog/Notes/97765437415756031a4b064691e3fc5e.png" alt="清水路·环湖大道 | 苏州"></p><h3 id="Mar-11-2023">Mar 11, 2023</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/d1c296dc199be5df7087034673e4267a.png" alt="On the Rock @ NamPhrae | Thailand"></p><h3 id="Mar-10-2023">Mar 10, 2023</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/3454d0d46192236de9293e1261e6a65c.png" alt="Route 107 | Thailand"></p><h3 id="Mar-4-2023">Mar 4, 2023</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/7d8d79718363def5e1f35fc64c131589.png" alt="米堆山 | 苏州"></p><h3 id="Jan-30-2023">Jan 30, 2023</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/cb9d279f773c498fb81893d84e3801a1.png" alt="滨江中路 | 上海"></p><h3 id="Nov-8-2022">Nov 8, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/2e7efec1c357cc31f2d9d9111263fcf6.png" alt="自家楼顶 | 上海"></p><h3 id="Oct-29-2022">Oct 29, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/480778a07a8ad62f4667758d7b9f1838.png" alt="自家阳台 | 上海"></p><h3 id="Oct-21-2022">Oct 21, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/60a5da12bed4e3378b8b06b131961ada.png" alt="望金路 | 苏州"></p><h3 id="Oct-12-2022">Oct 12, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/ea79b3cb30dbbd25b2a8f50cdc6e787e.png" alt="隔离酒店 | 上海"></p><h3 id="Oct-3-2022">Oct 3, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/a64244c75a29eb7f5229965d1019dd9f.png" alt="新巴尔虎左旗 | 呼伦贝尔"></p><h3 id="Sept-9-2022">Sept 9, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/588fe20a7f83e4ebea9248bffc3c02cd.png" alt="正大立方大厦 | 上海"></p><h3 id="Aug-12-2022">Aug 12, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/256ec6bd4be58d507b83a7674bbf3e5d.png" alt="陆家嘴 | 上海"></p><h3 id="Jul-2-2022">Jul 2, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/5b3a941292b14a6ed57e5d8472e4d166.png" alt="淀峰村 | 上海"></p><h3 id="Feb-26-2022">Feb 26, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/ed0aed30bd1f5eb7e812f6b910e2d272.png" alt="广富林郊野公园 | 上海"></p><h3 id="Feb-5-2022">Feb 5, 2022</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/252a930e286996cb70257dc33cdfd179.png" alt="申嘉湖高速 | 嘉兴"></p><h3 id="Dec-19-2021">Dec 19, 2021</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/20658e52339f61f6fa2cacc6378f1597.png" alt="望金路 | 苏州"></p><h3 id="Oct-23-2021">Oct 23, 2021</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/096adcde8f85c1e11e0935ec5c1c33b8.png" alt="广富林郊野公园 | 上海"></p><h3 id="Oct-5-2021">Oct 5, 2021</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/431a40cf0e17f31eca88b6d183bf0977.png" alt="小柴旦湖 | 海西"></p><h3 id="Oct-2-2021">Oct 2, 2021</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/c0d17cf3a1e1d0ce86aef91b63b49166.png" alt="黑独山 | 海西"></p><h3 id="Sept-21-2021">Sept 21, 2021</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/8150b4294afbf75dce10bd0e35c53328.png" alt="望金路 | 苏州"></p><h3 id="Sept-5-2021">Sept 5, 2021</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/cfe9e81ec7bb7732d7c37059e7719356.png" alt="望金路 | 苏州"></p><h3 id="Oct-5-2020">Oct 5, 2020</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/d9332c6fae74c9383402562a60efd961.png" alt="石头公园 | 文昌"></p><h3 id="Aug-30-2020">Aug 30, 2020</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/bf9d8d69f72adfa310a5e43609ffe447.png" alt="家附近 | 上海"></p><h3 id="Jul-28-2019">Jul 28, 2019</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/74b15296714f1663e085455cf6203a36.png" alt="欢乐谷 | 上海"></p><h3 id="Jun-16-2019">Jun 16, 2019</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/6c4c12c624df285655f099d06dc25497.png" alt="中谭路 | 上海"></p><h3 id="Apr-27-2019">Apr 27, 2019</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/69ab3d05f731687f45ddbfb7a101b883.png" alt="直岛 | 日本"></p><h3 id="Sept-9-2018">Sept 9, 2018</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/39920b32a2d354c61351c98100d5d341.png" alt="松江新城 | 上海"></p><h3 id="Jul-14-2018">Jul 14, 2018</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/1d03b9cc2dad2f2a729a619a34d9ea14.png" alt="中谭路 | 上海"></p><h3 id="May-11-2017">May 11, 2017</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/5722cafa310b4aeb6aedcf4dbbe07543.png" alt="Baiyoke Tower | Bankok"></p><h3 id="May-4-2017">May 4, 2017</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/75728df9ad538b27fdaf54abf50d6534.png" alt="Lanta | Krabi"></p><h3 id="May-3-2017">May 3, 2017</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/a90e458e957d32e3bcc8a54fe7870ccc.png" alt="Lanta | Krabi"></p><h3 id="May-1-2017">May 1, 2017</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/6bcd6f83a888c1beb19625c29e47f7d3.png" alt="Patong | Phuket"></p><h3 id="Apr-22-2017">Apr 22, 2017</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/15dc820477de2bc560c2f4495f8f913e.png" alt="陆家嘴金融广场 | 上海"></p><h3 id="Oct-5-2016">Oct 5, 2016</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/70da8fd8fa5ef68c87b457bc1165b69e.png" alt="家楼顶 | 官港"></p><h3 id="Mar-31-2016">Mar 31, 2016</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/bd759acc6ffdcad5637434f94911b3ea.png" alt="曾厝垵 | 厦门"></p><h3 id="Mar-6-2016">Mar 6, 2016</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/eb64a0ff6ac66640838a0eea11eaa59d.png" alt="康桥 | 上海"></p><h3 id="Feb-20-2016">Feb 20, 2016</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/dbeaec79d916ce1f60a75f3afef56d69.png" alt="淮海中路 | 上海"></p><h3 id="Jan-25-2016">Jan 25, 2016</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/27e5876049c7553a7cd3062d692679e0.png" alt="浦软 | 上海"></p><h3 id="Sept-24-2015">Sept 24, 2015</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/1e9de7867582054def65e0a6e12c337c.png" alt="浦软 | 上海"></p><h3 id="Jan-15-2014">Jan 15, 2014</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/a87b05e7aeeb54bc490bc283fbec81af.png" alt="创智天地 | 上海"></p><h3 id="Oct-3-2013">Oct 3, 2013</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/3fd71e4380cd0024aa7967dcbe0b3c1a.png" alt="阳朔 | 桂林"></p><h3 id="May-11-2013">May 11, 2013</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/825ec397bfc4623828ed3eb7566ead05.png" alt="广储门 | 扬州"></p><h3 id="Mar-3-2013">Mar 3, 2013</h3><p><img src="//s3.mindex.xyz/blog/Sunsets/9614bee33966b93edfdaebd9ba865da8.png" alt="世纪公园 | 上海"></p>]]></content>
    
    <summary type="html">
    
      日落尤其温柔，人间皆是浪漫。
    
    </summary>
    
    
      <category term="SeizeTheDay" scheme="https://neo1989.net/categories/SeizeTheDay/"/>
    
    
  </entry>
  
</feed>
