<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="我读书少，你莫骗我。"><title>Way2AI · 神经网络 (二) | 愚苏记</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/lib/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="/lib/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="/lib/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="/lib/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="/lib/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '96c862f2728296588ae9849f3bcb95db';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Way2AI · 神经网络 (二)</h1><a id="logo" href="/.">愚苏记</a><p class="description">To no avail but try.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-5-5"><div class="content_container no-padding-right"><div class="post"><h1 class="post-title">Way2AI · 神经网络 (二)</h1><div class="post-meta">Jun 12, 2023<span> | </span><span class="category"><a href="/categories/Way2AI/">Way2AI</a></span></div><a class="disqus-comment-count" data-disqus-identifier="Way2AI/Way2AI-neural-networks-2/" href="/Way2AI/Way2AI-neural-networks-2/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#TL-DR"><span class="toc-number">1.</span> <span class="toc-text">TL;DR</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model"><span class="toc-number">2.</span> <span class="toc-text">Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training"><span class="toc-number">3.</span> <span class="toc-text">Training</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Evaluation"><span class="toc-number">4.</span> <span class="toc-text">Evaluation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Initializing-weights"><span class="toc-number">5.</span> <span class="toc-text">Initializing weights</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dropout"><span class="toc-number">6.</span> <span class="toc-text">Dropout</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Overfitting"><span class="toc-number">7.</span> <span class="toc-text">Overfitting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Citation"><span class="toc-number">8.</span> <span class="toc-text">Citation</span></a></li></ol></div></div><div class="post-content"><h2 id="TL-DR">TL;DR</h2>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>接上篇，本文使用PyTorch实现一个相同的神经网络模型。</p>
<h2 id="Model">Model</h2>
<p>我们将使用两个线性连接层，并在前向传播中添加ReLU激活函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, num_classes)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = F.relu(self.fc1(x_in))</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initalize model</span></span><br><span class="line">model = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)</span><br><span class="line">print(model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of MLP(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=3, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="Training">Training</h2>
<p>训练模型的代码跟之前学到的逻辑回归几乎没有区别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">LEARNING_RATE = <span class="number">1e-2</span></span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(list(class_weights.values()))</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_fn</span><span class="params">(y_pred, y_true)</span>:</span></span><br><span class="line">    n_correct = torch.eq(y_pred, y_true).sum().item()</span><br><span class="line">    accuarcy = (n_correct / len(y_pred)) * <span class="number">100</span></span><br><span class="line">    <span class="keyword">return</span> accuarcy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.LongTensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS * <span class="number">10</span>):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        predictions = y_pred.max(dim=<span class="number">1</span>)[<span class="number">1</span>]  <span class="comment"># class</span></span><br><span class="line">        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 0.09, accuracy: 98.6</span></span><br><span class="line"><span class="comment"># Epoch: 10 | loss: 0.06, accuracy: 99.0</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.05, accuracy: 99.2</span></span><br><span class="line"><span class="comment"># Epoch: 30 | loss: 0.04, accuracy: 99.6</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.03, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 50 | loss: 0.03, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 60 | loss: 0.03, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 70 | loss: 0.02, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 80 | loss: 0.02, accuracy: 99.7</span></span><br><span class="line"><span class="comment"># Epoch: 90 | loss: 0.02, accuracy: 99.7</span></span><br></pre></td></tr></table></figure>
<h2 id="Evaluation">Evaluation</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predictiions</span></span><br><span class="line">y_prob = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line">y_pred = y_prob.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 1.0,</span></span><br><span class="line"><span class="comment">#     "recall": 1.0,</span></span><br><span class="line"><span class="comment">#     "f1": 1.0,</span></span><br><span class="line"><span class="comment">#     "num_samples": 225.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "c1": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 1.0,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c2": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 1.0,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c3": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 1.0,</span></span><br><span class="line"><span class="comment">#       "recall": 1.0,</span></span><br><span class="line"><span class="comment">#       "f1": 1.0,</span></span><br><span class="line"><span class="comment">#       "num_samples": 75.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="//s3.mindex.xyz/blog/Courses/3ac0940bf9a117acde72a4d36241c2b8.png" alt=""></p>
<p>如你所见，PyTorch的直观和易用性能让我的学习曲线相对平缓。</p>
<p>需要我们编写的核心代码，只集中在定义模型、定义损失函数和优化器、定义训练循环、验证和测试这个四个部分。</p>
<p>当然，还有许多细节需要考虑，比如说数据预处理、模型的保存和加载、使用GPU等。</p>
<h2 id="Initializing-weights">Initializing weights</h2>
<p>到目前为止，我们都是使用了一个很小的随机值初始化权重，这其实不是让模型在训练阶段能够收敛的最佳方式。</p>
<p>我们的目标是初始化一个合适的权重，使得我们激活的输出不会消失或者爆炸，因为这两种情况都会阻碍模型收敛。事实上我们可以<a href="https://pytorch.org/docs/stable/nn.init.html" target="_blank" rel="noopener" title="nn.init">自定义权重初始化</a>方法。目前比较常用的是<a href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_normal_" target="_blank" rel="noopener" title="nn.init.xavier_normal_">Xavier初始化方法</a>和<a href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_" target="_blank" rel="noopener" title="nn.init.kaiming_normal_">He初始化方法</a>。</p>
<p>事实上PyTorch的Linear类默认使用了kaiming_uniform_初始化方法，相关源代码看<a href="https://github.com/pytorch/pytorch/blob/af7dc23124a6e3e7b8af0637e3b027f3a8b3fb76/torch/nn/modules/linear.py#L101" target="_blank" rel="noopener" title="Linear源码">这里</a>，后续我们会学习到更高级的优化收敛的策略如batch normalization。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, num_classes)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        init.xavier_normal_(self.fc1.weight, gain=init.calculate_gain(<span class="string">"relu"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = F.relu(self.fc1(x_in))</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure>
<h2 id="Dropout">Dropout</h2>
<p>能够让我们的模型表现的好的最好的技术是增加数据，但这并不总是一个可选项。幸运的是，还有有一些帮助模型更健壮的其他办法，如正则化、dropout等。</p>
<p>Dropout是在训练过程中允许我们将神经元的输出置0的技术。由于我们每批次都会丢弃一组不同的神经元，所以Dropout可以作为一种采样策略，防止过拟合。</p>
<p><img src="//s3.mindex.xyz/blog/Courses/2c301aaf51dcbdc7fb1556b1cf547228.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">DROPOUT_P = <span class="number">0.1</span> <span class="comment"># percentage of weights that are dropped each pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, dropout_p, num_classes)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p) <span class="comment"># dropout</span></span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        init.xavier_normal(self.fc1.weight, gain=init.calculate_gain(<span class="string">"relu"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        z = F.relu(self.fc1(x_in))</span><br><span class="line">        z = self.dropout(z) <span class="comment"># dropout</span></span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span class="line">            dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of MLP(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=3, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="Overfitting">Overfitting</h2>
<p>虽然神经网络很擅长捕捉非线性关系，但它们非常容易对训练数据进行过度拟合，且无法对测试数据进行归纳。</p>
<p>看看下面的例子，我们使用完全随机的数据，并试图拟合含 $2 * N * C + D $ (其中N=样本数，C=标签，D表示输入纬度) 隐藏神经元的模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line">NUM_EPOCHS = <span class="number">500</span></span><br><span class="line">NUM_SAMPLES_PER_CLASS = <span class="number">50</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line">HIDDEN_DIM = <span class="number">2</span> * NUM_SAMPLES_PER_CLASS * NUM_CLASSES + INPUT_DIM <span class="comment"># 2*N*C + D</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate random data</span></span><br><span class="line">X = np.random.rand(NUM_SAMPLES_PER_CLASS * NUM_CLASSES, INPUT_DIM)</span><br><span class="line">y = np.array([[i] * NUM_SAMPLES_PER_CLASS <span class="keyword">for</span> i <span class="keyword">in</span> range(NUM_CLASSES)]).reshape(<span class="number">-1</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X: "</span>, format(np.shape(X)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y: "</span>, format(np.shape(y)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X:  (150, 2)</span></span><br><span class="line"><span class="comment"># y:  (150,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (105, 2), y_train: (105,)</span></span><br><span class="line"><span class="comment"># X_val: (23, 2), y_val: (23,)</span></span><br><span class="line"><span class="comment"># X_test: (22, 2), y_test: (22,)</span></span><br><span class="line"><span class="comment"># Sample point: [0.51102894 0.55377194] → 2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the inputs (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler().fit(X_train)</span><br><span class="line">X_train = X_scaler.transform(X_train)</span><br><span class="line">X_val = X_scaler.transform(X_val)</span><br><span class="line">X_test = X_scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.LongTensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span class="line">            dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of MLP(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=302, bias=True)</span></span><br><span class="line"><span class="comment">#   (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=302, out_features=3, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">50</span>==<span class="number">0</span>:</span><br><span class="line">        predictions = y_pred.max(dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># class</span></span><br><span class="line">        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 1.07, accuracy: 43.8</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.94, accuracy: 52.4</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.89, accuracy: 55.2</span></span><br><span class="line"><span class="comment"># Epoch: 60 | loss: 0.87, accuracy: 49.5</span></span><br><span class="line"><span class="comment"># Epoch: 80 | loss: 0.82, accuracy: 63.8</span></span><br><span class="line"><span class="comment"># Epoch: 100 | loss: 0.84, accuracy: 62.9</span></span><br><span class="line"><span class="comment"># Epoch: 120 | loss: 0.75, accuracy: 63.8</span></span><br><span class="line"><span class="comment"># Epoch: 140 | loss: 0.77, accuracy: 60.0</span></span><br><span class="line"><span class="comment"># Epoch: 160 | loss: 0.75, accuracy: 60.0</span></span><br><span class="line"><span class="comment"># Epoch: 180 | loss: 0.75, accuracy: 66.7</span></span><br><span class="line"><span class="comment"># Epoch: 200 | loss: 0.75, accuracy: 67.6</span></span><br><span class="line"><span class="comment"># Epoch: 220 | loss: 0.69, accuracy: 68.6</span></span><br><span class="line"><span class="comment"># Epoch: 240 | loss: 0.75, accuracy: 65.7</span></span><br><span class="line"><span class="comment"># Epoch: 260 | loss: 0.73, accuracy: 71.4</span></span><br><span class="line"><span class="comment"># Epoch: 280 | loss: 0.73, accuracy: 69.5</span></span><br><span class="line"><span class="comment"># Epoch: 300 | loss: 0.71, accuracy: 62.9</span></span><br><span class="line"><span class="comment"># Epoch: 320 | loss: 0.68, accuracy: 69.5</span></span><br><span class="line"><span class="comment"># Epoch: 340 | loss: 0.74, accuracy: 65.7</span></span><br><span class="line"><span class="comment"># Epoch: 360 | loss: 0.68, accuracy: 71.4</span></span><br><span class="line"><span class="comment"># Epoch: 380 | loss: 0.78, accuracy: 63.8</span></span><br><span class="line"><span class="comment"># Epoch: 400 | loss: 0.69, accuracy: 66.7</span></span><br><span class="line"><span class="comment"># Epoch: 420 | loss: 0.75, accuracy: 67.6</span></span><br><span class="line"><span class="comment"># Epoch: 440 | loss: 0.76, accuracy: 69.5</span></span><br><span class="line"><span class="comment"># Epoch: 460 | loss: 0.71, accuracy: 67.6</span></span><br><span class="line"><span class="comment"># Epoch: 480 | loss: 0.66, accuracy: 66.7</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">y_prob = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line">y_pred = y_prob.max(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)</span><br><span class="line"><span class="keyword">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   "overall": &#123;</span></span><br><span class="line"><span class="comment">#     "precision": 0.45959595959595956,</span></span><br><span class="line"><span class="comment">#     "recall": 0.45454545454545453,</span></span><br><span class="line"><span class="comment">#     "f1": 0.4512987012987013,</span></span><br><span class="line"><span class="comment">#     "num_samples": 22.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   "class": &#123;</span></span><br><span class="line"><span class="comment">#     "c1": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.5,</span></span><br><span class="line"><span class="comment">#       "recall": 0.375,</span></span><br><span class="line"><span class="comment">#       "f1": 0.42857142857142855,</span></span><br><span class="line"><span class="comment">#       "num_samples": 8.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c2": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.4444444444444444,</span></span><br><span class="line"><span class="comment">#       "recall": 0.5714285714285714,</span></span><br><span class="line"><span class="comment">#       "f1": 0.5,</span></span><br><span class="line"><span class="comment">#       "num_samples": 7.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     "c3": &#123;</span></span><br><span class="line"><span class="comment">#       "precision": 0.42857142857142855,</span></span><br><span class="line"><span class="comment">#       "recall": 0.42857142857142855,</span></span><br><span class="line"><span class="comment">#       "f1": 0.42857142857142855,</span></span><br><span class="line"><span class="comment">#       "num_samples": 7.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="//s3.mindex.xyz/blog/Courses/8b43366da170668bdeaee171c279362d.png" alt=""></p>
<p>正如你所见，虽然模型在训练集上做到了接近70%的准确率，但模型在测试集上的表现并不能令人满意。</p>
<p>重要的是我们需要进行实验，从不合适（高偏差）的简单模型开始，并试图改进到良好的拟合，以及避免过拟合。</p>
<p><img src="//s3.mindex.xyz/blog/Courses/9a3b5a8d871020ccda41430ca7958bc1.png" alt=""></p>
<h2 id="Citation">Citation</h2>
<figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Neural networks - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>
</div><iframe src="/donate/?AliPayQR=null&amp;WeChatQR=http://s3.mindex.xyz/mp/qrcode-s.jpg&amp;GitHub=http://github.com/neo1989&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Author: </strong>尼欧</li><li class="post-copyright-link"><strong>Blog Link: </strong><a href="/Way2AI/Way2AI-neural-networks-2/">https://neo1989.net/Way2AI/Way2AI-neural-networks-2/</a></li><li class="post-copyright-license"><strong>Copyright Declaration: </strong>转载请声明出处。</li></ul></div><br><div class="tags"><a href="/tags/Coder/">Coder</a><a href="/tags/AI/">AI</a><a href="/tags/PyTorch/">PyTorch</a></div><div class="post-nav"><a class="pre" href="/Way2AI/Way2AI-utilities/">Way2AI · PyTorch实现神经网络的基本套路</a><a class="next" href="/Way2AI/Way2AI-neural-networks-1/">Way2AI · 神经网络 (一)</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://neo1989.net/Way2AI/Way2AI-neural-networks-2/';
    this.page.identifier = 'Way2AI/Way2AI-neural-networks-2/';
    this.page.title = 'Way2AI · 神经网络 (二)';
  };</script><!-- script(type='text/javascript' id='disqus-lazy-load-script').--><!--   $.ajax({--><!--   url: 'https://disqus.com/next/config.json',--><!--   timeout: 2500,--><!--   type: 'GET',--><!--   success: function(){--><!--     var d = document;--><!--     var s = d.createElement('script');--><!--     s.src = '//#{theme.disqus}.disqus.com/embed.js';--><!--     s.setAttribute('data-timestamp', + new Date());--><!--     (d.head || d.body).appendChild(s);--><!--     $('.disqus_click_btn').css('display', 'none');--><!--   },--><!--   error: function() {--><!--     $('.disqus_click_btn').css('display', 'block');--><!--   }--><!--   });--><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//neo1989.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://neo1989.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div></div><div class="pure-u-1 pure-u-md-4-6"><div id="footer">Copyright © 2023 <a href="/." rel="nofollow">愚苏记.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/i-yard/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho/maupassant"> Cho.</a></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="/lib/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="/lib/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
  displayAlign: "left"
  });
</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script><script type="text/javascript" id="maid-script" mermaidoptioins="{&quot;startOnload&quot;:true,&quot;theme&quot;:&quot;forest&quot;}" src="/js/mermaid.min.js?v=1.0.0"></script><script>if (window.mermaid) {
  var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins'));
  mermaid.initialize(options);
}</script></div><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>