<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="我读书少，你莫骗我。"><title>Way2AI · 机器学习之Logistic Regression (二) | 愚苏记</title><link rel="stylesheet" type="text/css" href="https://cdn.staticfile.org/lxgw-wenkai-webfont/1.6.0/style.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/lib/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="/lib/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="/lib/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="/lib/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="/lib/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '96c862f2728296588ae9849f3bcb95db';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><link rel="alternate" type="application/atom+xml" href="/atom.xml"><meta name="generator" content="Hexo 7.1.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Way2AI · 机器学习之Logistic Regression (二)</h1><a id="logo" href="/.">愚苏记</a><p class="description">To no avail but try.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-4-4"><div class="content_container no-padding-right"><div class="post"><h1 class="post-title">Way2AI · 机器学习之Logistic Regression (二)</h1><div class="post-meta">May 29, 2023<span> | </span><span class="category"><a href="/categories/Way2AI/">Way2AI</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.5k</span><span class="post-meta-item-text"> Words</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 8</span><span class="post-meta-item-text"> Minutes</span></span></span></div><a class="disqus-comment-count" data-disqus-identifier="Way2AI/Way2AI-LogisticRegression-2/" href="/Way2AI/Way2AI-LogisticRegression-2/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#TL-DR"><span class="toc-number">1.</span> <span class="toc-text">TL;DR</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Get-ready"><span class="toc-number">2.</span> <span class="toc-text">Get ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model"><span class="toc-number">3.</span> <span class="toc-text">Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Loss"><span class="toc-number">4.</span> <span class="toc-text">Loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Metrics"><span class="toc-number">5.</span> <span class="toc-text">Metrics</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Optimizer"><span class="toc-number">6.</span> <span class="toc-text">Optimizer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training"><span class="toc-number">7.</span> <span class="toc-text">Training</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Evaluation"><span class="toc-number">8.</span> <span class="toc-text">Evaluation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Inference"><span class="toc-number">9.</span> <span class="toc-text">Inference</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Unscaled-weights"><span class="toc-number">10.</span> <span class="toc-text">Unscaled weights</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ending"><span class="toc-number">11.</span> <span class="toc-text">Ending</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Citation"><span class="toc-number">12.</span> <span class="toc-text">Citation</span></a></li></ol></div></div><div class="post-content"><h2 id="TL-DR">TL;DR</h2>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p>
<p>接上篇，本文使用PyTorch实现一个简单的逻辑回归。</p>
<h2 id="Get-ready">Get ready</h2>
<p>复用前篇的数据准备及预处理工作，这里直接建模。</p>
<h2 id="Model">Model</h2>
<p>我们使用PyTorch的<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#linear-layers" title="Linear layers">Linear layers</a> 来构建与前篇相同的模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LogisticRegression</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(LogisticRegression, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_in</span>):</span><br><span class="line">        z = self.fc1(x_in)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = LogisticRegression(input_dim=INPUT_DIM, num_classes=NUM_CLASSES)</span><br><span class="line"><span class="built_in">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of LogisticRegression(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=2, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="Loss">Loss</h2>
<p>这里使用<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss" title="nn.CrossEntropyLoss">交叉熵损失</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">y_pred = torch.randn(<span class="number">3</span>, NUM_CLASSES, requires_grad=<span class="literal">False</span>)</span><br><span class="line">y_true = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(NUM_CLASSES)</span><br><span class="line"><span class="built_in">print</span> (y_true)</span><br><span class="line">loss = loss_fn(y_pred, y_true)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Loss: <span class="subst">&#123;loss.numpy()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># tensor([0, 1, 1])</span></span><br><span class="line"><span class="comment"># Loss: 1.0754622220993042</span></span><br></pre></td></tr></table></figure>
<p>在这个任务中，我们将数据的类别权重纳入到损失函数中，以对抗样本的类别不平衡。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(<span class="built_in">list</span>(class_weights.values()))</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br></pre></td></tr></table></figure>
<h2 id="Metrics">Metrics</h2>
<p>我们将在训练模型时引入准确度来衡量模型的性能，因为仅查看损失值并不是非常直观。</p>
<p>后面的章节会介绍相关指标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Accuracy</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy_fn</span>(<span class="params">y_pred, y_true</span>):</span><br><span class="line">    n_correct = torch.eq(y_pred, y_true).<span class="built_in">sum</span>().item()</span><br><span class="line">    accuracy = (n_correct / <span class="built_in">len</span>(y_pred)) * <span class="number">100</span></span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line">y_pred = torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">y_true = torch.Tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy: &#123;accuracy_fn(y_pred, y_true):.1f&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Accuracy: &#123;accuracy_fn(y_pred, y_true):.1f&#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="Optimizer">Optimizer</h2>
<p>与之前介绍的线性回归一样，这里同样使用Adam优化器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br></pre></td></tr></table></figure>
<h2 id="Training">Training</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.LongTensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line">NUM_EPOCHS = <span class="number">50</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">        predictions = y_pred.<span class="built_in">max</span>(dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># class</span></span><br><span class="line">        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 0.71, accuracy: 49.6</span></span><br><span class="line"><span class="comment"># Epoch: 10 | loss: 0.23, accuracy: 93.1</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.14, accuracy: 97.4</span></span><br><span class="line"><span class="comment"># Epoch: 30 | loss: 0.11, accuracy: 98.3</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.09, accuracy: 98.0</span></span><br></pre></td></tr></table></figure>
<h2 id="Evaluation">Evaluation</h2>
<p>首先，我们看看一下测试集的准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">pred_train = F.softmax(model(X_train), dim=<span class="number">1</span>)</span><br><span class="line">pred_test = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;sample probability: <span class="subst">&#123;pred_test[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line">pred_train = pred_train.<span class="built_in">max</span>(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">pred_test = pred_test.<span class="built_in">max</span>(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;sample class: <span class="subst">&#123;pred_test[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># sample probability: tensor([0.9934, 0.0066], grad_fn=&lt;SelectBackward0&gt;)</span></span><br><span class="line"><span class="comment"># sample class: 0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy (could&#x27;ve also used our own accuracy function)</span></span><br><span class="line">train_acc = accuracy_score(y_train, pred_train)</span><br><span class="line">test_acc = accuracy_score(y_test, pred_test)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;train acc: <span class="subst">&#123;train_acc:<span class="number">.2</span>f&#125;</span>, test acc: <span class="subst">&#123;test_acc:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train acc: 0.98, test acc: 0.97</span></span><br></pre></td></tr></table></figure>
<p>我们还可以根据其他有意义的指标来评估我们的模型，如精确度和召回率。<br>
$$<br>
accuracy = \frac{TP + FN}{TP + TN + FP + FN}<br>
$$<br>
$$<br>
recall = \frac{TP}{TP + FN}<br>
$$<br>
$$<br>
precision = \frac{TP}{TP + FP}<br>
$$<br>
$$<br>
F1 = 2 * \frac{precision * recall}{precision + recall}<br>
$$</p>
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">TP</td>
<td style="text-align:center">truly predicted to be positive and were positive</td>
</tr>
<tr>
<td style="text-align:center">TN</td>
<td style="text-align:center">truly predicted to negative and where negative</td>
</tr>
<tr>
<td style="text-align:center">FP</td>
<td style="text-align:center">falsely predicted to be positive but where negative</td>
</tr>
<tr>
<td style="text-align:center">FN</td>
<td style="text-align:center">falsely predicted to be negative but where positive</td>
</tr>
</tbody>
</table>
<p>格式化指标，以供前端展示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_metrics</span>(<span class="params">y_true, y_pred, classes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Per-class performance metrics.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">&quot;overall&quot;</span>: &#123;&#125;, <span class="string">&quot;class&quot;</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">&quot;weighted&quot;</span>)</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;precision&quot;</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;recall&quot;</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;f1&quot;</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;num_samples&quot;</span>] = np.float64(<span class="built_in">len</span>(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(classes)):</span><br><span class="line">        performance[<span class="string">&quot;class&quot;</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">&quot;precision&quot;</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">&quot;recall&quot;</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">&quot;f1&quot;</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">&quot;num_samples&quot;</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br><span class="line"></span><br><span class="line"><span class="comment"># # Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=pred_test, classes=label_encoder.classes)</span><br><span class="line"><span class="built_in">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &quot;overall&quot;: &#123;</span></span><br><span class="line"><span class="comment">#     &quot;precision&quot;: 0.9744444444444446,</span></span><br><span class="line"><span class="comment">#     &quot;recall&quot;: 0.9733333333333334,</span></span><br><span class="line"><span class="comment">#     &quot;f1&quot;: 0.9731408308004051,</span></span><br><span class="line"><span class="comment">#     &quot;num_samples&quot;: 150.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   &quot;class&quot;: &#123;</span></span><br><span class="line"><span class="comment">#     &quot;benign&quot;: &#123;</span></span><br><span class="line"><span class="comment">#       &quot;precision&quot;: 1.0,</span></span><br><span class="line"><span class="comment">#       &quot;recall&quot;: 0.9310344827586207,</span></span><br><span class="line"><span class="comment">#       &quot;f1&quot;: 0.9642857142857143,</span></span><br><span class="line"><span class="comment">#       &quot;num_samples&quot;: 58.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     &quot;malignant&quot;: &#123;</span></span><br><span class="line"><span class="comment">#       &quot;precision&quot;: 0.9583333333333334,</span></span><br><span class="line"><span class="comment">#       &quot;recall&quot;: 1.0,</span></span><br><span class="line"><span class="comment">#       &quot;f1&quot;: 0.9787234042553191,</span></span><br><span class="line"><span class="comment">#       &quot;num_samples&quot;: 92.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure>
<p>同样的，利用PyTorch实现的逻辑回归模型建模了一个线性决策边界，我们可视化一下结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_multiclass_decision_boundary</span>(<span class="params">model, X, y</span>):</span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">0.1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">0.1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.linspace(x_min, x_max, <span class="number">101</span>), np.linspace(y_min, y_max, <span class="number">101</span>))</span><br><span class="line">    cmap = plt.cm.Spectral</span><br><span class="line"></span><br><span class="line">    X_test = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).<span class="built_in">float</span>()</span><br><span class="line">    y_pred = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line">    _, y_pred = y_pred.<span class="built_in">max</span>(dim=<span class="number">1</span>)</span><br><span class="line">    y_pred = y_pred.reshape(xx.shape)</span><br><span class="line">    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.RdYlBu)</span><br><span class="line">    plt.xlim(xx.<span class="built_in">min</span>(), xx.<span class="built_in">max</span>())</span><br><span class="line">    plt.ylim(yy.<span class="built_in">min</span>(), yy.<span class="built_in">max</span>())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Train&quot;</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="//s3.mindex.xyz/blog/Courses/8d309a3958c1769ef2403cc39a31dd17.png" alt=""></p>
<h2 id="Inference">Inference</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inputs for inference</span></span><br><span class="line">X_infer = pd.DataFrame([&#123;<span class="string">&quot;leukocyte_count&quot;</span>: <span class="number">13</span>, <span class="string">&quot;blood_pressure&quot;</span>: <span class="number">12</span>&#125;])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize</span></span><br><span class="line">X_infer = X_scaler.transform(X_infer)</span><br><span class="line"><span class="built_in">print</span> (X_infer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[-0.66859939 -3.09473005]]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">y_infer = F.softmax(model(torch.Tensor(X_infer)), dim=<span class="number">1</span>)</span><br><span class="line">prob, _<span class="keyword">class</span> = y_infer.<span class="built_in">max</span>(dim=<span class="number">1</span>)</span><br><span class="line">label = label_encoder.decode(_<span class="keyword">class</span>.detach().numpy())[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;The probability that you have a <span class="subst">&#123;label&#125;</span> tumor is <span class="subst">&#123;prob.detach().numpy()[<span class="number">0</span>]*<span class="number">100.0</span>:<span class="number">.0</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># The probability that you have a benign tumor is 90%</span></span><br></pre></td></tr></table></figure>
<h2 id="Unscaled-weights">Unscaled weights</h2>
<p>同样的，我们亦可以逆标准化我们的权重和偏差。</p>
<p>注意到只有$X$被标准化过<br>
$$<br>
\hat{y}_{unscaled} = \sum_{j=1}^k W_{scaled(j)} x_{scaled(j)} + b_{scaled}<br>
$$</p>
<p>已知<br>
$$<br>
\hat{x}_{scaled} = \frac{x_{j} - \overline{x}_{j}}{\sigma_{j}}<br>
$$</p>
<p>于是<br>
$$<br>
\hat{y}_{unscaled} = (b_{scaled} - \sum_{j=1}^k W_{scaled(j)} \frac{\overline{x}_j}{\sigma_{j}}) + \sum_j{\frac{W_{scaled(j)}}{\sigma_j}}x_j<br>
$$</p>
<p>对比公式</p>
<p>$$<br>
\hat{y}_{unscaled} = W_{unscaled} x + b_{unscaled}<br>
$$</p>
<p>便可得知<br>
$$<br>
W_{unscaled} = \frac{W_{scaled(j)}}{\sigma_j}<br>
$$</p>
<p>$$<br>
b_{unscaled} = b_{scaled} - \sum_{j=1}^k W_{unscaled(j)} \overline{x}_j<br>
$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unstandardize weights</span></span><br><span class="line">W = model.fc1.weight.data.numpy()</span><br><span class="line">b = model.fc1.bias.data.numpy()</span><br><span class="line">W_unscaled = W / X_scaler.scale_</span><br><span class="line">b_unscaled = b - np.<span class="built_in">sum</span>((W_unscaled * X_scaler.mean_))</span><br><span class="line"><span class="built_in">print</span> (W_unscaled)</span><br><span class="line"><span class="built_in">print</span> (b_unscaled)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [[ 0.80800055 -1.47212977]</span></span><br><span class="line"><span class="comment">#  [-0.88854214  0.77129243]]</span></span><br><span class="line"><span class="comment"># [11.4279   13.336911]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Ending">Ending</h2>
<p>到这里，我们便完成了PyTorch的逻辑回归任务的介绍。</p>
<h2 id="Citation">Citation</h2>
<figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="language-xml">,</span></span><br><span class="line"><span class="language-xml">    title        = </span><span class="template-variable">&#123; Logistic regression - Made With ML &#125;</span><span class="language-xml">,</span></span><br><span class="line"><span class="language-xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="language-xml">&#125;,</span></span><br><span class="line"><span class="language-xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">&#125;</span></span><br></pre></td></tr></table></figure>
</div><iframe src="/donate/?AliPayQR=null&amp;WeChatQR=http://s3.mindex.xyz/mp/qrcode-s.jpg&amp;GitHub=http://github.com/neo1989&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Author: </strong>尼欧</li><li class="post-copyright-link"><strong>Blog Link: </strong><a href="/Way2AI/Way2AI-LogisticRegression-2/">https://neo1989.net/Way2AI/Way2AI-LogisticRegression-2/</a></li><li class="post-copyright-license"><strong>Copyright Declaration: </strong>转载请声明出处。</li></ul></div><br><div class="tags"><a href="/tags/Coder/">Coder</a><a href="/tags/AI/">AI</a></div><div class="post-nav"><a class="pre" href="/Way2AI/Way2AI-neural-networks-1/">Way2AI · 神经网络 (一)</a><a class="next" href="/Way2AI/Way2AI-LogisticRegression-1/">Way2AI · 机器学习之Logistic Regression (一)</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://neo1989.net/Way2AI/Way2AI-LogisticRegression-2/';
    this.page.identifier = 'Way2AI/Way2AI-LogisticRegression-2/';
    this.page.title = 'Way2AI · 机器学习之Logistic Regression (二)';
  };</script><!-- script(type='text/javascript' id='disqus-lazy-load-script').--><!--   $.ajax({--><!--   url: 'https://disqus.com/next/config.json',--><!--   timeout: 2500,--><!--   type: 'GET',--><!--   success: function(){--><!--     var d = document;--><!--     var s = d.createElement('script');--><!--     s.src = '//#{theme.disqus}.disqus.com/embed.js';--><!--     s.setAttribute('data-timestamp', + new Date());--><!--     (d.head || d.body).appendChild(s);--><!--     $('.disqus_click_btn').css('display', 'none');--><!--   },--><!--   error: function() {--><!--     $('.disqus_click_btn').css('display', 'block');--><!--   }--><!--   });--><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//neo1989.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://neo1989.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div></div><div class="pure-u-1 pure-u-md-4-6"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">愚苏记.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/i-yard/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho/maupassant"> Cho.</a></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="/lib/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="/lib/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
  displayAlign: "left"
  });
</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script><script type="text/javascript" id="maid-script" mermaidoptioins="{&quot;startOnload&quot;:true,&quot;theme&quot;:&quot;forest&quot;}" src="/js/mermaid.min.js?v=1.0.0"></script><script>if (window.mermaid) {
  var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins'));
  mermaid.initialize(options);
}</script></div><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>