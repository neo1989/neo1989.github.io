<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="我读书少，你莫骗我。"><title>语义搜索 | 愚苏记</title><link rel="stylesheet" type="text/css" href="https://cdn.staticfile.org/lxgw-wenkai-webfont/1.6.0/style.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/lib/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="/lib/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="/lib/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="/lib/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="/lib/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '96c862f2728296588ae9849f3bcb95db';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><link rel="alternate" type="application/atom+xml" href="/atom.xml"><meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">语义搜索</h1><a id="logo" href="/.">愚苏记</a><p class="description">To no avail but try.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-5-5"><div class="content_container no-padding-right"><div class="post"><h1 class="post-title">语义搜索</h1><div class="post-meta">Jan 10, 2024<span> | </span><span class="category"><a href="/categories/Way2AI/">Way2AI</a></span></div><a class="disqus-comment-count" data-disqus-identifier="Way2AI/Way2AI-Semantic-Search/" href="/Way2AI/Way2AI-Semantic-Search/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#背景"><span class="toc-number">1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#对称与非对称"><span class="toc-number">2.</span> <span class="toc-text">对称与非对称</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Python"><span class="toc-number">3.</span> <span class="toc-text">Python</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#速度优化"><span class="toc-number">4.</span> <span class="toc-text">速度优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ElasticSearch"><span class="toc-number">5.</span> <span class="toc-text">ElasticSearch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#近似最近邻点"><span class="toc-number">6.</span> <span class="toc-text">近似最近邻点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#召回和重排"><span class="toc-number">7.</span> <span class="toc-text">召回和重排</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#完整示例"><span class="toc-number">8.</span> <span class="toc-text">完整示例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#相似问题检索"><span class="toc-number">8.1.</span> <span class="toc-text">相似问题检索</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#相似出版物检索"><span class="toc-number">8.2.</span> <span class="toc-text">相似出版物检索</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问答检索"><span class="toc-number">8.3.</span> <span class="toc-text">问答检索</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reference"><span class="toc-number">9.</span> <span class="toc-text">Reference</span></a></li></ol></div></div><div class="post-content"><p>语义搜索通过理解搜索查询的内容来提高搜索准确性。与传统搜索引擎不同，传统搜索引擎仅根据词法匹配查找文档，而语义搜索还可以找到同义词。</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><h3 id="背景">背景</h3>
<p>语义搜索的基本思想是将语料库中的所有条目（无论是句子、段落还是文档）都嵌入到一个向量空间中。</p>
<p>在搜索时，查询会被嵌入到相同的向量空间中，然后从语料库中找到与查询最接近的嵌入。这些嵌入应该与查询具有高度语义重叠。</p>
<p><img src="//s3.mindex.xyz/blog/Courses/a88a6ad41612242bacf9371252618da4.png" alt=""></p>
<h3 id="对称与非对称">对称与非对称</h3>
<p>对称语义搜索是指你的查询和语料库中的条目长度大致相同，并且具有相同数量的内容。一个例子是搜索类似的问题：你的查询可以例如是“如何在线学习 Python？”，你想找到一个像“如何在网络上学习 Python？”这样的条目。对于对称任务，你可能可以在语料库中翻找到查询和对应的条目。</p>
<p>非对称语义搜索是指你通常有一个简短的查询（例如一个问题或一些关键词），你想找到一个更长的段落来回答查询。一个例子是一个像“什么是 Python”的查询，你想找到段落“Python 是一种解释型、高级且通用的编程语言。Python 的设计理念是……”。对于非对称任务，在语料库中翻找通常没有意义。</p>
<p>选择适合任务类型的模型非常重要。</p>
<p>适合对称语义搜索的模型：<a href="https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models" target="_blank" rel="noopener">Pre-Trained Sentence Embedding Models</a></p>
<p>适合非对称语义搜索的模型：<a href="https://www.sbert.net/docs/pretrained-models/msmarco-v3.html" target="_blank" rel="noopener">Pre-Trained MS MARCO Models</a></p>
<h3 id="Python">Python</h3>
<p>在数据量不大的语料库中（条目数量最多大约100万），我们有能力计算出搜索词与语料库内每一个条目之间的余弦相似度。</p>
<p>在接下来的示例中，我们创建了一个包括几个样本句子的小型语料库，并为这个语料库以及我们的搜索词分别计算了它们的嵌入向量。</p>
<p>接着，我们运用 <code>sentence_transformers.util.cos_sim()</code> 函数来测量搜索词与语料库中所有条目之间的余弦相似性。</p>
<p>面对庞大的语料库，对所有评分逐一排序实在是效率太低。所以，我们采用了 <code>torch.topk</code> 函数来直接提取得分最高的前 k 个条目。</p>
<p>下面是一个简单的示例；参见 <a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search.py" target="_blank" rel="noopener">semantic_search.py</a>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">This is a simple application for sentence embeddings: semantic search</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">We have a corpus with various sentences. Then, for a given query sentence,</span></span><br><span class="line"><span class="string">we want to find the most similar sentence in this corpus.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This script outputs for various queries the top 5 most similar sentences in the corpus.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer, util</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">embedder = SentenceTransformer(<span class="string">'all-MiniLM-L6-v2'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Corpus with example sentences</span></span><br><span class="line">corpus = [</span><br><span class="line">    <span class="string">'A man is eating food.'</span>,</span><br><span class="line">    <span class="string">'A man is eating a piece of bread.'</span>,</span><br><span class="line">    <span class="string">'The girl is carrying a baby.'</span>,</span><br><span class="line">    <span class="string">'A man is riding a horse.'</span>,</span><br><span class="line">    <span class="string">'A woman is playing violin.'</span>,</span><br><span class="line">    <span class="string">'Two men pushed carts through the woods.'</span>,</span><br><span class="line">    <span class="string">'A man is riding a white horse on an enclosed ground.'</span>,</span><br><span class="line">    <span class="string">'A monkey is playing drums.'</span>,</span><br><span class="line">    <span class="string">'A cheetah is running behind its prey.'</span></span><br><span class="line">]</span><br><span class="line">corpus_embeddings = embedder.encode(corpus, convert_to_tensor=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Query sentences:</span></span><br><span class="line">queries = [<span class="string">'A man is eating pasta.'</span>, <span class="string">'Someone in a gorilla costume is playing a set of drums.'</span>, <span class="string">'A cheetah chases prey on across a field.'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity</span></span><br><span class="line">top_k = min(<span class="number">5</span>, len(corpus))</span><br><span class="line"><span class="keyword">for</span> query <span class="keyword">in</span> queries:</span><br><span class="line">    query_embedding = embedder.encode(query, convert_to_tensor=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We use cosine-similarity and torch.topk to find the highest 5 scores</span></span><br><span class="line">    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[<span class="number">0</span>]</span><br><span class="line">    top_results = torch.topk(cos_scores, k=top_k)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"\n\n======================\n\n"</span>)</span><br><span class="line">    print(<span class="string">"Query:"</span>, query)</span><br><span class="line">    print(<span class="string">"\nTop 5 most similar sentences in corpus:"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> score, idx <span class="keyword">in</span> zip(top_results[<span class="number">0</span>], top_results[<span class="number">1</span>]):</span><br><span class="line">        print(corpus[idx], <span class="string">"(Score: &#123;:.4f&#125;)"</span>.format(score))</span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk</span></span><br><span class="line"><span class="string">    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)</span></span><br><span class="line"><span class="string">    hits = hits[0]      #Get the hits for the first query</span></span><br><span class="line"><span class="string">    for hit in hits:</span></span><br><span class="line"><span class="string">        print(corpus[hit['corpus_id']], "(Score: &#123;:.4f&#125;)".format(hit['score']))</span></span><br><span class="line"><span class="string">    """</span></span><br></pre></td></tr></table></figure>
<h3 id="速度优化">速度优化</h3>
<p>要想让 <code>sentence_transformers.util.cos_sim()</code> 方法运行得更快，最好的做法是将 <code>query_embeddings</code> 和 <code>corpus_embeddings</code> 存在同一块 GPU 设备上。这样做可以明显提升处理性能。</p>
<p>另外，我们还可以对语料库嵌入进行标准化处理，使每个语料库嵌入的长度都为 1。这样，我们就可以通过点积运算来计算得分了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus_embeddings = corpus_embeddings.to(<span class="string">'cuda'</span>)</span><br><span class="line">corpus_embeddings = util.normalize_embeddings(corpus_embeddings)</span><br><span class="line"></span><br><span class="line">query_embeddings = query_embeddings.to(<span class="string">'cuda'</span>)</span><br><span class="line">query_embeddings = util.normalize_embeddings(query_embeddings)</span><br><span class="line">hits = util.semantic_search(query_embeddings, corpus_embeddings, score_function=util.dot_score)</span><br></pre></td></tr></table></figure>
<h3 id="ElasticSearch">ElasticSearch</h3>
<p>从 7.3 版本开始，ElasticSearch 推出了一个新功能，即能够索引密集向量 (dense vectors)，并将其用于对文档进行评分。所以，我们可以利用 ElasticSearch 对文档以及嵌入向量（embeddings）进行索引，以此在搜索时使用对应的嵌入向量寻找相关的文档信息。</p>
<p>ElasticSearch的一个优点是，它便于向索引中添加新的文档，而且能够和我们的向量一起存储其他数据。但缺点是它的性能较慢，这是因为它需要将搜索的嵌入内容和每一个已经存储的嵌入内容进行比较。这种操作的时间成本是线性的，对于大规模（超过 100k）的数据集来说，可能会慢得无法接受。</p>
<p>更多详细信息，请参见 <a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_elasticsearch.py" target="_blank" rel="noopener">semantic_search_quora_elasticsearch.py</a>。</p>
<h3 id="近似最近邻点">近似最近邻点</h3>
<p>如果使用精确的最近邻搜索方法（如 <code>sentence_transformers.util.semantic_search</code> 所采用的方式），在一个巨大的语料库中进行查找，特别是这个语料库中包含数百万个嵌入，可能会花费大量的时间。</p>
<p>在这种情况下，近似最近邻（Approximate Nearest Neighor，ANN）可能会很有帮助。这里，数据被划分为相似的嵌入小部分。利用索引可以有效地进行搜索，甚至在有数百万的向量时也能在毫秒内检索到最高相似性的嵌入（即最近的邻居）。</p>
<p>不过，结果未必都是精确的。可能有些具有高度相似性的向量被遗漏了。这就是我们称它为“近似最近邻居”的原因。</p>
<p>所有的人工神经网络（ANN）方法都通常需要调整一到多个参数，以达到召回率与搜索速度的权衡。如果你追求极高的搜索速度，可能会错过一些重要的搜索结果。反之，如果你期望得到高召回率，搜索的速度就可能会变慢。</p>
<p>近似最近邻搜索库中，<a href="https://github.com/spotify/annoy" target="_blank" rel="noopener" title="Annoy">Annoy</a>、<a href="https://github.com/facebookresearch/faiss" target="_blank" rel="noopener" title="FAISS">FAISS</a> 和 <a href="https://github.com/nmslib/hnswlib/" target="_blank" rel="noopener" title="hnswlib">hnswlib</a> 都很热门。但是，我个人更偏向 <code>hnswlib</code>，因为它不仅使用起来十分简单，性能卓越，而且包含了许多在实际应用中至关重要的特色功能。</p>
<p>示例：</p>
<ul>
<li><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_hnswlib.py" target="_blank" rel="noopener" title="semantic_search_quora_hnswlib.py">semantic_search_quora_hnswlib.py</a></li>
<li><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_annoy.py" target="_blank" rel="noopener" title="semantic_search_quora_annoy.py">semantic_search_quora_annoy.py</a></li>
<li><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_faiss.py" target="_blank" rel="noopener" title="semantic_search_quora_faiss.py">semantic_search_quora_faiss.py</a></li>
</ul>
<h3 id="召回和重排">召回和重排</h3>
<p>对于复杂的语义搜索场景，建议使用「召回和重排」流程：</p>
<p><img src="//s3.mindex.xyz/blog/Courses/3d66117e5374e1f95d858d7d422fc22e.png" alt=""></p>
<p>更多详细信息，请参见 <a href="https://www.sbert.net/examples/applications/retrieve_rerank/README.html" target="_blank" rel="noopener">Retrieve &amp; Re-rank</a></p>
<h3 id="完整示例">完整示例</h3>
<h4 id="相似问题检索">相似问题检索</h4>
<p><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_quora_pytorch.py" target="_blank" rel="noopener">semantic_search_quora_pytorch.py</a> [<a href="https://colab.research.google.com/drive/11GunvCqJuebfeTlgbJWkIMT0xJH6PWF1?usp=sharing" target="_blank" rel="noopener">Colab Version</a>] 是一个基于Quora重复问题数据集的应用案例。通过它，用户可以输入任何问题，然后代码会运用 <code>sentence_transformers.util.semantic_search</code> 方法从数据集中找出与输入问题最相近的问题。模型是 distilbert-multilingual-nli-stsb-quora-ranking，它的主要任务是去识别类似的问题，并且它支持超过50种语言。所以，无论用户用这50多种语言中的何种来提问，都可以得到有效的答案。这是一个对称的搜索任务，因为搜索查询的长度和内容与语料库中的问题相同。</p>
<h4 id="相似出版物检索">相似出版物检索</h4>
<p><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_publications.py" target="_blank" rel="noopener">semantic_search_publications.py</a> [<a href="https://colab.research.google.com/drive/12hfBveGHRsxhPIUMmJYrll2lFU4fOX06?usp=sharing" target="_blank" rel="noopener">Colab Version</a>] 这个示例演示了如何找到与某篇科学论文相似的其他论文。我们的语料库由在 EMNLP 2016 - 2018 会议上发表的所有论文组成。在搜索过程中，我们会输入最近发表的论文的标题和摘要，然后在我们的语料库中寻找相关的论文。我们使用的是 <a href="https://arxiv.org/abs/2004.07180" target="_blank" rel="noopener">SPECTER</a> 模型。这个搜索任务是对称的，因为我们的语料库中的论文和我们搜索的内容都是由标题和摘要组成。</p>
<h4 id="问答检索">问答检索</h4>
<p><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search/semantic_search_wikipedia_qa.py" target="_blank" rel="noopener">semantic_search_wikipedia_qa.py</a> [<a href="">Colab Version</a><a href="https://colab.research.google.com/drive/11GunvCqJuebfeTlgbJWkIMT0xJH6PWF1?usp=sharing">https://colab.research.google.com/drive/11GunvCqJuebfeTlgbJWkIMT0xJH6PWF1?usp=sharing</a>]：这个例子展示了一个在 <a href="https://ai.google.com/research/NaturalQuestions/" target="_blank" rel="noopener">Natural Questions dataset</a> 数据集上进行训练的模型。这个数据集包含了大约十万条真实的 Google 搜索请求，以及从维基百科获取并附带注解的段落，这些段落提供了问题的答案。这是一个非对称搜索任务的典型例子。在这个例子中，我们使用了体积较小的 <a href="https://simple.wikipedia.org/wiki/Main_Page" target="_blank" rel="noopener">Simple English Wikipedia</a> 作为语料库，这样它就可以轻松地加载到内存中。</p>
<p><a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.py" target="_blank" rel="noopener">retrieve_rerank_simple_wikipedia.py</a> [<a href="https://colab.research.google.com/github/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb" target="_blank" rel="noopener">Colab Version </a>]：这个脚本采用了 <strong>召回和重排</strong>的策略，是一个非对称搜索任务的典型例子。我们把所有维基百科的文章切分成各个段落，并用双向编码器进行编码处理。当有新的查询或问题输入时，我们也用同样的双向编码器进行编码，然后找出与之余弦相似度最高的段落。然后，我们用一个交叉编码器对找到的候选段落进行重新排序，最终将得分最高的5个段落展示给用户。我们使用的模型是在 <a href="https://github.com/microsoft/MSMARCO-Passage-Ranking/" target="_blank" rel="noopener">MS Marco Passage Reranking datase</a> 数据集上进行训练的，这个数据集包含了大约 500k 来自 Bing 搜索的真实查询。</p>
<h3 id="Reference">Reference</h3>
<p><a href="https://www.sbert.net/examples/applications/semantic-search/README.html" target="_blank" rel="noopener">Semantic Search</a></p>
</div><iframe src="/donate/?AliPayQR=null&amp;WeChatQR=http://s3.mindex.xyz/mp/qrcode-s.jpg&amp;GitHub=http://github.com/neo1989&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Author: </strong>尼欧</li><li class="post-copyright-link"><strong>Blog Link: </strong><a href="/Way2AI/Way2AI-Semantic-Search/">https://neo1989.net/Way2AI/Way2AI-Semantic-Search/</a></li><li class="post-copyright-license"><strong>Copyright Declaration: </strong>转载请声明出处。</li></ul></div><br><div class="tags"><a href="/tags/Coder/">Coder</a><a href="/tags/AI/">AI</a></div><div class="post-nav"><a class="next" href="/Notes/NOTE-Sept-graines-de-lumiere-dans-le-coeur-des-guerriers/">《我就是你啊》</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://neo1989.net/Way2AI/Way2AI-Semantic-Search/';
    this.page.identifier = 'Way2AI/Way2AI-Semantic-Search/';
    this.page.title = '语义搜索';
  };</script><!-- script(type='text/javascript' id='disqus-lazy-load-script').--><!--   $.ajax({--><!--   url: 'https://disqus.com/next/config.json',--><!--   timeout: 2500,--><!--   type: 'GET',--><!--   success: function(){--><!--     var d = document;--><!--     var s = d.createElement('script');--><!--     s.src = '//#{theme.disqus}.disqus.com/embed.js';--><!--     s.setAttribute('data-timestamp', + new Date());--><!--     (d.head || d.body).appendChild(s);--><!--     $('.disqus_click_btn').css('display', 'none');--><!--   },--><!--   error: function() {--><!--     $('.disqus_click_btn').css('display', 'block');--><!--   }--><!--   });--><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//neo1989.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://neo1989.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div></div><div class="pure-u-1 pure-u-md-4-6"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">愚苏记.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/i-yard/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho/maupassant"> Cho.</a></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="/lib/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="/lib/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script><script type="text/javascript" id="maid-script" mermaidoptioins="{&quot;startOnload&quot;:true,&quot;theme&quot;:&quot;forest&quot;}" src="/js/mermaid.min.js?v=1.0.0"></script><script>if (window.mermaid) {
  var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins'));
  mermaid.initialize(options);
}</script></div><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>