<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="我读书少，你莫骗我。"><title>Way2AI · 机器学习之Linear Regression (二) | 愚苏记</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/lib/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="/lib/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="/lib/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="/lib/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="/lib/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '96c862f2728296588ae9849f3bcb95db';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Way2AI · 机器学习之Linear Regression (二)</h1><a id="logo" href="/.">愚苏记</a><p class="description">To no avail but try.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-5-5"><div class="content_container no-padding-right"><div class="post"><h1 class="post-title">Way2AI · 机器学习之Linear Regression (二)</h1><div class="post-meta">May 21, 2023<span> | </span><span class="category"><a href="/categories/Way2AI/">Way2AI</a></span></div><a class="disqus-comment-count" data-disqus-identifier="Way2AI/Way2AI-LinearRegression-2/" href="/Way2AI/Way2AI-LinearRegression-2/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#TL-DR"><span class="toc-number">1.</span> <span class="toc-text">TL;DR</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Generate-data"><span class="toc-number">2.</span> <span class="toc-text">Generate data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Split-data"><span class="toc-number">3.</span> <span class="toc-text">Split data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Standardize-data"><span class="toc-number">4.</span> <span class="toc-text">Standardize data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Weights"><span class="toc-number">5.</span> <span class="toc-text">Weights</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model"><span class="toc-number">6.</span> <span class="toc-text">Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Loss"><span class="toc-number">7.</span> <span class="toc-text">Loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Optimizer"><span class="toc-number">8.</span> <span class="toc-text">Optimizer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training"><span class="toc-number">9.</span> <span class="toc-text">Training</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Evaluation"><span class="toc-number">10.</span> <span class="toc-text">Evaluation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Inference"><span class="toc-number">11.</span> <span class="toc-text">Inference</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Interpretability"><span class="toc-number">12.</span> <span class="toc-text">Interpretability</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Regularization"><span class="toc-number">13.</span> <span class="toc-text">Regularization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ending"><span class="toc-number">14.</span> <span class="toc-text">Ending</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Citation"><span class="toc-number">15.</span> <span class="toc-text">Citation</span></a></li></ol></div></div><div class="post-content"><h2 id="TL-DR">TL;DR</h2>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>Way2AI系列，确保出发去&quot;改变世界&quot;之前，我们已经打下了一个坚实的基础。</p>
<p>接上篇，本文的目标是使用PyTorch实现一个线性回归模型。</p>
<h2 id="Generate-data">Generate data</h2>
<p>我们复用上篇生成的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line">NUM_SAMPLES = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate synthetic data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_data</span><span class="params">(num_samples)</span>:</span></span><br><span class="line">    <span class="string">"""Generate dummy data for linear regression."""</span></span><br><span class="line">    X = np.array(range(num_samples))</span><br><span class="line">    random_noise = np.random.uniform(<span class="number">-10</span>, <span class="number">20</span>, size=num_samples)</span><br><span class="line">    y = <span class="number">3.5</span>*X + random_noise <span class="comment"># add some noise</span></span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate random (linear) data</span></span><br><span class="line">X, y = generate_data(num_samples=NUM_SAMPLES)</span><br><span class="line">data = np.vstack([X, y]).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load into a Pandas DataFrame</span></span><br><span class="line">df = pd.DataFrame(data, columns=[<span class="string">"X"</span>, <span class="string">"y"</span>])</span><br><span class="line">X = df[[<span class="string">"X"</span>]].values</span><br><span class="line">y = df[[<span class="string">"y"</span>]].values</span><br><span class="line"></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<p><img src="//s3.mindex.xyz/blog/Courses/44b3f9578128ebd663285e3dd1ac4120.png" alt=""></p>
<p>我们将数据绘制成散点图，可以看到它们有很明显的线性趋势。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scatter plot</span></span><br><span class="line">plt.title(<span class="string">"Generated data"</span>)</span><br><span class="line">plt.scatter(x=df[<span class="string">"X"</span>], y=df[<span class="string">"y"</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="//s3.mindex.xyz/blog/Courses/48c4cc75b277978a0b5e3eadf9e252e7.png" alt=""></p>
<h2 id="Split-data">Split data</h2>
<p>区别于上一篇中我们使用自定义摇骰子的方式分割数据，这里选择使用<a href="https://scikit-learn.org/stable/" target="_blank" rel="noopener" title="scikit-learn">scikit-learn</a>包里提供的<code>train_test_split</code>方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Split (train)</span></span><br><span class="line">X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"train: <span class="subst">&#123;len(X_train)&#125;</span> (<span class="subst">&#123;(len(X_train) / len(X)):<span class="number">.2</span>f&#125;</span>)\n"</span></span><br><span class="line">       <span class="string">f"remaining: <span class="subst">&#123;len(X_)&#125;</span> (<span class="subst">&#123;(len(X_) / len(X)):<span class="number">.2</span>f&#125;</span>)"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train: 35 (0.70)</span></span><br><span class="line"><span class="comment"># remaining: 15 (0.30)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Split (test)</span></span><br><span class="line">X_val, X_test, y_val, y_test = train_test_split(</span><br><span class="line">    X_, y_, train_size=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"train: <span class="subst">&#123;len(X_train)&#125;</span> (<span class="subst">&#123;len(X_train)/len(X):<span class="number">.2</span>f&#125;</span>)\n"</span></span><br><span class="line">  <span class="string">f"val: <span class="subst">&#123;len(X_val)&#125;</span> (<span class="subst">&#123;len(X_val)/len(X):<span class="number">.2</span>f&#125;</span>)\n"</span></span><br><span class="line">  <span class="string">f"test: <span class="subst">&#123;len(X_test)&#125;</span> (<span class="subst">&#123;len(X_test)/len(X):<span class="number">.2</span>f&#125;</span>)"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train: 35 (0.70)</span></span><br><span class="line"><span class="comment"># val: 7 (0.14)</span></span><br><span class="line"><span class="comment"># test: 8 (0.16)</span></span><br></pre></td></tr></table></figure>
<h2 id="Standardize-data">Standardize data</h2>
<p>同样的，我们需要对数据进行标准化处理。这里使用<code>scikit-learn</code>里提供的<code>StandardScaler</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the data (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler().fit(X_train)</span><br><span class="line">y_scaler = StandardScaler().fit(y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply scaler on training and test data</span></span><br><span class="line">X_train = X_scaler.transform(X_train)</span><br><span class="line">y_train = y_scaler.transform(y_train).ravel().reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">X_val = X_scaler.transform(X_val)</span><br><span class="line">y_val = y_scaler.transform(y_val).ravel().reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">X_test = X_scaler.transform(X_test)</span><br><span class="line">y_test = y_scaler.transform(y_test).ravel().reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;np.mean(X_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"mean: <span class="subst">&#123;np.mean(y_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(y_test, axis=<span class="number">0</span>)[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># mean: 0.4, std: 1.0</span></span><br><span class="line"><span class="comment"># mean: 0.3, std: 0.9</span></span><br></pre></td></tr></table></figure>
<h2 id="Weights">Weights</h2>
<p>我们将使用PyTorch的<code>Linear layers</code>来实现一个没有隐含层的神经网络。<br>
关于神经网络我们后面会具体学习。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">torch.manual_seed(SEED)</span><br><span class="line"></span><br><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>] <span class="comment"># X is 1-dimensional</span></span><br><span class="line">OUTPUT_DIM = y_train.shape[<span class="number">1</span>] <span class="comment"># y is 1-dimensional</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Inputs</span></span><br><span class="line">N = <span class="number">3</span> <span class="comment"># num samples</span></span><br><span class="line">x = torch.randn(N, INPUT_DIM)</span><br><span class="line"><span class="keyword">print</span> (x.shape)</span><br><span class="line"><span class="keyword">print</span> (x.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"><span class="comment"># [[-1.4836688 ]</span></span><br><span class="line"><span class="comment">#  [ 0.26714355]</span></span><br><span class="line"><span class="comment">#  [-1.8336787 ]]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Weights</span></span><br><span class="line">m = nn.Linear(INPUT_DIM, OUTPUT_DIM)</span><br><span class="line"><span class="keyword">print</span> (m)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"weights (<span class="subst">&#123;m.weight.shape&#125;</span>): <span class="subst">&#123;m.weight[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"bias (<span class="subst">&#123;m.bias.shape&#125;</span>): <span class="subst">&#123;m.bias[<span class="number">0</span>]&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Linear(in_features=1, out_features=1, bias=True)</span></span><br><span class="line"><span class="comment"># weights (torch.Size([1, 1])): -0.2795013189315796</span></span><br><span class="line"><span class="comment"># bias (torch.Size([1])): -0.7643394470214844</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Forward pass</span></span><br><span class="line">z = m(x)</span><br><span class="line"><span class="keyword">print</span> (z.shape)</span><br><span class="line"><span class="keyword">print</span> (z.detach().numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"><span class="comment"># [[-0.34965205]</span></span><br><span class="line"><span class="comment">#  [-0.8390064 ]</span></span><br><span class="line"><span class="comment">#  [-0.25182384]]</span></span><br></pre></td></tr></table></figure>
<h2 id="Model">Model</h2>
<p>$$<br>
\hat{y} = WX + b<br>
$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, output_dim)</span>:</span></span><br><span class="line">        super(LinearRegression, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x_in)</span>:</span></span><br><span class="line">        y_pred = self.fc1(x_in)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = LinearRegression(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM)</span><br><span class="line"><span class="keyword">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of LinearRegression(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=1, out_features=1, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="Loss">Loss</h2>
<p>同样的，我们使用PyTorch自带的<a href="">Loss Functions</a>, 这里指定<code>MSELoss</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line">y_pred = torch.Tensor([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line">y_true =  torch.Tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>])</span><br><span class="line">loss = loss_fn(y_pred, y_true)</span><br><span class="line">print(<span class="string">"Loss: "</span>, loss.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Loss:  0.75</span></span><br></pre></td></tr></table></figure>
<h2 id="Optimizer">Optimizer</h2>
<p>上一篇中我们介绍了使用梯度下降的方法来更新我们的权重。PyTorch中有很多不同的权重更新方法，需要根据不同的场景来选择合适的。详见<a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener" title="TORCH.OPTIM">TORCH.OPTIM</a>。<br>
这里我们采用适合大多数场景的<code>ADAM optimizer</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">LEARNING_RATE = <span class="number">1e-1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br></pre></td></tr></table></figure>
<h2 id="Training">Training</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.Tensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.Tensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.Tensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line">NUM_EPOCHS = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    <span class="comment"># https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 1.11</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.10</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.04</span></span><br><span class="line"><span class="comment"># Epoch: 60 | loss: 0.03</span></span><br><span class="line"><span class="comment"># Epoch: 80 | loss: 0.03</span></span><br></pre></td></tr></table></figure>
<h2 id="Evaluation">Evaluation</h2>
<p>现在我们准备评估我们训练好的模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">pred_train = model(X_train)</span><br><span class="line">pred_test = model(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">train_error = loss_fn(pred_train, y_train)</span><br><span class="line">test_error = loss_fn(pred_test, y_test)</span><br><span class="line">print(<span class="string">f"train_error: <span class="subst">&#123;train_error:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"test_error: <span class="subst">&#123;test_error:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># train_error: 0.03</span></span><br><span class="line"><span class="comment"># test_error: 0.04</span></span><br></pre></td></tr></table></figure>
<p>由于我们只有一个特征，因此可以轻松地对模型进行可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Figure size</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot train data</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Train"</span>)</span><br><span class="line">plt.scatter(X_train, y_train, label=<span class="string">"y_train"</span>)</span><br><span class="line">plt.plot(X_train, pred_train.detach().numpy(), color=<span class="string">"red"</span>, linewidth=<span class="number">1</span>, linestyle=<span class="string">"-"</span>, label=<span class="string">"model"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot test data</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Test"</span>)</span><br><span class="line">plt.scatter(X_test, y_test, label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.plot(X_test, pred_test.detach().numpy(), color=<span class="string">"red"</span>, linewidth=<span class="number">1</span>, linestyle=<span class="string">"-"</span>, label=<span class="string">"model"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show plots</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="//s3.mindex.xyz/blog/Courses/332563148a19dae3df2c54de08dafccd.png" alt=""></p>
<h2 id="Inference">Inference</h2>
<p>训练完模型后，我们可以使用它来对新数据进行预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Feed in your own inputs</span></span><br><span class="line">sample_indices = [<span class="number">10</span>, <span class="number">15</span>, <span class="number">25</span>]</span><br><span class="line">X_infer = np.array(sample_indices, dtype=np.float32)</span><br><span class="line">X_infer = torch.Tensor(X_scaler.transform(X_infer.reshape(<span class="number">-1</span>, <span class="number">1</span>)))</span><br></pre></td></tr></table></figure>
<p>由于我们对数据都进行了标准化，所以对预测值需要进行逆操作。<br>
$$<br>
\hat{y}_{scaled} = \frac{\hat{y} - \mu_{\hat{y}}}{\sigma_{\hat{y}}}<br>
$$</p>
<p>$$<br>
\hat{y} = \hat{y}_{scaled} * \sigma_{\hat{y}} + \mu_{\hat{y}}<br>
$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Unstandardize predictions</span></span><br><span class="line">pred_infer = model(X_infer).detach().numpy() * np.sqrt(y_scaler.var_) + y_scaler.mean_</span><br><span class="line"><span class="keyword">for</span> i, index <span class="keyword">in</span> enumerate(sample_indices):</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">f"<span class="subst">&#123;df.iloc[index][<span class="string">'y'</span>]:<span class="number">.2</span>f&#125;</span> (actual) → <span class="subst">&#123;pred_infer[i][<span class="number">0</span>]:<span class="number">.2</span>f&#125;</span> (predicted)"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 28.10 (actual) → 40.99 (predicted)</span></span><br><span class="line"><span class="comment"># 56.45 (actual) → 58.62 (predicted)</span></span><br><span class="line"><span class="comment"># 100.83 (actual) → 93.88 (predicted)</span></span><br></pre></td></tr></table></figure>
<h2 id="Interpretability">Interpretability</h2>
<p>线性回归具有高度可解释性的巨大优势。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unstandardize coefficients</span></span><br><span class="line">W = model.fc1.weight.data.numpy()[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">b = model.fc1.bias.data.numpy()[<span class="number">0</span>]</span><br><span class="line">W_unscaled = W * (y_scaler.scale_/X_scaler.scale_)</span><br><span class="line">b_unscaled = b * y_scaler.scale_ + y_scaler.mean_ - np.sum(W_unscaled*X_scaler.mean_)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"[actual] y = 3.5X + noise"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"[model] y_hat = <span class="subst">&#123;W_unscaled[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>X + <span class="subst">&#123;b_unscaled[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [actual] y = 3.5X + noise</span></span><br><span class="line"><span class="comment"># [model] y_hat = 3.5X + 5.7</span></span><br></pre></td></tr></table></figure>
<h2 id="Regularization">Regularization</h2>
<p>正则化有助于减少过拟合。本例使用<code>L2正则化</code> (岭回归)。</p>
<p>通过L2正则化，我们对大的权重值进行惩罚，鼓励权重是较小值。 还有其他类型的正则化，比如L1（套索回归），它可以用于创建稀疏模型，其中一些特征系数被清零，或者结合了L1和L2惩罚的弹性正则化。</p>
<p>正则化不仅适用于线性回归，您可以使用它来处理任何模型的权重，包括我们将在未来学习到的模型。</p>
<p>$$<br>
J(\theta) = \frac{1}{2} \sum_{i}(WX_i - y_i)^2 + \frac{\lambda}{2} \sum_i{W_i}^2<br>
$$</p>
<p>$$<br>
\frac{\partial(J)}{\partial(W)} = (\hat{y} - y)X + \lambda{W}<br>
$$</p>
<p>$$<br>
W = W - \alpha{\frac{\partial{J}}{\partial{W}}}<br>
$$</p>
<p>$\lambda$: 正则化系数; $\alpha$: 学习率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">L2_LAMBDA = <span class="number">1e-2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = LinearRegression(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer (w/ L2 regularization)</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">f"Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 0.67</span></span><br><span class="line"><span class="comment"># Epoch: 20 | loss: 0.06</span></span><br><span class="line"><span class="comment"># Epoch: 40 | loss: 0.03</span></span><br><span class="line"><span class="comment"># Epoch: 60 | loss: 0.03</span></span><br><span class="line"><span class="comment"># Epoch: 80 | loss: 0.03</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">pred_train = model(X_train)</span><br><span class="line">pred_test = model(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">train_error = loss_fn(pred_train, y_train)</span><br><span class="line">test_error = loss_fn(pred_test, y_test)</span><br><span class="line">print(<span class="string">f"train_error: <span class="subst">&#123;train_error:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"test_error: <span class="subst">&#123;test_error:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line">train_error: <span class="number">0.03</span></span><br><span class="line">test_error: <span class="number">0.03</span></span><br></pre></td></tr></table></figure>
<p>对于这个特定的例子，正则化并没有在性能上产生差异，因为我们的数据是从一个完美的线性方程生成的。但是对于大规模真实数据，正则化可以帮助我们的模型很好地泛化。</p>
<h2 id="Ending">Ending</h2>
<p>本篇基于上一篇的基础，简单介绍了如何用PyTorch实现线性回归。</p>
<p>Peace out.</p>
<h2 id="Citation">Citation</h2>
<figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    title        = </span><span class="template-variable">&#123; Linear regression - Made With ML &#125;</span><span class="xml">,</span></span><br><span class="line"><span class="xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="xml">&#125;,</span></span><br><span class="line"><span class="xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span></span><br><span class="line"><span class="xml">&#125;</span></span><br></pre></td></tr></table></figure>
</div><iframe src="/donate/?AliPayQR=null&amp;WeChatQR=http://s3.mindex.xyz/mp/qrcode-s.jpg&amp;GitHub=http://github.com/neo1989&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Author: </strong>尼欧</li><li class="post-copyright-link"><strong>Blog Link: </strong><a href="/Way2AI/Way2AI-LinearRegression-2/">https://neo1989.net/Way2AI/Way2AI-LinearRegression-2/</a></li><li class="post-copyright-license"><strong>Copyright Declaration: </strong>转载请声明出处。</li></ul></div><br><div class="tags"><a href="/tags/Coder/">Coder</a><a href="/tags/AI/">AI</a></div><div class="post-nav"><a class="pre" href="/Way2AI/Way2AI-CrossEntropyLoss/">Way2AI · PyTorch的CrossEntropyLoss实现的不对？</a><a class="next" href="/Way2AI/Way2AI-LinearRegression-1/">Way2AI · 机器学习之Linear Regression (一)</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://neo1989.net/Way2AI/Way2AI-LinearRegression-2/';
    this.page.identifier = 'Way2AI/Way2AI-LinearRegression-2/';
    this.page.title = 'Way2AI · 机器学习之Linear Regression (二)';
  };</script><!-- script(type='text/javascript' id='disqus-lazy-load-script').--><!--   $.ajax({--><!--   url: 'https://disqus.com/next/config.json',--><!--   timeout: 2500,--><!--   type: 'GET',--><!--   success: function(){--><!--     var d = document;--><!--     var s = d.createElement('script');--><!--     s.src = '//#{theme.disqus}.disqus.com/embed.js';--><!--     s.setAttribute('data-timestamp', + new Date());--><!--     (d.head || d.body).appendChild(s);--><!--     $('.disqus_click_btn').css('display', 'none');--><!--   },--><!--   error: function() {--><!--     $('.disqus_click_btn').css('display', 'block');--><!--   }--><!--   });--><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//neo1989.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://neo1989.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div></div><div class="pure-u-1 pure-u-md-4-6"><div id="footer">Copyright © 2023 <a href="/." rel="nofollow">愚苏记.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/i-yard/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho/maupassant"> Cho.</a></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="/lib/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="/lib/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
  displayAlign: "left"
  });
</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script><script type="text/javascript" id="maid-script" mermaidoptioins="{&quot;startOnload&quot;:true,&quot;theme&quot;:&quot;forest&quot;}" src="/js/mermaid.min.js?v=1.0.0"></script><script>if (window.mermaid) {
  var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins'));
  mermaid.initialize(options);
}</script></div><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>