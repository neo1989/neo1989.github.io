<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="我读书少，你莫骗我。"><title>Way2AI · RAG 综述 | 愚苏记</title><link rel="stylesheet" type="text/css" href="https://cdn.staticfile.org/lxgw-wenkai-webfont/1.6.0/style.css"><link rel="stylesheet" type="text/css" href="../../css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="../../lib/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="../../lib/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="../../lib/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="../../lib/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="../../lib/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="../../favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="../../favicon.ico"><link rel="apple-touch-icon" href="../../apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="../../apple-touch-icon.png"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '96c862f2728296588ae9849f3bcb95db';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><link rel="alternate" type="application/atom+xml" href="../../atom.xml"><meta name="generator" content="Hexo 7.1.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Way2AI · RAG 综述</h1><a id="logo" href="../../.">愚苏记</a><p class="description">To no avail but try.</p></div><div id="nav-menu"><a class="current" href="../../."><i class="fa fa-home"> Home</i></a><a href="../../archives/"><i class="fa fa-archive"> Archive</i></a><a href="../../about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-4-4"><div class="content_container no-padding-right"><div class="post"><h1 class="post-title">Way2AI · RAG 综述</h1><div class="post-meta">Mar 20, 2024<span> | </span><span class="category"><a href="../../categories/Way2AI/">Way2AI</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 25k</span><span class="post-meta-item-text"> Words</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 86</span><span class="post-meta-item-text"> Minutes</span></span></span></div><a class="disqus-comment-count" data-disqus-identifier="Way2AI/Way2AI-RAG-Survey/" href="#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#TL-DR"><span class="toc-number">1.</span> <span class="toc-text">TL;DR</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Definition"><span class="toc-number">3.</span> <span class="toc-text">Definition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RAG-Framework"><span class="toc-number">4.</span> <span class="toc-text">RAG Framework</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Naive-RAG"><span class="toc-number">4.1.</span> <span class="toc-text">Naive RAG</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Indexing"><span class="toc-number">4.1.1.</span> <span class="toc-text">Indexing</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Retrieval"><span class="toc-number">4.1.2.</span> <span class="toc-text">Retrieval</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Generation"><span class="toc-number">4.1.3.</span> <span class="toc-text">Generation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Naive-RAG-%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="toc-number">4.1.4.</span> <span class="toc-text">Naive RAG 的缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Advanced-RAG"><span class="toc-number">4.2.</span> <span class="toc-text">Advanced RAG</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Pre-Retrieval-Process"><span class="toc-number">4.2.1.</span> <span class="toc-text">Pre-Retrieval Process</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Retrieval-2"><span class="toc-number">4.2.2.</span> <span class="toc-text">Retrieval</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Post-Retrieval-Process"><span class="toc-number">4.2.3.</span> <span class="toc-text">Post-Retrieval Process</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Modular-RAG"><span class="toc-number">4.3.</span> <span class="toc-text">Modular RAG</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#New-Modules"><span class="toc-number">4.3.1.</span> <span class="toc-text">New Modules</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#New-Patterns"><span class="toc-number">4.3.2.</span> <span class="toc-text">New Patterns</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Optimizing-the-RAG-Pipeline"><span class="toc-number">4.3.3.</span> <span class="toc-text">Optimizing the RAG Pipeline</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Retrieval-3"><span class="toc-number">5.</span> <span class="toc-text">Retrieval</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Enhancing-Semantic-Representations"><span class="toc-number">5.1.</span> <span class="toc-text">Enhancing Semantic Representations</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Chunk-optimization"><span class="toc-number">5.1.1.</span> <span class="toc-text">Chunk optimization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Fine-tuning-Embedding-Models"><span class="toc-number">5.1.2.</span> <span class="toc-text">Fine-tuning Embedding Models</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Aligning-Queries-and-Documents"><span class="toc-number">5.2.</span> <span class="toc-text">Aligning Queries and Documents</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Query-Rewriting"><span class="toc-number">5.2.1.</span> <span class="toc-text">Query Rewriting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Embedding-Transformation"><span class="toc-number">5.2.2.</span> <span class="toc-text">Embedding Transformation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Aligning-Retriever-and-LLM"><span class="toc-number">5.3.</span> <span class="toc-text">Aligning Retriever and LLM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Fine-tuning-Retrievers"><span class="toc-number">5.3.1.</span> <span class="toc-text">Fine-tuning Retrievers</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Adapters"><span class="toc-number">5.3.2.</span> <span class="toc-text">Adapters</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Generation-2"><span class="toc-number">6.</span> <span class="toc-text">Generation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Post-retrieval-with-Frozen-LLM"><span class="toc-number">6.1.</span> <span class="toc-text">Post-retrieval with Frozen LLM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Information-Compression"><span class="toc-number">6.1.1.</span> <span class="toc-text">Information Compression</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reranking"><span class="toc-number">6.1.2.</span> <span class="toc-text">Reranking</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fine-tuning-LLM-for-RAG"><span class="toc-number">6.2.</span> <span class="toc-text">Fine-tuning LLM for RAG</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#General-Optimization-Process"><span class="toc-number">6.2.1.</span> <span class="toc-text">General Optimization Process</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Utilizing-Contrastive-Learning"><span class="toc-number">6.2.2.</span> <span class="toc-text">Utilizing Contrastive Learning</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Augmentation-in-RAG"><span class="toc-number">7.</span> <span class="toc-text">Augmentation in RAG</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RAG-in-Augmentation-Stages"><span class="toc-number">7.1.</span> <span class="toc-text">RAG in Augmentation Stages</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Pre-training-Stage"><span class="toc-number">7.1.1.</span> <span class="toc-text">Pre-training Stage</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Fine-tuning-Stage"><span class="toc-number">7.1.2.</span> <span class="toc-text">Fine-tuning Stage</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Inference-Stage"><span class="toc-number">7.1.3.</span> <span class="toc-text">Inference Stage</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Augmentation-Source"><span class="toc-number">7.2.</span> <span class="toc-text">Augmentation Source</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Augmented-with-Unstructured-Data"><span class="toc-number">7.2.1.</span> <span class="toc-text">Augmented with Unstructured Data</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Augmented-with-Structured-Data"><span class="toc-number">7.2.2.</span> <span class="toc-text">Augmented with Structured Data</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LLMs-Generated-Content-in-RAG"><span class="toc-number">7.2.3.</span> <span class="toc-text">LLMs-Generated Content in RAG</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Augmentation-Process"><span class="toc-number">7.3.</span> <span class="toc-text">Augmentation Process</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Iterative-Retrieval"><span class="toc-number">7.3.1.</span> <span class="toc-text">Iterative Retrieval</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Recursive-Retrieval"><span class="toc-number">7.3.2.</span> <span class="toc-text">Recursive Retrieval</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Adaptive-Retrieval"><span class="toc-number">7.3.3.</span> <span class="toc-text">Adaptive Retrieval</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RAG-vs-Fine-Tuning"><span class="toc-number">7.4.</span> <span class="toc-text">RAG vs Fine-Tuning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RAG-Evaluation"><span class="toc-number">8.</span> <span class="toc-text">RAG Evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Evaluation-Targets"><span class="toc-number">8.1.</span> <span class="toc-text">Evaluation Targets</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Retrieval-Quality"><span class="toc-number">8.1.1.</span> <span class="toc-text">Retrieval Quality</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Generation-Quality"><span class="toc-number">8.1.2.</span> <span class="toc-text">Generation Quality</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Evaluation-Aspects"><span class="toc-number">8.2.</span> <span class="toc-text">Evaluation Aspects</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Quality-Scores"><span class="toc-number">8.2.1.</span> <span class="toc-text">Quality Scores</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Required-Abilities"><span class="toc-number">8.2.2.</span> <span class="toc-text">Required Abilities</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Evaluation-Benchmarks-and-Tools"><span class="toc-number">8.3.</span> <span class="toc-text">Evaluation Benchmarks and Tools</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Future-Prospects"><span class="toc-number">9.</span> <span class="toc-text">Future Prospects</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Future-Challenges-of-RAG"><span class="toc-number">9.1.</span> <span class="toc-text">Future Challenges of RAG</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Modality-Extension-of-RAG"><span class="toc-number">9.1.1.</span> <span class="toc-text">Modality Extension of RAG</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Ecosystem-of-RAG"><span class="toc-number">9.2.</span> <span class="toc-text">Ecosystem of RAG</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Downstream-Tasks-and-Evaluation"><span class="toc-number">9.2.1.</span> <span class="toc-text">Downstream Tasks and Evaluation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Technical-Stack"><span class="toc-number">9.2.2.</span> <span class="toc-text">Technical Stack</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conclusion"><span class="toc-number">10.</span> <span class="toc-text">Conclusion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Source"><span class="toc-number">11.</span> <span class="toc-text">Source</span></a></li></ol></div></div><div class="post-content"><h2 id="TL-DR">TL;DR</h2>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>RAG 通过融合外部数据库的知识，提高了模型的准确性和可靠性，尤其在知识密集型任务中更是如此。RAG 允许进行持续的知识更新，并整合特定领域的信息。RAG 将大语言模型的内在知识与外部数据库广大、不断更新的知识库协同融合。</p>
<p>本文深入探讨了RAG 范式的发展过程，包括 Naive RAG、Advanced RAG 以及 Modular RAG； 仔细剖析了构成 RAG 框架的三大核心部分，包括检索、生成以及增强技术；详细阐述了每个关键部分所采用的最先进技术，深入剖析了 RAG 系统的最新进展；同时，也介绍了评估 RAG 模型的各项指标和基准，以及最新的评估框架；最后，勾勒出了未来的研究方向，包括面临的挑战、多模态的拓展，以及 RAG 基础设施和生态系统的进步。</p>
<h2 id="Introduction">Introduction</h2>
<p>大语言模型在处理特定领域或高度专业化的查询时，存在显著的局限性。例如，当查询超出模型的训练数据范围或需要获取最新信息时，模型常常会生成错误的信息，这种现象被称为“幻觉”。这些问题使得在没有额外保护措施的情况下，直接将大语言模型作为黑箱解决方案用于实际生产环境变得不现实。然而，RAG 提供了一种有效的解决方式，它通过将外部数据检索融入生成过程，从而提升了模型提供准确和相关回应的能力。</p>
<p>具体来说，RAG 的工作流程首先包含一个检索步骤，在回答问题或生成文本之前，大语言模型 (LLMs) 会先向外部数据源发出查询，以获取相关信息。这个过程不仅为接下来的文本生成阶段提供依据，同时也确保了生成的内容基于检索到的证据，这大大增强了输出结果的准确性和相关性。在推理阶段，RAG 能够从知识库中动态检索信息，以解决生成内容中的事实错误，这种现象通常被称为 “幻觉” (hallucinations)。RAG 的引入极大地推动了大语言模型的发展，它已经迅速被采用，并成为了提升聊天机器人能力和使大语言模型更具实用价值的关键技术。</p>
<p><img src="//s3.mindex.xyz/blog/Courses/47723527970e1cda0cb0e887269fc6fa.png" alt="图1"></p>
<p>如 图1 所示，RAG 的发展过程可以分为四个明显的阶段。在 2017 年，RAG 刚刚诞生，这个阶段正好与 Transformer 架构的出现同时发生。这个时期，RAG 的主要工作是通过预训练模型（PTM）来学习和获得更多的知识，以此来提升语言模型的性能。在这个阶段，RAG 的主要努力集中在优化预训练的方法上。</p>
<p>在初始阶段过后，RAG相关研究进入了一段相对平静的时期，直到聊天型GPT的出现。聊天型GPT的问世无疑是一个转折点，它将大语言模型推向了研究的前沿。研究社区开始将关注点转向如何利用大语言模型的能力，以提高模型的控制性并满足日益变化的需求。因此，大部分的RAG研究开始集中在推理上，只有少数研究者专注于微调过程。随着大语言模型的能力的不断提升，尤其是在GPT-4的推出后，RAG技术的发展面貌发生了显著的变化。研究重点开始转向一种混合方法，这种方法结合了RAG和微调的优势，同时，仍有一小部分研究者持续关注预训练方法的优化。</p>
<p>尽管 RAG 研究取得了飞速的发展，但我们仍然缺乏对该领域的系统性总结和概括，这使得我们难以全面理解 RAG 的技术进步。本次调研的目标就是梳理 RAG 的全过程，并深入探讨其现状和未来发展趋势，我们将通过深入研究大语言模型中的检索增强技术来实现这一目标。</p>
<p>因此，本文的目标是全面梳理和组织 RAG 的技术原理、发展历程、内容，特别是在大语言模型诞生后的相关方法和应用，以及 RAG 的评估方法和应用场景。我们希望通过对现有 RAG 技术的全面概述和分析，为未来的发展方向提供洞见和展望。本研究的意图是让读者和实践者对大模型和 RAG 有一个深入且系统的理解，清楚地理解检索增强的发展进程和关键技术，明确各种技术的优点和局限以及它们适用的情境，并预测未来可能的发展趋势。</p>
<p>本文的主要贡献包括：</p>
<ul>
<li>
<p>全面而系统地回顾了最新的 RAG 技术，梳理了其从初级 RAG、高级 RAG 到模块化 RAG 的发展历程。这次回顾将 RAG 研究的更广泛的背景置于大语言模型的发展脉络中。</p>
</li>
<li>
<p>确定并讨论了 RAG 过程中的关键技术，特别是“检索”、“生成器”和“增强”这几个环节，并深入探讨他们的协同作用，明确阐述这些组件如何紧密协作以构建一个连贯和有效的 RAG 框架。</p>
</li>
<li>
<p>为 RAG 构建了一个全面的评估框架，明确了评估的目标和评价指标。本文的比较分析揭示了 RAG 相比于模型微调方法的优势和劣势。此外，本文也预测了 RAG 的未来发展方向，强调了解决当前挑战的可能改进，扩展到多模态环境的可能性，以及其生态系统的发展。</p>
</li>
</ul>
<h2 id="Definition">Definition</h2>
<p>我们可以通过其工作流程来理解RAG的定义。</p>
<p>如 图2 所示，这是一个典型的 RAG 应用工作流程。</p>
<p>假设有用户向 ChatGPT 提问了一个最近引发了大量公众讨论的重大事件。 作为目前最知名且使用最广泛的大语言模型，ChatGPT 受到其预训练数据的限制，对最新的事件并不了解。RAG 就能解决这个问题，它能从外部知识库中检索到最新的文档片段。在这个例子中，RAG 检索到了一些与用户提问相关的新闻文章。接着，这些文章和用户的问题一起，被整合成一个信息丰富的提示，从而使 ChatGPT 能够生成一个更有深度的回答。</p>
<p>这个例子展示了 RAG 的工作过程，以及它如何通过实时信息检索来提升模型的回答质量。</p>
<p><img src="//s3.mindex.xyz/blog/Courses/2e656cad5ada2a3fc179e5c211322434.png" alt="图2"></p>
<p>从技术角度来看，RAG 已经通过各种创新方法得到了丰富，解决了如“应检索什么”、“何时进行检索”以及“如何利用检索到的信息”等核心问题。对于“应检索什么”的问题，研究已经从简单的 Token 和实体检索发展到更复杂的结构，如数据块和知识图，这些研究主要关注检索的精细程度以及数据的结构化程度。粗粒度的检索可以提供更多的信息，但精度较低。而检索结构化的文本虽然可以提供更多的信息，但会牺牲一些效率。对于“何时进行检索”的问题，已经产生了从单次检索到自适应和多次检索的策略。频繁的检索可以带来更多的信息，但效率会降低。至于“如何利用”检索到的数据，已经在模型架构的各个层次（包括输入层、中间层和输出层）中开发了集成技术。尽管在中间层和输出层的集成表现得更有效，但这需要更多的训练，并可能导致效率降低。</p>
<p>RAG 是一种通过整合外部知识库来增强大语言模型的新范式。它采用了一种协同的策略，将信息检索机制和上下文学习（即 In-Context Learning，ICL）结合起来，以此提升大语言模型的性能。在这个框架下，用户发起的查询会通过搜索算法触发相关信息的检索。然后，这些信息被融入到大语言模型的提示中，为生成过程提供了更丰富的上下文。RAG 的主要优势在于，它避免了为特定任务重新训练大语言模型的需求。开发者可以直接连接一个外部知识库，通过丰富输入来提升模型的输出精度。由于其高实用性和低门槛，RAG 已经成为大语言模型系统中最受欢迎的架构之一，许多对话产品几乎完全基于 RAG 构建。</p>
<p>RAG 的工作流程可以概括为三个步骤。<br>
首先，我们将大量的文本数据（语料库）分割为一小块一小块，然后使用一种叫做编码模型的工具，对这些小块进行标记，好比给每一块都打上了一个独特的标签。<br>
接下来，当 RAG 收到一个查询请求时，它会根据这个请求和已经标记好的小块之间的相似度，找出最相关的几块。<br>
最后，RAG 会根据这些找出的相关小块，生成一个回答。<br>
这个过程就像是在一大堆书中，找出最相关的几本，然后根据这几本书的内容，回答一个问题。这三个步骤构成了 RAG 的基本框架，使得 RAG 能够找到相关信息，并生成与上下文相关的回答。</p>
<p>接下来，我们会详细介绍 RAG 的研究框架。</p>
<h2 id="RAG-Framework">RAG Framework</h2>
<p>尽管 RAG 在成本效益上表现出色，并且超过了原生大语言模型的表现，但它们也存在一些明显的限制。Advanced RAG 和 Modular RAG 的开发就是针对 Naive RAG 中的这些特定短板的改进。</p>
<h3 id="Naive-RAG">Naive RAG</h3>
<p>Naive RAG 研究范式代表了最初的方法，这种方法在 ChatGPT 得到广泛采用后不久就开始崭露头角。Naive RAG 遵循一种传统的流程，包括索引、检索和生成。它也被形象地称为 “Retrieve-Read” 模式。</p>
<h4 id="Indexing">Indexing</h4>
<p>Indexing 是数据准备的关键环节，这一过程在离线环境中进行，并涉及多个阶段。</p>
<p>首先，我们需要对原始数据进行索引，这一步骤包括清洗和提取数据，将各种文件格式（如 PDF，HTML，Word，Markdown）转换为标准化的纯文本。为了适应语言模型的上下文处理能力，我们会将这些文本分割成更小、更易于处理的片段，这个过程被称为 “切块”。<br>
接着，我们会使用一个嵌入模型将这些片段转换为向量表示，这种嵌入模型的选择考虑了推理效率和模型大小的平衡，这样能在检索阶段方便地进行相似性比较。<br>
最后，我们会创建一个索引来存储这些文本片段及其向量嵌入，这些信息以键值对的形式存储，从而实现高效、可扩展的搜索功能。</p>
<h4 id="Retrieval">Retrieval</h4>
<p>当系统接收到用户的查询请求后，它会使用与索引阶段相同的编码模型，将输入的信息转化为向量形式。接着，它会计算出这个查询向量与索引库中各个向量片段的相似程度。系统会优先选择出与查询向量最相似的前K个片段。这些片段将作为更丰富的上下文信息，帮助系统更好地回应用户的请求。</p>
<h4 id="Generation">Generation</h4>
<p>用户的查询请求和被选中的文档会被整合成一个连贯的指令，大语言模型需要根据这个指令来形成回应。模型回答问题的方式可能会根据具体任务的要求而有所不同，既可以依赖于其内在的参数知识，也可以仅限于使用所提供文档中的信息。在进行连续对话的情况下，任何已有的对话历史都可以被整合到指令中，这样模型就能有效地进行多轮对话交互。</p>
<h4 id="Naive-RAG-的缺点">Naive RAG 的缺点</h4>
<p>Naive RAG 技术在“信息检索”、“答案生成”和“信息增强”三个关键环节面临着重大挑战。</p>
<p>信息检索的质量问题主要体现在精准度低，导致检索到的信息片段对齐不佳，可能出现信息错位或者信息断裂等问题。同时也存在召回率低的情况，即无法检索到所有相关的信息片段，这会阻碍大语言模型（LLMs）生成全面的回应。此外，过时的信息也会使问题变得更加复杂，可能会导致检索结果的准确性降低。</p>
<p>在答案生成的质量方面，模型可能会产生“幻觉”问题，即生成的答案并未基于所提供的上下文，还有可能出现与上下文无关或者模型输出存在潜在偏见的问题。</p>
<p>在信息增强的过程中，如何有效地将检索到的段落的上下文与当前的生成任务结合起来也是一大挑战，可能会导致生成的内容不连贯或者不一致。当多个检索到的段落包含相似的信息时，生成的回应可能会出现重复或者过度冗余的问题。</p>
<p>如何判断多个检索到的段落对生成任务的重要性和相关性也是一个挑战，需要适当权衡每个段落的价值。此外，如何在保证输出一致性的前提下，处理不同的写作风格和语气也是一项重要的任务。</p>
<p>最后，如果生成模型过度依赖检索到的信息，可能会导致输出的内容只是重复检索到的内容，而没有提供新的价值或合成信息。</p>
<h3 id="Advanced-RAG">Advanced RAG</h3>
<p>在提高检索质量上，Advanced RAG 增加了预检索和后检索策略。Advanced RAG 利用滑动窗口技术、细粒度的切割方法，以及元数据等手段，对其索引方式进行了优化。 同时，它也引入了多种方法，以进一步优化检索过程。</p>
<h4 id="Pre-Retrieval-Process">Pre-Retrieval Process</h4>
<p><strong>Optimizing Data Indexing</strong> 优化数据索引的目标是提高索引内容的质量。实现这一目标主要涉及五大策略：提升数据的精细度，优化索引的结构，添加元数据，调整数据对齐方式，以及混合检索策略。</p>
<p>提高数据粒度的目标是为了提升文本的规范性、一致性、真实性和丰富的背景信息，从而进一步提高 RAG 系统的性能。这涉及到删除无关的信息，澄清实体和术语的含义，核实事实的真实性，保持信息的上下文关联性，以及更新过时的文档。</p>
<p>优化索引结构主要包括调整数据块的大小以便更好地捕获相关的上下文信息，跨越多个索引路径进行查询，以及利用图数据索引中节点间的关系，借助图结构的信息来获取相关的上下文。</p>
<p>添加元数据信息主要包括将一些元数据（如日期、目的等）融入到数据块中用于过滤，同时也包括整合如章节、参考文献的子部分等元数据，以提高检索的效率。</p>
<p>对齐优化是一种解决文档间不一致问题的方法，它通过在文档中引入<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.19912">“假设性问题”</a>，来修正这些对齐的问题和差异。</p>
<h4 id="Retrieval-2">Retrieval</h4>
<p>在检索阶段，我们主要通过计算查询内容和信息片段（chunks）之间的相似度来确定最合适的上下文。在这个过程中，嵌入模型扮演着至关重要的角色。在 Advanced RAG 技术中，我们有潜力对这些嵌入模型进行进一步的优化。</p>
<p><strong>Fine-tuning Embedding</strong> 微调嵌入模型对 RAG 系统检索到的内容的相关性有着重大影响。这个过程包括定制嵌入模型，以提升在特定领域，尤其是在处理持续发展或罕见术语的专业领域中的检索相关性。比如，由 BAAI 开发的 BGE 嵌入模型（例如 BGE-large-EN）就是一种高性能的嵌入模型，可以通过微调来优化检索的相关性。微调所需的训练数据可以用类似 GPT-3.5-turbo 这样的语言模型生成，这种模型可以构造出基于文档片段的问题，然后将这些问题与对应的答案作为微调的配对数据。</p>
<p><strong>Dynamic Embedding</strong> 动态嵌入能够根据词语在语境中的使用而变化，这与<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.04906">静态嵌入</a>不同，后者为每个词语分配一个固定的向量。例如，在像 BERT 这样的 Transformer 模型中，同一个词语在不同的语境中可能会有不同的嵌入表示。OpenAI 的 embeddings-ada-02 模型，是基于 GPT 等大语言模型（LLMs）原理构建的一种复杂的动态嵌入模型，能够理解并捕获语境信息。然而，它可能对语境的敏感度并不如最新的全尺寸语言模型，如 GPT-4 那样高。</p>
<h4 id="Post-Retrieval-Process">Post-Retrieval Process</h4>
<p>在从数据库中检索出有价值的上下文后，必须将其与查询内容一同输入到大语言模型（LLMs）中，同时也要应对上下文窗口大小限制带来的挑战。如果一次性将所有相关文档全都输入到大语言模型中，可能会超出模型处理的上下文窗口大小限制，导致信息混乱，干扰对关键信息的提取。因此，我们需要对检索到的内容进行进一步的处理，以解决这些问题。</p>
<p><strong>Re-Ranking</strong></p>
<p>将检索到的信息重新排序，以将最相关的内容重新定位到提示的边缘，是一项关键策略。这个概念已经在像 LlamaIndex、LangChain 和 HayStack 这样的框架中实现。例如，Diversity Ranker 根据文档的多样性优先进行重新排序，而 LostInTheMiddleRanker 则交替地将最佳文档放在上下文窗口的开头和结尾。此外，像 cohereAI rerank、BGE rerank 和 LongLLMLingua 这样的方法重新计算了相关文本和查询之间的语义相似性，解决了解释基于向量的模拟搜索语义相似性的挑战。</p>
<p><strong>Prompt Compression</strong></p>
<p>研究发现，检索文档中的噪声会对 RAG 的性能产生不良影响。在后处理阶段，我们主要关注如何压缩无关的上下文，凸显出关键段落，并减少整体上下文的长度。有些方法，如 Selective Context 和 LLMLingua，使用小型语言模型来计算提示的互信息（即两个变量之间的信息共享程度）或困惑度（用于衡量模型预测的准确性），以此来评估各个元素的重要性。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.04408" title="RECOMP">RECOMP</a> 方法则通过训练不同级别的压缩算法来解决这个问题，而 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.03025" title="Long Context">Long Context</a> 和 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.05029" title="Walking Down the Memory Maze">“Walking in the Memory Maze”</a> 则通过设计总结技术来提高大语言模型对关键信息的识别能力，尤其是在处理大量上下文信息时。</p>
<h3 id="Modular-RAG">Modular RAG</h3>
<p>模块化的 RAG 结构进一步拓展了传统的 Naive RAG 框架，提供了更高的灵活性和多样性。它采用了多种方法来提升系统性能，如引入了一个用于相似度检索的搜索模块，并在检索器中采用了微调技术。为了解决特定的问题，研究人员开发了改进的 RAG 模块和迭代方法。模块化的 RAG 范式在 RAG 领域中逐渐成为主流，允许进行序列化的处理流程，或者在多个模块之间进行端到端的训练。图3 展示了三种 RAG 范式之间的关系。然而，模块化 RAG 并不是孤立存在的。Advanced RAG 是模块化 RAG 的一种特化形式，而 Naive RAG 则是 Advanced RAG 的一个特例。这三种范式之间的关系是一种继承和发展的过程。</p>
<p><img src="//s3.mindex.xyz/blog/Courses/de3edf0ee0b239184c3eae0197716cd4.png" alt="图3"></p>
<h4 id="New-Modules">New Modules</h4>
<p><strong>Search Module</strong> 相较于 Naive/Advanced RAG 的相似性检索，搜索模块更针对特定场景进行定制，进一步在额外的语料库上直接进行搜索。这种集成方式是通过大语言模型（LLM）生成的代码，以及如 SQL 或 Cypher 等查询语言和其他自定义工具实现的。进行搜索的数据源可以包括搜索引擎、文本数据、表格数据，以及知识图谱。</p>
<p><strong>Memory Module</strong> 该模块运用大语言模型（LLM）的记忆功能来引导信息检索。这种方法主要是找出与当前输入最匹配的记忆信息。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.02437" title="Self-Memory">Self-Memory</a> 则是通过一个检索增强的生成器，迭代地创建一个无界的记忆池，这个过程中会结合“原始问题”和“双向问题”。通过运用一个能够自我改进，即利用自身输出进行优化的检索增强生成模型，使得在推理过程中，文本的表述更能贴近数据的实际分布。因此，模型会优先使用自身的输出结果，而非<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.08773">训练数据</a>。</p>
<p><strong>Fusion</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.03367" title="RAG-Fusion">RAG-Fusion</a> 改进了传统搜索系统的某些局限性，它采用一种多角度查询方法。这种方法利用大语言模型（LLM）将用户的搜索请求扩展到多个不同的角度去考虑。这样不仅能够抓住用户明确寻求的信息，还能挖掘出更深层次、更具变革性的知识。在这个过程中，系统会同时对原始的搜索请求和扩展后的请求进行搜索，然后通过智能排序优化搜索结果，再把最佳的搜索结果与新的查询请求配对。这种高级的方法确保搜索结果不仅符合用户明确的需求，还能捕捉到用户可能没有明确表达出来的需求，从而帮助用户发现更深入、更相关的信息。</p>
<p><strong>Routing</strong> RAG 系统在检索过程中会利用各种不同的资源，这些资源在领域、语言和格式上各不相同，根据具体情况，可以选择交替使用或者合并。查询路由能够根据用户的查询来决定接下来的操作，这些操作可能包括生成摘要、搜索特定的数据库，或者将不同的路径合并成一个响应。此外，查询路由还会选择最适合当前查询的数据存储，可能包括各种不同的资源，如向量存储、图数据库或关系数据库，甚至是索引的层次结构——比如，用于多文档存储的摘要索引和文档块向量索引。查询路由的决策是预定义的，通过调用大语言模型 (LLMs) 来执行，从而将查询指向选定的索引。</p>
<p><strong>Predict</strong> 不同于直接从数据源进行检索，此模块更倾向于利用大语言模型来生成必要的上下文。相较于直接检索得到的信息，大语言模型生成的内容更能准确地包含我们所需要的、与问题紧密相关的信息。</p>
<p><strong>Task Adapter</strong> 这个模块致力于让 RAG 在各种下游任务中发挥作用。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.08518" title="UPRISE">UPRISE</a> 系统能够自动从预先构建的数据池中提取出用于零样本任务的输入提示，从而增强了不同任务和模型之间的通用性。同时，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2209.11755" title="Promptagator">Promptagator</a> 则运用大语言模型作为少样本查询生成器，根据产生的数据，进一步构建出针对特定任务的检索器。通过发挥大语言模型的泛化能力，我们可以仅用极少的样本就能开发出针对特定任务的端到端检索系统。</p>
<h4 id="New-Patterns">New Patterns</h4>
<p>Modular RAG 的架构具有极高的灵活性，可以根据特定问题的需求，调整或替换 RAG 流程中的模块。</p>
<p>无论是 Naive RAG 还是 Advanced RAG，都可以看作是由一些固定的模块构成。如 图3 所示，Naive RAG 主要由 “检索”（Retrieve）和 “阅读”（Read）模块组成。而 Advanced RAG 在 Naive RAG 的基础上，增加了 “重写”（Rewrite）和 “重新排序”（Rerank）模块。总的来说，Modular RAG 具有更大的多样性和灵活性。</p>
<p>目前的研究主要探讨两种组织模式。第一种是添加或替换模块，第二种则关注调整模块间的工作流程。这种灵活性使得 RAG 流程可以定制化，从而有效地应对各种任务。</p>
<p><strong>Adding or Replacing Modules</strong></p>
<p>引入或替换模块的策略的主要目标是在保持检索-阅读过程的基本架构的同时，融入额外的模块以增强某些特定的功能。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14283">Rewrite-Retrieve-Read</a> 就是一个很好的例子，它在原有的检索-阅读过程中加入了重写环节，形成了一种新的“重写-检索-阅读”的流程。在这个过程中，大语言模型（LLM）的表现被用作一种强化学习的奖励机制，用来驱动重写模块。这样，重写模块就能够对检索的查询进行微调，从而提升读取器在完成下游任务时的性能。</p>
<p>同样，我们也可以在一些方法中选择性地替换模块，比如在 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2209.10063" title="Generate-Read">Generate-Read</a> 的方法中，大语言模型（LLM）的生成模块就可以替代原有的检索模块。另一个例子是 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.01296" title="Recite-Read">Recite-Read</a> 的方法，它将检索的过程从外部数据转移到了模型的权重之中。在这种方法中，大语言模型（LLM）需要首先记住特定任务的信息，然后再产生能够处理知识密集型自然语言处理任务的输出。</p>
<p><strong>Adjusting the Flow between Modules</strong></p>
<p>在调整各个模块的运行流程这个领域，人们正在努力增强语言模型和检索模型之间的互动。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.14024" title="Demonstrate-Search-Predict">DSP</a> 提出了一种被称为“展示-搜索-预测”的框架，它将上下文学习系统视为一个具体的程序，而不仅仅是完成任务的最终提示，从而能更有效地处理那些需要大量知识的任务。而 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.15294" title="Iterative Retrieval-Generation">ITER-RETGEN</a> 方法则利用生成的内容来引导信息检索，在一个不断循环的“检索-阅读-再检索-再阅读”的过程中，实现了“通过检索来增强生成”和“通过生成来增强检索”。这种方法展示了一种新颖的方式，即利用一个模块的输出来提升另一个模块的功能。</p>
<h4 id="Optimizing-the-RAG-Pipeline">Optimizing the RAG Pipeline</h4>
<p>检索过程的优化旨在提高 RAG 系统中信息的获取效率和质量。</p>
<p>当前的研究主要集中在融合各种搜索技术，精化检索步骤，引入认知回溯机制（这是一种根据已获得信息反向调整搜索策略的方法），实行灵活多样的查询策略，以及运用嵌入相似性（这是一种通过比较向量在多维空间中的位置关系来衡量其相似性的方法）。这些方法的共同目标是在 RAG 系统中实现检索效率与上下文信息深度的平衡。</p>
<p><strong>Hybrid Search Exploration</strong> RAG 系统通过巧妙地融合各类技术，如关键词检索、语义理解和向量匹配，从而提升其性能。这种方式充分利用了每种技术的特长，以适应各种不同的搜索需求和信息类型，保证我们能够始终找到与查询内容高度匹配且包含丰富上下文信息的答案。混合搜索的使用，为我们的检索策略提供了强有力的支持，从而提升了整个 RAG 系统的效果。</p>
<p><strong>Recursive Retrieval and Query Engine</strong> 递归检索的过程分为两个步骤。首先，在初始阶段，我们会获取一些较小的信息片段，以便抓住关键的语义含义。然后，在后续阶段，我们会向 LLM 提供含有更多上下文信息的较大的信息片段。这种两步式的检索方法旨在在效率和提供丰富的上下文响应之间找到一个平衡点。</p>
<p><strong>StepBack-prompt</strong> 让大语言模型（LLM）不再只关注具体的实例，而是参与到对更广泛概念和原则的推理中去。实验结果表明，当我们使用这种后向提示的方法时，对各种需要推理的挑战性任务的处理性能有了显著的提高，这也突出了这种方法与 RAG 过程的良好适配性。不仅在生成对后向提示的响应时，我们可以应用这些增强检索的步骤，而且在最终的问答过程中也可以使用。</p>
<p><strong>Sub-Queries</strong> 根据不同的场景，我们可以采用各种不同的搜索策略。比如，我们可以利用 LlamaIndex 等框架提供的搜索引擎，或者利用树形结构进行搜索，还可以通过向量搜索，或者简单地按顺序搜索数据的各个部分。</p>
<p><strong>Hypothetical Document Embeddings</strong> HyDE的工作原理基于一个假设，那就是生成的答案在特征空间中可能比直接查询更接近。HyDE利用 LLM 根据查询生成一个假设的答案（即一份文档），然后将这份文档转化为特征向量，并使用这个特征向量来检索与假设文档相似的真实文档。这种方法并不是直接根据查询来寻找特征向量的相似性，而是关注从一个答案到另一个答案的特征向量的相似性。然而，这种方法可能并不总是能得到理想的结果，特别是当语言模型对某个主题不够熟悉时，可能会导致更多的错误。</p>
<h2 id="Retrieval-3">Retrieval</h2>
<p>在使用 RAG 的过程中，我们必须要能够从数据源中高效地找到相关的文档，这是至关重要的。然而，构建一个高效的检索器却充满了挑战。<br>
在这一部分，我们要探讨三个核心问题：1) 我们如何构建出准确的语义表达？2) 我们应该采用什么方法来确保查询内容和文档内容在语义层面上的一致性？3) 我们如何让检索器的结果更符合大语言模型的处理方式？</p>
<h3 id="Enhancing-Semantic-Representations">Enhancing Semantic Representations</h3>
<p>在 RAG 系统中，语义空间的作用非常重要，因为它负责将查询和文档进行多维度的对应关系映射。在这个语义空间中，检索的精确度会直接影响到 RAG 的最终效果。</p>
<h4 id="Chunk-optimization">Chunk optimization</h4>
<p>在处理外部文档时，首先我们需要将文档分解成更小的片段，以便提取出更细致的特征，然后将这些特征嵌入到一个代表其语义的模型中。但是，如果嵌入的文本片段过大或者过小，都可能导致我们得到的结果并不理想。因此，找出文档在语料库中最适合的片段大小，对于保证我们检索到的结果既准确又相关，是非常关键的。</p>
<p>选择合适的信息分块策略需要我们细心考虑多个关键因素，包括索引内容的特性、嵌入模型及其理想的分块大小、用户查询的可能长度和复杂度，以及特定应用对搜索结果的使用方式。例如，我们应根据内容的长度——长或短——来选择分块模型。另外，不同的嵌入模型在处理不同大小的信息块时，表现会有所不同。比如，sentence-transformer 在处理单句时效果更好，而 text-embedding-ada-002 在处理包含256或512个 tokens 的信息块时表现优异。</p>
<p>另外，用户输入问题的长度和复杂性，以及应用程序的特定需求（如语义搜索或问答），都会影响我们如何选择切分策略（chunking strategy）。这个选择可能直接受到我们选择的 LLMs 的 token 限制，需要我们对切分的块大小（block size）进行调整。实际上，要获取精确的查询结果，我们需要灵活地运用不同的切分策略。并没有一种“一刀切”的最佳策略，只有最适应特定情境的策略。</p>
<p>最新的研究正在探索各种用于提高检索效率和准确性的块优化技术。其中一种方法使用了类似滑动窗口的技术，通过在多个检索过程中融合全局相关信息，实现了分层检索。另一种策略，被称为 “small2big” 方法，它在初始搜索阶段使用小的文本块，然后在后续阶段向语言模型提供更大的相关文本块进行处理。</p>
<p>摘要嵌入技术依据文档的摘要（或概述）对检索结果进行优先级排序，能够帮助我们更全面地理解整个文档的上下文。另一方面，元数据过滤技术则是运用文档的元数据来优化过滤流程。还有一种创新的做法，即图索引技术，它将实体和关系转换为图中的节点和连接，这种转换在处理多跳问题时，能够显著提升结果的相关性。</p>
<p>这些多元化的方法相互结合，已经带来了显著的技术进步，这使得 RAG 在检索结果的质量和整体性能上都有了显著的提升。</p>
<h4 id="Fine-tuning-Embedding-Models">Fine-tuning Embedding Models</h4>
<p>确定了合适的信息块大小后，下一个重要步骤就是利用一种叫做“嵌入模型”的技术，把这些信息块和查询问题转化到一个叫做“语义空间”的地方。这个转化过程的好坏至关重要，因为它直接影响了我们的模型理解和表示所有信息的能力。最近的研究中，出现了一些重要的嵌入模型，比如 ‘AngIE’、Voyage 和 ‘BGE’ 等。这些模型已经在大量的信息中进行了预训练。然而，当它们被应用到专业领域时，可能在准确理解特定领域的信息方面会有所不足。</p>
<p>此外，为了确保模型能够从内容相关性的角度理解用户的查询，对嵌入模型进行针对特定任务的微调是必不可少的。如果一个模型没有经过微调，它可能无法充分满足特定任务的需求。因此，对嵌入模型进行微调对于其后续的实际应用来说至关重要。在微调嵌入模型的方法中，主要有两种不同的思路。</p>
<p><strong>Domain Knowledge Fine-tuning</strong> 要让嵌入式检索模型准确地反映特定领域的信息，使用特定领域的数据集进行微调是非常重要的。这种过程与常规的语言模型微调有所不同，主要体现在所使用的数据集的特性上。通常，嵌入式检索模型微调的数据集主要包括三个元素：查询，语料库，以及相关文档。模型使用这些查询在语料库中找出相关的文档。然后，通过看模型是否能够在回应查询时检索到这些相关文档来评估模型的效果。数据集的构建，模型的微调，以及评估阶段都有各自的挑战。‘LlamaIndex’ 提供了一套核心的类和函数，目的是为了优化嵌入式检索模型微调的工作流程，使这些复杂的过程变得更加简单。通过创建一个充满领域知识的语料库，并利用 ‘LlamaIndex’ 提供的方法，我们可以有效地微调嵌入式检索模型，使其更好地满足目标领域的特定需求。</p>
<p><strong>Fine-tuning for Downstream Tasks</strong> 为了提高模型性能，微调嵌入模型以适应各种下游任务是至关重要的一步。在采用 RAG 处理这些任务的领域，已经出现了一些创新的方法，它们通过充分利用 LLMs 的功能来微调嵌入模型。例如，Promptagator 通过使用 LLM 作为生成少样本查询的工具，创建出针对特定任务的检索器，从而解决了在有监督微调中，特别是在数据稀疏的领域中所面临的挑战。另一种方法 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.07554" title="LLM-Embedder">LLM-Embedder</a> ，则是利用 LLMs 的功能，为多个下游任务的数据生成奖励信号。检索器的微调过程依赖于两种类型的监督信号：一种是数据集的明确标签，另一种是来自 LLMs 的奖励信号。这种双信号方法可以更有效地进行微调，使得嵌入模型能够更好地适应各种不同的下游应用。</p>
<p>虽然一些方法通过融入领域知识和特定任务的微调来改进语义的表达，但是，检索器并不总是能与特定的 LLM 达到最佳的配合效果。为了解决这个问题，一些研究者尝试通过使用来自 LLM 的反馈信息，对微调过程进行直接的指导和监督。这种直接监督的目标就是让检索器更好地与 LLM 对齐，从而在后续任务中提升性能表现。</p>
<h3 id="Aligning-Queries-and-Documents">Aligning Queries and Documents</h3>
<p>在 RAG 的应用场景中，检索器可能使用一个嵌入模型来同时编码查询和文档，或者为每个部分分别使用独立的模型。还有，用户的初始查询可能由于措辞不够准确或者缺乏足够的语义信息而受到影响。因此，使用户查询的语义空间与文档的语义空间对齐就显得至关重要。</p>
<h4 id="Query-Rewriting">Query Rewriting</h4>
<p>查询重写是一种基本的方法，用于使查询与文档的语义保持一致。例如，Query2Doc 和 ITER-RETGEN 等方法利用 LLM ，将原始查询和其他指导信息结合起来，生成一个类似于文档的内容（我们称之为“伪文档”）。HyDE 方法则是通过文本提示来构建查询向量，生成一个“假设文档”，以此来抓住核心的模式。还有一个叫做 RRR 的框架，它改变了传统的检索和阅读顺序，将重心放在了查询重写上。STEP-BACKPROMPTING 则让 LLM 能够进行基于高级概念的抽象推理和检索。此外，多查询检索方法利用 LLM 同时生成并执行多个搜索查询，这对于解决那些包含多个子问题的复杂问题具有明显的优势。</p>
<h4 id="Embedding-Transformation">Embedding Transformation</h4>
<p>除了像查询重构这样的大体策略之外，还有一些更为精细的技巧，这些技巧是专门为了改进嵌入转换而设计的。LlamaIndex 就是一个很好的例子，它引入了一种可以接在查询编码器后的适配器模块。这个适配器模块的作用是帮助进行微调，从而优化查询嵌入的表示方式，使其能更好地映射到与目标任务更为吻合的潜在空间中。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.19912" title="SANTA">SANTA</a>  应对了将查询内容与结构化的外部文档进行匹配的挑战，尤其是在处理结构化和非结构化数据之间的不兼容性时。它通过两种预训练策略，提升了检索器对结构化信息的敏感性：首先，通过利用结构化和非结构化数据之间的内在关联，以此为基础进行对比学习的预训练，使模型更具结构化数据感知能力；其次，通过执行一种名为 “蒙版实体预测” 的策略。这种策略采用了以实体为中心的遮蔽方法，鼓励语言模型预测并填充被遮蔽的实体信息，从而加深对结构化数据的理解。</p>
<p>SANTA 解决了将查询内容与结构化的外部文档进行匹配的问题，尤其是在处理结构化和非结构化数据之间的不同之处时。该方法通过两种预训练策略，提升了检索器对结构化信息的识别能力：首先，通过利用结构化和非结构化数据之间的内在关联，以此为基础进行对比学习的预训练，使模型更具结构化数据感知能力；其次，通过执行一种名为 “蒙版实体预测” 的策略。这种策略采用了以实体为中心的遮蔽方法，鼓励语言模型预测并填充被遮蔽的实体信息，从而加深对结构化数据的理解。</p>
<h3 id="Aligning-Retriever-and-LLM">Aligning Retriever and LLM</h3>
<p>在 RAG 流程中，仅仅通过各种技术提升检索的命中率，并不一定能改善最终的效果，因为检索到的文档可能并不能满足 LLMs 的具体要求。因此，本节将介绍两种方法，目的是使检索的结果更好地适应大型语言模型的偏好。</p>
<h4 id="Fine-tuning-Retrievers">Fine-tuning Retrievers</h4>
<p>有很多研究利用来自大型语言模型（LLM）的反馈信号来改进检索模型。</p>
<p>例如，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.17331" title="AAR">AAR</a> 通过使用编解码器结构，为预训练的检索器引入了监督信号。这是通过识别语言模型偏好的文档，这些文档是通过 FiD 交叉注意力得分来确定的。接着，检索器会通过硬负样本采样和标准交叉熵损失进行微调。最后，改进后的检索器可以直接用于提升未接触过的目标语言模型，从而提高目标任务的性能。此外，有研究建议，大型语言模型可能更倾向于关注可读性强，而非信息丰富的文档。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.12652" title="REPLUG">REPLUG</a> 利用搜索工具和大型语言模型（LLM）来估算搜索到的文档的可能性，并通过计算这些可能性之间的差异（专业术语为 KL 散度）进行训练。这种直接而有效的训练方式，通过使用语言模型（LM）作为指导，提高了搜索模型的性能，同时还避免了需要使用复杂的&quot;交叉关注&quot;技术。</p>
<p>UPRISE 也采用预设的 LLM 来调整搜索提示器的性能。这个大型语言模型和搜索提示器都接收同样的输入——提示和输入的配对信息，然后利用大型语言模型给出的分数来指导搜索提示器的训练，就好像大型语言模型是给数据打标签的人一样。</p>
<p>另外，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2208.03299" title="Atlas">Atlas</a> 提出了四种策略来优化和提升 AI 模型的学习效果：</p>
<ul>
<li>注意力蒸馏（Attention Distillation）。这个方法是通过使用 LLM（大型语言模型）在处理信息时产生的&quot;注意力分数&quot;，来帮助模型提取和学习知识。</li>
<li>EMDR2。这个策略是运用一种叫做&quot;期望最大化&quot;的算法，将搜寻到的信息（文档）作为一种&quot;隐藏的变量&quot;来训练模型。</li>
<li>困惑度蒸馏（Perplexity Distillation）。这个方法直接使用模型在生成信息（token）过程中的&quot;困惑度&quot;（一种衡量模型预测准确性的指标）来进行训练。</li>
<li>LOOP。这个策略提出了一种新的优化函数，它考虑到在删除某些信息（文档）后，对模型预测结果的影响。这种方法提供了一种高效的训练策略，使模型能更好地适应特定的任务。</li>
</ul>
<p>这些策略的目标是提升检索器与 LLM 的协同效应，从而提高检索的效果，并更准确地回答用户的问题。</p>
<h4 id="Adapters">Adapters</h4>
<p>微调模型可能会遇到一些挑战，比如需要通过 API 来实现功能集成，或者需要应对由于本地计算资源有限带来的一些限制。因此，有些方法会选择引入一个外部适配器，以帮助模型的调整和优化。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.18347" title="PRCA">PRCA</a> 通过一个上下文提取阶段和一个受奖励驱动的阶段来训练适配器。然后，它使用一种基于 token 的自回归策略来优化检索器的输出。这种基于 token 的过滤方法利用交叉注意力得分来有效地过滤 token，只选择得分最高的输入 token。RECOMP 则引入了用于生成摘要的提取式和生成式压缩器。这些压缩器要么选择相关的句子，要么合成文档信息，为多文档查询创建定制的摘要。</p>
<p>另外，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.04757" title="PKG">PKG</a> 提出了一种创新的方法，通过指令微调将知识集成到透明的模型中。在这种方法中，检索模块直接被替换，以根据查询生成相关的文档。这种方法有助于解决在微调过程中遇到的困难，并提高模型的性能。</p>
<h2 id="Generation-2">Generation</h2>
<p>RAG 的核心部分是它的生成器，这个生成器的任务是把检索到的信息转换成流畅且连贯的文本。和传统的语言模型不同，RAG 的生成器通过融合检索到的数据，使其在准确性和相关性上有所提升。在 RAG 中，生成器接收的输入不仅包括了常规的上下文信息，还包括通过检索器获取的相关文本段落。这种全面的输入让生成器能更深入地理解问题的上下文，从而产生更丰富、更贴近上下文的回应。</p>
<p>此外，生成器在生成内容时，会受到检索到的文本的指引，以确保生成的内容与检索的信息保持一致。多样化的输入数据使得在生成阶段的工作变得更有针对性，所有的努力都是为了让大型模型更好地适应从查询和文档中获取的输入数据。在接下来的小节中，我们将通过深入探讨后检索处理和微调的各个方面，来详细介绍生成器的功能。</p>
<h3 id="Post-retrieval-with-Frozen-LLM">Post-retrieval with Frozen LLM</h3>
<p>在 LLM 的领域，许多研究都选择使用像 GPT-4 这样的成熟模型，它们可以利用自身丰富的内部知识，系统地整合各种文档中检索到的信息。</p>
<p>然而，这些大型模型也面临着一些挑战，例如上下文长度的限制，以及对冗余信息的过度敏感。为了应对这些问题，一些研究开始将注意力转向了如何在信息检索后进行更好的处理。</p>
<p>后检索处理主要涉及对检索器从大型文档数据库中获取的相关信息进行处理、过滤或优化。其主要目标是提升检索结果的质量，使其更符合用户的需求或后续的任务。我们可以将其视为对在检索阶段获取的文档进行的再次处理。在后检索处理中，常见的操作包括信息压缩和结果的重新排序。</p>
<h4 id="Information-Compression">Information Compression</h4>
<p>检索器擅长从海量的知识库中寻找相关信息，但管理检索文档中众多的信息则是一项挑战。当前的研究正在试图通过扩大 LLM 的上下文长度来应对这个问题。然而，现有的大型模型在处理上下文限制方面仍存在困难。因此，在某些情况下，我们需要对信息进行压缩。信息压缩在降低噪声，解决上下文长度限制，以及提升生成效果方面具有重要的作用。</p>
<p>PRCA 通过训练一个能够提取关键信息的模块来应对这个问题。在处理上下文信息的阶段，只要给这个模块提供一个输入文本 $S_{input}$ ，它就能生成一个输出序列 $C_{extracted}$。这个输出序列就是对输入文档中的上下文信息的精炼和提炼。整个训练过程的目标就是让 $C_{extracted}$ 尽可能接近实际的上下文信息 $C_{truth}$。</p>
<p>类似地，RECOMP 也采取了一个相似的策略，它通过对比学习（Contrastive Learning）训练了一个能够压缩信息的模块。在这个训练过程中，每个训练数据点都包含一个正样本和五个负样本。在整个训练过程中，这个编码模块会通过最小化对比损失（Contrastive Loss）来进行学习和优化。</p>
<p>另一项研究选择了一种不同的策略，旨在通过减少文档数量来提高模型的答案准确性。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.08559" title="filter-then-rerank">filter-then-rerank paradigm</a> 巧妙地结合了大型语言模型（LLMs）和小型语言模型（SLMs）的优势。在这个模型中，SLMs 起到过滤的作用，而 LLMs 则负责重新排序。研究发现，让 LLMs 对 SLMs 所识别出的复杂样本进行重新排序，可以在各种需要从大量信息中提取关键内容的任务（IE, Information Extraction）中获得显著的改进。</p>
<h4 id="Reranking">Reranking</h4>
<p>重排模型对于优化检索器获取的文档集的质量起着至关重要的作用。当语言模型需要处理更多的上下文信息时，其性能通常会有所下降，而重排模型能有效地解决这个问题。其核心思想是重新整理文档的顺序，将最相关的内容优先放在前面，从而控制了文档的总数量。这种方法不仅解决了在检索过程中需要处理更多信息（“上下文窗口扩展”）的挑战，而且还提高了检索的效率和反应速度。</p>
<p>重排模型在信息检索过程中起到了关键的双重作用，既像一个调整者，优化处理过程，又像一个提炼者，精炼信息的质量。这样，它就能为接下来的语言模型处理提供更有效、更精确的输入信息。</p>
<p>一种叫做上下文压缩的技术被引入重排序过程中，以提供更精确的检索信息。这种方法就像用一个筛子，把单个文档的冗余内容筛掉，只保留最重要的信息。最终的目标是在搜索结果中展示最相关的信息，让读者能看到更精确、更有针对性的内容。</p>
<h3 id="Fine-tuning-LLM-for-RAG">Fine-tuning LLM for RAG</h3>
<p>在 RAG 模型中，优化生成器是极其重要的一环。生成器的职责是将检索得到的信息转化为相关的文本，这些文本就构成了模型的最终输出。通过优化生成器，我们希望生成的文本不仅流畅自然，而且能够有效地利用检索到的文档，以更好地满足用户的查询需求。</p>
<p>在传统的 LLM 生成任务中，我们通常只需输入一个查询。然而，RAG 的独特之处在于，它不仅接受查询作为输入，还将检索器找到的各种文档（无论是结构化的还是非结构化的）纳入输入。这些额外的信息对模型的理解有着重大影响，特别是对于规模较小的模型。因此，我们需要对模型进行微调，使其能够适应同时包含查询和检索文档的输入。在将输入提供给经过微调的模型之前，我们通常需要对检索器找到的文档进行后检索处理。值得注意的是，对 RAG 中生成器的微调方法与对 LLM 的一般微调方法是一致的。</p>
<p>接下来，我们将简要介绍一些涉及数据（无论是格式化的还是非格式化的）和优化函数的典型研究。</p>
<h4 id="General-Optimization-Process">General Optimization Process</h4>
<p>在通用的优化过程中，训练数据通常由输入和输出的配对构成，目标是训练模型在接收到输入 $x$ 时能够产生对应的输出 $y$。在 Self-Memory 的研究中，我们采用了传统的训练流程，也就是在给定输入 $x$ 的情况下，检索出相关的文档 $z$，然后在将输入 $x$ 和文档 $z$ 组合后，模型就能够生成输出 $y$。Self-Memory 使用了两种常见的微调模式，分别被称为联合编码器（Joint-Encoder）和双重编码器（Dual-Encoder）。</p>
<p>在联合编码器（Joint-Encoder）范式中，我们使用一个基于编码器-解码器的标准模型。在这里，编码器首先将输入进行编码，然后解码器通过注意力机制，将编码的结果结合起来，以自回归的方式生成 tokens。另一方面，在双重编码器（Dual-Encoder）范式中，系统设置了两个独立的编码器，每个编码器分别对输入（查询，上下文）和文档进行编码。然后，解码器会按顺序对生成的输出进行双向交叉注意力处理。这两种架构都以 Transformer 作为基础构件，并通过优化负对数似然损失来进行训练。</p>
<h4 id="Utilizing-Contrastive-Learning">Utilizing Contrastive Learning</h4>
<p>在为语言模型准备训练数据的过程中，我们通常会生成输入和输出的配对数据。这种传统方式可能导致一种称为&quot;暴露偏差&quot;的问题，也就是说，模型只在单一的正确输出样本上进行训练，这限制了模型接触和学习各种可能输出引用的机会 。这个限制可能会影响模型在实际应用中的性能，因为模型可能会过度拟合训练集中的特定样本，从而降低其在不同上下文中的泛化能力。</p>
<p>为了缓解&quot;暴露偏差&quot;，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.18846" title="SURGE">SURGE (SUbgraph Retrieval-augmented GEneration)</a> 提出了采用图文对比学习的方法。这种方法包含一个对比学习目标，鼓励模型生成一系列可能且连贯的回应，这些回应超越了训练数据中的样本。这种策略对于减少过拟合和提升模型的泛化能力至关重要。</p>
<p>在处理涉及结构化数据的检索任务时，SANTA 框架采用了三阶段的训练方法，旨在有效地捕捉数据的结构特性和语义差异。在训练的初始阶段，主要关注的是检索器，通过使用&quot;对比学习&quot;（一种通过比较不同样本来提升模型学习效果的方法）来优化查询和文档的嵌入表示。</p>
<p>接下来，在生成器的初级训练阶段，我们采用一种名为“对比学习”（Contrastive Learning）的方法，它的目标是将结构化的数据与其对应的非结构化文档描述相匹配。在生成器训练的后期阶段，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.07129">模型开始重视实体的语义（entity semantics）在理解文本数据中的重要性</a>。这个过程首先从结构化数据中找出实体，然后在输入给生成器的数据中，将这些实体进行隐藏或者说“遮罩”（masking），这样模型就可以试图预测这些被隐藏的部分，从而更好地理解和学习数据的内在结构。</p>
<p>训练的过程中，模型会学习如何利用上下文信息来重新构建被隐藏的实体。这一步骤有助于模型理解文本数据的内在结构和含义，也帮助模型更好地匹配结构化数据中的相关实体。最终的优化目标是让语言模型学会准确地恢复被隐藏的部分，这样可以进一步提升模型对实体语义的理解。</p>
<h2 id="Augmentation-in-RAG">Augmentation in RAG</h2>
<p>这部分内容主要围绕三个关键方面展开：增强阶段，增强数据的来源以及增强过程。这些方面揭示了对 RAG 发展至关重要的关键技术。RAG 的核心组件的分类在 图4 中进行了展示。</p>
<p><img src="//s3.mindex.xyz/blog/Courses/ec7ce0716d2010d7ddd953c4d967eaaf.png" alt="图4：RAG 的核心组件分类"></p>
<h3 id="RAG-in-Augmentation-Stages">RAG in Augmentation Stages</h3>
<p>RAG 是一项知识密集的任务，它在语言模型训练的预训练、微调和推理阶段，融合了多种技术方法。</p>
<h4 id="Pre-training-Stage">Pre-training Stage</h4>
<p>在预训练阶段，研究人员已经研究了如何通过基于检索的策略来强化开放领域问答（Question Answering）的预训练模型（PTMs）。REALM 模型采用了一种结构化、可解释的知识嵌入方法，在遮蔽语言模型（MLM）框架内，将预训练和微调构建为“先检索再预测”的工作流程。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.04426" title="RETRO">RETRO</a> 利用检索增强技术进行大规模的从零开始的预训练，成功地减少了模型的参数数量，同时在模型的困惑度上超越了标准的 GPT 模型。RETRO 的独特之处在于，它增加了一个额外的编码器，专门处理从外部知识库中检索得到的实体的特征，这是在 GPT 模型的基础架构上进行的创新。</p>
<p>Atlas 在预训练和微调阶段都将检索机制整合进了 T5 的框架中。它使用预训练好的 T5 来初始化编码器-解码器语言模型，同时使用预训练好的 Contriever 作为密集型检索器，从而提升了处理复杂语言建模任务的效率。</p>
<p>此外，COG 推出了一种创新的文本生成方式，仿佛是从已有的文本集合中拷贝文字片段。通过高效的向量搜索工具，COG 能够计算并索引出具有上下文含义的文本片段表示，这种方法在诸如问答系统和领域适应性等应用中，相比 RETRO 显示出更强的性能。</p>
<p>缩放定律（scaling laws）的出现，为模型参数的大规模增长注入了动力，使得自回归模型成为了主流。研究人员正在将 RAG 方法应用到更大的预训练模型中，RETRO++ 就是这种趋势的一个例证，它在扩大模型参数的同时，保证甚至提升了性能。</p>
<p>实际的研究结果强烈表明，无论是在文本生成的质量、事实的准确性，还是在减少生成文本的有害内容以及提高任务处理能力等方面，都有了明显的提升，尤其在需要大量知识支持的应用中，如开放领域的问答系统。这些成果表明，将检索机制融入到自回归语言模型的预训练过程中，是一个非常有潜力的研究方向，这种方法将复杂的检索技术与大规模语言模型相结合，可以实现更精确、更高效的语言生成。</p>
<p>增强预训练带来了许多优势，例如，它可以创建出一种强大的基础模型，该模型在困惑度、文本生成质量和特定任务的性能方面，都超过了标准的 GPT 模型，而且使用的参数更少。这种方法在处理知识密集型任务上表现出色，并且可以通过在专门的语料库上进行训练，来方便地开发出特定领域的模型。</p>
<p>尽管如此，这种方法也面临着一些挑战，比如需要大量的预训练数据集和资源，而且随着模型大小的增加，更新频率也会降低。但即便存在这些难题，这种方法在增强模型的韧性方面却展现出了显著的优势。一旦完成训练，增强检索的模型就可以摆脱对外部库的依赖，从而提升生成速度和运行效率。这种方法所揭示出的潜在优势，使其成为人工智能和机器学习领域中持续研究和创新的热门话题。</p>
<h4 id="Fine-tuning-Stage">Fine-tuning Stage</h4>
<p>RAG 和 Fine-tuning 是增强 LLMs 的强大工具，它们的结合可以满足更多特定场景的需求。一方面，Fine-tuning 可以帮助我们检索出具有独特风格的文档，从而实现更好的语义表达，并缩小查询和文档之间的差距，这样就能确保检索器的输出更适应当前的场景。另一方面，Fine-tuning 可以满足我们进行风格化和有针对性的调整的生成需求。而且，Fine-tuning 还可以用来调整检索器和生成器，以提高模型的协同效果。</p>
<p>微调检索器的主要目标是利用语料库直接优化嵌入模型，从而提升语义表示的质量。通过反馈信号，我们可以更好地将检索器的能力与 LLM 的偏好相对应，实现二者的协同效果。针对特定的下游任务对检索器进行微调，可以提高其适应性 。同时，引入任务无关的微调策略，旨在提升检索器在多任务场景中的通用性。</p>
<p>微调生成器可以让输出结果更加具有风格和个性化。一方面，这种方法可以让我们更好地适应不同的输入数据格式。例如，我们可以微调 LLM 以适应知识图谱的结构，文本对的结构，以及其他特定的结构。另一方面，通过建立指令性的数据集，我们可以让 LLM 生成特定格式的内容。比如，在适应性或迭代式的检索场景中，我们可以微调 LLM 以生成能够帮助我们确定下一步行动时机的内容。</p>
<p>通过同时调整 AI 智能体的&quot;检索器&quot;（用于从大量信息中找到相关的部分）和&quot;生成器&quot;（用于根据检索到的信息产生回答），我们可以提高 AI 的适应性，并避免&quot;过拟合&quot;（即 AI 过度适应训练数据，而在面对新的数据时表现不佳）。但是，这种同时调整的方法也会增加计算资源的消耗。RA-DIT 提供了一种轻量级的解决方案，它是一个双向调整框架，可以有效地为任何 LLMs 添加检索功能。这种&quot;增强检索的调整方法&quot;可以更新 LLM，使其更有效地利用检索到的信息，并忽视那些可能干扰其判断的无关内容。</p>
<p>尽管微调具有不少优点，但它也有其局限性，比如需要专为 RAG 微调设计的数据集，以及对大量计算资源的需求。然而，在这个阶段，我们可以根据特定的需求和数据格式来定制模型，这可能相比预训练阶段能更有效地利用资源，同时还能微调模型的输出风格。</p>
<p>总的来说，微调阶段对于让 RAG 模型更好地适应特定任务是至关重要的，它能够优化检索器和生成器的性能。尽管存在资源和数据集需求的挑战，但这个阶段仍能提高模型对各种任务的灵活性和适应性。因此，策略性地微调 RAG 模型是开发高效且有效的检索增强系统的一个关键环节。</p>
<h4 id="Inference-Stage">Inference Stage</h4>
<p>在 RAG 模型中，推理阶段极为关键，它需要大量地与 LLMs 进行交互。传统的 RAG 方法，也就是我们所说的 Naive RAG，这一阶段会利用检索到的内容来引导生成过程。</p>
<p>为了克服 Naive RAG 的局限性，一些先进的技术在推理阶段引入了更富有上下文的信息。DSP 框架采用了一种复杂的交换机制，可以在 Frozen LLMs 和检索模型（RMs）之间交换自然语言文本，这样就可以丰富上下文，从而改善生成的结果。PKG 方法则为大型语言模型配备了一个知识引导模块，这个模块可以在不改变模型参数的情况下检索相关的信息，使得模型能够执行更复杂的任务。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.06595" title="CREA-ICL">CREA-ICL</a> 通过同步检索跨语言知识来增强上下文，而 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.01296" title="RECITE">RECITE</a> 则通过直接从大型语言模型中抽取段落来生成上下文。</p>
<p>在推理阶段，我们可以看到一些方法进一步优化了 RAG 过程，这些方法主要应用于需要多步推理的任务。例如，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.05149" title="ITRG">ITRG</a> 采用迭代式的检索信息方法，以便找出正确的推理路径，从而更好地适应不同的任务。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.15294" title="ITER-RETGEN">ITER-RETGEN</a> 则采用了一种循环策略，将信息检索和信息生成步骤交替进行，一会儿利用检索的信息来增强生成，一会儿又利用生成的信息来增强检索。对于那些不需要大量知识的任务，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.17653" title="PGRA">PGRA</a> 提出了一个两阶段的框架，首先使用一个通用的信息检索器，然后使用一个由提示引导的重新排序器来选择和优先考虑证据。相比之下，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.10509" title="IRCoT">IRCoT</a> 则将 RAG 与思维链 (CoT) 的方法结合，交替进行由 CoT 引导的信息检索与由检索信息引导的 CoT 过程，这显著提升了 GPT-3 在各种问答任务中的表现。</p>
<p>总的来说，这些在推理阶段的增强手段提供了一种轻量级且经济高效的选择。它们能够借助预训练模型的能力，而无需再进行额外的训练。其主要优势在于，我们可以在保持 LLM 参数不变的同时，提供与特定任务需求紧密相关的信息。然而，这种方法也有其局限性，它需要精细的数据处理和优化，并受到基础模型固有能力的制约。为了更有效地应对各种任务需求，人们通常会将这种方法与一些步骤优化策略结合起来，如分步推理（step-wise reasoning）、迭代式检索（iterative retrieval）以及自适应检索策略（adaptive retrieval strategies）。</p>
<h3 id="Augmentation-Source">Augmentation Source</h3>
<p>RAG 模型的效能受到增强数据源选择的深度影响。不同类型的知识和数据需要应用不同的处理方法。这些数据源通常被划分为非结构化数据、结构化数据，以及 LLM 生成的内容。图5 展示了一颗技术树，描绘了各种代表性的 RAG 研究及其不同的数据增强方向。技术树的叶节点以三种不同的颜色标识，分别代表利用非结构化数据、结构化数据，以及由 LLM 生成的内容进行数据增强。从图中可以清楚地看出，最初的数据增强主要依赖非结构化数据，如纯文本。后来，这种方法逐渐扩展，开始利用结构化数据（如知识图谱）进行增强以进一步提高效能。最近，越来越多的研究开始利用由 LLM 自身生成的内容进行检索和数据增强，这是一个明显的研究趋势。</p>
<p><img src="//s3.mindex.xyz/blog/Courses/31d82ae1f4b367a4e0ec4257d8309d2b.png" alt="图5"></p>
<h4 id="Augmented-with-Unstructured-Data">Augmented with Unstructured Data</h4>
<p>非结构化文本通常从各种语料库中收集，如用于微调大型模型的提示数据，以及跨语言数据。检索的单位可以多种多样，包括 Token（如 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.00172" title="kNN-LMs">kNN-LMs</a>），短语（如 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.06962" title="CoG">CoG</a>）以及文档的段落。需要注意的是，单位越细，虽然可以提高检索的精度，但同时也会增加检索的复杂性。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.06983" title="FLARE">FLARE, Forward-Looking Active REtrieval augmented generation</a> 提出了一种主动检索方法，该方法在大型语言模型生成低概率词汇时触发。它创建一个临时的句子来进行文档检索，然后利用检索到的上下文信息重新生成句子，以预测接下来的句子。而 RETRO 则是利用前一段内容（chunk）来在同一级别中检索最近的相邻内容，结合前一段内容的上下文，以此来引导下一段内容的生成。为了保持因果性，生成下一个内容块 $C_i$ 时，只会利用前一个内容块 $N(C_{i−1})$ 的最近邻居，而不是 $N(C_i)$。</p>
<h4 id="Augmented-with-Structured-Data">Augmented with Structured Data</h4>
<p>结构化的数据，例如知识图谱（KGs），能够提供高质量的上下文信息，减少模型产生的错误预测。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14322" title="RET-LLMs">RET-LLMs</a> 会从过去的对话中提取信息，构建出一个知识图谱，这个图谱可以作为一个内存库在未来被模型引用。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.18846" title="SUGRE">SUGRE</a> 则是利用了图神经网络（GNNs）技术，通过编码知识图谱的相关部分，确保模型生成的文本与从知识图谱中检索到的信息一致，这个过程是通过多模态对比学习实现的。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.11761" title="KnowledGPT">KnowledGPT</a> 则是通过生成针对知识库（KB）的搜索查询，并将查询结果存储起来，以此来增强 RAG 模型的知识丰富度和上下文相关性。</p>
<h4 id="LLMs-Generated-Content-in-RAG">LLMs-Generated Content in RAG</h4>
<p>为了解决 RAG 中外部辅助信息的限制问题，一些研究开始尝试挖掘 LLMs 内部的知识。例如，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.05002" title="SKR">SKR</a> 这个方法，它可以将问题分为“已知”和“未知”两类，并根据分类结果有选择性地增强信息检索的效果。另一种方法叫做 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2209.10063" title="GenRead">GenRead</a>，它用 LLM 生成器取代了传统的信息检索器，结果发现，由 LLM 生成的上下文信息往往能提供更精确的答案，这是因为这些上下文信息更符合因果语言建模预训练目标的要求。还有一种叫做 Self-Memory 的方法，它能够通过一个增强了检索功能的生成器，迭代地创建一个无界的记忆池，并用一个记忆选择器选择出能对原始问题提供双重问题视角的输出，以此来自我增强生成模型的能力。<br>
这些方法都突显了在 RAG 中，如何创新性地利用各种数据源，以提高模型的性能和完成任务的效果。</p>
<h3 id="Augmentation-Process">Augmentation Process</h3>
<p>在 RAG 领域，常规的做法往往是通过一次检索后进行生成，但这样可能导致效率不高。一个被称为 “lost in the middle”（中途迷失）的问题就是这样产生的，当一次检索得到的内容过于冗余，可能会淡化或与重要信息产生冲突，从而降低生成的质量。更进一步，对于那些需要多步推理的复杂问题来说，这种一次性的检索往往无法提供足够广泛的信息，因此显得力不从心。</p>
<p>如 图5  所示，为了应对这些挑战，现代研究提出了几种改进检索过程的方法，包括迭代检索、递归检索和自适应检索。迭代检索让模型可以进行多轮检索，从而提升获取信息的深度和相关性。递归检索则是将一次检索的结果作为下一次检索的输入，这有助于更深入地探索相关信息，尤其是在处理复杂或多步骤的查询时。递归检索常被应用于需要逐步逼近最终答案的场景，比如学术研究、法律案例分析或者某些类型的数据挖掘任务中。而自适应检索则提供了一种动态的调节机制，可以根据各种任务和上下文的具体需求来调整检索过程。</p>
<h4 id="Iterative-Retrieval">Iterative Retrieval</h4>
<p>RAG 模型中，我们采用了一种叫做“迭代检索”的技术。这项技术的工作方式是，根据最初的查询以及已经生成的文本，反复地收集相关的文档，以此为大型语言模型（LLM）构建一个更加丰富的知识库。实践证明，这种技术可以通过在多次检索中引入更多的上下文参考信息，有效提高模型生成答案的稳定性。但是，这种方法也有其局限性，比如可能会出现语义的断裂，或者积累了太多的无关信息。这是因为我们通常会依赖于一串 n 个 tokens 来划分生成的文本和检索到的文档之间的界限。</p>
<p>为了应对特定的数据场景，我们采用了递归检索和多跳检索这两种技术。递归检索使用一种结构化的索引，按照层次结构处理和检索数据。这可能包括在基于摘要进行检索之前，先对文档或长篇PDF的部分内容进行概括。接着，在文档内部进行第二次检索，进一步优化搜索结果，这就体现了这个过程的递归特性。与此不同，多跳检索的目标是深入挖掘图结构的数据源，提取出相互关联的信息。</p>
<p>另外，有些方法把检索和生成的步骤融合在一起。ITER-RETGEN 采用了一种协同策略，对于需要复制特定信息的任务，它并行地运用了“检索增强生成”和“生成增强检索”。这个模型将解决输入任务所需的内容作为检索相关知识的上下文基础，这反过来又有助于在接下来的迭代中生成更优质的回应。</p>
<h4 id="Recursive-Retrieval">Recursive Retrieval</h4>
<p>递归检索是一种在信息检索和自然语言处理中常用的手段，它能有效提升搜索结果的深度和相关度。这种方法的核心在于，我们会根据前一次搜索得到的结果，反复优化我们的搜索问题。递归检索的目标，就是通过这样的反馈循环，逐步找到最相关的信息，从而提升我们的搜索体验。IRCoT 采用了一种名为 “思维链条” 的策略来引导搜索过程，并会根据搜索结果对这个 “思维链条” 进行优化。而 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.14696" title="Tree of Clarifications">Tree of Clarifications</a> 则会创建一个 “澄清树”，这是一种系统性的方法，用于优化查询中不明确的部分。在一些复杂的搜索场景中，比如用户一开始并不完全清楚自己需要什么，或者他们正在寻找的信息非常专业或微妙，这种方法就会显得特别有用。递归检索的特性允许它能够不断学习和适应用户的需求，往往能提高用户对搜索结果的满意度。</p>
<h4 id="Adaptive-Retrieval">Adaptive Retrieval</h4>
<p>FLARE 和 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.11511" title="Self-RAG">Self-RAG</a> 等自适应检索方法优化了 RAG 框架，使大型语言模型（LLMs）能主动决定何时以及获取何种内容进行检索，从而提高了信息获取的效率和相关性。这些方法是更大趋势的一部分，其中 LLMs 在其操作中都表现出主动判断的能力，这在 AutoGPT、Toolformer 和 Graph-Toolformer 等模型智能体中有所体现。以 Graph-Toolformer 为例，它将检索过程分解为若干个步骤，LLMs 在其中主动使用检索器，运用 Self-Ask 技术，并通过少样本提示发起搜索查询。这种主动态度使得 LLMs 能在需要的时候进行搜索，就像智能体使用工具一样。</p>
<p>WebGPT 通过融入强化学习框架，使得 GPT-3 模型能够在生成文本时自我运用搜索引擎。它使用一些特别设定的 Token （代表特定操作的标记）来完成这一流程，这些 Token 使得模型可以进行搜索引擎查询、结果浏览和参考文献引用等动作，从而借助外部搜索引擎进一步增强了 GPT-3 的功能。</p>
<p>Flare 通过监测生成过程中的置信度来自动化调整检索的时机，这个置信度是通过生成内容的概率来反映的。当这个概率低于某个设定的阈值时，Flare 会启动检索系统，收集相关的信息，从而优化检索的流程。</p>
<p>Self-RAG 引入了一种特别的 Token，我们叫它“反射 Token”，这让模型具有了自我审视其输出的能力。这些 Token 分为两类：“检索”和“评价”。模型可以自行决定何时启动检索，或者，我们也可以设定一个预设阈值来触发检索过程。在检索阶段，生成器会在多段文本中进行“片段级别”的 beam 搜索，目的是找出最连贯的文本序列。而“评价”分数则用来更新各个子区的分数，我们可以在推理过程中灵活地调整这些权重，以便根据需要调整模型的行为。Self-RAG 的设计避免了需要额外的分类器或依赖于自然语言推理（NLI）模型，这使得决定何时启动检索机制的过程更加简洁，并提升了模型在生成准确回应时的自主判断能力。</p>
<p><img src="//s3.mindex.xyz/blog/Courses/ed615ec174d8671515a77b90ebe27e5f.png" alt="图6"></p>
<p>由于使用越来越广泛，LLM（大型语言模型）的优化已经受到了大量的关注。诸如提示设计（prompt engineering）、微调（Fine-Tuning，FT）和 RAG 等技术各有其独特的特性，这些特性在 图6 中进行了直观的展示。尽管提示设计方法是利用模型的内在能力，但在优化 LLMs 的过程中，我们通常需要同时采用 RAG 和 FT 方法。在选择 RAG 和 FT 之间，应根据具体场景的需求以及每种方法本身的特性进行决定。下表 中提供了 RAG 和 FT 的详细对比。</p>
<table>
<thead>
<tr>
<th style="text-align:center">特征</th>
<th style="text-align:center">RAG</th>
<th style="text-align:center">FT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">知识更新</td>
<td style="text-align:center">直接更新检索的知识库，可以使得信息始终保持最新状态，从而避免频繁的重新训练，这使得它非常适用于动态的数据环境中。</td>
<td style="text-align:center">存储静态数据，需要对知识和数据更新进行再训练。</td>
</tr>
<tr>
<td style="text-align:center">外部知识</td>
<td style="text-align:center">对于利用外部资源方面表现出色，特别是在访问文档或其他结构化/非结构化数据库的应用上更是得心应手。</td>
<td style="text-align:center">可以用于将预训练得到的外部知识与LLM 进行整合，但对于频繁变化的数据源可能并不实用。</td>
</tr>
<tr>
<td style="text-align:center">数据处理</td>
<td style="text-align:center">只需要进行极少量的数据处理操作。</td>
<td style="text-align:center">其效果高度依赖于高质量数据集，如果数据集有限，可能并不会带来显著的性能提升。</td>
</tr>
<tr>
<td style="text-align:center">模型定制</td>
<td style="text-align:center">虽然主要关注信息检索和整合外部知识，但可能无法充分定制模型的行为或写作风格。</td>
<td style="text-align:center">可以根据特定的语调或术语，调整LLM的行为、写作风格或者特定领域的知识。</td>
</tr>
<tr>
<td style="text-align:center">可解释性</td>
<td style="text-align:center">可以将响应追溯到特定的数据源，从而提高其可解释性和可追溯性。</td>
<td style="text-align:center">就像黑盒子一样，并不总能明白模型为何会以某种特定的方式作出反应，这导致了其可解释性相对较低。</td>
</tr>
<tr>
<td style="text-align:center">计算资源</td>
<td style="text-align:center">它依赖于计算资源来支持相关的检索策略和数据库技术。另外，还需要维护和更新外部数据源的集成。</td>
<td style="text-align:center">我们需要准备和整理高质量的训练数据集，明确微调的目标，并提供相应的计算资源。</td>
</tr>
<tr>
<td style="text-align:center">延迟要求</td>
<td style="text-align:center">这个过程会涉及到数据检索，可能会导致处理延迟增加。</td>
<td style="text-align:center">微调过后的语言模型可以直接做出响应，无需进行数据检索，这样可以降低延迟。</td>
</tr>
<tr>
<td style="text-align:center">减少幻觉</td>
<td style="text-align:center">由于每个答案都是基于检索到的证据，因此在本质上，这种方法不容易出现无据可依的推测。</td>
<td style="text-align:center">通过针对特定领域的数据进行训练，可以帮助减少模型的误判，但是当面对不熟悉的输入时，模型可能仍会产生误判。</td>
</tr>
<tr>
<td style="text-align:center">道德和隐私问题</td>
<td style="text-align:center">从外部数据库存储和检索文本的过程中，会引发道德和隐私方面的问题。</td>
<td style="text-align:center">如果训练数据中含有敏感内容，可能会触发一些关于道德和隐私的担忧。</td>
</tr>
</tbody>
</table>
<h3 id="RAG-vs-Fine-Tuning">RAG vs Fine-Tuning</h3>
<p>RAG 就好比给模型提供了一本可以精准检索信息的教科书，对于特定的查询来说非常完美。另一方面，FT 就好比一个学生随着时间的积累逐渐吸收和掌握知识，更擅长于复制特定的结构、样式或格式。FT 可以通过强化模型的基础知识、调整输出和教授复杂的指令，提升模型的性能和效率。然而，对于引入新知识或迅速应对新的使用场景，FT 的表现并不理想。</p>
<p>RAG 和 FT 这两种方法并不是互斥的，实际上它们可以相互补充，从不同的角度增强模型的能力。在某些情况下，同时应用这两种方法可能会带来最优的性能。涉及 RAG 和 FT 的优化过程可能需要经过多次迭代才能取得满意的结果。</p>
<h2 id="RAG-Evaluation">RAG Evaluation</h2>
<p>在自然语言处理（NLP）领域，由于 RAG 的快速进步和广泛使用，对 RAG 模型的评估已经成为 LLM 社区研究的重点。这种评估的核心目标是深入理解 RAG 模型，并优化其在各种应用场景中的表现。</p>
<p>历史上，对 RAG 模型的评估主要关注的是它们在特定的下游任务中的表现。这些评估使用的是适应当前任务的既定度量标准。例如，对问题回答的评估可能会依赖 EM 和 F1 分数，而对事实核查任务的评估通常会以准确性作为主要的度量标准。像 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.10633" title="RALLE">RALLE</a> 这样的工具，也是基于这些特定任务的度量标准来设计的，以便自动评估 RAG 的应用。然而，针对 RAG 模型特性的独立评估研究却显得匮乏，只有少数相关的研究。</p>
<p>接下来的部分，我们将从特定任务的评估方法和指标转向，去综合分析各类研究文献中的独特观点。这个探索过程包括 RAG 模型评估的目标，评估模型的各个要素，以及用于评估的基准和工具。我们的目标是提供一个全面的 RAG 模型评估概述，明确指出那些专门针对这些先进的生成系统独特特性的评估方法</p>
<h3 id="Evaluation-Targets">Evaluation Targets</h3>
<p>RAG 模型的评估主要关注两个关键部分：检索模块和生成模块。这样的划分方式确保了我们能全面评估提供的上下文的质量和生成的内容的质量。</p>
<h4 id="Retrieval-Quality">Retrieval Quality</h4>
<p>对检索质量的评估是决定 RAG 检索模块所获取的上下文信息有效性的关键。我们通常会借用搜索引擎、推荐系统以及信息检索系统中的标准度量方法来衡量 RAG 检索模块的表现。例如，<a target="_blank" rel="noopener" href="https://docs.cloud.deepset.ai/docs/experiments-and-metrics">我们常用的度量标准有命中率（Hit Rate）、平均倒数排名（MRR）和归一化折损累积增益（NDCG）</a>。</p>
<h4 id="Generation-Quality">Generation Quality</h4>
<p>生成质量的评估主要关注生成器是否能从检索到的上下文中生成连贯且相关的答案。这种评估可以根据内容的性质进行分类：无标签内容和有标签内容。对于无标签内容，评估的重点是生成的答案是否忠实于原文、是否相关且无误导性。相反，对于有标签内容，我们主要关注模型生成的信息的准确性。另外，无论是对检索质量还是生成质量的评估，都可以通过手动或<a target="_blank" rel="noopener" href="https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG" title="auto-eval">自动</a>的方式进行。</p>
<h3 id="Evaluation-Aspects">Evaluation Aspects</h3>
<p>当前对 RAG 模型的评估方法强调三个主要的质量指标和四项关键技能，这些因素共同影响着 RAG 模型两个主要目标的评估：检索和生成。</p>
<h4 id="Quality-Scores">Quality Scores</h4>
<p>质量评分包括上下文的相关性、答案的忠实度以及答案的适应性。这些评分指标从不同角度对 RAG 模型在信息检索和生成过程中的效率进行了评估。</p>
<p>上下文相关性是评估检索到的上下文是否精准、具体，它确保了内容的相关性，并降低了处理无关信息所产生的额外成本。</p>
<p>答案的忠实度保证了生成的答案能够真实反映检索到的上下文，保持信息的连贯性，避免产生自相矛盾的情况。</p>
<p>答案的相关性要求生成的答案直接针对提出的问题，有效地解决了核心询问。</p>
<h4 id="Required-Abilities">Required Abilities</h4>
<p>RAG 的评估也涵盖了四种显示其适应性和效率的能力：抗噪声能力、负面拒绝、信息整合能力和反事实鲁棒性。这些能力对于模型在面对各种挑战和复杂情境下的性能表现至关重要，它们将直接影响质量评分。</p>
<p>&quot;抗噪声能力&quot;用以评估模型处理那些与问题相关但又缺乏实质性信息的噪声文档的能力。</p>
<p>&quot;负面拒绝&quot;评估了模型在面对检索不到解答问题所需知识的文档时，选择不做回应的判断力。</p>
<p>&quot;信息整合&quot;则衡量了模型在从众多文档中提炼并融合信息，以解答复杂问题的能力。</p>
<p>至于&quot;反事实鲁棒性&quot;，则测试了模型在面对文档中的已知错误信息时，即便被告知可能存在误导，也能够识别并忽视这些不准确信息的能力。</p>
<p>上下文的相关性和对噪声的鲁棒性对于评估检索质量至关重要，而回答的准确性、回答的相关性、在无法回答时选择不回答的判断力、信息的整合能力，以及对于文档中已知错误信息的识别和忽视能力，都对评估生成质量起着重要的作用。</p>
<p>下表 总结了每个评估方面的具体指标。需要明白的是，这些从相关工作中得出的指标，尽管是传统的测量方法，但并不意味着它们已经形成了一种成熟或标准化的方式来量化 RAG 的评估方面。此外，尽管本文未包含，但在一些评估研究中，也开发出了专门针对 RAG 模型的特性的定制指标。</p>
<p><img src="//s3.mindex.xyz/blog/Courses/33aac527b2c8cac60cca8325d7b23a54.png" alt=""></p>
<h3 id="Evaluation-Benchmarks-and-Tools">Evaluation Benchmarks and Tools</h3>
<p>这一部分详述了 RAG 模型的评估框架，包括基准测试和自动评估工具。这些工具提供定量指标，不仅可以衡量 RAG 模型的性能，更能帮助我们更好地理解模型在各个评估方面的能力。一些知名的基准，如 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.01431" title="RGB">RGB</a> 和 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.08147" title="RECALL">RECALL</a>，主要是用来评估 RAG 模型的基本能力。与此同时，一些最新的自动工具，如 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.15217" title="RAGAS">RAGAS</a>、<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.09476" title="ARES">ARES</a> 和 <a target="_blank" rel="noopener" href="https://github.com/truera/trulens" title="TruLens">TruLens</a>，利用 LLMs 来确定质量分数。这些工具和基准一起构成了一个健壮的框架，用于系统性地评估 RAG 模型，如 下表 所总结的。</p>
<p><img src="//s3.mindex.xyz/blog/Courses/3223d20139581f95e1d3484c2b75bc18.png" alt=""></p>
<h2 id="Future-Prospects">Future Prospects</h2>
<h3 id="Future-Challenges-of-RAG">Future Challenges of RAG</h3>
<p>尽管 RAG 技术已有显著进步，但仍有一些挑战需要深入研究：</p>
<p><strong>上下文长度</strong>。RAG 的效果受到大型语言模型（LLMs）的上下文窗口大小的限制。如何在信息不足的过短窗口和信息稀释的过长窗口之间找到平衡，这是至关重要的。而随着当前不断扩大 LLM 上下文窗口的努力，如何让 RAG 适应这些变化，这是个重要的研究课题。</p>
<p><strong>鲁棒性</strong>。在信息检索过程中，如果出现噪声或矛盾信息，可能会严重影响 RAG 的输出质量。这种情况就像是“错误的信息有时比没有信息还糟糕”。如何提高 RAG 对这种敌对或反事实输入的抵抗力，这已经成为一个研究的热点，并已成为衡量性能的关键指标。</p>
<p><strong>混合方法（RAG+FT）</strong>。将 RAG 与微调结合起来正在成为一种主导策略。如何最佳地集成 RAG 和微调——无论是顺序的、交替的，还是通过端到端联合训练——以及如何充分利用参数化和非参数化的优势，这些都是当前研究的重要领域。</p>
<p><strong>拓宽 LLM 的应用领域</strong>。在 RAG 系统中，LLM 不仅被用于生成最终答案，也被用于信息检索和结果评估。如何进一步挖掘 LLM 在 RAG 系统中的潜力，已经成为了一个日益重要的研究方向。</p>
<p><strong>规模效应</strong>。虽然 LLM 的规模效应（即模型规模越大，性能越好的现象）已经得到了确认，但这种效应是否适用于 RAG 系统，仍然是一个未知数。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.06762">初步的研究</a> 已经开始探讨这个问题，但 RAG 模型的参数数量仍然远远落后于 LLM。有一种特别引人关注的可能性，那就是 “反向规模效应”，即在某些情况下，较小的模型可能会表现得比大模型还要好，这个问题值得进一步研究。</p>
<p><strong>RAG 的实际应用</strong>。由于 RAG 系统的实用性强，且能够满足工程需求，因此在实际应用中得到了广泛的采用。然而，如何提高信息检索的效率，如何在大型知识库中更准确地找到相关的文档（这个过程被称为 “召回”），以及如何确保数据安全，防止 LLM 无意中泄露文档的来源或元数据，这些都是需要解决的重要工程问题。</p>
<h4 id="Modality-Extension-of-RAG">Modality Extension of RAG</h4>
<p>RAG 已经突破了其最初的基于文本的问答边界，开始处理多种形式的模态数据。这种扩展推动了各种领域的 RAG 概念集成到创新的多模态模型中：</p>
<p><strong>图像</strong>。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.12561" title="RA-CM3">RA-CM3</a> 是一种开创性的多模态模型，能够检索并生成文本和图像。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.12597" title="BLIP-2">BLIP-2</a> 利用冻结的图像编码器与大型语言模型（LLM）一起，实现了高效的视觉语言预训练，使得图像到文本的零样本（zero-shot）转换成为可能。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.03765" title="Visualize Before You Write">“Visualize Before You Write”</a> 的方法使用图像生成来引导语言模型的文本生成，在开放式文本生成任务中展现出了潜力。</p>
<p><strong>音频和视频</strong>。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.08174" title="GSS">GSS</a> 方法通过检索并拼接音频片段，实现了将机器翻译的数据转化为语音翻译的数据。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.02736" title="UEOP">UEOP</a> 通过结合外部的、离线的语音转文本策略，实现了端到端自动语音识别的重大进步。此外，基于 KNN 的注意力融合策略利用音频嵌入和与之语义相关的文本嵌入来提升 ASR 的性能，从而加速了领域适应。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.14115" title="Vid2Seq">Vid2Seq</a> 则通过在语言模型中加入专门的时间标记，有助于预测事件的边界和文本描述，生成统一的输出序列。</p>
<p><strong>代码</strong>。<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1109/ICSE48619.2023.00205" title="RBPS">RBPS</a> 在处理小规模学习任务上表现出色，它能通过对代码进行编码和频率分析，找出与开发者目标最匹配的代码示例。这种方法在如生成测试断言（test assertion generation）和修复程序错误（program repair）等任务中已展现出其强大的效果。对于结构化知识处理，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.13269" title="CoK">CoK</a> 方法会首先从知识图谱中找出与输入问题相关的信息，然后将这些信息作为提示融入到输入中，这样做能显著提升在知识图谱问答任务中的表现。</p>
<h3 id="Ecosystem-of-RAG">Ecosystem of RAG</h3>
<h4 id="Downstream-Tasks-and-Evaluation">Downstream Tasks and Evaluation</h4>
<p>RAG 展现出了巨大的潜力，它能够利用大量的知识库，使语言模型有能力处理复杂的查询并生成详细的回答。实践证明，RAG 在各种实际应用中表现出色，包括回答开放式问题和验证事实的真实性。采用 RAG 不仅可以提高回答的准确性和相关性，还能增加回答的多样性和深度。<br>
RAG 在多个领域的可扩展性和通用性值得我们深入研究，尤其是在医学、法律和教育等专业领域。在这些领域，RAG 可能有助于降低训练成本，并相比传统的微调方法，提高专业领域知识问答的性能。<br>
同时，我们需要优化 RAG 的评估框架，以最大化其在各种任务中的效能和实用性。这需要我们开发更精细化的度量和评估工具，能够衡量上下文的相关性、内容的创新性和避免造成伤害等方面。<br>
此外，提高 RAG 驱动模型的可解释性也是我们的重要目标。这将使用户能够理解模型生成回答的理由，从而在使用 RAG 应用时提升信任感和透明度。</p>
<h4 id="Technical-Stack">Technical Stack</h4>
<p>RAG 生态系统的发展受到其技术栈进步的深刻影响。随着 ChatGPT 的出现，像 LangChain 和 LLamaIndex 这样的关键工具迅速走红，它们提供了丰富的 RAG 相关 API，并在大型语言模型（LLM）领域变得必不可少。尽管新兴的技术栈在功能上可能不如 LangChain 和 LLamaIndex 丰富，但它们通过提供专门的服务而获得了区别。例如，Flowise AI 优先考虑低代码策略，让用户可以通过直观的拖放界面部署包括 RAG 在内的 AI 应用。其他如 HayStack，Meltano，和 Cohere Coral 的技术也因其对该领域的独特贡献而受到关注。</p>
<p>除了专注于 AI 的提供商，传统的软件和云服务提供商也在扩展他们的服务范围，包括专注于 RAG 的服务。例如，Verba 是 Weaviate 设计的一款专为个人助理应用程序设计的工具，而 Amazon 的 Kendra14 是一个智能企业搜索服务，它允许用户使用内置连接器浏览各种内容库。</p>
<p>在 RAG 技术领域的演变过程中，我们可以看到一个明显的趋势，即向不同的专化方向发展，包括：</p>
<ol>
<li>定制化。根据特定需求定制 RAG。</li>
<li>简化。使 RAG 更易于使用，从而降低初学者的学习难度。</li>
<li>专业化。优化 RAG 以更有效地适应生产环境的需求。</li>
</ol>
<h2 id="Conclusion">Conclusion</h2>
<p><img src="//s3.mindex.xyz/blog/Courses/b758594171be86db55d0071885972bb8.png" alt="图7"></p>
<p>如 图7 所示，本文总结凸显了 RAG 通过整合语言模型的参数化知识和外部知识库的大量非参数化数据，对提升 LLM 能力作出了重大贡献。我们的调查展示了 RAG 技术的演变以及其对知识密集型任务的影响。我们的分析在 RAG 框架内划分出三个发展范式：Naive RAG、Advanced RAG 和 Modular RAG，每一个都在前者的基础上取得了进一步的提升。Advanced RAG 范式在 Naive 方法的基础上，通过引入复杂的架构元素，如查询重写、块重新排列和提示摘要，实现了超越。这些创新导向了一个更为细致和模块化的架构，提高了 LLM 的性能和可解释性。RAG 与微调和强化学习等其他 AI 方法的技术集成，进一步扩展了其能力。在内容检索方面，一种结合结构化和非结构化数据源的混合方法正逐渐兴起，为检索过程提供了更多元的内容。在 RAG 框架内的前沿研究正在探索新的概念，如从 LLM 中进行自我检索和信息检索的动态时间。</p>
<p>尽管 RAG 技术已经取得了显著的进步，但在提高其鲁棒性和处理大量上下文信息的能力方面，仍有大量的研究机会。RAG 的应用领域也正在向多模态扩展，适应解读和处理各种数据形式，比如图像、视频和代码。这种扩展展现了 RAG 对 AI 实际应用的重大影响，吸引了学术和工业界的广泛关注。RAG 生态系统的发展趋势在于 RAG 为中心的 AI 应用的增加和相关工具的持续开发。然而，随着 RAG 应用领域的不断扩大，我们迫切需要改进评估方法以跟上其发展步伐。确保性能评估的准确性和代表性，对于全面理解 RAG 对 AI 研究和开发社区的贡献至关重要。</p>
<h2 id="Source">Source</h2>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.10997" title="RAG for LLMs: A Survey">RAG for LLMs: A Survey</a></p>
</div><iframe src="/donate/?AliPayQR=null&amp;WeChatQR=http://s3.mindex.xyz/mp/qrcode-s.jpg&amp;GitHub=http://github.com/neo1989&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Author: </strong>尼欧</li><li class="post-copyright-link"><strong>Blog Link: </strong><a href="/Way2AI/Way2AI-RAG-Survey/">https://neo1989.net/Way2AI/Way2AI-RAG-Survey/</a></li><li class="post-copyright-license"><strong>Copyright Declaration: </strong>转载请声明出处。</li></ul></div><br><div class="tags"><a href="../../tags/Coder/">Coder</a><a href="../../tags/AI/">AI</a><a href="../../tags/RAG/">RAG</a><a href="../../tags/LLM/">LLM</a></div><div class="post-nav"><a class="pre" href="../Way2AI-LLM-Agents/">Way2AI · LLM Agent 简介</a><a class="next" href="../../HandMades/HANDMADE-sse-with-go/">Server Sent Event (SSE) with Go</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://neo1989.net/Way2AI/Way2AI-RAG-Survey/';
    this.page.identifier = 'Way2AI/Way2AI-RAG-Survey/';
    this.page.title = 'Way2AI · RAG 综述';
  };</script><!-- script(type='text/javascript' id='disqus-lazy-load-script').--><!--   $.ajax({--><!--   url: 'https://disqus.com/next/config.json',--><!--   timeout: 2500,--><!--   type: 'GET',--><!--   success: function(){--><!--     var d = document;--><!--     var s = d.createElement('script');--><!--     s.src = '//#{theme.disqus}.disqus.com/embed.js';--><!--     s.setAttribute('data-timestamp', + new Date());--><!--     (d.head || d.body).appendChild(s);--><!--     $('.disqus_click_btn').css('display', 'none');--><!--   },--><!--   error: function() {--><!--     $('.disqus_click_btn').css('display', 'block');--><!--   }--><!--   });--><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//neo1989.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://neo1989.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div></div><div class="pure-u-1 pure-u-md-4-6"><div id="footer">Copyright © 2024 <a href="../../." rel="nofollow">愚苏记.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/i-yard/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho/maupassant"> Cho.</a></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="../../js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="../../lib/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="../../js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="../../lib/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
  displayAlign: "left"
  });
</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="../../js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="../../js/smartresize.js?v=1.0.0"></script><script type="text/javascript" id="maid-script" mermaidoptioins="{&quot;startOnload&quot;:true,&quot;theme&quot;:&quot;forest&quot;}" src="../../js/mermaid.min.js?v=1.0.0"></script><script>if (window.mermaid) {
  var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins'));
  mermaid.initialize(options);
}</script></div><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>