<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="我读书少，你莫骗我。"><title>Way2AI · PyTorch实现神经网络的基本套路 | 愚苏记</title><link rel="stylesheet" type="text/css" href="https://cdn.staticfile.org/lxgw-wenkai-webfont/1.6.0/style.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/lib/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="/lib/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="/lib/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="/lib/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="/lib/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '96c862f2728296588ae9849f3bcb95db';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><link rel="alternate" type="application/atom+xml" href="/atom.xml"><meta name="generator" content="Hexo 7.1.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Way2AI · PyTorch实现神经网络的基本套路</h1><a id="logo" href="/.">愚苏记</a><p class="description">To no avail but try.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-5-5"><div class="content_container no-padding-right"><div class="post"><h1 class="post-title">Way2AI · PyTorch实现神经网络的基本套路</h1><div class="post-meta">Jun 15, 2023<span> | </span><span class="category"><a href="/categories/Way2AI/">Way2AI</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3.4k</span><span class="post-meta-item-text"> Words</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 18</span><span class="post-meta-item-text"> Minutes</span></span></span></div><a class="disqus-comment-count" data-disqus-identifier="Way2AI/Way2AI-utilities/" href="/Way2AI/Way2AI-utilities/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#TL-DR"><span class="toc-number">1.</span> <span class="toc-text">TL;DR</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Set-up"><span class="toc-number">2.</span> <span class="toc-text">Set up</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Device"><span class="toc-number">3.</span> <span class="toc-text">Device</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Load-data"><span class="toc-number">4.</span> <span class="toc-text">Load data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Split-data"><span class="toc-number">5.</span> <span class="toc-text">Split data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Label-encoding"><span class="toc-number">6.</span> <span class="toc-text">Label encoding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Standardize-data"><span class="toc-number">7.</span> <span class="toc-text">Standardize data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataLoader"><span class="toc-number">8.</span> <span class="toc-text">DataLoader</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model"><span class="toc-number">9.</span> <span class="toc-text">Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Trainer"><span class="toc-number">10.</span> <span class="toc-text">Trainer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LR-scheduler"><span class="toc-number">11.</span> <span class="toc-text">LR scheduler</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Early-stopping"><span class="toc-number">12.</span> <span class="toc-text">Early stopping</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training"><span class="toc-number">13.</span> <span class="toc-text">Training</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Evaluation"><span class="toc-number">14.</span> <span class="toc-text">Evaluation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Saving-loading"><span class="toc-number">15.</span> <span class="toc-text">Saving &amp; loading</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ending"><span class="toc-number">16.</span> <span class="toc-text">Ending</span></a></li></ol></div></div><div class="post-content"><h2 id="TL-DR">TL;DR</h2>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>本文简单交代了神经网络的基本套路以及部分实用组件，以简化开发过程。</p>
<h2 id="Set-up">Set up</h2>
<p>通常我们需要为重复实验设置很多seed，所以我们可以将其打包到一个函数里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_seeds</span>(<span class="params">seed=<span class="number">1024</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Set seeds for reproducibility.&quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    touch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)  <span class="comment"># multi-GPU</span></span><br><span class="line"></span><br><span class="line">set_seeds(seed=<span class="number">1024</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Device">Device</h2>
<p>当我们有大型数据集和更大的模型要训练时，我们可以通过在 GPU 上并行化张量操作来加速。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cuda = <span class="literal">True</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span>(torch.cuda.is_available() <span class="keyword">and</span> cuda) <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">torch.set_default_tensor_type(&#123;<span class="string">&quot;cuda&quot;</span>: <span class="string">&quot;torch.cuda.FloatTensor&quot;</span>, <span class="string">&quot;cpu&quot;</span>: <span class="string">&quot;torch.FloatTensor&quot;</span>&#125;.get(<span class="built_in">str</span>(device)))</span><br></pre></td></tr></table></figure>
<h2 id="Load-data">Load data</h2>
<p>这里依然使用前文引入的螺旋数据作为演示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">url = <span class="string">&quot;http://s3.mindex.xyz/datasets/9378f64fc8dd2817e4c92be0a3bae8e7.csv&quot;</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>) <span class="comment"># load</span></span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>) <span class="comment"># shuffle</span></span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Data shapes</span></span><br><span class="line">X = df[[<span class="string">&quot;X1&quot;</span>, <span class="string">&quot;X2&quot;</span>]].values</span><br><span class="line">y = df[<span class="string">&quot;color&quot;</span>].values</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;X: &quot;</span>, np.shape(X))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;y: &quot;</span>, np.shape(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X:  (1500, 2)</span></span><br><span class="line"><span class="comment"># y:  (1500,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize data</span></span><br><span class="line">plt.title(<span class="string">&quot;Generated non-linear data&quot;</span>)</span><br><span class="line">colors = &#123;<span class="string">&quot;c1&quot;</span>: <span class="string">&quot;red&quot;</span>, <span class="string">&quot;c2&quot;</span>: <span class="string">&quot;yellow&quot;</span>, <span class="string">&quot;c3&quot;</span>: <span class="string">&quot;blue&quot;</span>&#125;</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=[colors[_y] <span class="keyword">for</span> _y <span class="keyword">in</span> y], edgecolors=<span class="string">&quot;k&quot;</span>, s=<span class="number">25</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="//s3.mindex.xyz/blog/Courses/147e784e6ecae3fd226abce4f3905550.png" alt=""></p>
<h2 id="Split-data">Split data</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_val_test_split</span>(<span class="params">X, y, train_size</span>):</span><br><span class="line">    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)</span><br><span class="line">    X_test, X_val, y_test, y_val = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (1050, 2), y_train: (1050,)</span></span><br><span class="line"><span class="comment"># X_val: (225, 2), y_val: (225,)</span></span><br><span class="line"><span class="comment"># X_test: (225, 2), y_test: (225,)</span></span><br><span class="line"><span class="comment"># Sample point: [0.17003003 0.63079261] → c3</span></span><br></pre></td></tr></table></figure>
<h2 id="Label-encoding">Label encoding</h2>
<p>接下来定义一个 LabelEncoder 来将文本标签编码成唯一的索引。</p>
<p>这里不再使用 scikit-learn 的 LabelEncoder，因为我们希望能够以我们想要的方式保存和加载我们的实例。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LabelEncoder</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Label encoder for tag labels.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, class_to_index=<span class="literal">None</span></span>):</span><br><span class="line">        self.class_to_index = class_to_index <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = <span class="built_in">list</span>(self.class_to_index.keys())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.class_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&lt;LabelEncoder(num_classes=<span class="subst">&#123;<span class="built_in">len</span>(self)&#125;</span>&gt;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, y</span>):</span><br><span class="line">        classes = np.unique(y)</span><br><span class="line">        <span class="keyword">for</span> i, class_ <span class="keyword">in</span> <span class="built_in">enumerate</span>(classes):</span><br><span class="line">            self.class_to_index[class_] = i</span><br><span class="line">        self.index_to_class = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.class_to_index.items()&#125;</span><br><span class="line">        self.classes = <span class="built_in">list</span>(self.class_to_index.keys())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, y</span>):</span><br><span class="line">        encoded = np.zeros((<span class="built_in">len</span>(y)), dtype=<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(y):</span><br><span class="line">            encoded[i] = self.class_to_index[item]</span><br><span class="line">        <span class="keyword">return</span> encoded</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, y</span>):</span><br><span class="line">        classes = []</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(y):</span><br><span class="line">            classes.append(self.index_to_class[item])</span><br><span class="line">        <span class="keyword">return</span> classes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, fp</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fp, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;<span class="string">&#x27;class_to_index&#x27;</span>: self.class_to_index&#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">cls, fp</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fp, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Encode</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">label_encoder.fit(y_train)</span><br><span class="line">label_encoder.class_to_index</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert labels to tokens</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line">y_train = label_encoder.encode(y_train)</span><br><span class="line">y_val = label_encoder.encode(y_val)</span><br><span class="line">y_test = label_encoder.encode(y_test)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> <span class="built_in">enumerate</span>(counts)&#125;</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: c3</span></span><br><span class="line"><span class="comment"># y_train[0]: 2</span></span><br><span class="line"><span class="comment"># counts: [350 350 350]</span></span><br><span class="line"><span class="comment"># weights: &#123;0: 0.002857142857142857, 1: 0.002857142857142857, 2: 0.002857142857142857&#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="Standardize-data">Standardize data</h2>
<p>我们需要标准化我们的数据（零均值和单位方差），这样特定特征的大小就不会影响模型学习其权重的方式。</p>
<p>我们只对输入X进行标准化，因为我们的输出y是类值。</p>
<p>我们将编写自己的 StandardScaler 类，以便在推理过程中轻松保存和加载它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">StandardScaler</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, mean=<span class="literal">None</span>, std=<span class="literal">None</span></span>):</span><br><span class="line">        self.mean = np.array(mean)</span><br><span class="line">        self.std = np.array(std)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X</span>):</span><br><span class="line">        self.mean = np.mean(X_train, axis=<span class="number">0</span>)</span><br><span class="line">        self.std = np.std(X_train, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">scale</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> (X - self.mean) / self.std</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">unscale</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> (X * self.std) + self.mean</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, fp</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fp, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            contents = &#123;<span class="string">&quot;mean&quot;</span>: self.mean.tolist(), <span class="string">&quot;std&quot;</span>: self.std.tolist()&#125;</span><br><span class="line">            json.dump(contents, fp, indent=<span class="number">4</span>, sort_keys=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">cls, fp</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fp, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            kwargs = json.load(fp=fp)</span><br><span class="line">        <span class="keyword">return</span> cls(**kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the data (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler()</span><br><span class="line">X_scaler.fit(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_test[0]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_test[1]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_test[0]: mean: 0.0, std: 1.0</span></span><br><span class="line"><span class="comment"># X_test[1]: mean: -0.0, std: 1.0</span></span><br></pre></td></tr></table></figure>
<h2 id="DataLoader">DataLoader</h2>
<p>我们将把数据放在 Dataset 中，并使用 DataLoader 来有效地创建用于训练和验证的批次。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Dataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        self.X = X</span><br><span class="line">        self.y = y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&lt;Dataset(N=<span class="subst">&#123;<span class="built_in">len</span>(self)&#125;</span>)&gt;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        X = self.X[index]</span><br><span class="line">        y = self.y[index]</span><br><span class="line">        <span class="keyword">return</span> [X, y]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">self, batch</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Processing on a batch.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Get inputs</span></span><br><span class="line">        batch = np.array(batch)</span><br><span class="line">        X = np.stack(batch[:, <span class="number">0</span>], axis=<span class="number">0</span>)</span><br><span class="line">        y = batch[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cast</span></span><br><span class="line">        X = torch.FloatTensor(X.astype(np.float32))</span><br><span class="line">        y = torch.LongTensor(y.astype(np.int32))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_dataloader</span>(<span class="params">self, batch_size, shuffle=<span class="literal">False</span>, drop_last=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">return</span> torch.utils.data.DataLoader(</span><br><span class="line">            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,</span><br><span class="line">            shuffle=shuffle, drop_last=drop_last, pin_memory=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>事实上我们并不需要 collate_fn ，但我们可以让它透明（无副作用），因为当我想要对批处理做一些处理的时候，需要用到这个方法。(如：数据padding）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create datasets</span></span><br><span class="line">train_dataset = Dataset(X=X_train, y=y_train)</span><br><span class="line">val_dataset = Dataset(X=X_val, y=y_val)</span><br><span class="line">test_dataset = Dataset(X=X_test, y=y_test)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Datasets:\n&quot;</span></span><br><span class="line">       <span class="string">f&quot;  Train dataset:<span class="subst">&#123;train_dataset.__str__()&#125;</span>\n&quot;</span></span><br><span class="line">       <span class="string">f&quot;  Val dataset: <span class="subst">&#123;val_dataset.__str__()&#125;</span>\n&quot;</span></span><br><span class="line">       <span class="string">f&quot;  Test dataset: <span class="subst">&#123;test_dataset.__str__()&#125;</span>\n&quot;</span></span><br><span class="line">       <span class="string">&quot;Sample point:\n&quot;</span></span><br><span class="line">       <span class="string">f&quot;  X: <span class="subst">&#123;train_dataset[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>\n&quot;</span></span><br><span class="line">       <span class="string">f&quot;  y: <span class="subst">&#123;train_dataset[<span class="number">0</span>][<span class="number">1</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Datasets:</span></span><br><span class="line"><span class="comment">#   Train dataset: &lt;Dataset(N=1050)&gt;</span></span><br><span class="line"><span class="comment">#   Val dataset: &lt;Dataset(N=225)&gt;</span></span><br><span class="line"><span class="comment">#   Test dataset: &lt;Dataset(N=225)&gt;</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: [-1.47355106 -1.67417243]</span></span><br><span class="line"><span class="comment">#   y: 0</span></span><br></pre></td></tr></table></figure>
<p>之前的文章中都是利用全部的数据进行梯度计算，然而更标准的做法是 <strong>mini-batch</strong> 随机梯度下降，也就是将样本分成多个只有 n(BATCH_SIZE) 个样本的 mini-batch。这就是 Dataloader 派上用场的地方。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create dataloaders</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_dataloader = train_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">val_dataloader = val_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">test_dataloader = test_dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line">batch_X, batch_y = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Sample batch:\n&quot;</span></span><br><span class="line">       <span class="string">f&quot;  X: <span class="subst">&#123;<span class="built_in">list</span>(batch_X.size())&#125;</span>\n&quot;</span></span><br><span class="line">       <span class="string">f&quot;  y: <span class="subst">&#123;<span class="built_in">list</span>(batch_y.size())&#125;</span>\n&quot;</span></span><br><span class="line">       <span class="string">&quot;Sample point:\n&quot;</span></span><br><span class="line">       <span class="string">f&quot;  X: <span class="subst">&#123;batch_X[<span class="number">0</span>]&#125;</span>\n&quot;</span></span><br><span class="line">       <span class="string">f&quot;  y: <span class="subst">&#123;batch_y[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Sample batch:</span></span><br><span class="line"><span class="comment">#   X: [64, 2]</span></span><br><span class="line"><span class="comment">#   y: [64]</span></span><br><span class="line"><span class="comment"># Sample point:</span></span><br><span class="line"><span class="comment">#   X: tensor([ 0.4535, -0.3570], dtype=torch.float64)</span></span><br><span class="line"><span class="comment">#   y: 0</span></span><br></pre></td></tr></table></figure>
<h2 id="Model">Model</h2>
<p>我们需要定义一个模型，以便继续给出训练阶段的实用组件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>]  <span class="comment"># 2D</span></span><br><span class="line">HIDDEN_DIM = <span class="number">100</span></span><br><span class="line">DROPOUT_P = <span class="number">.01</span></span><br><span class="line">NUM_CLASSES = <span class="built_in">len</span>(label_encoder.classes)</span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, hidden_dim, dropout_p, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_in</span>):</span><br><span class="line">        z = F.relu(self.fc1(x_in))</span><br><span class="line">        z = self.dropout(z)</span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span class="line">            dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="built_in">print</span> (model.named_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &lt;bound method Module.named_parameters of MLP(</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=2, out_features=100, bias=True)</span></span><br><span class="line"><span class="comment">#   (droput): Dropout(p=0.01, inplace=False)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=100, out_features=3, bias=True)</span></span><br><span class="line"><span class="comment"># )&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="Trainer">Trainer</h2>
<p>之前的文章，我们一直在编写只使用循环来训练分割后的训练数据，然后在测试集上评估。</p>
<p>但实际工作中，我们会遵循下面这个过程：</p>
<ul>
<li>使用mini-batches进行训练</li>
<li>在验证集上评估损失，并更新超参</li>
<li>训练结束后，在测试集上评估模型</li>
</ul>
<p>所以我们需要创建 Trainer 类来组织这些过程。</p>
<p>首先，train_step 用来执行小批量数据训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">self, dataloader</span>):</span><br><span class="line">    self.model.train()</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]</span><br><span class="line">        inputs, targets = batch[:-<span class="number">1</span>], batch[-<span class="number">1</span>]</span><br><span class="line">        self.optimizer.zero_grad()  <span class="comment"># reset gradients</span></span><br><span class="line">        z = self.model(inputs)  <span class="comment"># forward pass</span></span><br><span class="line">        J = self.loss_fn(z, targets)</span><br><span class="line">        J.backward()  <span class="comment"># backward pass</span></span><br><span class="line">        self.optimizer.step()  <span class="comment"># Update weights</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cumulative Metrics</span></span><br><span class="line">        loss += (j.detach().item() - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p>然后 eval_step，用于验证</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">eval_step</span>(<span class="params">self, dataloader</span>):</span><br><span class="line">    self.model.<span class="built_in">eval</span>()</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    y_trues, x_probs = [], []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.inference_model():</span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">            batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]</span><br><span class="line">            inputs, y_trye = batch[:-<span class="number">1</span>], batch[-<span class="number">1</span>]</span><br><span class="line">            z = self.model(inputs)</span><br><span class="line">            J = self.loss_fn(z, y_true).item()</span><br><span class="line"></span><br><span class="line">            loss += (J - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Store outputs</span></span><br><span class="line">            y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">            y_probs.extend(y_prob)</span><br><span class="line">            y_trues.extend(y_true.cpu().numpy())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, np.vstack(y_trues), np.vstack(y_probs)</span><br></pre></td></tr></table></figure>
<p>最后 predict_step, 只是用来对数据进行预测</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_step</span>(<span class="params">self, dataloader</span>):</span><br><span class="line">    self.model.<span class="built_in">eval</span>()</span><br><span class="line">    y_prods = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.inference_model():</span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line"></span><br><span class="line">            inputs, y_trye = batch[:-<span class="number">1</span>], batch[-<span class="number">1</span>]</span><br><span class="line">            z = self.model(inputs)</span><br><span class="line">            y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">            y_probs.extend(y_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.vstack(y_probs)</span><br></pre></td></tr></table></figure>
<h2 id="LR-scheduler">LR scheduler</h2>
<p>我们将向优化器添加一个学习率调度器，以在训练期间调整我们的学习率。</p>
<p>有许多<a href="%22https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate%22" title="How to adjust learning rate">调度器</a>可供选择，但最受欢迎的是 ReduceLROnPlateau ，它在指标（例如：验证损失）停止改进的时候，减少学习率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the LR scheduler</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="string">&quot;min&quot;</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(NUM_EPOCHS * <span class="number">10</span>):</span><br><span class="line">    ...</span><br><span class="line">    train_loss = trainer.train_step(dataloader=train_dataloader)</span><br><span class="line">    val_loss, _, _ = trainer.eval_step(dataloader=val_dataloader)</span><br><span class="line">    scheduler.step(val_loss)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<h2 id="Early-stopping">Early stopping</h2>
<p>我们不应该拍脑袋训练足够多的epoch，而是应该有个明确的停止标准。</p>
<p>常见的停止标准，是模型达到一个期望的性能时，即停止训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Early stopping</span></span><br><span class="line"><span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">    best_val_loss = val_loss</span><br><span class="line">    best_model = trainer.model</span><br><span class="line">    _patience = patience  <span class="comment"># reset _patience</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    _patience -= <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> _patience:  <span class="comment"># 0</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Stopping early!&quot;</span>)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Training">Training</h2>
<p>现在把上面这些放到一起</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">LEARNING_RATE = <span class="number">1e-2</span></span><br><span class="line">NUM_EPOCHS = <span class="number">100</span></span><br><span class="line">PATIENCE = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Loss</span></span><br><span class="line">class_weights_tensor = torch.Tensor(<span class="built_in">list</span>(class_weights.values())).to(device)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer &amp; scheduler</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(</span><br><span class="line">    optimizer, mode=<span class="string">&quot;min&quot;</span>, factor=<span class="number">0.1</span>, patience=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Trainer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, device, loss_fn=<span class="literal">None</span>, optimizer=<span class="literal">None</span>, scheduler=<span class="literal">None</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set params</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.device = device</span><br><span class="line">        self.loss_fn = loss_fn</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self.scheduler = scheduler</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">self, dataloader</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Train step.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Set model to train mode</span></span><br><span class="line">        self.model.train()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over train batches</span></span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Step</span></span><br><span class="line">            batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">            inputs, targets = batch[:-<span class="number">1</span>], batch[-<span class="number">1</span>]</span><br><span class="line">            self.optimizer.zero_grad()  <span class="comment"># Reset gradients</span></span><br><span class="line">            z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">            J = self.loss_fn(z, targets)  <span class="comment"># Define loss</span></span><br><span class="line">            J.backward()  <span class="comment"># Backward pass</span></span><br><span class="line">            self.optimizer.step()  <span class="comment"># Update weights</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Cumulative Metrics</span></span><br><span class="line">            loss += (J.detach().item() - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">eval_step</span>(<span class="params">self, dataloader</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Validation or test step.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line">        y_trues, y_probs = [], []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Step</span></span><br><span class="line">                batch = [item.to(self.device) <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment"># Set device</span></span><br><span class="line">                inputs, y_true = batch[:-<span class="number">1</span>], batch[-<span class="number">1</span>]</span><br><span class="line">                z = self.model(inputs)  <span class="comment"># Forward pass</span></span><br><span class="line">                J = self.loss_fn(z, y_true).item()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Cumulative Metrics</span></span><br><span class="line">                loss += (J - loss) / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line">                y_trues.extend(y_true.cpu().numpy())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, np.vstack(y_trues), np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict_step</span>(<span class="params">self, dataloader</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Prediction step.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Set model to eval mode</span></span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">        y_probs = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over val batches</span></span><br><span class="line">        <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Forward pass w/ inputs</span></span><br><span class="line">                inputs, targets = batch[:-<span class="number">1</span>], batch[-<span class="number">1</span>]</span><br><span class="line">                z = self.model(inputs)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Store outputs</span></span><br><span class="line">                y_prob = F.softmax(z).cpu().numpy()</span><br><span class="line">                y_probs.extend(y_prob)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.vstack(y_probs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, num_epochs, patience, train_dataloader, val_dataloader</span>):</span><br><span class="line">        best_val_loss = np.inf</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">            <span class="comment"># Steps</span></span><br><span class="line">            train_loss = self.train_step(dataloader=train_dataloader)</span><br><span class="line">            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)</span><br><span class="line">            self.scheduler.step(val_loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Early stopping</span></span><br><span class="line">            <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">                best_val_loss = val_loss</span><br><span class="line">                best_model = self.model</span><br><span class="line">                _patience = patience  <span class="comment"># reset _patience</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                _patience -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> _patience:  <span class="comment"># 0</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Stopping early!&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Logging</span></span><br><span class="line">            <span class="built_in">print</span>(</span><br><span class="line">                <span class="string">f&quot;Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> | &quot;</span></span><br><span class="line">                <span class="string">f&quot;train_loss: <span class="subst">&#123;train_loss:<span class="number">.5</span>f&#125;</span>, &quot;</span></span><br><span class="line">                <span class="string">f&quot;val_loss: <span class="subst">&#123;val_loss:<span class="number">.5</span>f&#125;</span>, &quot;</span></span><br><span class="line">                <span class="string">f&quot;lr: <span class="subst">&#123;self.optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]:<span class="number">.2</span>E&#125;</span>, &quot;</span></span><br><span class="line">                <span class="string">f&quot;_patience: <span class="subst">&#123;_patience&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> best_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trainer module</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, device=device, loss_fn=loss_fn,</span><br><span class="line">    optimizer=optimizer, scheduler=scheduler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line">best_model = trainer.train(</span><br><span class="line">    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 1 | train_loss: 0.87488, val_loss: 0.66353, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 2 | train_loss: 0.66368, val_loss: 0.55748, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># Epoch: 67 | train_loss: 0.03002, val_loss: 0.02305, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 68 | train_loss: 0.03011, val_loss: 0.02309, lr: 1.00E-02, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 69 | train_loss: 0.02544, val_loss: 0.02227, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 70 | train_loss: 0.02680, val_loss: 0.02154, lr: 1.00E-02, _patience: 3</span></span><br><span class="line"><span class="comment"># Epoch: 71 | train_loss: 0.02897, val_loss: 0.02162, lr: 1.00E-02, _patience: 2</span></span><br><span class="line"><span class="comment"># Epoch: 72 | train_loss: 0.02737, val_loss: 0.02190, lr: 1.00E-02, _patience: 1</span></span><br><span class="line"><span class="comment"># Stopping early!</span></span><br></pre></td></tr></table></figure>
<h2 id="Evaluation">Evaluation</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_metrics</span>(<span class="params">y_true, y_pred, classes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Per-class performance metrics.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">&quot;overall&quot;</span>: &#123;&#125;, <span class="string">&quot;class&quot;</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">&quot;weighted&quot;</span>)</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;precision&quot;</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;recall&quot;</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;f1&quot;</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;num_samples&quot;</span>] = np.float64(<span class="built_in">len</span>(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(classes)):</span><br><span class="line">        performance[<span class="string">&quot;class&quot;</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">&quot;precision&quot;</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">&quot;recall&quot;</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">&quot;f1&quot;</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">&quot;num_samples&quot;</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions</span></span><br><span class="line">test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Determine performance</span></span><br><span class="line">performance = get_metrics(</span><br><span class="line">    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)</span><br><span class="line"><span class="built_in">print</span> (json.dumps(performance[<span class="string">&quot;overall&quot;</span>], indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &quot;precision&quot;: 0.9956140350877192,</span></span><br><span class="line"><span class="comment">#   &quot;recall&quot;: 0.9955555555555555,</span></span><br><span class="line"><span class="comment">#   &quot;f1&quot;: 0.9955553580159118,</span></span><br><span class="line"><span class="comment">#   &quot;num_samples&quot;: 225.0</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="Saving-loading">Saving &amp; loading</h2>
<p>我们需要保存一些必要的模型数据，以供后续能够完整的加载和使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Save artifacts</span></span><br><span class="line"><span class="built_in">dir</span> = Path(<span class="string">&quot;mlp&quot;</span>)</span><br><span class="line"><span class="built_in">dir</span>.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">label_encoder.save(fp=Path(<span class="built_in">dir</span>, <span class="string">&quot;label_encoder.json&quot;</span>))</span><br><span class="line">X_scaler.save(fp=Path(<span class="built_in">dir</span>, <span class="string">&quot;X_scaler.json&quot;</span>))</span><br><span class="line">torch.save(best_model.state_dict(), Path(<span class="built_in">dir</span>, <span class="string">&quot;model.pt&quot;</span>))</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(Path(<span class="built_in">dir</span>, <span class="string">&#x27;performance.json&#x27;</span>), <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    json.dump(performance, indent=<span class="number">2</span>, sort_keys=<span class="literal">False</span>, fp=fp)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load artifacts</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">label_encoder = LabelEncoder.load(fp=Path(<span class="built_in">dir</span>, <span class="string">&quot;label_encoder.json&quot;</span>))</span><br><span class="line">X_scaler = StandardScaler.load(fp=Path(<span class="built_in">dir</span>, <span class="string">&quot;X_scaler.json&quot;</span>))</span><br><span class="line">model = MLP(</span><br><span class="line">    input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span class="line">    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)</span><br><span class="line">model.load_state_dict(torch.load(Path(<span class="built_in">dir</span>, <span class="string">&quot;model.pt&quot;</span>), map_location=device))</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize trainer</span></span><br><span class="line">trainer = Trainer(model=model, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dataloader</span></span><br><span class="line">sample = [[<span class="number">0.106737</span>, <span class="number">0.114197</span>]] <span class="comment"># c1</span></span><br><span class="line">X = X_scaler.scale(sample)</span><br><span class="line">y_filler = label_encoder.encode([label_encoder.classes[<span class="number">0</span>]]*<span class="built_in">len</span>(X))</span><br><span class="line">dataset = Dataset(X=X, y=y_filler)</span><br><span class="line">dataloader = dataset.create_dataloader(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line">y_prob = trainer.predict_step(dataloader)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line">label_encoder.decode(y_pred)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Ending">Ending</h2>
<p>本文给出了一个机器学习项目的基本组件， 事实上，还有一些其他的重要组成没有覆盖到。比如：</p>
<ul>
<li>文本序列化的Tokenizers</li>
<li>表征数据的Encoders</li>
<li>数据padding</li>
<li>实验跟踪及可视化结果</li>
<li>超惨优化</li>
<li>等等</li>
</ul>
<p>后续我们会继续学习，至少到这里，我们有了入门深度学习的基础了。</p>
</div><iframe src="/donate/?AliPayQR=null&amp;WeChatQR=http://s3.mindex.xyz/mp/qrcode-s.jpg&amp;GitHub=http://github.com/neo1989&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Author: </strong>尼欧</li><li class="post-copyright-link"><strong>Blog Link: </strong><a href="/Way2AI/Way2AI-utilities/">https://neo1989.net/Way2AI/Way2AI-utilities/</a></li><li class="post-copyright-license"><strong>Copyright Declaration: </strong>转载请声明出处。</li></ul></div><br><div class="tags"><a href="/tags/Coder/">Coder</a><a href="/tags/AI/">AI</a><a href="/tags/PyTorch/">PyTorch</a></div><div class="post-nav"><a class="pre" href="/Notes/NOTE-openai-function-calling/">ChatGPT开放函数调用能力 · 好用到震惊！</a><a class="next" href="/Way2AI/Way2AI-neural-networks-2/">Way2AI · 神经网络 (二)</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://neo1989.net/Way2AI/Way2AI-utilities/';
    this.page.identifier = 'Way2AI/Way2AI-utilities/';
    this.page.title = 'Way2AI · PyTorch实现神经网络的基本套路';
  };</script><!-- script(type='text/javascript' id='disqus-lazy-load-script').--><!--   $.ajax({--><!--   url: 'https://disqus.com/next/config.json',--><!--   timeout: 2500,--><!--   type: 'GET',--><!--   success: function(){--><!--     var d = document;--><!--     var s = d.createElement('script');--><!--     s.src = '//#{theme.disqus}.disqus.com/embed.js';--><!--     s.setAttribute('data-timestamp', + new Date());--><!--     (d.head || d.body).appendChild(s);--><!--     $('.disqus_click_btn').css('display', 'none');--><!--   },--><!--   error: function() {--><!--     $('.disqus_click_btn').css('display', 'block');--><!--   }--><!--   });--><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//neo1989.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://neo1989.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div></div><div class="pure-u-1 pure-u-md-4-6"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">愚苏记.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/i-yard/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho/maupassant"> Cho.</a></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="/lib/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="/lib/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
  displayAlign: "left"
  });
</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script><script type="text/javascript" id="maid-script" mermaidoptioins="{&quot;startOnload&quot;:true,&quot;theme&quot;:&quot;forest&quot;}" src="/js/mermaid.min.js?v=1.0.0"></script><script>if (window.mermaid) {
  var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins'));
  mermaid.initialize(options);
}</script></div><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>