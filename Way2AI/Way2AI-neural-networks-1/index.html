<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="我读书少，你莫骗我。"><title>Way2AI · 神经网络 (一) | 愚苏记</title><link rel="stylesheet" type="text/css" href="https://cdn.staticfile.org/lxgw-wenkai-webfont/1.6.0/style.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/lib/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="/lib/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="/lib/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="/lib/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="/lib/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '96c862f2728296588ae9849f3bcb95db';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><link rel="alternate" type="application/atom+xml" href="/atom.xml"><meta name="generator" content="Hexo 7.1.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Way2AI · 神经网络 (一)</h1><a id="logo" href="/.">愚苏记</a><p class="description">To no avail but try.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-5-5"><div class="content_container no-padding-right"><div class="post"><h1 class="post-title">Way2AI · 神经网络 (一)</h1><div class="post-meta">Jun 1, 2023<span> | </span><span class="category"><a href="/categories/Way2AI/">Way2AI</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3.6k</span><span class="post-meta-item-text"> Words</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 20</span><span class="post-meta-item-text"> Minutes</span></span></span></div><a class="disqus-comment-count" data-disqus-identifier="Way2AI/Way2AI-neural-networks-1/" href="/Way2AI/Way2AI-neural-networks-1/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#TL-DR"><span class="toc-number">1.</span> <span class="toc-text">TL;DR</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Set-up"><span class="toc-number">2.</span> <span class="toc-text">Set up</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Load-data"><span class="toc-number">2.1.</span> <span class="toc-text">Load data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Split-data"><span class="toc-number">2.2.</span> <span class="toc-text">Split data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Label-encoding"><span class="toc-number">2.3.</span> <span class="toc-text">Label encoding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Standardize-data"><span class="toc-number">2.4.</span> <span class="toc-text">Standardize data</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-model"><span class="toc-number">3.</span> <span class="toc-text">Linear model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-Train"><span class="toc-number">3.1.</span> <span class="toc-text">Model &amp; Train</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Evaluation"><span class="toc-number">3.2.</span> <span class="toc-text">Evaluation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Activation-functions"><span class="toc-number">4.</span> <span class="toc-text">Activation functions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NumPy"><span class="toc-number">5.</span> <span class="toc-text">NumPy</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Initialize-weights"><span class="toc-number">5.1.</span> <span class="toc-text">Initialize weights</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Model"><span class="toc-number">5.2.</span> <span class="toc-text">Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Loss"><span class="toc-number">5.3.</span> <span class="toc-text">Loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradients"><span class="toc-number">5.4.</span> <span class="toc-text">Gradients</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Update-weights"><span class="toc-number">5.5.</span> <span class="toc-text">Update weights</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Training"><span class="toc-number">5.6.</span> <span class="toc-text">Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Evaluation-2"><span class="toc-number">5.7.</span> <span class="toc-text">Evaluation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ending"><span class="toc-number">6.</span> <span class="toc-text">Ending</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Citation"><span class="toc-number">7.</span> <span class="toc-text">Citation</span></a></li></ol></div></div><div class="post-content"><h2 id="TL-DR">TL;DR</h2>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>本章的目标依然是学习一种模型 $\hat{y}$，能准确对输入 $X$ 及对应的输出 $y$ 进行建模。</p>
<p>你会注意到神经网络只是我们迄今为止看到的广义线性方法的扩展，但具有非线性激活函数，因为我们的数据是高度非线性的。</p>
<p><img src="//s3.mindex.xyz/blog/Courses/908c174b73d8c8bb1c1ec3ba9e4cf885.png" alt=""></p>
<p>$$<br>
z_1 = XW_1<br>
$$</p>
<p>$$<br>
a_1 = f(z_1)<br>
$$</p>
<p>$$<br>
z_2 = a_1W_2<br>
$$</p>
<p>$$<br>
\hat{y} = softmax(x)<br>
$$</p>
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$N$</td>
<td style="text-align:center">样本数</td>
</tr>
<tr>
<td style="text-align:center">$D$</td>
<td style="text-align:center">特征数</td>
</tr>
<tr>
<td style="text-align:center">$H$</td>
<td style="text-align:center">隐藏神经元</td>
</tr>
<tr>
<td style="text-align:center">$C$</td>
<td style="text-align:center">标签数</td>
</tr>
<tr>
<td style="text-align:center">$W_1$</td>
<td style="text-align:center">第一层的权重</td>
</tr>
<tr>
<td style="text-align:center">$z_1$</td>
<td style="text-align:center">第一层的输出</td>
</tr>
<tr>
<td style="text-align:center">$f$</td>
<td style="text-align:center">非线性激活函数</td>
</tr>
<tr>
<td style="text-align:center">$a_1$</td>
<td style="text-align:center">第一层的激活值</td>
</tr>
<tr>
<td style="text-align:center">$W_2$</td>
<td style="text-align:center">第二层的权重</td>
</tr>
<tr>
<td style="text-align:center">$z_2$</td>
<td style="text-align:center">第二层的输出</td>
</tr>
</tbody>
</table>
<h2 id="Set-up">Set up</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1024</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">np.random.seed(SEED)</span><br><span class="line">random.seed(SEED)</span><br></pre></td></tr></table></figure>
<h3 id="Load-data">Load data</h3>
<p>这里准备了一份非线性可分的螺旋数据来学习。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">url = <span class="string">&quot;http://s3.mindex.xyz/datasets/9378f64fc8dd2817e4c92be0a3bae8e7.csv&quot;</span></span><br><span class="line">df = pd.read_csv(url, header=<span class="number">0</span>) <span class="comment"># load</span></span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>) <span class="comment"># shuffle</span></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<p><img src="//s3.mindex.xyz/blog/Courses/4022744de8e63b11599cdd95aab6ac62.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Data shapes</span></span><br><span class="line">X = df[[<span class="string">&quot;X1&quot;</span>, <span class="string">&quot;X2&quot;</span>]].values</span><br><span class="line">y = df[<span class="string">&quot;color&quot;</span>].values</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;X: &quot;</span>, np.shape(X))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;y: &quot;</span>, np.shape(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X:  (1500, 2)</span></span><br><span class="line"><span class="comment"># y:  (1500,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize data</span></span><br><span class="line">plt.title(<span class="string">&quot;Generated non-linear data&quot;</span>)</span><br><span class="line">colors = &#123;<span class="string">&quot;c1&quot;</span>: <span class="string">&quot;red&quot;</span>, <span class="string">&quot;c2&quot;</span>: <span class="string">&quot;yellow&quot;</span>, <span class="string">&quot;c3&quot;</span>: <span class="string">&quot;blue&quot;</span>&#125;</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=[colors[_y] <span class="keyword">for</span> _y <span class="keyword">in</span> y], edgecolors=<span class="string">&quot;k&quot;</span>, s=<span class="number">25</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="//s3.mindex.xyz/blog/Courses/147e784e6ecae3fd226abce4f3905550.png" alt=""></p>
<h3 id="Split-data">Split data</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">TRAIN_SIZE = <span class="number">0.7</span></span><br><span class="line">VAL_SIZE = <span class="number">0.15</span></span><br><span class="line">TEST_SIZE = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_val_test_split</span>(<span class="params">X, y, train_size</span>):</span><br><span class="line">    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)</span><br><span class="line">    X_test, X_val, y_test, y_val = train_test_split(X_, y_, train_size=<span class="number">0.5</span>, stratify=y_)</span><br><span class="line">    <span class="keyword">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data splits</span></span><br><span class="line">X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(</span><br><span class="line">    X=X, y=y, train_size=TRAIN_SIZE)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_train: <span class="subst">&#123;X_train.shape&#125;</span>, y_train: <span class="subst">&#123;y_train.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_val: <span class="subst">&#123;X_val.shape&#125;</span>, y_val: <span class="subst">&#123;y_val.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_test: <span class="subst">&#123;X_test.shape&#125;</span>, y_test: <span class="subst">&#123;y_test.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;Sample point: <span class="subst">&#123;X_train[<span class="number">0</span>]&#125;</span> → <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_train: (1050, 2), y_train: (1050,)</span></span><br><span class="line"><span class="comment"># X_val: (225, 2), y_val: (225,)</span></span><br><span class="line"><span class="comment"># X_test: (225, 2), y_test: (225,)</span></span><br><span class="line"><span class="comment"># Sample point: [0.17003003 0.63079261] → c3</span></span><br></pre></td></tr></table></figure>
<h3 id="Label-encoding">Label encoding</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output vectorizer</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line"></span><br><span class="line"><span class="comment"># FIt on train date</span></span><br><span class="line">label_encoder = label_encoder.fit(y_train)</span><br><span class="line">classes = <span class="built_in">list</span>(label_encoder.classes_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;classes: <span class="subst">&#123;classes&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># classes: [&#x27;c1&#x27;, &#x27;c2&#x27;, &#x27;c3&#x27;]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert labels to tokens</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line">y_train = label_encoder.transform(y_train)</span><br><span class="line">y_val = label_encoder.transform(y_val)</span><br><span class="line">y_test = label_encoder.transform(y_test)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;y_train[0]: <span class="subst">&#123;y_train[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_train[0]: c3</span></span><br><span class="line"><span class="comment"># y_train[0]: 2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Class weights</span></span><br><span class="line">counts = np.bincount(y_train)</span><br><span class="line">class_weights = &#123;i: <span class="number">1.0</span>/count <span class="keyword">for</span> i, count <span class="keyword">in</span> <span class="built_in">enumerate</span>(counts)&#125;</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;counts: <span class="subst">&#123;counts&#125;</span>\nweights: <span class="subst">&#123;class_weights&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># counts: [350 350 350]</span></span><br><span class="line"><span class="comment"># weights: &#123;0: 0.002857142857142857, 1: 0.002857142857142857, 2: 0.002857142857142857&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="Standardize-data">Standardize data</h3>
<p>因为 $y$ 是类别值，所以我们只标准化 $X$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the data (mean=0, std=1) using training data</span></span><br><span class="line">X_scaler = StandardScaler().fit(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply scaler on training and test data (don&#x27;t standardize outputs for classification)</span></span><br><span class="line">X_train = X_scaler.transform(X_train)</span><br><span class="line">X_val = X_scaler.transform(X_val)</span><br><span class="line">X_test = X_scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check (means should be ~0 and std should be ~1)</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_test[0]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">0</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;X_test[1]: mean: <span class="subst">&#123;np.mean(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>, std: <span class="subst">&#123;np.std(X_test[:, <span class="number">1</span>], axis=<span class="number">0</span>):<span class="number">.1</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># X_test[0]: mean: 0.0, std: 1.0</span></span><br><span class="line"><span class="comment"># X_test[1]: mean: -0.0, std: 1.0</span></span><br></pre></td></tr></table></figure>
<h2 id="Linear-model">Linear model</h2>
<p>在尝试使用神经网络之前，为了解释激活函数，我们先用前面学到的逻辑回归模型来学习我们的数据。</p>
<p>你会发现一个用线性激活函数的线性模型对我们的数据来说并不是合适的。</p>
<h3 id="Model-Train">Model &amp; Train</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">INPUT_DIM = X_train.shape[<span class="number">1</span>] <span class="comment"># X is 2-dimensional</span></span><br><span class="line">HIDDEN_DIM = <span class="number">100</span></span><br><span class="line">NUM_CLASSES = <span class="built_in">len</span>(classes) <span class="comment"># 3 classes</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, hidden_dim, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_in</span>):</span><br><span class="line">        z = self.fc1(x_in) <span class="comment"># linear activation</span></span><br><span class="line">        z = self.fc2(z)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = LinearModel(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)</span><br><span class="line"></span><br><span class="line">LEARNING_RATE = <span class="number">1e-2</span></span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">class_weights_tensor = torch.Tensor(<span class="built_in">list</span>(class_weights.values()))</span><br><span class="line">loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy_fn</span>(<span class="params">y_pred, y_true</span>):</span><br><span class="line">    n_correct = torch.eq(y_pred, y_true).<span class="built_in">sum</span>().item()</span><br><span class="line">    accuracy = (n_correct / <span class="built_in">len</span>(y_pred)) * <span class="number">100</span></span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert data to tensors</span></span><br><span class="line">X_train = torch.Tensor(X_train)</span><br><span class="line">y_train = torch.LongTensor(y_train)</span><br><span class="line">X_val = torch.Tensor(X_val)</span><br><span class="line">y_val = torch.LongTensor(y_val)</span><br><span class="line">X_test = torch.Tensor(X_test)</span><br><span class="line">y_test = torch.LongTensor(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    y_pred = model(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero all gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">1</span>==<span class="number">0</span>:</span><br><span class="line">        predictions = y_pred.<span class="built_in">max</span>(dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># class</span></span><br><span class="line">        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.1</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0 | loss: 1.18, accuracy: 43.7</span></span><br><span class="line"><span class="comment"># Epoch: 1 | loss: 0.92, accuracy: 55.6</span></span><br><span class="line"><span class="comment"># Epoch: 2 | loss: 0.79, accuracy: 54.5</span></span><br><span class="line"><span class="comment"># Epoch: 3 | loss: 0.74, accuracy: 54.4</span></span><br><span class="line"><span class="comment"># Epoch: 4 | loss: 0.73, accuracy: 53.9</span></span><br><span class="line"><span class="comment"># Epoch: 5 | loss: 0.73, accuracy: 53.9</span></span><br><span class="line"><span class="comment"># Epoch: 6 | loss: 0.74, accuracy: 55.0</span></span><br><span class="line"><span class="comment"># Epoch: 7 | loss: 0.75, accuracy: 55.8</span></span><br><span class="line"><span class="comment"># Epoch: 8 | loss: 0.76, accuracy: 56.2</span></span><br><span class="line"><span class="comment"># Epoch: 9 | loss: 0.77, accuracy: 56.7</span></span><br></pre></td></tr></table></figure>
<h3 id="Evaluation">Evaluation</h3>
<p>我们来看一下这个线性模型在螺旋数据上的表现如何。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_metrics</span>(<span class="params">y_true, y_pred, classes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Per-class performance metrics.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Performance</span></span><br><span class="line">    performance = &#123;<span class="string">&quot;overall&quot;</span>: &#123;&#125;, <span class="string">&quot;class&quot;</span>: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Overall performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="string">&quot;weighted&quot;</span>)</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;precision&quot;</span>] = metrics[<span class="number">0</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;recall&quot;</span>] = metrics[<span class="number">1</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;f1&quot;</span>] = metrics[<span class="number">2</span>]</span><br><span class="line">    performance[<span class="string">&quot;overall&quot;</span>][<span class="string">&quot;num_samples&quot;</span>] = np.float64(<span class="built_in">len</span>(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Per-class performance</span></span><br><span class="line">    metrics = precision_recall_fscore_support(y_true, y_pred, average=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(classes)):</span><br><span class="line">        performance[<span class="string">&quot;class&quot;</span>][classes[i]] = &#123;</span><br><span class="line">            <span class="string">&quot;precision&quot;</span>: metrics[<span class="number">0</span>][i],</span><br><span class="line">            <span class="string">&quot;recall&quot;</span>: metrics[<span class="number">1</span>][i],</span><br><span class="line">            <span class="string">&quot;f1&quot;</span>: metrics[<span class="number">2</span>][i],</span><br><span class="line">            <span class="string">&quot;num_samples&quot;</span>: np.float64(metrics[<span class="number">3</span>][i]),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> performance</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predictions</span></span><br><span class="line">y_prob = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;sample probability: <span class="subst">&#123;y_prob[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line">y_pred = y_prob.<span class="built_in">max</span>(dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;sample class: <span class="subst">&#123;y_pred[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># sample probability: tensor([0.3424, 0.0918, 0.5659], grad_fn=&lt;SelectBackward0&gt;)</span></span><br><span class="line"><span class="comment"># sample class: 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)</span><br><span class="line"><span class="built_in">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &quot;overall&quot;: &#123;</span></span><br><span class="line"><span class="comment">#     &quot;precision&quot;: 0.5174825174825175,</span></span><br><span class="line"><span class="comment">#     &quot;recall&quot;: 0.5155555555555555,</span></span><br><span class="line"><span class="comment">#     &quot;f1&quot;: 0.5162093875662788,</span></span><br><span class="line"><span class="comment">#     &quot;num_samples&quot;: 225.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   &quot;class&quot;: &#123;</span></span><br><span class="line"><span class="comment">#     &quot;c1&quot;: &#123;</span></span><br><span class="line"><span class="comment">#       &quot;precision&quot;: 0.5194805194805194,</span></span><br><span class="line"><span class="comment">#       &quot;recall&quot;: 0.5333333333333333,</span></span><br><span class="line"><span class="comment">#       &quot;f1&quot;: 0.5263157894736841,</span></span><br><span class="line"><span class="comment">#       &quot;num_samples&quot;: 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     &quot;c2&quot;: &#123;</span></span><br><span class="line"><span class="comment">#       &quot;precision&quot;: 0.46153846153846156,</span></span><br><span class="line"><span class="comment">#       &quot;recall&quot;: 0.48,</span></span><br><span class="line"><span class="comment">#       &quot;f1&quot;: 0.47058823529411764,</span></span><br><span class="line"><span class="comment">#       &quot;num_samples&quot;: 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     &quot;c3&quot;: &#123;</span></span><br><span class="line"><span class="comment">#       &quot;precision&quot;: 0.5714285714285714,</span></span><br><span class="line"><span class="comment">#       &quot;recall&quot;: 0.5333333333333333,</span></span><br><span class="line"><span class="comment">#       &quot;f1&quot;: 0.5517241379310344,</span></span><br><span class="line"><span class="comment">#       &quot;num_samples&quot;: 75.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_multiclass_decision_boundary</span>(<span class="params">model, X, y</span>):</span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">0.1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">0.1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.linspace(x_min, x_max, <span class="number">101</span>), np.linspace(y_min, y_max, <span class="number">101</span>))</span><br><span class="line">    cmap = plt.cm.Spectral</span><br><span class="line"></span><br><span class="line">    X_test = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).<span class="built_in">float</span>()</span><br><span class="line">    y_pred = F.softmax(model(X_test), dim=<span class="number">1</span>)</span><br><span class="line">    _, y_pred = y_pred.<span class="built_in">max</span>(dim=<span class="number">1</span>)</span><br><span class="line">    y_pred = y_pred.reshape(xx.shape)</span><br><span class="line">    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.RdYlBu)</span><br><span class="line">    plt.xlim(xx.<span class="built_in">min</span>(), xx.<span class="built_in">max</span>())</span><br><span class="line">    plt.ylim(yy.<span class="built_in">min</span>(), yy.<span class="built_in">max</span>())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Train&quot;</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="//s3.mindex.xyz/blog/Courses/e722233774a0fd9b91f4d77a3068287d.png" alt=""></p>
<h2 id="Activation-functions">Activation functions</h2>
<p>使用广义的线性方法产生了较差的结果，因为我们试图用线性激活函数去学习非线性数据。</p>
<p>所以我们需要一个可以能让模型学习到数据中的非线性的激活函数。有几种不同的选择，我们稍微探索一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fig size</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Data</span></span><br><span class="line">x = torch.arange(-<span class="number">5.</span>, <span class="number">5.</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sigmoid activation (constrain a value between 0 and 1.)</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Sigmoid activation&quot;</span>)</span><br><span class="line">y = torch.sigmoid(x)</span><br><span class="line">plt.plot(x.numpy(), y.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tanh activation (constrain a value between -1 and 1.)</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">y = torch.tanh(x)</span><br><span class="line">plt.title(<span class="string">&quot;Tanh activation&quot;</span>)</span><br><span class="line">plt.plot(x.numpy(), y.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Relu (clip the negative values to 0)</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">y = F.relu(x)</span><br><span class="line">plt.title(<span class="string">&quot;ReLU activation&quot;</span>)</span><br><span class="line">plt.plot(x.numpy(), y.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show plots</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="//s3.mindex.xyz/blog/Courses/499e55bee0461d4b7471247d91ec78b1.png" alt=""></p>
<p>ReLU激活函数$(max(0, z))$ 是目前为止用的最广泛的激活函数。但每个激活函数都有自己适用场景。比如：如果我们需要输出在0和1之间，那么sigmoid是合适的选择。</p>
<p>*（在某些情况下，ReLU函数也是不够的。例如，当神经元的输出大多为负时，激活函数的输出为0，这将导致神经元“死去”。为了减轻这种影响，我们可以降低学习率活着使用<a target="_blank" rel="noopener" href="https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7" title="ReLU">“ReLU变种”</a>。 如 Leaky ReLU 或 PRelu，它们会适当倾斜于神经元的负输出。）</p>
<h2 id="NumPy">NumPy</h2>
<p>现在，我们创建一个与逻辑回归模型完全相似的多层感知机，但包含一个学习数据中非线性的激活函数。</p>
<h3 id="Initialize-weights">Initialize weights</h3>
<p><strong>第一步</strong>: 随机初始化模型的权重$W$。（后面会介绍更有效的初始化策略）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize first layer&#x27;s weights</span></span><br><span class="line">W1 = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, HIDDEN_DIM)</span><br><span class="line">b1 = np.zeros((<span class="number">1</span>, HIDDEN_DIM))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;W1: <span class="subst">&#123;W1.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;b1: <span class="subst">&#123;b1.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># W1: (2, 100)</span></span><br><span class="line"><span class="comment"># b1: (1, 100)</span></span><br></pre></td></tr></table></figure>
<h3 id="Model">Model</h3>
<p><strong>第二步</strong>: 讲输入 $X$ 送到模型中进行前向传播以得到网络的输出。</p>
<p>首先，我们将输入传给第一层。<br>
$$<br>
z_1 = XW_1<br>
$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># z1 = [NX2] · [2X100] + [1X100] = [NX100]</span></span><br><span class="line">z1 = np.dot(X_train, W1) + b1</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;z1: <span class="subst">&#123;z1.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># z1: (1050, 100)</span></span><br></pre></td></tr></table></figure>
<p>接下来，我们应用非线性激活函数Relu。<br>
$$<br>
a_1 = f(z_1)<br>
$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply activation function</span></span><br><span class="line">a1 = np.maximum(<span class="number">0</span>, z1) <span class="comment"># ReLU</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;a_1: <span class="subst">&#123;a1.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># a_1: (1050, 100)</span></span><br></pre></td></tr></table></figure>
<p>然着我们将激活函数的输出传给第二层，以获得logit。<br>
$$<br>
z_2 = a_1W_2<br>
$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize second layer&#x27;s weights</span></span><br><span class="line">W2 = <span class="number">0.01</span> * np.random.randn(HIDDEN_DIM, NUM_CLASSES)</span><br><span class="line">b2 = np.zeros((<span class="number">1</span>, NUM_CLASSES))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;W2: <span class="subst">&#123;W2.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;b2: <span class="subst">&#123;b2.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># W2: (100, 3)</span></span><br><span class="line"><span class="comment"># b2: (1, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># z2 = logits = [NX100] · [100X3] + [1X3] = [NX3]</span></span><br><span class="line">logits = np.dot(a1, W2) + b2</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;logits: <span class="subst">&#123;logits.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;sample: <span class="subst">&#123;logits[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># logits: (1050, 3)</span></span><br><span class="line"><span class="comment"># sample: [ 0.00017606 -0.0023457   0.00035913]</span></span><br></pre></td></tr></table></figure>
<p>之后，我们将应用softmax来获得网络的概率输出。</p>
<p>$$<br>
\hat{y} = softmax(z_2)<br>
$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normalization via softmax to obtain class probabilities</span></span><br><span class="line">exp_logits = np.exp(logits)</span><br><span class="line">y_hat = exp_logits / np.<span class="built_in">sum</span>(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;y_hat: <span class="subst">&#123;y_hat.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;sample: <span class="subst">&#123;y_hat[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># y_hat: (1050, 3)</span></span><br><span class="line"><span class="comment"># sample: [0.33359304 0.33275285 0.33365411]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="Loss">Loss</h3>
<p><strong>第三步</strong>： 利用交叉熵计算我们分类任务的损失。<br>
$$<br>
J(\theta) = - \sum_i^K{log(\hat{y}_i)} = - \sum_i^K{log(\frac{e^{W_yX_i}}{\sum_j{e^{WX_i}}})}\\<br>
$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss</span></span><br><span class="line">correct_class_logprobs = -np.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y_train])</span><br><span class="line">loss = np.<span class="built_in">sum</span>(correct_class_logprobs) / <span class="built_in">len</span>(y_train)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;loss: <span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># loss: 1.10</span></span><br></pre></td></tr></table></figure>
<h3 id="Gradients">Gradients</h3>
<p><strong>第四步</strong> 计算损失函数 $J(\theta)$ 相对于权重的梯度。</p>
<p>对于$W_2$的梯度，与前篇逻辑回归的梯度相同，因为 $\hat{y} = softmax(z_2)$</p>
<p>$$<br>
\begin{split}<br>
\frac{\partial{J}}{\partial{W_{2j}}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{W_{2j}}}  \\<br>
&amp;= - \frac{1}{\hat{y}} \frac{\partial{\hat{y}}}{\partial{W_{2j}}} \\<br>
&amp;= - \frac{1}{\frac{e^{a_1 W_{2y}}}{\sum_j{e^{a_1 W}}}} \frac{\sum_j{e^{a_1 W}e^{a_1 W_{2y}} 0 - e^{a_1 W_{2y}} e^{a_1 W_{2j}} a_1 }}{(\sum_j{e^{a_1 W}})^2} \\<br>
&amp;= \frac{a_1 e^{a_1 W_{2j}}}{\sum_j{e^{a_1 W}}} \\<br>
&amp;= a_1 \hat{y}<br>
\end{split}<br>
$$</p>
<p>$$<br>
\begin{split}<br>
\frac{\partial{J}}{\partial{W_{2y}}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{W_{2y}}}  \\<br>
&amp;= - \frac{1}{\hat{y}} \frac{\partial{\hat{y}}}{\partial{W_{2y}}} \\<br>
&amp;= - \frac{1}{\frac{e^{a_1 W_{2y}}}{\sum_j{e^{a_1 W}}}} \frac{\sum_j{e^{a_1 W}e^{a_1 W_{2y}} a_1 - e^{W_{2y} a_1}e^{a_1 W_{2y}} a_1}}{(\sum_j{e^{a_1 W}})^2} = \frac{1}{\hat{y}} (a_1 \hat{y}^2 - a_1 \hat{y}) \\<br>
&amp;= a_1 (\hat{y} - 1)<br>
\end{split}<br>
$$</p>
<p>对于 $W_1$ 的梯度计算有点棘手，因为我们必须要通过两组权重进行反向传播。</p>
<p>$$<br>
\begin{split}<br>
\frac{\partial{J}}{\partial{W_1}} &amp;= \frac{\partial{J}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{X}} \frac{\partial{X}}{\partial{z_1}}  \frac{\partial{z_1}}{\partial{W_1}} \\<br>
&amp;= W_2 (\partial{\hat{y}})(\partial{ReLU})X<br>
\end{split}<br>
$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dJ/dW2</span></span><br><span class="line">dscores = y_hat</span><br><span class="line">dscores[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y_train] -= <span class="number">1</span></span><br><span class="line">dscores /= <span class="built_in">len</span>(y_train)</span><br><span class="line">dW2 = np.dot(a1.T, dscores)</span><br><span class="line">db2 = np.<span class="built_in">sum</span>(dscores, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dJ/dW1</span></span><br><span class="line">dhidden = np.dot(dscores, W2.T)</span><br><span class="line">dhidden[a1 &lt;= <span class="number">0</span>] = <span class="number">0</span> <span class="comment"># ReLu backprop</span></span><br><span class="line">dW1 = np.dot(X_train.T, dhidden)</span><br><span class="line">db1 = np.<span class="built_in">sum</span>(dhidden, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Update-weights">Update weights</h3>
<p><strong>第五步</strong> 指定一个学习率来更新权重 $W$，惩罚错误的分类奖励正确的分类。<br>
$$<br>
W_i = W_i - \alpha \frac{\partial{J}}{\partial{W_i}}<br>
$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Update weights</span></span><br><span class="line">W1 += -LEARNING_RATE * dW1</span><br><span class="line">b1 += -LEARNING_RATE * db1</span><br><span class="line">W2 += -LEARNING_RATE * dW2</span><br><span class="line">b2 += -LEARNING_RATE * db2</span><br></pre></td></tr></table></figure>
<h3 id="Training">Training</h3>
<p><strong>第六步</strong>: 重复步骤 2 ~ 5，以最小化损失为目的来训练模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert tensors to NumPy arrays</span></span><br><span class="line">X_train = X_train.numpy()</span><br><span class="line">y_train = y_train.numpy()</span><br><span class="line">X_val = X_val.numpy()</span><br><span class="line">y_val = y_val.numpy()</span><br><span class="line">X_test = X_test.numpy()</span><br><span class="line">y_test = y_test.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize random weights</span></span><br><span class="line">W1 = <span class="number">0.01</span> * np.random.randn(INPUT_DIM, HIDDEN_DIM)</span><br><span class="line">b1 = np.zeros((<span class="number">1</span>, HIDDEN_DIM))</span><br><span class="line">W2 = <span class="number">0.01</span> * np.random.randn(HIDDEN_DIM, NUM_CLASSES)</span><br><span class="line">b2 = np.zeros((<span class="number">1</span>, NUM_CLASSES))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training loop</span></span><br><span class="line"><span class="keyword">for</span> epoch_num <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># First layer forward pass [NX2] · [2X100] = [NX100]</span></span><br><span class="line">    z1 = np.dot(X_train, W1) + b1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply activation function</span></span><br><span class="line">    a1 = np.maximum(<span class="number">0</span>, z1) <span class="comment"># ReLU</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># z2 = logits = [NX100] · [100X3] = [NX3]</span></span><br><span class="line">    logits = np.dot(a1, W2) + b2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Normalization via softmax to obtain class probabilities</span></span><br><span class="line">    exp_logits = np.exp(logits)</span><br><span class="line">    y_hat = exp_logits / np.<span class="built_in">sum</span>(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss</span></span><br><span class="line">    correct_class_logprobs = -np.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y_train])</span><br><span class="line">    loss = np.<span class="built_in">sum</span>(correct_class_logprobs) / <span class="built_in">len</span>(y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># show progress</span></span><br><span class="line">    <span class="keyword">if</span> epoch_num%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Accuracy</span></span><br><span class="line">        y_pred = np.argmax(logits, axis=<span class="number">1</span>)</span><br><span class="line">        accuracy =  np.mean(np.equal(y_train, y_pred))</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch_num&#125;</span>, loss: <span class="subst">&#123;loss:<span class="number">.3</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dJ/dW2</span></span><br><span class="line">    dscores = y_hat</span><br><span class="line">    dscores[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y_train] -= <span class="number">1</span></span><br><span class="line">    dscores /= <span class="built_in">len</span>(y_train)</span><br><span class="line">    dW2 = np.dot(a1.T, dscores)</span><br><span class="line">    db2 = np.<span class="built_in">sum</span>(dscores, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dJ/dW1</span></span><br><span class="line">    dhidden = np.dot(dscores, W2.T)</span><br><span class="line">    dhidden[a1 &lt;= <span class="number">0</span>] = <span class="number">0</span> <span class="comment"># ReLu backprop</span></span><br><span class="line">    dW1 = np.dot(X_train.T, dhidden)</span><br><span class="line">    db1 = np.<span class="built_in">sum</span>(dhidden, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    W1 += -<span class="number">1e0</span> * dW1</span><br><span class="line">    b1 += -<span class="number">1e0</span> * db1</span><br><span class="line">    W2 += -<span class="number">1e0</span> * dW2</span><br><span class="line">    b2 += -<span class="number">1e0</span> * db2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Epoch: 0, loss: 1.098, accuracy: 0.519</span></span><br><span class="line"><span class="comment"># Epoch: 100, loss: 0.541, accuracy: 0.680</span></span><br><span class="line"><span class="comment"># Epoch: 200, loss: 0.305, accuracy: 0.893</span></span><br><span class="line"><span class="comment"># Epoch: 300, loss: 0.135, accuracy: 0.951</span></span><br><span class="line"><span class="comment"># Epoch: 400, loss: 0.091, accuracy: 0.976</span></span><br><span class="line"><span class="comment"># Epoch: 500, loss: 0.069, accuracy: 0.984</span></span><br><span class="line"><span class="comment"># Epoch: 600, loss: 0.056, accuracy: 0.989</span></span><br><span class="line"><span class="comment"># Epoch: 700, loss: 0.048, accuracy: 0.991</span></span><br><span class="line"><span class="comment"># Epoch: 800, loss: 0.043, accuracy: 0.994</span></span><br><span class="line"><span class="comment"># Epoch: 900, loss: 0.039, accuracy: 0.994</span></span><br></pre></td></tr></table></figure>
<h3 id="Evaluation-2">Evaluation</h3>
<p>在测试集上评估这个模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLPFromScratch</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, x</span>):</span><br><span class="line">        z1 = np.dot(x, W1) + b1</span><br><span class="line">        a1 = np.maximum(<span class="number">0</span>, z1)</span><br><span class="line">        logits = np.dot(a1, W2) + b2</span><br><span class="line">        exp_logits = np.exp(logits)</span><br><span class="line">        y_hat = exp_logits / np.<span class="built_in">sum</span>(exp_logits, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> y_hat</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluation</span></span><br><span class="line">model = MLPFromScratch()</span><br><span class="line">y_prob = model.predict(X_test)</span><br><span class="line">y_pred = np.argmax(y_prob, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # Performance</span></span><br><span class="line">performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)</span><br><span class="line"><span class="built_in">print</span> (json.dumps(performance, indent=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &quot;overall&quot;: &#123;</span></span><br><span class="line"><span class="comment">#     &quot;precision&quot;: 0.9826749826749827,</span></span><br><span class="line"><span class="comment">#     &quot;recall&quot;: 0.9822222222222222,</span></span><br><span class="line"><span class="comment">#     &quot;f1&quot;: 0.9822481383871041,</span></span><br><span class="line"><span class="comment">#     &quot;num_samples&quot;: 225.0</span></span><br><span class="line"><span class="comment">#   &#125;,</span></span><br><span class="line"><span class="comment">#   &quot;class&quot;: &#123;</span></span><br><span class="line"><span class="comment">#     &quot;c1&quot;: &#123;</span></span><br><span class="line"><span class="comment">#       &quot;precision&quot;: 1.0,</span></span><br><span class="line"><span class="comment">#       &quot;recall&quot;: 0.9733333333333334,</span></span><br><span class="line"><span class="comment">#       &quot;f1&quot;: 0.9864864864864865,</span></span><br><span class="line"><span class="comment">#       &quot;num_samples&quot;: 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     &quot;c2&quot;: &#123;</span></span><br><span class="line"><span class="comment">#       &quot;precision&quot;: 0.9615384615384616,</span></span><br><span class="line"><span class="comment">#       &quot;recall&quot;: 1.0,</span></span><br><span class="line"><span class="comment">#       &quot;f1&quot;: 0.9803921568627451,</span></span><br><span class="line"><span class="comment">#       &quot;num_samples&quot;: 75.0</span></span><br><span class="line"><span class="comment">#     &#125;,</span></span><br><span class="line"><span class="comment">#     &quot;c3&quot;: &#123;</span></span><br><span class="line"><span class="comment">#       &quot;precision&quot;: 0.9864864864864865,</span></span><br><span class="line"><span class="comment">#       &quot;recall&quot;: 0.9733333333333334,</span></span><br><span class="line"><span class="comment">#       &quot;f1&quot;: 0.9798657718120806,</span></span><br><span class="line"><span class="comment">#       &quot;num_samples&quot;: 75.0</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_multiclass_decision_boundary_numpy</span>(<span class="params">model, X, y, savefig_fp=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Plot the multiclass decision boundary for a model that accepts 2D inputs.</span></span><br><span class="line"><span class="string">    Credit: https://cs231n.github.io/neural-networks-case-study/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        model &#123;function&#125; -- trained model with function model.predict(x_in).</span></span><br><span class="line"><span class="string">        X &#123;numpy.ndarray&#125; -- 2D inputs with shape (N, 2).</span></span><br><span class="line"><span class="string">        y &#123;numpy.ndarray&#125; -- 1D outputs with shape (N,).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Axis boundaries</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">0.1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">0.1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.linspace(x_min, x_max, <span class="number">101</span>),</span><br><span class="line">                         np.linspace(y_min, y_max, <span class="number">101</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create predictions</span></span><br><span class="line">    x_in = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    y_pred = model.predict(x_in)</span><br><span class="line">    y_pred = np.argmax(y_pred, axis=<span class="number">1</span>).reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot decision boundary</span></span><br><span class="line">    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.RdYlBu)</span><br><span class="line">    plt.xlim(xx.<span class="built_in">min</span>(), xx.<span class="built_in">max</span>())</span><br><span class="line">    plt.ylim(yy.<span class="built_in">min</span>(), yy.<span class="built_in">max</span>())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot</span></span><br><span class="line">    <span class="keyword">if</span> savefig_fp:</span><br><span class="line">        plt.savefig(savefig_fp, <span class="built_in">format</span>=<span class="string">&quot;png&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the decision boundary</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Train&quot;</span>)</span><br><span class="line">plot_multiclass_decision_boundary_numpy(model=model, X=X_train, y=y_train)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">plot_multiclass_decision_boundary_numpy(model=model, X=X_test, y=y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="//s3.mindex.xyz/blog/Courses/0878670f7a00ef84e2037f39161b6fa5.png" alt=""></p>
<h2 id="Ending">Ending</h2>
<p>神经网络是机器学习和人工智能领域的基础，我们必须彻底掌握。</p>
<h2 id="Citation">Citation</h2>
<figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml">@article</span><span class="template-variable">&#123;madewithml,</span></span><br><span class="line"><span class="template-variable">    author       = &#123;Goku Mohandas&#125;</span><span class="language-xml">,</span></span><br><span class="line"><span class="language-xml">    title        = </span><span class="template-variable">&#123; Neural networks - Made With ML &#125;</span><span class="language-xml">,</span></span><br><span class="line"><span class="language-xml">    howpublished = </span><span class="template-variable">&#123;\url&#123;https://madewithml.com/&#125;</span><span class="language-xml">&#125;,</span></span><br><span class="line"><span class="language-xml">    year         = </span><span class="template-variable">&#123;2022&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">&#125;</span></span><br></pre></td></tr></table></figure>
</div><iframe src="/donate/?AliPayQR=null&amp;WeChatQR=http://s3.mindex.xyz/mp/qrcode-s.jpg&amp;GitHub=http://github.com/neo1989&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Author: </strong>尼欧</li><li class="post-copyright-link"><strong>Blog Link: </strong><a href="/Way2AI/Way2AI-neural-networks-1/">https://neo1989.net/Way2AI/Way2AI-neural-networks-1/</a></li><li class="post-copyright-license"><strong>Copyright Declaration: </strong>转载请声明出处。</li></ul></div><br><div class="tags"><a href="/tags/Coder/">Coder</a><a href="/tags/AI/">AI</a><a href="/tags/PyTorch/">PyTorch</a></div><div class="post-nav"><a class="pre" href="/Way2AI/Way2AI-neural-networks-2/">Way2AI · 神经网络 (二)</a><a class="next" href="/Way2AI/Way2AI-LogisticRegression-2/">Way2AI · 机器学习之Logistic Regression (二)</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://neo1989.net/Way2AI/Way2AI-neural-networks-1/';
    this.page.identifier = 'Way2AI/Way2AI-neural-networks-1/';
    this.page.title = 'Way2AI · 神经网络 (一)';
  };</script><!-- script(type='text/javascript' id='disqus-lazy-load-script').--><!--   $.ajax({--><!--   url: 'https://disqus.com/next/config.json',--><!--   timeout: 2500,--><!--   type: 'GET',--><!--   success: function(){--><!--     var d = document;--><!--     var s = d.createElement('script');--><!--     s.src = '//#{theme.disqus}.disqus.com/embed.js';--><!--     s.setAttribute('data-timestamp', + new Date());--><!--     (d.head || d.body).appendChild(s);--><!--     $('.disqus_click_btn').css('display', 'none');--><!--   },--><!--   error: function() {--><!--     $('.disqus_click_btn').css('display', 'block');--><!--   }--><!--   });--><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//neo1989.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://neo1989.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div></div><div class="pure-u-1 pure-u-md-4-6"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">愚苏记.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/i-yard/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho/maupassant"> Cho.</a></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="/lib/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="/lib/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
  displayAlign: "left"
  });
</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script><script type="text/javascript" id="maid-script" mermaidoptioins="{&quot;startOnload&quot;:true,&quot;theme&quot;:&quot;forest&quot;}" src="/js/mermaid.min.js?v=1.0.0"></script><script>if (window.mermaid) {
  var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins'));
  mermaid.initialize(options);
}</script></div><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>