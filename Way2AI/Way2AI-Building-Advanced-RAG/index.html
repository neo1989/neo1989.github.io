<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="我读书少，你莫骗我。"><title>Way2AI · Building Advanced RAG | 愚苏记</title><link rel="stylesheet" type="text/css" href="https://cdn.staticfile.org/lxgw-wenkai-webfont/1.6.0/style.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/lib/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="/lib/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="/lib/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="/lib/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="/lib/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '96c862f2728296588ae9849f3bcb95db';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><link rel="alternate" type="application/atom+xml" href="/atom.xml"><meta name="generator" content="Hexo 7.1.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Way2AI · Building Advanced RAG</h1><a id="logo" href="/.">愚苏记</a><p class="description">To no avail but try.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-4-4"><div class="content_container no-padding-right"><div class="post"><h1 class="post-title">Way2AI · Building Advanced RAG</h1><div class="post-meta">Jan 24, 2024<span> | </span><span class="category"><a href="/categories/Way2AI/">Way2AI</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2k</span><span class="post-meta-item-text"> Words</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 9</span><span class="post-meta-item-text"> Minutes</span></span></span></div><a class="disqus-comment-count" data-disqus-identifier="Way2AI/Way2AI-Building-Advanced-RAG/" href="/Way2AI/Way2AI-Building-Advanced-RAG/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Basic-RAG"><span class="toc-number">1.</span> <span class="toc-text">Basic RAG</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Success-Requirements-for-RAG"><span class="toc-number">2.</span> <span class="toc-text">Success Requirements for RAG</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Advanced-RAG"><span class="toc-number">3.</span> <span class="toc-text">Advanced RAG</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AC%E5%9B%9E"><span class="toc-number">3.1.</span> <span class="toc-text">召回</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Chunk-Size-Optimization"><span class="toc-number">3.1.1.</span> <span class="toc-text">Chunk-Size Optimization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Structured-External-Knowledge"><span class="toc-number">3.1.2.</span> <span class="toc-text">Structured External Knowledge</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Other-useful-links"><span class="toc-number">3.1.3.</span> <span class="toc-text">Other useful links</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90"><span class="toc-number">3.2.</span> <span class="toc-text">生成</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Information-Compression"><span class="toc-number">3.2.1.</span> <span class="toc-text">Information Compression</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Result-Re-Rank"><span class="toc-number">3.2.2.</span> <span class="toc-text">Result Re-Rank</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AC%E5%9B%9E-%E7%94%9F%E6%88%90"><span class="toc-number">3.3.</span> <span class="toc-text">召回&amp;生成</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Generator-Enhanced-Retrieval"><span class="toc-number">3.3.1.</span> <span class="toc-text">Generator-Enhanced Retrieval</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Iterative-Retrieval-Generator-RAG"><span class="toc-number">3.3.2.</span> <span class="toc-text">Iterative Retrieval-Generator RAG</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RAG%E7%9A%84%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-number">4.</span> <span class="toc-text">RAG的评估指标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number">5.</span> <span class="toc-text">Reference</span></a></li></ol></div></div><div class="post-content"><p><img src="//s3.mindex.xyz/blog/Courses/fa53038bd6116d6d7897b88d8b3e59b7.png" alt="A comprehensive RAG CheatSheet detailing motivations for RAG as well as techniques and strategies for progressing beyond Basic or Naive RAG builds."></p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><h2 id="Basic-RAG">Basic RAG</h2>
<p>主流的 RAG 主要涉及从外部知识库召回文档，并将这些文档连同用户的查询一起传递给大语言模型，以此生成回应。<br>
也就是说，RAG 包含了 <strong>召回部分</strong>、<strong>外部知识库</strong> 以及 <strong>生成部分</strong> 三个组成部分。</p>
<p>LlamaIndex Basic RAG 示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> SimpleDirectoryReader, VectorStoreIndex</span><br><span class="line"></span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">documents = SimpleDirectoryReader(input_dir=<span class="string">&quot;...&quot;</span>).load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># build VectorStoreIndex that takes care of chunking documents</span></span><br><span class="line"><span class="comment"># and encoding chunks to embeddings for future retrieval</span></span><br><span class="line">index = VectorStoreIndex.from_documents(documents=documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The QueryEngine class is equipped with the generator</span></span><br><span class="line"><span class="comment"># and facilitates the retrieval and generation steps</span></span><br><span class="line">query_engine = index.as_query_engine()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your Default RAG</span></span><br><span class="line">response = query_engine.query(<span class="string">&quot;A user&#x27;s query&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Success-Requirements-for-RAG">Success Requirements for RAG</h2>
<p>要想让一个RAG被认定为成功（即能为用户的问题提供有用且相关的答案），实际上有两个核心要求：</p>
<ul>
<li><strong>召回</strong> 必须是用户查询最相关的文档。</li>
<li><strong>生成</strong> 必须能够充分利用召回的文档来有效的回答用户查询。</li>
</ul>
<h2 id="Advanced-RAG">Advanced RAG</h2>
<p>一旦我们明确了成功的标准，就可以说，构建先进的 RAG 主要是要运用更精细的技术和策略（应用于召回或生成组件），以确保最终达到这些标准。<br>
进一步说，我们可以把这些精细的技术分为两类：一类是独立解决两大成功要求中的一项（或多或少）的技术，另一类则是同时应对这两大要求的技术。</p>
<h3 id="召回">召回</h3>
<p>接下来，我们将简述几种更高级的技术，这些技术能帮助我们实现第一个要求：</p>
<h4 id="Chunk-Size-Optimization">Chunk-Size Optimization</h4>
<p>因为大语言模型的上下文长度有限，所以在构建外部知识库时，我们需要将文档切分成多个部分。如果切分的部分过大或过小，都可能给生成组件带来问题，从而导致生成的回答不准确。</p>
<p>LlamaIndex Chunk Size Optimization <a target="_blank" rel="noopener" href="https://github.com/run-llama/llama_index/blob/main/docs/examples/param_optimizer/param_optimizer.ipynb">示例</a>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> ServiceContext</span><br><span class="line"><span class="keyword">from</span> llama_index.param_tuner.base <span class="keyword">import</span> ParamTuner, RunResult</span><br><span class="line"><span class="keyword">from</span> llama_index.evaluation <span class="keyword">import</span> SemanticSimilarityEvaluator, BatchEvalRunner</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Perform hyperparameter tuning as in traditional ML via grid-search</span></span><br><span class="line"><span class="comment">### 1. Define an objective function that ranks different parameter combos</span></span><br><span class="line"><span class="comment">### 2. Build ParamTuner object</span></span><br><span class="line"><span class="comment">### 3. Execute hyperparameter tuning with ParamTuner.tune()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Define objective function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">objective_function</span>(<span class="params">params_dict</span>):</span><br><span class="line">    chunk_size = params_dict[<span class="string">&quot;chunk_size&quot;</span>]</span><br><span class="line">    docs = params_dict[<span class="string">&quot;docs&quot;</span>]</span><br><span class="line">    top_k = params_dict[<span class="string">&quot;top_k&quot;</span>]</span><br><span class="line">    evals_qs = params_dict[<span class="string">&quot;eval_qs&quot;</span>]</span><br><span class="line">    ref_response_strs = params_dict[<span class="string">&quot;ref_response_strs&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># build RAG pipeline</span></span><br><span class="line">    index = _build_index(chunk_size, docs)  <span class="comment"># hlper function</span></span><br><span class="line">    query_engine = index.as_query_engine(similarity_top_k=top_k)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform inference with RAG pipeline on a privoded questions `eval_qs`</span></span><br><span class="line">    pred_response_objs = get_responses(</span><br><span class="line">        eval_qs, query_engine, show_progress=true</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform evaluations of predictions by comparing them to reference</span></span><br><span class="line">    <span class="comment"># response `ref_response_strs`</span></span><br><span class="line">    evaluator = SemanticSimilarityEvaluator(...)</span><br><span class="line">    eval_batch_runner = BatchEvalRunner(</span><br><span class="line">        &#123;<span class="string">&quot;semantic_similarity&quot;</span>: evaluator&#125;, workers=<span class="number">2</span>, show_progress=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    eval_results = eval_batch_runner.evaluate_responses(</span><br><span class="line">        evals_qs, responses=pred_response_objs, reference=ref_response_strs</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get semantic similarity metric</span></span><br><span class="line">    mean_score = np.array(</span><br><span class="line">        [r.score <span class="keyword">for</span> r <span class="keyword">in</span> eval_results[<span class="string">&quot;semantic_similarity&quot;</span>]]</span><br><span class="line">    ).mean()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> RunResult(score=mean_score, params=params_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Build ParamTuner object</span></span><br><span class="line">param_dict = &#123;<span class="string">&quot;chunk_size&quot;</span>: [<span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>]&#125;</span><br><span class="line">fixed_param_dict = &#123;</span><br><span class="line">    <span class="string">&quot;top_k&quot;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">&quot;docs&quot;</span>: docs,</span><br><span class="line">    <span class="string">&quot;evals_qs&quot;</span>: evals_qs[:<span class="number">10</span>],</span><br><span class="line">    <span class="string">&quot;ref_response_strs&quot;</span>: ref_response_strs[:<span class="number">10</span>],</span><br><span class="line">&#125;</span><br><span class="line">param_tuner = ParamTuner(</span><br><span class="line">    param_fn=objective_function,</span><br><span class="line">    param_dict=param_dict,</span><br><span class="line">    fixed_param_dict=fixed_param_dict,</span><br><span class="line">    show_progress=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Execute hyperparameter search</span></span><br><span class="line">results = param_tuner.tune()</span><br><span class="line">best_result = results.best_run_result</span><br><span class="line">best_chunk_size = results.best_run_result.params[<span class="string">&quot;chunk_size&quot;</span>]</span><br></pre></td></tr></table></figure>
<h4 id="Structured-External-Knowledge">Structured External Knowledge</h4>
<p>在复杂的情况下，我们可能需要构建一个比基本向量索引更有结构的外部知识库，这样才能在处理有明显区别的外部知识源时，进行递归召回或者路径召回。</p>
<p>LlamaIndex Recursive Retrieval <a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/examples/retrievers/recursive_retriever_nodes.html">示例</a>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> SimpleDirectoryReader, VectorStoreIndex</span><br><span class="line"><span class="keyword">from</span> llama_index.node_parser <span class="keyword">import</span> SentenceSplitter</span><br><span class="line"><span class="keyword">from</span> llama_index.schema <span class="keyword">import</span> IndexNode</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Build a recursive retriever that retrieves using small chunks</span></span><br><span class="line"><span class="comment">### but passes associated larger chunks to the generation stage</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">documents = SimpleDirectoryReader(</span><br><span class="line">    input_file=<span class="string">&quot;...&quot;</span></span><br><span class="line">).load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># build parent chunks via NodeParser</span></span><br><span class="line">node_parser = SentenceSplitter(chunk_size=<span class="number">1024</span>)</span><br><span class="line">base_nodes = node_parser.get_nodes_from_documents(documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define smaller child chunks</span></span><br><span class="line">sub_chunk_sizes = [<span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line">sub_node_parsers = [</span><br><span class="line">    SentenceSplitter(chunk_size=c, chunk_overlap=<span class="number">20</span>) <span class="keyword">for</span> c <span class="keyword">in</span> sub_chunk_sizes</span><br><span class="line">]</span><br><span class="line">all_nodes = []</span><br><span class="line"><span class="keyword">for</span> base_node <span class="keyword">in</span> base_nodes;</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> sub_node_parsers:</span><br><span class="line">        sub_nodes = n.get_nodes_from_documents([base_node])</span><br><span class="line">        sub_inodes = [</span><br><span class="line">            IndexNode.from_text_node(sn, base_node.node_id) <span class="keyword">for</span> sn <span class="keyword">in</span> sub_nodes</span><br><span class="line">        ]</span><br><span class="line">        all_nodes.extend(sub_inodes)</span><br><span class="line">    <span class="comment"># also add original node to node</span></span><br><span class="line">    original_node = IndexNode.from_text_node(base_node, base_node.node_id)</span><br><span class="line">    all_nodes.append(original_node)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define a VectorStoreIndex with all of the nodes</span></span><br><span class="line">vector_index_chunk = VectorStoreIndex(</span><br><span class="line">    all_nodes, service_context=service_context</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define a VectorStoreIndex with all of the nodes</span></span><br><span class="line">vector_index_chunk = VectorStoreIndex(</span><br><span class="line">    all_nodes, service_context=service_context</span><br><span class="line">)</span><br><span class="line">vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build RecursiveRetriever</span></span><br><span class="line">all_node_dict = &#123;n.node_id: n <span class="keyword">for</span> n <span class="keyword">in</span> all_nodes&#125;</span><br><span class="line">retriever_chunk = RecursiveRetriever(</span><br><span class="line">    <span class="string">&quot;vector&quot;</span>,</span><br><span class="line">    retriever_dict=&#123;<span class="string">&quot;vector&quot;</span>: vector_retriever_chunk&#125;,</span><br><span class="line">    node_dict=all_nodes_dict,</span><br><span class="line">    verbose=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build RetrieverQueryEngine using recursive_retriever</span></span><br><span class="line">query_engine_chunk = RetrieverQueryEngine.from_args(</span><br><span class="line">    retriever_chunk, service_context=service_context</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># perform inference with advanced RAG (i.e. query engine)</span></span><br><span class="line">response = query_engine_chunk.query(</span><br><span class="line">    <span class="string">&quot;Can you tell me about the key concepts for safety finetuning&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="Other-useful-links">Other useful links</h4>
<p>我们提供了一些指南，展示了在复杂情况下如何应用其他高级技术来确保准确的召回。以下是其中一些选定的链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine.html">Building External Knowledge using Knowledge Graphs</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/examples/vector_stores/elasticsearch_auto_retriever.html">Performing Mixed Retrieval with Auto Retrievers</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/examples/retrievers/simple_fusion.html">Building Fusion Retrievers</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding.html">Fine-tuning Embedding Models used in Retrieval</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/examples/query_transformations/HyDEQueryTransformDemo.html">Transforming Query Embeddings (HyDE)</a></li>
</ul>
<h3 id="生成">生成</h3>
<p>与上一节类似，我们提供了一些高级技术的示例，这些技术的目的是确保召回到的文档能够很好地对齐LLM的生成器。</p>
<h4 id="Information-Compression">Information Compression</h4>
<p>大语言模型（LLM）不仅受到上下文长度的限制，而且如果召回到的文档中含有太多的无关信息（也就是噪声），可能会使生成的回答质量下降。</p>
<p>LlamaIndex Information Compression <a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/LongLLMLingua.html">示例</a> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> SimpleDirectoryReader, VectorStoreIndex</span><br><span class="line"><span class="keyword">from</span> llama_index.query_engine <span class="keyword">import</span> RetrieverQueryEngine</span><br><span class="line"><span class="keyword">from</span> llama_index.postprocessor <span class="keyword">import</span> LongLLMLinguaPostprocessor</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Define a Postprocessor object, here LongLLMLinguaPostprocessor</span></span><br><span class="line"><span class="comment">### Build QueryEngine that uses this Postprocessor on retrieved docs</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Postprocessor</span></span><br><span class="line">node_postprocessor = LongLLMLinguaPostprocessor(</span><br><span class="line">    instruction_str=<span class="string">&quot;Given the context, please answer the final question&quot;</span>,</span><br><span class="line">    target_token=<span class="number">300</span>,</span><br><span class="line">    rank_method=<span class="string">&quot;longllmlingua&quot;</span>,</span><br><span class="line">    additional_compress_kwargs=&#123;</span><br><span class="line">        <span class="string">&quot;condition_compare&quot;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&quot;condition_in_question&quot;</span>: <span class="string">&quot;after&quot;</span>,</span><br><span class="line">        <span class="string">&quot;context_budget&quot;</span>: <span class="string">&quot;+100&quot;</span>,</span><br><span class="line">        <span class="string">&quot;reorder_context&quot;</span>: <span class="string">&quot;sort&quot;</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define VectorStoreIndex</span></span><br><span class="line">documents = SimpleDirectoryReader(input_dir=<span class="string">&quot;...&quot;</span>).load_data()</span><br><span class="line">index = VectorStoreIndex.from_documents(documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define QueryEngine</span></span><br><span class="line">retriever = index.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br><span class="line">retriever_query_engine = RetrieverQueryEngine.from_args(</span><br><span class="line">    retriever, node_postprocessor=[node_postprocessor]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Used your advanced RAG</span></span><br><span class="line">response = retriever_query_engine.query(<span class="string">&quot;A user&#x27;s query&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Result-Re-Rank">Result Re-Rank</h4>
<p>大语言模型（LLM）存在被称为“<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.03172">中间迷失</a>”的现象，即模型主要关注输入提示的两端。因此，在将召回到的文档传递给生成部分之前，对它们进行重新排序是有帮助的。</p>
<p>LlamaIndex Re-Ranking For Better Generation <a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/CohereRerank.html">示例</a>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> SimpleDirectoryReader, VectorStoreIndex</span><br><span class="line"><span class="keyword">from</span> llama_index.postprocessor.cohere_rerank <span class="keyword">import</span> CohereRerank</span><br><span class="line"><span class="keyword">from</span> llama_index.postprocessor <span class="keyword">import</span> LongLLMLinguaPostprocessor</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Define a Postprocessor object, here CohereRerank</span></span><br><span class="line"><span class="comment">### Build QueryEngine that uses this Postprocessor on retrieved docs</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Build CohereRerank post retrieval processor</span></span><br><span class="line">api_key = os.environ[<span class="string">&quot;COHERE_API_KEY&quot;</span>]</span><br><span class="line">cohere_rerank = CohereRerank(api_key=api_key, top_n=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build QueryEngine (RAG) using the post processor</span></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;...&quot;</span>).load_data()</span><br><span class="line">index = VectorStoreIndex.from_documents(documents=documents)</span><br><span class="line">query_engine = index.as_query_engine(</span><br><span class="line">    similarity_top_k=<span class="number">10</span>,</span><br><span class="line">    node_postprocessor=[cohere_rerank],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your advanced RAG</span></span><br><span class="line">response = query_engine.query(</span><br><span class="line">    <span class="string">&quot;What did Sam Altman do in this essay?&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="召回-生成">召回&amp;生成</h3>
<p>在这一部分，我们研究了一些高级方法，这些方法利用召回和生成的相互配合，旨在提高召回效果，同时生成出更精确的对用户查询的回应。</p>
<h4 id="Generator-Enhanced-Retrieval">Generator-Enhanced Retrieval</h4>
<p>这些方法利用大语言模型的内在推理能力，在进行召回之前优化用户的查询，以便更准确地找出生成有用回应所需的信息。</p>
<p>LlamaIndex Generator-Enhanced Retrieval <a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/examples/query_engine/flare_query_engine.html">示例</a>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> llama_index.query_engine <span class="keyword">import</span> FLAREInstructQueryEngine</span><br><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> (</span><br><span class="line">    VectorStoreIndex,</span><br><span class="line">    SimpleDirectoryReader,</span><br><span class="line">    ServiceContext,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Build a FLAREInstructQueryEngine which has the generator LLM play</span></span><br><span class="line"><span class="comment">### a more active role in retrieval by prompting it to elicit retrieval</span></span><br><span class="line"><span class="comment">### instructions on what it needs to answer the user query.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Build FLAREInstructQueryEngine</span></span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;...&quot;</span>).load_data()</span><br><span class="line">index = VectorStoreIndex.from_documents(documents=documents)</span><br><span class="line">index_query_engine = index.as_query_engine(similarity_top_k=<span class="number">2</span>)</span><br><span class="line">service_context = ServiceContext.from_defaults(llm=OpenAI(model=<span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line">flare_query_engine = FLAREInstructQueryEngine(</span><br><span class="line">    query_engine=index_query_engine,</span><br><span class="line">    service_context=service_context,</span><br><span class="line">    max_iterations=<span class="number">7</span>,</span><br><span class="line">    verbose=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your advanced RAG</span></span><br><span class="line">response = flare_query_engine(</span><br><span class="line">    <span class="string">&quot;Can you tell me about the author&#x27;s trajectory in the startup world?&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="Iterative-Retrieval-Generator-RAG">Iterative Retrieval-Generator RAG</h4>
<p>在一些复杂的场景下，我们可能需要进行多步骤的推理，才能给出对用户查询的有用且相关的回答。</p>
<p>LlamaIndex Iterative Retrieval-Generator <a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/examples/evaluation/RetryQuery.html#retry-query-engine">示例</a>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.query_engine <span class="keyword">import</span> RetryQueryEngine</span><br><span class="line"><span class="keyword">from</span> llama_index.evaluation <span class="keyword">import</span> RelevancyEvaluator</span><br><span class="line"></span><br><span class="line"><span class="comment">### Recipe</span></span><br><span class="line"><span class="comment">### Build a RetryQueryEngine which performs retrieval-generation cycles</span></span><br><span class="line"><span class="comment">### until it either achieves a passing evaluation or a max number of </span></span><br><span class="line"><span class="comment">### cycles has been reached</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Build RetryQueryEngine</span></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;...&quot;</span>).load_data()</span><br><span class="line">index = VectorStoreIndex.from_documents(documents=documents)</span><br><span class="line">base_query_engine = index.as_query_engine()</span><br><span class="line">query_response_evaluator = RelevancyEvaluator() <span class="comment"># evaluator to critique retrieval-generation cycles</span></span><br><span class="line"></span><br><span class="line">retry_query_engine = RetryQueryEngine(</span><br><span class="line">    base_query_engine, query_response_evaluator</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your advanced rag</span></span><br><span class="line">retry_response = retry_query_engine(<span class="string">&quot;A user&#x27;s query&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="RAG的评估指标">RAG的评估指标</h2>
<p>对 RAG 系统进行评估，无疑是非常重要的。在<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.10997.pdf">《Retrieval-Augmented Generation for Large Language Models: A Survey》</a>中，作者们提出了7个评估指标，这些指标在CheetSheet右上角部分有所体现。</p>
<p>llama-index 库包含了一些评估抽象，还集成了对 RAGAs 的支持，以帮助开发者从这些评估指标的角度，理解他们的 RAG 系统在多大程度上达到了预期的成功要求。下面，我们列举了一些精选的评估笔记本指南:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/latest/examples/evaluation/answer_and_context_relevancy.html">Answer Relevancy and Context Relevancy</a></li>
<li><a target="_blank" rel="noopener" href="https://www.notion.so/LlamaIndex-Platform-0754edd9af1c4159bde12649c184c8ef?pvs=21">Faithfulness</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/run-llama/llama_index/blob/main/docs/examples/evaluation/retrieval/retriever_eval.ipynb">Retrieval Evaluation</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/examples/evaluation/batch_eval.html">Batch Evaluations with BatchEvalRunner</a></li>
</ul>
<h2 id="Reference">Reference</h2>
<p>希望在你阅读完这篇博客文章后，能有更多的信心和准备，去运用这些精妙的技术来打造先进的 RAG 系统！</p>
<p><a target="_blank" rel="noopener" href="https://blog.llamaindex.ai/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b">A Cheat Sheet and Some Recipes For Building Advanced RAG</a></p>
</div><iframe src="/donate/?AliPayQR=null&amp;WeChatQR=http://s3.mindex.xyz/mp/qrcode-s.jpg&amp;GitHub=http://github.com/neo1989&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Author: </strong>尼欧</li><li class="post-copyright-link"><strong>Blog Link: </strong><a href="/Way2AI/Way2AI-Building-Advanced-RAG/">https://neo1989.net/Way2AI/Way2AI-Building-Advanced-RAG/</a></li><li class="post-copyright-license"><strong>Copyright Declaration: </strong>转载请声明出处。</li></ul></div><br><div class="tags"><a href="/tags/Coder/">Coder</a><a href="/tags/AI/">AI</a></div><div class="post-nav"><a class="pre" href="/Notes/NOTE-18-Emotional-Equations/">18 Emotional Equations</a><a class="next" href="/CheatSheet/CHEATSHEET-Regular-Expressions/">正则表达式</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://neo1989.net/Way2AI/Way2AI-Building-Advanced-RAG/';
    this.page.identifier = 'Way2AI/Way2AI-Building-Advanced-RAG/';
    this.page.title = 'Way2AI · Building Advanced RAG';
  };</script><!-- script(type='text/javascript' id='disqus-lazy-load-script').--><!--   $.ajax({--><!--   url: 'https://disqus.com/next/config.json',--><!--   timeout: 2500,--><!--   type: 'GET',--><!--   success: function(){--><!--     var d = document;--><!--     var s = d.createElement('script');--><!--     s.src = '//#{theme.disqus}.disqus.com/embed.js';--><!--     s.setAttribute('data-timestamp', + new Date());--><!--     (d.head || d.body).appendChild(s);--><!--     $('.disqus_click_btn').css('display', 'none');--><!--   },--><!--   error: function() {--><!--     $('.disqus_click_btn').css('display', 'block');--><!--   }--><!--   });--><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//neo1989.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://neo1989.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div></div><div class="pure-u-1 pure-u-md-4-6"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">愚苏记.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/i-yard/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho/maupassant"> Cho.</a></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="/lib/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="/lib/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script><script type="text/javascript" id="maid-script" mermaidoptioins="{&quot;startOnload&quot;:true,&quot;theme&quot;:&quot;forest&quot;}" src="/js/mermaid.min.js?v=1.0.0"></script><script>if (window.mermaid) {
  var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins'));
  mermaid.initialize(options);
}</script></div><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>