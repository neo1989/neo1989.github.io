<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="我读书少，你莫骗我。"><title>Way2AI · Embeddings （上） | 愚苏记</title><link rel="stylesheet" type="text/css" href="https://cdn.staticfile.org/lxgw-wenkai-webfont/1.6.0/style.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/lib/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="/lib/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="/lib/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="/lib/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="/lib/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '96c862f2728296588ae9849f3bcb95db';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><link rel="alternate" type="application/atom+xml" href="/atom.xml"><meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Way2AI · Embeddings （上）</h1><a id="logo" href="/.">愚苏记</a><p class="description">To no avail but try.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-5-5"><div class="content_container no-padding-right"><div class="post"><h1 class="post-title">Way2AI · Embeddings （上）</h1><div class="post-meta">Jul 4, 2023<span> | </span><span class="category"><a href="/categories/Way2AI/">Way2AI</a></span></div><a class="disqus-comment-count" data-disqus-identifier="Way2AI/Way2AI-Embeddings-1/" href="/Way2AI/Way2AI-Embeddings-1/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#TL-DR"><span class="toc-number">1.</span> <span class="toc-text">TL;DR</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Learning-embeddings"><span class="toc-number">2.</span> <span class="toc-text">Learning embeddings</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Word2Vec"><span class="toc-number">2.1.</span> <span class="toc-text">Word2Vec</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#FastText"><span class="toc-number">2.2.</span> <span class="toc-text">FastText</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pretrained-embeddings"><span class="toc-number">3.</span> <span class="toc-text">Pretrained embeddings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ending"><span class="toc-number">4.</span> <span class="toc-text">Ending</span></a></li></ol></div></div><div class="post-content"><h2 id="TL-DR">TL;DR</h2>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>虽然One-Hot编码能够将离散变量表示为二进制向量，且能保留结构化信息，但它有两个主要的缺点：</p>
<ul>
<li>线性依赖词表的大小。这在语料库很大的情况下会带来问题如维数巨大且稀疏</li>
<li>单个token的表示，不保留其相对于其它token的关系</li>
</ul>
<p>本文将简单介绍embeddings，及它是如何解决one-hot编码的所有缺点。</p>
<h2 id="Learning-embeddings">Learning embeddings</h2>
<p>我们将通过使用PyTorch建模来学习embeddings，不过首先，我们学习一下专门用于嵌入和主题建模的库<a href="https://radimrehurek.com/gensim/" target="_blank" rel="noopener" title="Gensim">Gensim</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.download(<span class="string">"punkt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split text into sentence</span></span><br><span class="line">tokenizer = nltk.data.load(<span class="string">"tokenizers/punkt/english.pickle"</span>)</span><br><span class="line">book = requests.get(<span class="string">"https://s3.mindex.xyz/datasets/harrypotter.txt"</span>).content</span><br><span class="line">sentences = tokenizer.tokenize(str(book))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">f"<span class="subst">&#123;len(sentences)&#125;</span> sentences"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># 12449 sentences</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="string">"""Conditional preprocessing on our text."""</span></span><br><span class="line">    <span class="comment"># Lower</span></span><br><span class="line">    text = text.lower()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Spacing and filters</span></span><br><span class="line">    text = re.sub(<span class="string">r"([-;;.,!?&lt;=&gt;])"</span>, <span class="string">r" \1 "</span>, text)  <span class="comment"># separate punctuation tied to words</span></span><br><span class="line">    text = re.sub(<span class="string">"[^A-Za-z0-9]+"</span>, <span class="string">" "</span>, text)  <span class="comment"># remove non alphanumeric chars</span></span><br><span class="line">    text = re.sub(<span class="string">" +"</span>, <span class="string">" "</span>, text)  <span class="comment"># remove multiple spaces</span></span><br><span class="line">    text = text.strip()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Separate into word tokens</span></span><br><span class="line">    text = text.split(<span class="string">" "</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocess sentences</span></span><br><span class="line"><span class="keyword">print</span> (sentences[<span class="number">11</span>])</span><br><span class="line">sentences = [preprocess(s) <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line"><span class="keyword">print</span> (sentences[<span class="number">11</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Snape nodded, but did not elaborate.</span></span><br><span class="line"><span class="comment"># ['snape', 'nodded', 'but', 'did', 'not', 'elaborate']</span></span><br></pre></td></tr></table></figure>
<p>embeddings的核心就是单词表示，且这种表示不只是依赖单词本身，而且依赖它的上下文。我们有几种不同的方法可以实现这一目标：</p>
<ul>
<li>给定上下文中的单词，预测目标单词（CBOW )</li>
<li>给定目标词，预测上下文词（skip-gram)</li>
<li>给定一个文本序列，预测下一个单词（ LM ）</li>
</ul>
<p>上面这些方法都涉及到创建数据来训练模型。句子中的每个单词都成为目标单词，上下文由窗口决定。</p>
<p>如下图（skip-gram），窗口大小为2。我们对语料库中的每个句子重复此操作，以产生用于无监督任务的训练数据。这个任务的核心逻辑是，相似的词会出现在相似的上下文中，我们可以通过反复的使用这种(target, context)文本对来学习这种关系。</p>
<p><img src="//s3.mindex.xyz/tmp/7bf17eefb4ff89642692d20685c9cb1a.webp" alt=""></p>
<p>我们可以使用上述任何一种方法来应用Embeddings。在任务中到底选择哪种方案，可能更多的需要依靠在监督任务上的表现来做选择。</p>
<h3 id="Word2Vec">Word2Vec</h3>
<p>当我们有大量的词汇表需要应用Embeddings时，事情会变得复杂。回想一下在反向传播中使用softmax更新正确的和不正确的分类权重，这种情况下每一次反向传播都意味着一个巨大的计算。因此解决方案是使用<a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" target="_blank" rel="noopener" title="Word2Vec Negative Sampling">负采样</a>，它只更新正确的类和随机一部分不正确的类（NEGATIVE_SAMPLING = 20）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> KeyedVectors</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"></span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">WINDOW = <span class="number">5</span></span><br><span class="line">MIN_COUNT = <span class="number">3</span></span><br><span class="line">SKIP_GRAM = <span class="number">1</span></span><br><span class="line">NEGATIVE_SAMPLING = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">w2v = Word2Vec(</span><br><span class="line">    sentences=sentences, vector_size=EMBEDDING_DIM,</span><br><span class="line">    window=WINDOW, min_count=MIN_COUNT,</span><br><span class="line">    sg=SKIP_GRAM, negative=NEGATIVE_SAMPLING)</span><br><span class="line"><span class="keyword">print</span> (w2v)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># Word2Vec&lt;vocab=4937, vector_size=100, alpha=0.025&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Vector for each word</span></span><br><span class="line">w2v.wv.get_vector(<span class="string">"potter"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># array([ 0.04592679,  0.26393083, -0.29759625, -0.51007414,  0.02860732,</span></span><br><span class="line"><span class="comment">#        -0.01302573,  0.3703193 ,  0.14425582, -0.4187037 ,  0.04296769,</span></span><br><span class="line"><span class="comment">#        -0.13030362, -0.30441925, -0.14958233,  0.04964258,  0.14798391,</span></span><br><span class="line"><span class="comment">#        -0.18539314,  0.51730794,  0.01598365, -0.11325987, -0.6307836 ,</span></span><br><span class="line"><span class="comment">#         0.39244524,  0.25232184,  0.29555508, -0.22162063, -0.29100868,</span></span><br><span class="line"><span class="comment">#        -0.22083738, -0.52918744, -0.68654346, -0.09764519,  0.05514489,</span></span><br><span class="line"><span class="comment">#         0.06108054,  0.3587375 , -0.01166064, -0.42530054, -0.05000629,</span></span><br><span class="line"><span class="comment">#         0.45623606, -0.29811206, -0.09037815, -0.0024387 , -0.41930553,</span></span><br><span class="line"><span class="comment">#         0.12495753, -0.1773121 ,  0.19551197,  0.02754493,  0.25369856,</span></span><br><span class="line"><span class="comment">#         0.10022393, -0.38912103, -0.10274333, -0.24544689,  0.00851442,</span></span><br><span class="line"><span class="comment">#         0.26698554, -0.03026148,  0.12343717, -0.07433262,  0.0162609 ,</span></span><br><span class="line"><span class="comment">#         0.15033086,  0.09943663,  0.28371716, -0.26024884, -0.05571229,</span></span><br><span class="line"><span class="comment">#         0.0938114 , -0.00562614, -0.11472147,  0.21217017,  0.12490374,</span></span><br><span class="line"><span class="comment">#         0.34131378,  0.10346038,  0.38650215, -0.44265935, -0.02233333,</span></span><br><span class="line"><span class="comment">#        -0.47005087, -0.28585035,  0.06968105,  0.08989634,  0.22004889,</span></span><br><span class="line"><span class="comment">#        -0.22940454, -0.06248426,  0.089827  , -0.35011858,  0.11977731,</span></span><br><span class="line"><span class="comment">#        -0.06323916,  0.0940324 , -0.31842625,  0.53730965,  0.17043817,</span></span><br><span class="line"><span class="comment">#         0.15869781,  0.40275395,  0.04705542,  0.35397893,  0.00738561,</span></span><br><span class="line"><span class="comment">#         0.21539825,  0.14310665,  0.13341616, -0.0660746 ,  0.42496106,</span></span><br><span class="line"><span class="comment">#         0.09145384,  0.47487733, -0.23636843,  0.00715503,  0.05220298],</span></span><br><span class="line"><span class="comment">#       dtype=float32)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get nearest neighbors (excluding itself)</span></span><br><span class="line">w2v.wv.most_similar(positive=<span class="string">"scar"</span>, topn=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [('forehead', 0.9045635461807251),</span></span><br><span class="line"><span class="comment">#  ('pain', 0.9014869928359985),</span></span><br><span class="line"><span class="comment">#  ('mouth', 0.8918080925941467),</span></span><br><span class="line"><span class="comment">#  ('prickling', 0.890386164188385),</span></span><br><span class="line"><span class="comment">#  ('throat', 0.8795480728149414)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Saving and loading</span></span><br><span class="line">w2v.wv.save_word2vec_format(<span class="string">"w2v.bin"</span>, binary=<span class="literal">True</span>)</span><br><span class="line">wv = KeyedVectors.load_word2vec_format(<span class="string">"w2v.bin"</span>, binary=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="FastText">FastText</h3>
<p>当一个词在我们的词汇表中不存在时会发生什么？我们可以分配一个 UNK 标识来表示为未登录词，或者使用<a href="https://radimrehurek.com/gensim/models/fasttext.html" target="_blank" rel="noopener" title="FastText">FastText</a>，它使用字符级的n-grams算法来embed单词，这样有助于处理罕见词,拼错的词，以及语料库中不存在但相似的词。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> FastText</span><br><span class="line"></span><br><span class="line"><span class="comment"># Super fast because of optimized C code under the hood</span></span><br><span class="line">ft = FastText(sentences=sentences, vector_size=EMBEDDING_DIM,</span><br><span class="line">              window=WINDOW, min_count=MIN_COUNT,</span><br><span class="line">              sg=SKIP_GRAM, negative=NEGATIVE_SAMPLING)</span><br><span class="line"><span class="keyword">print</span> (ft)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># FastText&lt;vocab=4937, vector_size=100, alpha=0.025&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This word doesn't exist so the word2vec model will error out</span></span><br><span class="line">wv.most_similar(positive=<span class="string">'scarring'</span>, topn=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># KeyError: "Key 'scarring' not present in vocabulary"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># FastText will use n-grams to embed an OOV word</span></span><br><span class="line">ft.wv.most_similar(positive=<span class="string">'scarring'</span>, topn=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [('swimming', 0.9938331246376038),</span></span><br><span class="line"><span class="comment">#  ('howling', 0.9927006959915161),</span></span><br><span class="line"><span class="comment">#  ('dabbing', 0.9923058748245239),</span></span><br><span class="line"><span class="comment">#  ('wriggling', 0.9921060800552368),</span></span><br><span class="line"><span class="comment">#  ('bulging', 0.9919766783714294)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Save and loading</span></span><br><span class="line">ft.wv.save(<span class="string">"ft.bin"</span>)</span><br><span class="line">ftwv = KeyedVectors.load(<span class="string">"ft.bin"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Pretrained-embeddings">Pretrained embeddings</h2>
<p>我们可以利用上述方法从头开始应用embeddings，也可以利用已经在百万文档上训练过的预训练embeddings。流行的包括<a href="https://www.tensorflow.org/tutorials/text/word2vec" target="_blank" rel="noopener" title="Word2Vec">Word2Vec</a>、<a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener" title="GloVe">GloVe</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Preview of the GloVe embeddings file</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"glove.6B.100d.txt"</span>, <span class="string">"r"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    line = next(fp)</span><br><span class="line">    values = line.split()</span><br><span class="line">    word = values[<span class="number">0</span>]</span><br><span class="line">    embedding = np.asarray(values[<span class="number">1</span>:], dtype=<span class="string">'float32'</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">f"word: <span class="subst">&#123;word&#125;</span>"</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">f"embedding:\n<span class="subst">&#123;embedding&#125;</span>"</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">f"embedding dim: <span class="subst">&#123;len(embedding)&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># word: the</span></span><br><span class="line"><span class="comment"># embedding:</span></span><br><span class="line"><span class="comment"># [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141</span></span><br><span class="line"><span class="comment">#   0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384</span></span><br><span class="line"><span class="comment">#  -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464</span></span><br><span class="line"><span class="comment">#  -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155</span></span><br><span class="line"><span class="comment">#  -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021</span></span><br><span class="line"><span class="comment">#   0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531</span></span><br><span class="line"><span class="comment">#   0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559</span></span><br><span class="line"><span class="comment">#  -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243</span></span><br><span class="line"><span class="comment">#   0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514</span></span><br><span class="line"><span class="comment">#   0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044</span></span><br><span class="line"><span class="comment">#   0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212</span></span><br><span class="line"><span class="comment">#  -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148</span></span><br><span class="line"><span class="comment">#  -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215</span></span><br><span class="line"><span class="comment">#  -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459</span></span><br><span class="line"><span class="comment">#   0.8278    0.27062 ]</span></span><br><span class="line"><span class="comment"># embedding dim: 100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load embeddings (may take a minute)</span></span><br><span class="line">glove = KeyedVectors.load_word2vec_format(<span class="string">"glove.6B.100d.txt"</span>, binary=<span class="literal">False</span>, no_header=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># (king - man) + woman = ?</span></span><br><span class="line"><span class="comment"># king - man = ? -  woman</span></span><br><span class="line">glove.most_similar(positive=[<span class="string">"woman"</span>, <span class="string">"king"</span>], negative=[<span class="string">"man"</span>], topn=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [('queen', 0.7698540687561035),</span></span><br><span class="line"><span class="comment">#  ('monarch', 0.6843381524085999),</span></span><br><span class="line"><span class="comment">#  ('throne', 0.6755736470222473),</span></span><br><span class="line"><span class="comment">#  ('daughter', 0.6594556570053101),</span></span><br><span class="line"><span class="comment">#  ('princess', 0.6520534157752991)]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get nearest neighbors (excluding itself)</span></span><br><span class="line">glove.most_similar(positive=<span class="string">"goku"</span>, topn=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [('gohan', 0.7246542572975159),</span></span><br><span class="line"><span class="comment">#  ('bulma', 0.6497020125389099),</span></span><br><span class="line"><span class="comment">#  ('raistlin', 0.644360363483429),</span></span><br><span class="line"><span class="comment">#  ('skaar', 0.6316742897033691),</span></span><br><span class="line"><span class="comment">#  ('guybrush', 0.6231325268745422)]</span></span><br></pre></td></tr></table></figure>
<p>我们可视化一下 king, queen, man, woman 这四个单词的位置关系。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reduce dimensionality for plotting</span></span><br><span class="line">X = glove[glove.index_to_key]</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pca_results = pca.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_embeddings</span><span class="params">(words, embeddings, pca_results)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        idx = embeddings.key_to_index[word]</span><br><span class="line">        plt.scatter(pca_results[idx, <span class="number">0</span>], pca_results[idx, <span class="number">1</span>])</span><br><span class="line">        plt.annotate(word, xy=(pca_results[idx, <span class="number">0</span>], pca_results[idx, <span class="number">1</span>]))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize</span></span><br><span class="line">plot_embeddings(</span><br><span class="line">    words=[<span class="string">"king"</span>, <span class="string">"queen"</span>, <span class="string">"man"</span>, <span class="string">"woman"</span>], embeddings=glove,</span><br><span class="line">    pca_results=pca_results)</span><br></pre></td></tr></table></figure>
<p><img src="//s3.mindex.xyz/tmp/6c1b85445e28b27bfed73fc0d1f7d3ec.png" alt=""></p>
<p>再看一下，离woman和doctor近，但离man远的词有哪些</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Bias in embeddings</span></span><br><span class="line">glove.most_similar(positive=[<span class="string">"woman"</span>, <span class="string">"doctor"</span>], negative=[<span class="string">"man"</span>], topn=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line"><span class="comment"># [('nurse', 0.7735227942466736),</span></span><br><span class="line"><span class="comment">#  ('physician', 0.7189430594444275),</span></span><br><span class="line"><span class="comment">#  ('doctors', 0.6824328303337097),</span></span><br><span class="line"><span class="comment">#  ('patient', 0.6750683188438416),</span></span><br><span class="line"><span class="comment">#  ('dentist', 0.6726033091545105)]</span></span><br></pre></td></tr></table></figure>
<h2 id="Ending">Ending</h2>
<p>下一篇，我们将进一步介绍Embeddings如何提升我们前篇介绍的CNN分类模型。</p>
</div><iframe src="/donate/?AliPayQR=null&amp;WeChatQR=http://s3.mindex.xyz/mp/qrcode-s.jpg&amp;GitHub=http://github.com/neo1989&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Author: </strong>尼欧</li><li class="post-copyright-link"><strong>Blog Link: </strong><a href="/Way2AI/Way2AI-Embeddings-1/">https://neo1989.net/Way2AI/Way2AI-Embeddings-1/</a></li><li class="post-copyright-license"><strong>Copyright Declaration: </strong>转载请声明出处。</li></ul></div><br><div class="tags"><a href="/tags/Coder/">Coder</a><a href="/tags/AI/">AI</a><a href="/tags/PyTorch/">PyTorch</a></div><div class="post-nav"><a class="pre" href="/Way2AI/Way2AI-Embeddings-2/">Way2AI · Embeddings （下）</a><a class="next" href="/Way2AI/Way2AI-CNN/">Way2AI · 卷积神经网络</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://neo1989.net/Way2AI/Way2AI-Embeddings-1/';
    this.page.identifier = 'Way2AI/Way2AI-Embeddings-1/';
    this.page.title = 'Way2AI · Embeddings （上）';
  };</script><!-- script(type='text/javascript' id='disqus-lazy-load-script').--><!--   $.ajax({--><!--   url: 'https://disqus.com/next/config.json',--><!--   timeout: 2500,--><!--   type: 'GET',--><!--   success: function(){--><!--     var d = document;--><!--     var s = d.createElement('script');--><!--     s.src = '//#{theme.disqus}.disqus.com/embed.js';--><!--     s.setAttribute('data-timestamp', + new Date());--><!--     (d.head || d.body).appendChild(s);--><!--     $('.disqus_click_btn').css('display', 'none');--><!--   },--><!--   error: function() {--><!--     $('.disqus_click_btn').css('display', 'block');--><!--   }--><!--   });--><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//neo1989.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://neo1989.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div></div><div class="pure-u-1 pure-u-md-4-6"><div id="footer">Copyright © 2023 <a href="/." rel="nofollow">愚苏记.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/i-yard/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho/maupassant"> Cho.</a></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="/lib/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="/lib/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
  displayAlign: "left"
  });
</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script><script type="text/javascript" id="maid-script" mermaidoptioins="{&quot;startOnload&quot;:true,&quot;theme&quot;:&quot;forest&quot;}" src="/js/mermaid.min.js?v=1.0.0"></script><script>if (window.mermaid) {
  var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins'));
  mermaid.initialize(options);
}</script></div><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>